{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Enunciado_T3.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python [conda root]","language":"python","name":"conda-root-py"}},"cells":[{"metadata":{"id":"gOzCOJeGbYeX","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"hKwe4llYbYeZ","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","> c) Defina la sección que conecta a estas dos partes a través de un *sampleo* implicito ($g = \\mu_{z^{(i)}} + \\sigma_{z^{(i)}}\\cdot \\epsilon$), ésto es lo que lo hace que sea un enfoque probabilistico/bayesiano. Describa el modelo completo.\n","\n","```python\n","def sampling(args):\n","    epsilon_std = 1.0\n","    z_mean, z_log_var = args\n","    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),mean=0., stddev=epsilon_std)\n","    return z_mean + K.exp(z_log_var) * epsilon\n","from keras.layers import Lambda\n","z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n","hid_decoded = decoder_hid(z)\n","up_decoded = decoder_upsample(hid_decoded)\n","reshape_decoded =  decoder_reshape(up_decoded)\n","deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n","x_decoded_relu = decoder_deconv_2(deconv_1_decoded)\n","x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n","# instantiate VAE model\n","vae = Model(x, x_decoded_mean_squash)\n","vae.summary()\n","```\n","\n","> d) Como la función objetivo es *customizada* deberemos definirla y poner una distribución a *priori* sobre las variables latentes, en este caso se tendrá como media un vector de ceros y la matriz de covarianza la matriz identidad $p_{\\theta}(z) \\sim N (\\vec{0},I)$. Elija la función de pérdida para la reconstrucción. Comente porqué la *KL Divergence* podría funcionar como regularizador del criterio de entrenamiento obtenido.\n","\n","```python\n","from keras import backend as K\n","# Compute VAE loss\n","#choised_loss =  keras.metrics.binary_crossentropy(K.flatten(x),K.flatten(x_decoded_mean_squash))\n","#choised_loss =  keras.metrics.mean_squared_error(K.flatten(x),K.flatten(x_decoded_mean_squash))\n","reconstruction_loss = img_rows * img_cols * channel* choised_loss\n","kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n","vae_loss = K.mean(reconstruction_loss + kl_loss)\n","vae.add_loss(vae_loss)\n","vae.summary()\n","```\n","\n","> e) Entrene el modelo definido con los datos de MNIST entre 10 a 15 *epochs* con el optimizador de *RMSprop* y tamaño de batch el que estime conveniente.\n","\n","```python\n","batch_size = ...\n","epochs =  [10,15]\n","vae.compile(optimizer='rmsprop')\n","vae.fit(X_train,epochs=epochs, batch_size=batch_size,validation_data=(X_test, None))\n","```\n","\n","> f) Visualice la representación codificada $z$ (variables latentes) de los datos en base a su media $\\mu_{z^{(i)}}$. Además genere un histograma de la media y la varianza $\\sigma_{z^{(i)}}^2$ de las dos componentes. Comente\n","\n","```python\n","# display a 2D plot of the digit classes in the latent space\n","x_test_encoded = encoder.predict(X_test, batch_size=batch_size)\n","import matplotlib.pyplot as plt\n","plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n","plt.colorbar()\n","plt.show()\n","encoder_log_var = Model(x,z_log_var)\n","#histogram\n","```\n","\n","> g) Genere nuevos datos artificialmente a través del espacio de las variables latentes. Para esto deberá generar puntos linealmente separados por debajo de la distribución Normal. Comente qué significada cada eje en la imagen ¿qué sucede más allá en el espacio del 90% confianza de las variables latentes? ¿Qué objetos se generan?\n","\n","```python\n","#GENERATOR\n","decoder_input = Input(shape=(latent_dim,))\n","_hid_decoded = decoder_hid(decoder_input)\n","_up_decoded = decoder_upsample(_hid_decoded)\n","_reshape_decoded = decoder_reshape(_up_decoded)\n","_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n","_x_decoded_relu = decoder_deconv_2(_deconv_1_decoded)\n","_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n","generator = Model(decoder_input, _x_decoded_mean_squash)\n","##PLOT\n","n = 30  # figure with 15x15 images \n","image_size = img_cols\n","figure = np.zeros((image_size * n, image_size * n))\n","from scipy.stats import norm\n","#metodo de la transformada inversa\n","grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n","grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n","for i, yi in enumerate(grid_x):\n","    for j, xi in enumerate(grid_y):\n","        z_sample = np.array([[xi, yi]])            \n","        x_decoded = generator.predict(z_sample,batch_size=batch_size)\n","        figure[i * image_size: (i + 1) * image_size,\n","               j * image_size: (j + 1) * image_size] = x_decoded[0][:,:,0]\n","plt.figure(figsize=(10, 10))\n","plt.imshow(figure,cmap='gnuplot2')\n","pos = np.arange(image_size/2,image_size*n,image_size)\n","plt.yticks(pos,np.round(grid_y,1))\n","plt.xticks(pos,np.round(grid_x,1))\n","plt.show()\n","#en los extremos del intervalo de confianza\n","grid = norm.ppf(np.linspace(0.000005, 0.999995, n))\n","```\n","\n","> h) ¿Qué pasa al cambiar la distribución latente de los datos $z$? Comente sobre alguna distribución elegida diferente a la Normal y muestre sobre el cómo debiera ser implementada.\n","\n","> i) Comente sobre si mejora o empeora el desempeño al aumentar la dimensionalidad de las variables latentes $z$, explique.\n","\n","> j) Vea qué sucede al cambiar algún aspectro estructural de la red (en su arquitectura). Recuerde que la estructura del *decoder* debe ser análoga a la del *encoder.  \n","\n","### 1.2 *Generative Adversarial Networks* (GAN) [[4]](#refs)\n","\n","\n","Las GAN son un enfoque distinto de modelo generativo. A pesar de que tiene dos redes conectadas, las tareas que realiza cada una de ella son distintas. Se trata de un modelo adversario en que una red *compite* con la otra. Por un lado se tiene la red discriminadora $D$ que intenta disernir si un dato proviene de los datos reales o fue un dato generado artificialmente. Por otro lado se tiene la red generadora $G$ que intenta generar datos artificialmente de manera que la red discriminadora se confunda, es decir, sea lo más similar a los datos reales. \n","\n","<img src=\"https://oshearesearch.com/wp-content/uploads/2016/07/mnist_gan.png\" title=\"VAE\" width=\"60%\" />\n","\n","\n","El enfoque optimizador de los parámetros de la red neuronal es para $D$ el de maximizar la probabilidad de los datos que provienen de la distribución original minimizando la probabilidad de los datos que provienen del modelo generativo. Mientras que para $G$ es el de maximizzar la probabilidad de que $D$ asigne un dato de $G$ como real, o bien, minimizar la probabilidad de que $D$ asigne un dato de $G$ como corrupto.\n","\n","$$\n","min_G \\ max_G = E_{x\\sim p_{data}(x) }[logD(x)] + E_{z\\sim p_z(z)}[log(1-D(G(z))]\n","$$\n","\n","Ésto tiene un óptimo teórico que es cuando $p_g = p_{data}$, es decir, cuando el *generador* $G$ logra imitar la distribución de probabilidad de los datos.\n","\n","\n","\n","> a) Defina al *discriminador* de la GAN como el que se muestra en el código, de 3 bloque convolucionales y una tanda *fully conected*, con los Dropout para evitar *overfitting*. Describa la arquitectura utilizada y cuál es la función de activación seleccionada.\n","\n","```python\n","from keras.models import Model,Sequential\n","from keras.layers import LeakyReLU,Conv2D,Dropout,Flatten,Dense\n","## Discriminator\n","D = Sequential()\n","depth = 64\n","dropout = 0.4\n","input_shape = (img_rows, img_cols, channel)\n","D.add(Conv2D(depth*1, (5,5), strides=2, input_shape=input_shape,padding='same', activation=LeakyReLU(alpha=0.2)))\n","D.add(Dropout(dropout))\n","D.add(Conv2D(depth*2, (5,5), strides=2, padding='same',activation=LeakyReLU(alpha=0.2)))\n","D.add(Dropout(dropout))\n","D.add(Conv2D(depth*4, (5,5), strides=2, padding='same',activation=LeakyReLU(alpha=0.2)))\n","D.add(Dropout(dropout))\n","D.add(Flatten())\n","D.add(Dense(1024,activation=LeakyReLU(alpha=0.2)))\n","D.add(Dense(1,activation='sigmoid'))\n","D.summary()\n","```\n","\n","> b) Defina al *generador* de la GAN como el que se muestra en el código, con una tanda *fully conected* y 3 bloque convolucionales transpuesta además de agregar *BatchNormalization* entre ellas para tener un aprendizaje más estable. Describa la arquitectura utilizada (siendo del tipo *fully convolutional* puesto que la salida es un arreglo n-dimensional) y el porqué la función de activación de la salida es *sigmoidal*.\n","\n","```python\n","from keras.layers import BatchNormalization,Reshape,UpSampling2D,Conv2DTranspose,Activation\n","## Generator\n","G = Sequential()\n","dim = 14\n","input_dim= 2 #para que sea similar al vAE\n","G.add(Dense(128, input_dim=input_dim))\n","G.add(BatchNormalization())\n","G.add(Activation('relu'))\n","G.add(Dense(dim*dim*depth))\n","G.add(BatchNormalization())\n","G.add(Activation('relu'))\n","G.add(Reshape((dim, dim, depth)))\n","G.add(Conv2DTranspose(depth/2, (3,3), padding='same',strides=(2,2)))\n","G.add(BatchNormalization())\n","G.add(Activation('relu'))\n","G.add(Conv2DTranspose(depth/2, (3,3), padding='same'))\n","G.add(BatchNormalization())\n","G.add(Activation('relu'))\n","G.add(Conv2DTranspose(channel, (3,3), padding='same')) \n","G.add(Activation('sigmoid')) \n","G.summary()\n","```\n","\n","> c) Conecte los modelos a través del enfoque adversario, será necesario definir dos modelos debido a que el entrenamiento es iterativo, primero se entrena el discriminador el generador fijo, luego se entrena el generador con el discriminador fijo y así. \n","\n","```python\n","from keras.optimizers import RMSprop\n","## Discriminator model (police)\n","optimizer = RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8)\n","DM = Sequential()\n","DM.add(D)\n","DM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n","## Adversarial model (Generator->Discriminator)\n","D.trainable=False #set the discriminator freeze  (fixed params)\n","optimizer = RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8)\n","AM = Sequential()\n","AM.add(G)\n","AM.add(D)\n","AM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n","```\n","\n","> d) Entrene el modelo definido con el enfoque iterativo como se nombró, para ésto utilice la función que se presenta que lo realiza de manera manual. Grafique la pérdida *loss* de cada red (el generador y el discriminador/adversario) a través de los pasos de actualización de los pesos ¿Cómo se espera que sean estas curvas de aprendizaje?\n","\n","```python\n","def train_on_steps(X_train,DM,AM,G,steps,batch_size):\n","    history = {\"d\":[],\"g\":[]}\n","    for e in range(train_steps):\n","        # Make generative images\n","        image_batch = X_train[np.random.randint(0,X_train.shape[0],size=batch_size),:,:,:] #sample images from real data\n","        noise_gen = np.random.uniform(-1,1,size=[batch_size,input_dim]) #sample image from generated data\n","        generated_images = G.predict(noise_gen) #fake images\n","        # Train discriminator on generated images\n","        X = np.concatenate((image_batch, generated_images))\n","        #create labels\n","        y = np.ones([2*batch_size,1])\n","        y[batch_size:,:] = 0\n","        d_loss  = DM.train_on_batch(X,y)\n","        history[\"d\"].append(d_loss)\n","        # train Generator-Discriminator stack on input noise to non-generated output class\n","        noise_tr = np.random.uniform(-1,1,size=[batch_size,input_dim])\n","        y = np.ones([batch_size, 1])\n","        g_loss = AM.train_on_batch(noise_tr, y)\n","        history[\"g\"].append(g_loss)\n","        log_mesg = \"%d: [D loss: %f, acc: %f]\" % (e, d_loss[0], d_loss[1])\n","        log_mesg = \"%s  [G loss: %f, acc: %f]\" % (log_mesg, g_loss[0], g_loss[1])\n","        print(log_mesg)\n","        return history\n","train_steps = 5000 #or few if  you want\n","hist = train_on_steps(X_train,DM,AM,G,train_steps,64)\n","```\n","\n","> e) Genere nuevos datos artificialmente a través del modelo generador *G* ya entrenado, para esto inicialice aleatoriamente el espacio oculto de dimensiones del generador a través de una distribución Uniforme entre -1 y 1, al igual como fue entrenado. Comente sobre las imágenes generadas y compare con lo realizado con el VAE, en temas de calidad visual y en tiempos de ejecución.\n","\n","```python\n","N = 10\n","noise = np.random.uniform(-1.0, 1.0, size=[N, input_dim]) \n","images = G.predict(noise)\n","import matplotlib.pyplot as plt\n","for i in range(images.shape[0]):\n","    plt.subplot(4, 4, i+1)\n","    image = images[i, :, :, :]\n","    image = np.reshape(image, [img_rows, img_cols])\n","    plt.imshow(image, cmap='gray')\n","    plt.axis('off')\n","plt.tight_layout()\n","plt.show()\n","```\n","\n","> f) ¿Qué le parece que resulta más crucial al momento de entrenar las GAN, saber que se tiene un buen generador e intentar mejorar el discriminador o saber que se tiene un buen discriminador e intentar mejorar el generador? en ambos casos para que el generador mejore. Experimente con una de las ideas, modifique el generador o el discriminador e intente generar mejores imágenes artificiales.\n","\n","> g) ¿Qué pasa al cambiar la distribución latente de los datos $z$? Comente sobre alguna distribución elegida, diferente a la Uniforme, e **implementela** entrenando completamente el modelo.\n","\n","> h) Comente sobre el efecto de aumentar la dimensionalidad de las variables a generar datos $z$. Compare con lo acontecido con el método generativo VAE."]},{"metadata":{"id":"IWIa7yh_bYea","colab_type":"text"},"cell_type":"markdown","source":["<a id=\"segundo\"></a>\n","## 2. *Question Answering*\n","\n","Las redes neuronales recurrentes hoy en día han sido aplicadas a varios problemas que involucra dependencia temporal de los datos de entrada, en textos por lo común, tal como los modelos *sequence to sequence* de traducción, resumir textos, formular hipótesis de un extracto o, como veremos en esta actividad, generar respuesta en base a alguna pregunta. En imágenes también han sido aplicadas, ya sea a procesamiento de videos u a otro problema en que las imágenes tienen dependencia temporal unas con otras.\n","\n","Para ésta actividad trabajaremos el dataset de __[SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/)__  (The Stanford Question Answering Dataset), los datos se los entregamos en formato *csv*, sin ningún preprocesamiento, para que sea mas fácil la lectura. La tarea como ya se comentó consiste en predecir una respuesta (secuencia de palabras) que contesten una pregunta también en forma de secuencia de palabras, con un enfoque *encoder-decoder* con módulos de antención.\n","\n","\n","<img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2017/07/20/sockeye_1.gif\" title=\"Attention\" width=\"65%\" style=\"float: right;\"/>\n","\n","\n","<img src=\"http://www.wildml.com/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.16.08-PM.png\" title=\"Attention\" width=\"35%\" style=\"float: left;\"/>\n","\n","\n","\n","Los módulos de antención [[6]](#refs) son una variación a la arquitectura *encoder-decoder* en donde se agrega que para cada instante de tiempo de la **decodificación** $T'$ hay una combinación lineal del vector de codificación en todos los instantes tiempo $T$, ésto es para que en cada instante de tiempo de la decodificación se ponga atención a cierta información en toda la secuencia de entrada. \n","\n","\n","$$\n","y_{T'} = \\sum_{t}^{T} \\alpha_{T',t} \\cdot h_t^{codificacion}\n","$$\n","\n","\n","> a) Carge los datos y descríbalos ¿Cuántos ejemplos se tienen para entrenar y para predecir?\n","\n","```python\n","import pandas as pd\n","df_train = pd.read_csv('SQuAD/train_Q-A.csv')\n","df_train.dropna(inplace=True)\n","df_test = pd.read_csv('SQuAD/test_Q.csv')\n","df_train.head()\n","df_train.shape\n","```\n","\n","> b) Realice un preprocesamiento simple a los textos de entrada (preguntas) *tokenizandolos* y pasando a minúsculas para evitar ambiguedad, si desea agregar algun preprocesamiento éxtra ésto se verá reflajado en su nota. A los textos de salida (respuestas) no realice ningún preprocesamiento mas que *tokenizar*, puesto que para la evaluación se solicita retornar los textos en su forma natural. Comente lo realizado.\n","\n","```python\n","from nltk.tokenize import word_tokenize\n","train_questions = [word_tokenize(sentence.lower()) for sentence in df_train[\"question\"]] #or processing\n","test_questions = [word_tokenize(sentence.lower()) for sentence in df_test[\"question\"]]\n","train_answers = [word_tokenize(sentence) for sentence in df_train[\"answer\"]]\n","```\n","\n","> c) Cree un vocabulario para codificar las palabras en las respuestas a generar. Repita el procedimiento para las preguntas. Agrege un símbolo que signifique el fin de la respuesta a generar, así para tener un criterio de cuando una respuesta, valga la redundancia, está efectivamente *respondida* ¿Cuántas palabras tiene el vocabulario de las respuestas y de las preguntas? ¿Ésto podría ser un problema al momento de entrenar la red para que predizca de entre todas ellas?\n","\n","```python\n","vocab_answer = set()\n","for sentence in train_answers:\n","    for word in sentence:\n","        vocab_answer.add(word)\n","vocab_answer = [\"#end\"]+ list(vocab_answer)\n","print('posibles palabras para respuestas :', len(vocab_answer))\n","vocabA_indices = {c: i for i, c in enumerate(vocab_answer)}\n","indices_vocabA = {i: c for i, c in enumerate(vocab_answer)}\n","#sameforquestions\n","```\n","\n","> d) Codifique los tokens (palabras) de cada texto que utilizará.\n","\n","```python\n","#input and output to onehotvector\n","X_answers = [[vocabA_indices[palabra] for palabra in sentence] for sentence in train_answers]\n","Xtrain_question = #same for train question\n","Xtest_question = #same for test question\n","```\n","> Luego de ésto realice un *padding* a ambas secuencias, entrada y salida de entrenamiento y a la entrada del conjunto de pruebas. Comente sobre las dimensionalidades finales de los conjuntos de entrenamiento y de prueba.\n","\n","```python\n","import numpy as np\n","max_input_lenght = np.max(list(map(len,train_questions)))\n","max_output_lenght = np.max(list(map(len,train_answers)))+1\n","from keras.preprocessing import sequence\n","Xtrain_question = sequence.pad_sequences(Xtrain_question,maxlen=max_input_lenght,padding='pre or post',value=0)\n","Xtest_question = sequence.pad_sequences(Xtest_question,maxlen=max_input_lenght,padding='pre or post',value=0)\n","X_answers = sequence.pad_sequences(X_answers,maxlen=max_output_lenght,padding='post',value=vocabA_indices[\"#end\"])\n","```\n","\n","> e) Defina el modelo *encoder-decoder* con los módulos de atención.\n","\n","```python\n","#Encoder-Decoder modelo\n","from keras.layers import Input,RepeatVector,TimeDistributed,Dense,Embedding,Flatten,Activation,Permute,Lambda\n","from keras.models import Model\n","from keras import backend as K\n","lenght_output = max_output_lenght\n","hidden_dim = 128\n","```\n","> Defina el *encoder* y las compuertas que utilizará: CuDNNGRU,CuDNNLSTM, RNN u otra. Puede utilizar redes bidireccionales en el *encoder* ¿Esto mejora el resultado?\n","\n","```python\n","embedding_vector = 64 \n","encoder_input = Input(shape=(max_input_lenght,))\n","embedded = Embedding(input_dim=len(vocabQ_indices),output_dim=embedding_vector,input_length=max_input_lenght)(encoder_input)\n","encoder = gate(hidden_dim, return_sequences=True)(embedded)\n","```\n","> Defina la atención $\\alpha$ que se calculará sobre cada instante de tiempo $T$ computándo su atención en cada instante de tiempo de la decodificación $T'$.\n","\n","```python\n","# compute T' importance for each step T\n","attention = TimeDistributed(Dense(max_output_lenght, activation='tanh'))(encoder)\n","#softmax a las antenciones sobre todo T\n","attention = Permute([2, 1])(attention)\n","attention = Activation('softmax')(attention) \n","attention = Permute([2, 1])(attention)\n","```\n","> Aplique la atención sobre el *encoder* y genere las salidas correspondientes.\n","\n","```python\n","# apply the attention to encoder\n","def attention_multiply(vects):\n","    encoder, attention = vects\n","    return K.batch_dot(attention,encoder, axes=1)\n","sent_representation = Lambda(attention_multiply)([encoder, attention])\n","decoder = gate(hidden_dim, return_sequences=True)(sent_representation)\n","probabilities = TimeDistributed(Dense(len(vocab_answer), activation=\"softmax\"))(decoder)\n","```\n","> Defina el modelo y descríbalo adecuadamente.\n","\n","```python\n","model = Model(encoder_input,probabilities)\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n","model.summary()\n","```\n","\n","> f) Entrene el modelo por 10 *epochs* con el tamaño de batch que estime conveniente. Para ésto deberá redimensionar la salida para que tenga 3 dimensiones debido a la recurrencia.\n","\n","```python\n","X_answers = X_answers.reshape(X_answers.shape[0],X_answers.shape[1],1)\n","X_answers.shape\n","model.fit(Xtrain_question,X_answers,epochs=10,batch_size=BS,validation_split=0.2)\n","```\n","*Por temas de recursos puede optar con entrenar con una muestra más pequeña del conjunto de entrenamiento*.\n","\n","> g) Muestre ejemplos de la predicción del modelo, para ésto genere una función que prediga a través de la distribución de probabilidad de la salida, de la forma que estime conveniente, cada palabra en cada instante de tiempo.\n","\n","```python\n","def predict_words(model,example,diversity=?):\n","    #predict example\n","n=10\n","for i in range(n):\n","    indexs = np.random.randint(0,len(Xtest_question))\n","    example = Xtest_question[indexs]\n","    indexes_answer = predict_words(model,example,0.85)\n","    question = df_test[\"question\"][indexs]\n","    print(\"Pregunta: \",question)\n","    answer = \"\"\n","    for index in indexes_answer:\n","        if indices_vocabA[index]==\"#end\": # el final de la oracion\n","            continue\n","        else:\n","            answer+=indices_vocabA[index]+\" \"\n","    print(\"Respuesta: \",answer)\n","print(\"Los ha predecido todos!\")\n","```\n","\n","> h) Evalúe la calidad de su modelo con la métrica del *benchmark*, para ésto deberá descargar el archivo **evaluation script** y el dato **dev json** de la página oficial del dataset: https://rajpurkar.github.io/SQuAD-explorer/ y ejecutarlo de la siguiente manera dentro del *Jupyter Notebook*\n","\n","```python\n","#evaluar resultados\n","!python evaluate-v2.0.py dev-v2.0.json predictions\n","```\n","> Para generar las predicciones utilice la función anteriormente definida de la siguiente manera:\n","```python\n","dic_predictions = {}\n","for example,id_e in zip(Xtest_question,df_test[\"id\"]): #todos los ejemplos\n","    indexes_answer = predict_words(model,example) #predice palabra en cada instante\n","    answer = \"\"\n","    for index in indexes_answer:\n","        if indices_vocabA[index]==\"#end\": # el final de la oracion\n","            continue\n","        else:\n","            answer+=indices_vocabA[index]+\" \"\n","    dic_predictions[id_e] = answer\n","    contador+=1\n","print(\"Los ha predecido todos!\")\n","json_save = json.dumps(dic_predictions)\n","archivo = open(\"predictions\",\"w\")\n","archivo.write(json_save)\n","archivo.close()\n","```\n","Comente sobre el desempeño obtenido y porqué debiera deberse."]},{"metadata":{"id":"GBJtxe1ubYeb","colab_type":"text"},"cell_type":"markdown","source":["<a id=\"tercero\"></a>\n","## 3. Challenge: Crowd (Object) Counting\n","\n","<img src=\"http://personal.ie.cuhk.edu.hk/~ccloy/images/shopping_mall_annotated.jpg\" title=\"Crowd Counting\" width=\"30%\"/>\n","\n","Para esta sección final se evaluará todo lo que han aprendido a través de un desafío en donde puedan competir y medir sus resultados *in time* en la plataforma de __[Kaggle](https://www.kaggle.com/)__. El problema y todo su detalle puede ser encontrado en la página de la competencia a través del siguiente link:\n","\n","\n","<center><H2> __[Competencia Object Counting](https://www.kaggle.com/t/59c93ca0e8ae47999f9287a5751d6402)__ </H2></center>\n","\n","\n","\n","Para esto deberán crearse una cuenta en la plataforma *Kaggle* y subir sus respuestas a ésta. Por favor crearse nombres que sean fácil identificar después para saber quién fue quién, sino no podrán tener la nota (o en el correo de entrega ponen cual es su nombre de usuario en la competencia).\n","\n","*Nota: El puntaje que entregará al estar participando en la competencia (menos de 2 puntos al *benchmark* que se presenta) y el lugar obtenido se presenta en la página de Kaggle.*\n","\n","> Las entregas en *csv* pueden ser generadas de la siguiente manera:\n","```python\n","import pandas as pd\n","d = {'id': test_ids, 'count': prediction_test}\n","entrega = pd.DataFrame(data=d,columns=['id','count'])\n","entrega.to_csv('mysubmission.csv', index=False)\n","```\n","\n","> Los archivos binarios de numpy pueden ser cargados facilmente con el siguiente comando:\n","```python\n","import numpy as np\n","dato = np.load('binary_file.npy')\n","```\n","\n","> Las imagenes en python pueden ser cargadas con la librería Pillow y transformadas a numpy array con el siguiente comando:\n","```python\n","from PIL import Image\n","img = Image.open(\"path/to/image.jpg or png\")\n","np.asarray(img)\n","```"]},{"metadata":{"id":"CHhTRArwbYeb","colab_type":"text"},"cell_type":"markdown","source":["<a id=\"refs\"></a>\n","## Referencias\n","[1] Kingma, D. P., & Welling, M. (2013). *Auto-encoding variational bayes*. arXiv preprint arXiv:1312.6114.  \n","[2] Dumoulin, V., & Visin, F. (2016). *A guide to convolution arithmetic for deep learning*. arXiv preprint arXiv:1603.07285.  \n","[3] https://github.com/vdumoulin/conv_arithmetic  \n","[4] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). *Generative adversarial nets*. In Advances in neural information processing systems (pp. 2672-2680).  \n","[5] Joyce, J. M. (2011). *Kullback-leibler divergence*. In International Encyclopedia of Statistical Science (pp. 720-722). Springer Berlin Heidelberg.  \n","[6] Chorowski, J. K., Bahdanau, D., Serdyuk, D., Cho, K., & Bengio, Y. (2015). *Attention-based models for speech recognition*. In Advances in neural information processing systems (pp. 577-585)."]}]}
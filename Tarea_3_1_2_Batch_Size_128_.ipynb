{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea 3 - 1.2 - Batch Size 128 .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/gonzalezf/Tarea3-INF395/blob/master/Tarea_3_1_2_Batch_Size_128_.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "OduUJ0FnVYP6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input,Conv2D,Flatten,Dense,MaxPool2D\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.layers import Reshape,Conv2DTranspose,Activation\n",
        "from keras import backend as K\n",
        "from keras.layers import Lambda\n",
        "from keras import utils\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "from scipy.stats import norm\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDSl4qUjVchu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JspDUiSOVeaM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vS1w3Oh8Vfs-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0NjIC1b_Vhp6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_rows, img_cols, channel = X_train.shape[1:]\n",
        "# color channels (1 = grayscale, 3 = RGB) \n",
        "#si fueran imagenes a color, se utilizaria channel = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A29FZz7OVlMn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lXaBkY4fVozz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# GAN"
      ]
    },
    {
      "metadata": {
        "id": "1y2hDhluVpR7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model,Sequential\n",
        "from keras.layers import LeakyReLU,Conv2D,Dropout,Flatten,Dense\n",
        "from keras.layers import BatchNormalization,Reshape,UpSampling2D,Conv2DTranspose,Activation\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Model,Sequential\n",
        "from keras.layers import LeakyReLU,Conv2D,Dropout,Flatten,Dense\n",
        "from keras.layers import BatchNormalization,Reshape,UpSampling2D,Conv2DTranspose,Activation\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZXRraSQxVq0J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "9bba0fef-90d8-4209-e902-7510bedf22d0"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "## Discriminator\n",
        "D = Sequential()\n",
        "depth = 64\n",
        "dropout = 0.4\n",
        "input_shape = (img_rows, img_cols, channel)\n",
        "D.add(Conv2D(depth*1, (5,5), strides=2, input_shape=input_shape,padding='same', activation=LeakyReLU(alpha=0.2)))\n",
        "D.add(Dropout(dropout))\n",
        "D.add(Conv2D(depth*2, (5,5), strides=2, padding='same',activation=LeakyReLU(alpha=0.2)))\n",
        "D.add(Dropout(dropout))\n",
        "D.add(Conv2D(depth*4, (5,5), strides=2, padding='same',activation=LeakyReLU(alpha=0.2)))\n",
        "D.add(Dropout(dropout))\n",
        "D.add(Flatten())\n",
        "D.add(Dense(1024,activation=LeakyReLU(alpha=0.2)))\n",
        "D.add(Dense(1,activation='sigmoid'))\n",
        "D.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 14, 14, 64)        1664      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1024)              4195328   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 5,222,401\n",
            "Trainable params: 5,222,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\thena\\Redes Neuronales\\rna\\lib\\site-packages\\keras\\activations.py:115: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9gbhPus9Vwnk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "927c2884-e599-4056-f416-8cd6ae719928"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## Generator\n",
        "G = Sequential()\n",
        "dim = 14\n",
        "input_dim= 2 #para que sea similar al vAE\n",
        "G.add(Dense(128, input_dim=input_dim))\n",
        "G.add(BatchNormalization())\n",
        "G.add(Activation('relu'))\n",
        "G.add(Dense(dim*dim*depth))\n",
        "G.add(BatchNormalization())\n",
        "G.add(Activation('relu'))\n",
        "G.add(Reshape((dim, dim, depth)))\n",
        "G.add(Conv2DTranspose(int(depth/2), (3,3), padding='same',strides=(2,2)))\n",
        "G.add(BatchNormalization())\n",
        "G.add(Activation('relu'))\n",
        "G.add(Conv2DTranspose(int(depth/2), (3,3), padding='same'))\n",
        "G.add(BatchNormalization())\n",
        "G.add(Activation('relu'))\n",
        "G.add(Conv2DTranspose(channel, (3,3), padding='same')) \n",
        "G.add(Activation('sigmoid')) \n",
        "G.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_11 (Dense)             (None, 128)               384       \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 12544)             1618176   \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 12544)             50176     \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 28, 28, 32)        18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTr (None, 28, 28, 1)         289       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,697,505\n",
            "Trainable params: 1,672,033\n",
            "Non-trainable params: 25,472\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OqNVdOPmV1V2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "## Discriminator model (police)\n",
        "#optimizer = RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8)\n",
        "optimizer = Adam(0.001)\n",
        "DM = Sequential()\n",
        "DM.add(D)\n",
        "DM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
        "## Adversarial model (Generator->Discriminator)\n",
        "D.trainable=False #set the discriminator freeze  (fixed params)\n",
        "#optimizer = RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8)\n",
        "optimizer = Adam(lr = 0.001)\n",
        "AM = Sequential()\n",
        "AM.add(G)\n",
        "AM.add(D)\n",
        "AM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vVz3-3diGtQ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_on_steps(X_train,DM,AM,G,steps,batch_size):\n",
        "    history = {\"d\":[],\"g\":[]}\n",
        "    for e in range(train_steps):\n",
        "        # Make generative images\n",
        "        image_batch = X_train[np.random.randint(0,X_train.shape[0],size=batch_size),:,:,:] #sample images from real data\n",
        "        noise_gen = np.random.uniform(-1,1,size=[batch_size,input_dim]) #sample image from generated data\n",
        "        generated_images = G.predict(noise_gen) #fake images\n",
        "        # Train discriminator on generated images\n",
        "        X = np.concatenate((image_batch, generated_images))\n",
        "        #create labels\n",
        "        y = np.ones([2*batch_size,1])\n",
        "        y[batch_size:,:] = 0\n",
        "        d_loss  = DM.train_on_batch(X,y)\n",
        "        history[\"d\"].append(d_loss)\n",
        "        # train Generator-Discriminator stack on input noise to non-generated output class\n",
        "        noise_tr = np.random.uniform(-1,1,size=[batch_size,input_dim])\n",
        "        y = np.ones([batch_size, 1])\n",
        "        g_loss = AM.train_on_batch(noise_tr, y)\n",
        "        history[\"g\"].append(g_loss)\n",
        "        log_mesg = \"%d: [D loss: %f, acc: %f]\" % (e, d_loss[0], d_loss[1])\n",
        "        log_mesg = \"%s  [G loss: %f, acc: %f]\" % (log_mesg, g_loss[0], g_loss[1])\n",
        "        print(log_mesg)\n",
        "    return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KqxiO0utV7wP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85071
        },
        "outputId": "b1bd2f28-7249-405a-bf71-2b2bb5a39759"
      },
      "cell_type": "code",
      "source": [
        "train_steps = 5000 #or few if  you want\n",
        "hist = train_on_steps(X_train,DM,AM,G,train_steps,64)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: [D loss: 0.695814, acc: 0.468750]  [G loss: 1.443468, acc: 0.000000]\n",
            "1: [D loss: 0.589562, acc: 0.500000]  [G loss: 0.674172, acc: 0.625000]\n",
            "2: [D loss: 0.605599, acc: 0.593750]  [G loss: 3.510629, acc: 0.000000]\n",
            "3: [D loss: 0.514577, acc: 0.671875]  [G loss: 1.999466, acc: 0.000000]\n",
            "4: [D loss: 0.357756, acc: 0.851562]  [G loss: 0.759928, acc: 0.484375]\n",
            "5: [D loss: 0.521978, acc: 0.710938]  [G loss: 3.001046, acc: 0.000000]\n",
            "6: [D loss: 0.412749, acc: 0.835938]  [G loss: 2.990742, acc: 0.000000]\n",
            "7: [D loss: 0.382906, acc: 0.859375]  [G loss: 0.780683, acc: 0.515625]\n",
            "8: [D loss: 0.429757, acc: 0.750000]  [G loss: 1.874981, acc: 0.031250]\n",
            "9: [D loss: 0.284359, acc: 0.898438]  [G loss: 3.501664, acc: 0.000000]\n",
            "10: [D loss: 0.458759, acc: 0.796875]  [G loss: 1.401048, acc: 0.125000]\n",
            "11: [D loss: 0.297343, acc: 0.875000]  [G loss: 0.977304, acc: 0.296875]\n",
            "12: [D loss: 0.347261, acc: 0.828125]  [G loss: 3.420774, acc: 0.000000]\n",
            "13: [D loss: 0.310705, acc: 0.875000]  [G loss: 1.755478, acc: 0.046875]\n",
            "14: [D loss: 0.229251, acc: 0.937500]  [G loss: 1.674239, acc: 0.046875]\n",
            "15: [D loss: 0.166743, acc: 0.914062]  [G loss: 3.673211, acc: 0.000000]\n",
            "16: [D loss: 0.143810, acc: 0.953125]  [G loss: 2.470294, acc: 0.046875]\n",
            "17: [D loss: 0.072905, acc: 0.984375]  [G loss: 1.158051, acc: 0.265625]\n",
            "18: [D loss: 0.139435, acc: 0.921875]  [G loss: 5.208521, acc: 0.000000]\n",
            "19: [D loss: 0.401518, acc: 0.898438]  [G loss: 0.591758, acc: 0.687500]\n",
            "20: [D loss: 0.535113, acc: 0.773438]  [G loss: 5.978427, acc: 0.000000]\n",
            "21: [D loss: 0.905560, acc: 0.726562]  [G loss: 0.552450, acc: 0.703125]\n",
            "22: [D loss: 1.022646, acc: 0.656250]  [G loss: 1.117085, acc: 0.375000]\n",
            "23: [D loss: 0.439266, acc: 0.804688]  [G loss: 2.177322, acc: 0.078125]\n",
            "24: [D loss: 0.875577, acc: 0.750000]  [G loss: 0.260469, acc: 0.968750]\n",
            "25: [D loss: 0.435052, acc: 0.750000]  [G loss: 0.270265, acc: 0.937500]\n",
            "26: [D loss: 0.608528, acc: 0.671875]  [G loss: 0.689634, acc: 0.546875]\n",
            "27: [D loss: 0.489394, acc: 0.687500]  [G loss: 1.739498, acc: 0.000000]\n",
            "28: [D loss: 0.525479, acc: 0.812500]  [G loss: 2.112626, acc: 0.000000]\n",
            "29: [D loss: 0.578011, acc: 0.867188]  [G loss: 1.628573, acc: 0.000000]\n",
            "30: [D loss: 0.306320, acc: 0.921875]  [G loss: 1.265951, acc: 0.046875]\n",
            "31: [D loss: 0.250351, acc: 0.960938]  [G loss: 1.067672, acc: 0.265625]\n",
            "32: [D loss: 0.245708, acc: 0.953125]  [G loss: 1.115230, acc: 0.265625]\n",
            "33: [D loss: 0.312932, acc: 0.851562]  [G loss: 1.292409, acc: 0.218750]\n",
            "34: [D loss: 0.216505, acc: 0.937500]  [G loss: 1.749910, acc: 0.031250]\n",
            "35: [D loss: 0.259493, acc: 0.945312]  [G loss: 2.504632, acc: 0.015625]\n",
            "36: [D loss: 0.209673, acc: 0.937500]  [G loss: 2.945788, acc: 0.000000]\n",
            "37: [D loss: 0.284815, acc: 0.906250]  [G loss: 2.744557, acc: 0.000000]\n",
            "38: [D loss: 0.395669, acc: 0.867188]  [G loss: 1.932814, acc: 0.046875]\n",
            "39: [D loss: 0.362278, acc: 0.875000]  [G loss: 1.156499, acc: 0.328125]\n",
            "40: [D loss: 0.398758, acc: 0.789062]  [G loss: 1.138899, acc: 0.281250]\n",
            "41: [D loss: 0.323510, acc: 0.882812]  [G loss: 1.235601, acc: 0.234375]\n",
            "42: [D loss: 0.487584, acc: 0.867188]  [G loss: 1.024320, acc: 0.453125]\n",
            "43: [D loss: 0.200274, acc: 0.945312]  [G loss: 0.974058, acc: 0.375000]\n",
            "44: [D loss: 0.346834, acc: 0.875000]  [G loss: 0.876564, acc: 0.500000]\n",
            "45: [D loss: 0.430816, acc: 0.851562]  [G loss: 0.639118, acc: 0.703125]\n",
            "46: [D loss: 0.355286, acc: 0.851562]  [G loss: 1.011309, acc: 0.421875]\n",
            "47: [D loss: 0.581374, acc: 0.757812]  [G loss: 0.853146, acc: 0.484375]\n",
            "48: [D loss: 0.611420, acc: 0.695312]  [G loss: 1.833279, acc: 0.156250]\n",
            "49: [D loss: 0.939181, acc: 0.718750]  [G loss: 1.268496, acc: 0.312500]\n",
            "50: [D loss: 0.894247, acc: 0.585938]  [G loss: 0.908370, acc: 0.406250]\n",
            "51: [D loss: 0.975085, acc: 0.539062]  [G loss: 2.134174, acc: 0.062500]\n",
            "52: [D loss: 1.057448, acc: 0.539062]  [G loss: 2.014667, acc: 0.031250]\n",
            "53: [D loss: 0.999825, acc: 0.570312]  [G loss: 1.283218, acc: 0.109375]\n",
            "54: [D loss: 1.130343, acc: 0.398438]  [G loss: 0.737342, acc: 0.453125]\n",
            "55: [D loss: 1.140930, acc: 0.296875]  [G loss: 1.036970, acc: 0.156250]\n",
            "56: [D loss: 1.045045, acc: 0.304688]  [G loss: 1.501903, acc: 0.031250]\n",
            "57: [D loss: 1.086900, acc: 0.398438]  [G loss: 1.740666, acc: 0.031250]\n",
            "58: [D loss: 1.077625, acc: 0.437500]  [G loss: 1.486945, acc: 0.015625]\n",
            "59: [D loss: 0.875526, acc: 0.484375]  [G loss: 1.156796, acc: 0.125000]\n",
            "60: [D loss: 0.862059, acc: 0.390625]  [G loss: 0.950515, acc: 0.218750]\n",
            "61: [D loss: 0.778151, acc: 0.500000]  [G loss: 0.885272, acc: 0.250000]\n",
            "62: [D loss: 0.712432, acc: 0.515625]  [G loss: 0.945865, acc: 0.187500]\n",
            "63: [D loss: 0.710776, acc: 0.531250]  [G loss: 1.083377, acc: 0.125000]\n",
            "64: [D loss: 0.636474, acc: 0.617188]  [G loss: 1.261171, acc: 0.031250]\n",
            "65: [D loss: 0.586853, acc: 0.679688]  [G loss: 1.389461, acc: 0.000000]\n",
            "66: [D loss: 0.529072, acc: 0.734375]  [G loss: 1.435374, acc: 0.000000]\n",
            "67: [D loss: 0.496642, acc: 0.789062]  [G loss: 1.323126, acc: 0.000000]\n",
            "68: [D loss: 0.448275, acc: 0.851562]  [G loss: 1.316290, acc: 0.046875]\n",
            "69: [D loss: 0.365972, acc: 0.921875]  [G loss: 1.245531, acc: 0.078125]\n",
            "70: [D loss: 0.334854, acc: 0.945312]  [G loss: 1.359381, acc: 0.062500]\n",
            "71: [D loss: 0.305857, acc: 0.906250]  [G loss: 1.539704, acc: 0.062500]\n",
            "72: [D loss: 0.213877, acc: 0.968750]  [G loss: 2.053744, acc: 0.000000]\n",
            "73: [D loss: 0.188697, acc: 0.953125]  [G loss: 2.910696, acc: 0.000000]\n",
            "74: [D loss: 0.148213, acc: 0.976562]  [G loss: 3.135764, acc: 0.015625]\n",
            "75: [D loss: 0.134252, acc: 0.945312]  [G loss: 3.133376, acc: 0.000000]\n",
            "76: [D loss: 0.173775, acc: 0.921875]  [G loss: 3.357133, acc: 0.031250]\n",
            "77: [D loss: 0.412069, acc: 0.851562]  [G loss: 6.251730, acc: 0.000000]\n",
            "78: [D loss: 0.755456, acc: 0.750000]  [G loss: 3.205250, acc: 0.218750]\n",
            "79: [D loss: 0.930900, acc: 0.671875]  [G loss: 3.238480, acc: 0.140625]\n",
            "80: [D loss: 0.594247, acc: 0.742188]  [G loss: 3.409328, acc: 0.062500]\n",
            "81: [D loss: 0.505490, acc: 0.812500]  [G loss: 2.343606, acc: 0.203125]\n",
            "82: [D loss: 0.618758, acc: 0.742188]  [G loss: 1.448519, acc: 0.218750]\n",
            "83: [D loss: 0.516300, acc: 0.773438]  [G loss: 0.753888, acc: 0.578125]\n",
            "84: [D loss: 0.627451, acc: 0.687500]  [G loss: 0.882448, acc: 0.437500]\n",
            "85: [D loss: 0.483614, acc: 0.726562]  [G loss: 1.128815, acc: 0.312500]\n",
            "86: [D loss: 0.533102, acc: 0.648438]  [G loss: 1.145272, acc: 0.203125]\n",
            "87: [D loss: 0.362864, acc: 0.898438]  [G loss: 1.211720, acc: 0.093750]\n",
            "88: [D loss: 0.365355, acc: 0.890625]  [G loss: 1.215058, acc: 0.109375]\n",
            "89: [D loss: 0.221557, acc: 0.984375]  [G loss: 1.142963, acc: 0.203125]\n",
            "90: [D loss: 0.155223, acc: 1.000000]  [G loss: 1.086698, acc: 0.234375]\n",
            "91: [D loss: 0.121685, acc: 0.992188]  [G loss: 1.075028, acc: 0.328125]\n",
            "92: [D loss: 0.093265, acc: 1.000000]  [G loss: 0.985086, acc: 0.453125]\n",
            "93: [D loss: 0.073809, acc: 1.000000]  [G loss: 0.845930, acc: 0.484375]\n",
            "94: [D loss: 0.076623, acc: 1.000000]  [G loss: 0.707812, acc: 0.625000]\n",
            "95: [D loss: 0.112224, acc: 0.968750]  [G loss: 0.593930, acc: 0.687500]\n",
            "96: [D loss: 0.086863, acc: 0.976562]  [G loss: 0.587887, acc: 0.671875]\n",
            "97: [D loss: 0.224136, acc: 0.906250]  [G loss: 0.969563, acc: 0.468750]\n",
            "98: [D loss: 0.209278, acc: 0.937500]  [G loss: 2.548192, acc: 0.078125]\n",
            "99: [D loss: 0.290628, acc: 0.882812]  [G loss: 3.811812, acc: 0.015625]\n",
            "100: [D loss: 0.530581, acc: 0.835938]  [G loss: 2.499640, acc: 0.187500]\n",
            "101: [D loss: 0.432958, acc: 0.843750]  [G loss: 2.459436, acc: 0.140625]\n",
            "102: [D loss: 0.666683, acc: 0.695312]  [G loss: 3.890034, acc: 0.062500]\n",
            "103: [D loss: 1.015607, acc: 0.750000]  [G loss: 2.293232, acc: 0.234375]\n",
            "104: [D loss: 0.879027, acc: 0.695312]  [G loss: 1.649637, acc: 0.531250]\n",
            "105: [D loss: 0.983758, acc: 0.671875]  [G loss: 4.004818, acc: 0.062500]\n",
            "106: [D loss: 1.173848, acc: 0.625000]  [G loss: 2.612320, acc: 0.187500]\n",
            "107: [D loss: 0.639275, acc: 0.742188]  [G loss: 0.903540, acc: 0.609375]\n",
            "108: [D loss: 0.569463, acc: 0.734375]  [G loss: 1.180267, acc: 0.390625]\n",
            "109: [D loss: 0.583888, acc: 0.750000]  [G loss: 1.602544, acc: 0.218750]\n",
            "110: [D loss: 0.442412, acc: 0.812500]  [G loss: 1.699939, acc: 0.140625]\n",
            "111: [D loss: 0.459608, acc: 0.781250]  [G loss: 1.159747, acc: 0.359375]\n",
            "112: [D loss: 0.361123, acc: 0.867188]  [G loss: 0.940422, acc: 0.421875]\n",
            "113: [D loss: 0.310910, acc: 0.890625]  [G loss: 0.725568, acc: 0.546875]\n",
            "114: [D loss: 0.302906, acc: 0.851562]  [G loss: 0.759712, acc: 0.515625]\n",
            "115: [D loss: 0.304276, acc: 0.851562]  [G loss: 0.935788, acc: 0.359375]\n",
            "116: [D loss: 0.273057, acc: 0.898438]  [G loss: 0.853772, acc: 0.453125]\n",
            "117: [D loss: 0.231181, acc: 0.898438]  [G loss: 0.838042, acc: 0.468750]\n",
            "118: [D loss: 0.215710, acc: 0.914062]  [G loss: 0.968587, acc: 0.390625]\n",
            "119: [D loss: 0.197095, acc: 0.945312]  [G loss: 1.100662, acc: 0.406250]\n",
            "120: [D loss: 0.267336, acc: 0.890625]  [G loss: 1.098036, acc: 0.468750]\n",
            "121: [D loss: 0.196874, acc: 0.937500]  [G loss: 0.837006, acc: 0.562500]\n",
            "122: [D loss: 0.254548, acc: 0.921875]  [G loss: 0.442103, acc: 0.796875]\n",
            "123: [D loss: 0.158566, acc: 0.945312]  [G loss: 0.638732, acc: 0.640625]\n",
            "124: [D loss: 0.154404, acc: 0.968750]  [G loss: 1.068607, acc: 0.593750]\n",
            "125: [D loss: 0.127719, acc: 0.937500]  [G loss: 0.966457, acc: 0.609375]\n",
            "126: [D loss: 0.235302, acc: 0.898438]  [G loss: 0.986853, acc: 0.609375]\n",
            "127: [D loss: 0.114985, acc: 0.953125]  [G loss: 0.981122, acc: 0.687500]\n",
            "128: [D loss: 0.184907, acc: 0.945312]  [G loss: 1.023333, acc: 0.671875]\n",
            "129: [D loss: 0.211797, acc: 0.914062]  [G loss: 1.117981, acc: 0.593750]\n",
            "130: [D loss: 0.228524, acc: 0.929688]  [G loss: 1.527487, acc: 0.578125]\n",
            "131: [D loss: 0.214398, acc: 0.914062]  [G loss: 1.645706, acc: 0.484375]\n",
            "132: [D loss: 0.286726, acc: 0.875000]  [G loss: 1.298993, acc: 0.531250]\n",
            "133: [D loss: 0.288347, acc: 0.882812]  [G loss: 1.226155, acc: 0.515625]\n",
            "134: [D loss: 0.344845, acc: 0.835938]  [G loss: 1.375720, acc: 0.437500]\n",
            "135: [D loss: 0.306898, acc: 0.859375]  [G loss: 1.830241, acc: 0.328125]\n",
            "136: [D loss: 0.256067, acc: 0.906250]  [G loss: 1.494498, acc: 0.375000]\n",
            "137: [D loss: 0.324485, acc: 0.882812]  [G loss: 1.500524, acc: 0.390625]\n",
            "138: [D loss: 0.428103, acc: 0.804688]  [G loss: 2.440569, acc: 0.156250]\n",
            "139: [D loss: 0.382776, acc: 0.835938]  [G loss: 1.834087, acc: 0.312500]\n",
            "140: [D loss: 0.515752, acc: 0.773438]  [G loss: 0.733922, acc: 0.609375]\n",
            "141: [D loss: 0.631801, acc: 0.687500]  [G loss: 0.599180, acc: 0.640625]\n",
            "142: [D loss: 0.673695, acc: 0.648438]  [G loss: 1.387356, acc: 0.375000]\n",
            "143: [D loss: 0.474068, acc: 0.812500]  [G loss: 2.039768, acc: 0.109375]\n",
            "144: [D loss: 0.595217, acc: 0.796875]  [G loss: 1.842983, acc: 0.171875]\n",
            "145: [D loss: 0.682896, acc: 0.710938]  [G loss: 1.029890, acc: 0.468750]\n",
            "146: [D loss: 0.562522, acc: 0.710938]  [G loss: 0.593359, acc: 0.718750]\n",
            "147: [D loss: 0.624147, acc: 0.609375]  [G loss: 0.588372, acc: 0.703125]\n",
            "148: [D loss: 0.570894, acc: 0.640625]  [G loss: 0.710204, acc: 0.515625]\n",
            "149: [D loss: 0.537011, acc: 0.687500]  [G loss: 0.914187, acc: 0.312500]\n",
            "150: [D loss: 0.437986, acc: 0.812500]  [G loss: 1.270888, acc: 0.218750]\n",
            "151: [D loss: 0.435498, acc: 0.828125]  [G loss: 1.409093, acc: 0.187500]\n",
            "152: [D loss: 0.534285, acc: 0.796875]  [G loss: 1.223822, acc: 0.234375]\n",
            "153: [D loss: 0.358640, acc: 0.843750]  [G loss: 1.242038, acc: 0.250000]\n",
            "154: [D loss: 0.354064, acc: 0.843750]  [G loss: 1.046571, acc: 0.296875]\n",
            "155: [D loss: 0.301479, acc: 0.890625]  [G loss: 1.128840, acc: 0.359375]\n",
            "156: [D loss: 0.237367, acc: 0.906250]  [G loss: 1.160757, acc: 0.343750]\n",
            "157: [D loss: 0.246405, acc: 0.906250]  [G loss: 1.382561, acc: 0.375000]\n",
            "158: [D loss: 0.253378, acc: 0.875000]  [G loss: 1.560981, acc: 0.296875]\n",
            "159: [D loss: 0.263343, acc: 0.898438]  [G loss: 2.396288, acc: 0.125000]\n",
            "160: [D loss: 0.169408, acc: 0.937500]  [G loss: 3.299772, acc: 0.062500]\n",
            "161: [D loss: 0.122920, acc: 0.968750]  [G loss: 4.117265, acc: 0.046875]\n",
            "162: [D loss: 0.226846, acc: 0.914062]  [G loss: 4.402368, acc: 0.015625]\n",
            "163: [D loss: 0.208072, acc: 0.914062]  [G loss: 4.582696, acc: 0.109375]\n",
            "164: [D loss: 0.125314, acc: 0.945312]  [G loss: 3.161125, acc: 0.265625]\n",
            "165: [D loss: 0.221037, acc: 0.929688]  [G loss: 2.087358, acc: 0.484375]\n",
            "166: [D loss: 0.387585, acc: 0.890625]  [G loss: 1.286562, acc: 0.531250]\n",
            "167: [D loss: 0.468022, acc: 0.867188]  [G loss: 1.797114, acc: 0.406250]\n",
            "168: [D loss: 0.278601, acc: 0.867188]  [G loss: 2.751824, acc: 0.250000]\n",
            "169: [D loss: 0.581112, acc: 0.765625]  [G loss: 2.451208, acc: 0.281250]\n",
            "170: [D loss: 0.477740, acc: 0.796875]  [G loss: 1.235090, acc: 0.500000]\n",
            "171: [D loss: 0.384387, acc: 0.851562]  [G loss: 0.705049, acc: 0.640625]\n",
            "172: [D loss: 0.374992, acc: 0.804688]  [G loss: 0.723667, acc: 0.671875]\n",
            "173: [D loss: 0.425015, acc: 0.820312]  [G loss: 1.246791, acc: 0.546875]\n",
            "174: [D loss: 0.277118, acc: 0.859375]  [G loss: 1.365248, acc: 0.453125]\n",
            "175: [D loss: 0.419681, acc: 0.875000]  [G loss: 1.435542, acc: 0.500000]\n",
            "176: [D loss: 0.223154, acc: 0.937500]  [G loss: 1.507102, acc: 0.421875]\n",
            "177: [D loss: 0.252474, acc: 0.890625]  [G loss: 0.631354, acc: 0.750000]\n",
            "178: [D loss: 0.321598, acc: 0.820312]  [G loss: 0.732447, acc: 0.687500]\n",
            "179: [D loss: 0.339708, acc: 0.859375]  [G loss: 2.170242, acc: 0.218750]\n",
            "180: [D loss: 0.229373, acc: 0.937500]  [G loss: 3.460425, acc: 0.109375]\n",
            "181: [D loss: 0.368933, acc: 0.875000]  [G loss: 2.326706, acc: 0.140625]\n",
            "182: [D loss: 0.382535, acc: 0.921875]  [G loss: 0.625004, acc: 0.703125]\n",
            "183: [D loss: 0.288884, acc: 0.867188]  [G loss: 0.455558, acc: 0.781250]\n",
            "184: [D loss: 0.259494, acc: 0.890625]  [G loss: 0.861504, acc: 0.546875]\n",
            "185: [D loss: 0.135361, acc: 0.945312]  [G loss: 1.351703, acc: 0.359375]\n",
            "186: [D loss: 0.239198, acc: 0.921875]  [G loss: 0.866953, acc: 0.625000]\n",
            "187: [D loss: 0.093292, acc: 0.960938]  [G loss: 0.333552, acc: 0.859375]\n",
            "188: [D loss: 0.086267, acc: 0.976562]  [G loss: 0.098130, acc: 0.968750]\n",
            "189: [D loss: 0.066482, acc: 0.984375]  [G loss: 0.018369, acc: 1.000000]\n",
            "190: [D loss: 0.117863, acc: 0.953125]  [G loss: 0.061305, acc: 0.968750]\n",
            "191: [D loss: 0.049378, acc: 0.984375]  [G loss: 0.123054, acc: 0.968750]\n",
            "192: [D loss: 0.042137, acc: 0.984375]  [G loss: 0.227986, acc: 0.906250]\n",
            "193: [D loss: 0.059559, acc: 0.976562]  [G loss: 0.361363, acc: 0.843750]\n",
            "194: [D loss: 0.050645, acc: 0.976562]  [G loss: 0.266123, acc: 0.906250]\n",
            "195: [D loss: 0.073969, acc: 0.968750]  [G loss: 0.049279, acc: 0.984375]\n",
            "196: [D loss: 0.053486, acc: 0.992188]  [G loss: 0.197464, acc: 0.937500]\n",
            "197: [D loss: 0.070639, acc: 0.976562]  [G loss: 1.451318, acc: 0.500000]\n",
            "198: [D loss: 0.036577, acc: 0.976562]  [G loss: 2.301840, acc: 0.281250]\n",
            "199: [D loss: 0.273780, acc: 0.937500]  [G loss: 0.627663, acc: 0.734375]\n",
            "200: [D loss: 0.104462, acc: 0.945312]  [G loss: 0.111623, acc: 0.937500]\n",
            "201: [D loss: 0.551593, acc: 0.765625]  [G loss: 3.200054, acc: 0.031250]\n",
            "202: [D loss: 0.509444, acc: 0.835938]  [G loss: 2.868663, acc: 0.140625]\n",
            "203: [D loss: 0.692864, acc: 0.828125]  [G loss: 0.200463, acc: 0.906250]\n",
            "204: [D loss: 0.028108, acc: 1.000000]  [G loss: 0.006167, acc: 1.000000]\n",
            "205: [D loss: 0.893620, acc: 0.687500]  [G loss: 0.214339, acc: 0.921875]\n",
            "206: [D loss: 0.164753, acc: 0.914062]  [G loss: 0.894772, acc: 0.562500]\n",
            "207: [D loss: 0.541656, acc: 0.789062]  [G loss: 0.549183, acc: 0.765625]\n",
            "208: [D loss: 0.255356, acc: 0.867188]  [G loss: 0.229110, acc: 0.921875]\n",
            "209: [D loss: 0.330215, acc: 0.851562]  [G loss: 0.137160, acc: 0.968750]\n",
            "210: [D loss: 0.613296, acc: 0.671875]  [G loss: 0.338553, acc: 0.875000]\n",
            "211: [D loss: 0.356235, acc: 0.828125]  [G loss: 0.635880, acc: 0.656250]\n",
            "212: [D loss: 0.356169, acc: 0.914062]  [G loss: 0.842415, acc: 0.468750]\n",
            "213: [D loss: 0.491262, acc: 0.789062]  [G loss: 0.779964, acc: 0.625000]\n",
            "214: [D loss: 0.555382, acc: 0.765625]  [G loss: 0.400128, acc: 0.937500]\n",
            "215: [D loss: 0.446296, acc: 0.820312]  [G loss: 0.304501, acc: 0.953125]\n",
            "216: [D loss: 0.521503, acc: 0.656250]  [G loss: 0.372864, acc: 0.906250]\n",
            "217: [D loss: 0.410232, acc: 0.781250]  [G loss: 0.541728, acc: 0.734375]\n",
            "218: [D loss: 0.371113, acc: 0.796875]  [G loss: 0.857543, acc: 0.390625]\n",
            "219: [D loss: 0.366932, acc: 0.828125]  [G loss: 1.270531, acc: 0.281250]\n",
            "220: [D loss: 0.312264, acc: 0.890625]  [G loss: 1.693966, acc: 0.140625]\n",
            "221: [D loss: 0.286567, acc: 0.914062]  [G loss: 2.237154, acc: 0.062500]\n",
            "222: [D loss: 0.457530, acc: 0.820312]  [G loss: 2.008725, acc: 0.125000]\n",
            "223: [D loss: 0.366865, acc: 0.859375]  [G loss: 1.880546, acc: 0.140625]\n",
            "224: [D loss: 0.353689, acc: 0.859375]  [G loss: 1.498794, acc: 0.218750]\n",
            "225: [D loss: 0.322304, acc: 0.843750]  [G loss: 1.195812, acc: 0.328125]\n",
            "226: [D loss: 0.391965, acc: 0.820312]  [G loss: 1.092791, acc: 0.375000]\n",
            "227: [D loss: 0.327172, acc: 0.835938]  [G loss: 1.065673, acc: 0.343750]\n",
            "228: [D loss: 0.270624, acc: 0.921875]  [G loss: 1.175702, acc: 0.406250]\n",
            "229: [D loss: 0.222572, acc: 0.929688]  [G loss: 1.478587, acc: 0.390625]\n",
            "230: [D loss: 0.225246, acc: 0.937500]  [G loss: 1.310670, acc: 0.343750]\n",
            "231: [D loss: 0.176736, acc: 0.953125]  [G loss: 1.383032, acc: 0.375000]\n",
            "232: [D loss: 0.191333, acc: 0.929688]  [G loss: 1.360558, acc: 0.406250]\n",
            "233: [D loss: 0.075181, acc: 0.976562]  [G loss: 1.068977, acc: 0.437500]\n",
            "234: [D loss: 0.177511, acc: 0.937500]  [G loss: 0.829838, acc: 0.593750]\n",
            "235: [D loss: 0.144158, acc: 0.929688]  [G loss: 0.901236, acc: 0.625000]\n",
            "236: [D loss: 0.136531, acc: 0.960938]  [G loss: 0.628133, acc: 0.656250]\n",
            "237: [D loss: 0.295730, acc: 0.867188]  [G loss: 0.931529, acc: 0.671875]\n",
            "238: [D loss: 0.180036, acc: 0.953125]  [G loss: 1.036087, acc: 0.531250]\n",
            "239: [D loss: 0.229169, acc: 0.890625]  [G loss: 1.741662, acc: 0.468750]\n",
            "240: [D loss: 0.400814, acc: 0.851562]  [G loss: 2.268381, acc: 0.453125]\n",
            "241: [D loss: 0.747290, acc: 0.750000]  [G loss: 1.810790, acc: 0.421875]\n",
            "242: [D loss: 0.658602, acc: 0.804688]  [G loss: 1.070503, acc: 0.593750]\n",
            "243: [D loss: 1.348380, acc: 0.570312]  [G loss: 1.266446, acc: 0.390625]\n",
            "244: [D loss: 1.510782, acc: 0.539062]  [G loss: 3.754003, acc: 0.078125]\n",
            "245: [D loss: 1.587952, acc: 0.585938]  [G loss: 4.457406, acc: 0.109375]\n",
            "246: [D loss: 1.781757, acc: 0.554688]  [G loss: 2.744646, acc: 0.328125]\n",
            "247: [D loss: 1.405575, acc: 0.562500]  [G loss: 1.094434, acc: 0.625000]\n",
            "248: [D loss: 1.225362, acc: 0.546875]  [G loss: 0.760997, acc: 0.656250]\n",
            "249: [D loss: 1.025592, acc: 0.625000]  [G loss: 1.121476, acc: 0.531250]\n",
            "250: [D loss: 0.969447, acc: 0.601562]  [G loss: 1.531277, acc: 0.390625]\n",
            "251: [D loss: 0.671839, acc: 0.710938]  [G loss: 1.627617, acc: 0.343750]\n",
            "252: [D loss: 0.667113, acc: 0.765625]  [G loss: 1.168699, acc: 0.484375]\n",
            "253: [D loss: 0.550666, acc: 0.867188]  [G loss: 0.603861, acc: 0.734375]\n",
            "254: [D loss: 0.342959, acc: 0.875000]  [G loss: 0.451743, acc: 0.734375]\n",
            "255: [D loss: 0.449528, acc: 0.812500]  [G loss: 0.455448, acc: 0.796875]\n",
            "256: [D loss: 0.309163, acc: 0.890625]  [G loss: 0.596719, acc: 0.687500]\n",
            "257: [D loss: 0.179116, acc: 0.968750]  [G loss: 0.970065, acc: 0.656250]\n",
            "258: [D loss: 0.187882, acc: 0.960938]  [G loss: 1.201986, acc: 0.531250]\n",
            "259: [D loss: 0.242189, acc: 0.898438]  [G loss: 1.274756, acc: 0.406250]\n",
            "260: [D loss: 0.198107, acc: 0.921875]  [G loss: 1.330715, acc: 0.390625]\n",
            "261: [D loss: 0.187455, acc: 0.937500]  [G loss: 0.935432, acc: 0.593750]\n",
            "262: [D loss: 0.218415, acc: 0.945312]  [G loss: 0.702887, acc: 0.640625]\n",
            "263: [D loss: 0.212933, acc: 0.914062]  [G loss: 0.818318, acc: 0.593750]\n",
            "264: [D loss: 0.209751, acc: 0.921875]  [G loss: 0.747563, acc: 0.625000]\n",
            "265: [D loss: 0.216158, acc: 0.937500]  [G loss: 0.735332, acc: 0.546875]\n",
            "266: [D loss: 0.209877, acc: 0.960938]  [G loss: 1.129169, acc: 0.453125]\n",
            "267: [D loss: 0.126356, acc: 0.984375]  [G loss: 1.690691, acc: 0.359375]\n",
            "268: [D loss: 0.157862, acc: 0.937500]  [G loss: 1.918015, acc: 0.203125]\n",
            "269: [D loss: 0.165601, acc: 0.968750]  [G loss: 1.466259, acc: 0.312500]\n",
            "270: [D loss: 0.128809, acc: 0.968750]  [G loss: 1.600590, acc: 0.312500]\n",
            "271: [D loss: 0.184983, acc: 0.929688]  [G loss: 0.830820, acc: 0.593750]\n",
            "272: [D loss: 0.220063, acc: 0.929688]  [G loss: 1.202061, acc: 0.375000]\n",
            "273: [D loss: 0.260461, acc: 0.898438]  [G loss: 1.653179, acc: 0.171875]\n",
            "274: [D loss: 0.226622, acc: 0.929688]  [G loss: 1.586569, acc: 0.234375]\n",
            "275: [D loss: 0.266684, acc: 0.875000]  [G loss: 1.015242, acc: 0.484375]\n",
            "276: [D loss: 0.175353, acc: 0.945312]  [G loss: 0.866861, acc: 0.593750]\n",
            "277: [D loss: 0.245133, acc: 0.914062]  [G loss: 1.660015, acc: 0.296875]\n",
            "278: [D loss: 0.177251, acc: 0.929688]  [G loss: 2.210812, acc: 0.109375]\n",
            "279: [D loss: 0.398064, acc: 0.875000]  [G loss: 1.222514, acc: 0.406250]\n",
            "280: [D loss: 0.341478, acc: 0.906250]  [G loss: 0.453750, acc: 0.796875]\n",
            "281: [D loss: 0.528135, acc: 0.781250]  [G loss: 1.005608, acc: 0.546875]\n",
            "282: [D loss: 0.267246, acc: 0.914062]  [G loss: 1.148705, acc: 0.453125]\n",
            "283: [D loss: 0.297050, acc: 0.867188]  [G loss: 0.654462, acc: 0.640625]\n",
            "284: [D loss: 0.309638, acc: 0.875000]  [G loss: 0.537513, acc: 0.765625]\n",
            "285: [D loss: 0.395377, acc: 0.867188]  [G loss: 0.711664, acc: 0.656250]\n",
            "286: [D loss: 0.337506, acc: 0.859375]  [G loss: 1.762859, acc: 0.281250]\n",
            "287: [D loss: 0.782188, acc: 0.757812]  [G loss: 0.815116, acc: 0.578125]\n",
            "288: [D loss: 0.437191, acc: 0.765625]  [G loss: 1.143036, acc: 0.390625]\n",
            "289: [D loss: 0.543141, acc: 0.789062]  [G loss: 1.196397, acc: 0.421875]\n",
            "290: [D loss: 0.603743, acc: 0.765625]  [G loss: 1.222780, acc: 0.390625]\n",
            "291: [D loss: 0.626803, acc: 0.664062]  [G loss: 1.440823, acc: 0.343750]\n",
            "292: [D loss: 0.830466, acc: 0.656250]  [G loss: 1.191512, acc: 0.359375]\n",
            "293: [D loss: 0.745304, acc: 0.671875]  [G loss: 1.237955, acc: 0.343750]\n",
            "294: [D loss: 0.720347, acc: 0.656250]  [G loss: 1.292038, acc: 0.312500]\n",
            "295: [D loss: 0.803767, acc: 0.617188]  [G loss: 1.670111, acc: 0.281250]\n",
            "296: [D loss: 0.625542, acc: 0.710938]  [G loss: 1.784342, acc: 0.203125]\n",
            "297: [D loss: 0.473055, acc: 0.773438]  [G loss: 1.876378, acc: 0.125000]\n",
            "298: [D loss: 0.329419, acc: 0.867188]  [G loss: 1.681457, acc: 0.218750]\n",
            "299: [D loss: 0.217728, acc: 0.914062]  [G loss: 1.444927, acc: 0.437500]\n",
            "300: [D loss: 0.151467, acc: 0.945312]  [G loss: 1.252887, acc: 0.562500]\n",
            "301: [D loss: 0.160121, acc: 0.937500]  [G loss: 1.399876, acc: 0.406250]\n",
            "302: [D loss: 0.123805, acc: 0.976562]  [G loss: 1.532229, acc: 0.375000]\n",
            "303: [D loss: 0.090248, acc: 0.976562]  [G loss: 1.580378, acc: 0.343750]\n",
            "304: [D loss: 0.148297, acc: 0.937500]  [G loss: 1.158072, acc: 0.500000]\n",
            "305: [D loss: 0.107523, acc: 0.976562]  [G loss: 0.682153, acc: 0.656250]\n",
            "306: [D loss: 0.093787, acc: 0.984375]  [G loss: 0.637402, acc: 0.640625]\n",
            "307: [D loss: 0.112584, acc: 0.968750]  [G loss: 0.689153, acc: 0.640625]\n",
            "308: [D loss: 0.096989, acc: 0.968750]  [G loss: 0.896493, acc: 0.421875]\n",
            "309: [D loss: 0.095352, acc: 0.984375]  [G loss: 1.540608, acc: 0.031250]\n",
            "310: [D loss: 0.081323, acc: 0.984375]  [G loss: 3.012833, acc: 0.000000]\n",
            "311: [D loss: 0.040463, acc: 0.984375]  [G loss: 4.446341, acc: 0.000000]\n",
            "312: [D loss: 0.046101, acc: 0.968750]  [G loss: 5.921580, acc: 0.000000]\n",
            "313: [D loss: 0.020198, acc: 0.984375]  [G loss: 6.671877, acc: 0.000000]\n",
            "314: [D loss: 0.004999, acc: 1.000000]  [G loss: 7.517580, acc: 0.000000]\n",
            "315: [D loss: 0.004175, acc: 1.000000]  [G loss: 8.182964, acc: 0.000000]\n",
            "316: [D loss: 0.000539, acc: 1.000000]  [G loss: 8.751838, acc: 0.000000]\n",
            "317: [D loss: 0.000102, acc: 1.000000]  [G loss: 9.117474, acc: 0.000000]\n",
            "318: [D loss: 0.000388, acc: 1.000000]  [G loss: 8.908741, acc: 0.000000]\n",
            "319: [D loss: 0.000013, acc: 1.000000]  [G loss: 9.016336, acc: 0.046875]\n",
            "320: [D loss: 0.000008, acc: 1.000000]  [G loss: 7.714226, acc: 0.218750]\n",
            "321: [D loss: 0.000012, acc: 1.000000]  [G loss: 8.237007, acc: 0.296875]\n",
            "322: [D loss: 0.000008, acc: 1.000000]  [G loss: 7.248845, acc: 0.343750]\n",
            "323: [D loss: 0.000002, acc: 1.000000]  [G loss: 6.823910, acc: 0.312500]\n",
            "324: [D loss: 0.000006, acc: 1.000000]  [G loss: 5.656100, acc: 0.437500]\n",
            "325: [D loss: 0.000001, acc: 1.000000]  [G loss: 5.440356, acc: 0.515625]\n",
            "326: [D loss: 0.000003, acc: 1.000000]  [G loss: 4.704719, acc: 0.531250]\n",
            "327: [D loss: 0.000001, acc: 1.000000]  [G loss: 3.796282, acc: 0.625000]\n",
            "328: [D loss: 0.000005, acc: 1.000000]  [G loss: 2.652983, acc: 0.656250]\n",
            "329: [D loss: 0.000010, acc: 1.000000]  [G loss: 2.035421, acc: 0.703125]\n",
            "330: [D loss: 0.000264, acc: 1.000000]  [G loss: 1.312607, acc: 0.828125]\n",
            "331: [D loss: 0.000651, acc: 1.000000]  [G loss: 0.717325, acc: 0.921875]\n",
            "332: [D loss: 0.002302, acc: 1.000000]  [G loss: 0.229963, acc: 0.921875]\n",
            "333: [D loss: 0.080942, acc: 0.960938]  [G loss: 0.790151, acc: 0.906250]\n",
            "334: [D loss: 0.005926, acc: 1.000000]  [G loss: 0.448099, acc: 0.875000]\n",
            "335: [D loss: 0.034917, acc: 0.984375]  [G loss: 1.370783, acc: 0.796875]\n",
            "336: [D loss: 0.001337, acc: 1.000000]  [G loss: 0.720341, acc: 0.875000]\n",
            "337: [D loss: 0.004864, acc: 1.000000]  [G loss: 0.443646, acc: 0.875000]\n",
            "338: [D loss: 0.003240, acc: 1.000000]  [G loss: 0.882023, acc: 0.843750]\n",
            "339: [D loss: 0.000031, acc: 1.000000]  [G loss: 0.377344, acc: 0.875000]\n",
            "340: [D loss: 0.066192, acc: 0.976562]  [G loss: 1.667259, acc: 0.750000]\n",
            "341: [D loss: 0.305041, acc: 0.945312]  [G loss: 2.978573, acc: 0.750000]\n",
            "342: [D loss: 0.581653, acc: 0.914062]  [G loss: 4.741594, acc: 0.562500]\n",
            "343: [D loss: 1.298790, acc: 0.812500]  [G loss: 4.596885, acc: 0.578125]\n",
            "344: [D loss: 1.750825, acc: 0.734375]  [G loss: 6.304182, acc: 0.437500]\n",
            "345: [D loss: 1.776571, acc: 0.757812]  [G loss: 11.673256, acc: 0.078125]\n",
            "346: [D loss: 0.767817, acc: 0.851562]  [G loss: 3.134610, acc: 0.500000]\n",
            "347: [D loss: 0.522850, acc: 0.843750]  [G loss: 2.794503, acc: 0.546875]\n",
            "348: [D loss: 0.462723, acc: 0.851562]  [G loss: 7.065323, acc: 0.218750]\n",
            "349: [D loss: 0.231367, acc: 0.890625]  [G loss: 8.579599, acc: 0.031250]\n",
            "350: [D loss: 0.487872, acc: 0.820312]  [G loss: 6.546495, acc: 0.218750]\n",
            "351: [D loss: 0.273152, acc: 0.890625]  [G loss: 5.979800, acc: 0.421875]\n",
            "352: [D loss: 0.325789, acc: 0.882812]  [G loss: 5.180101, acc: 0.390625]\n",
            "353: [D loss: 0.311176, acc: 0.898438]  [G loss: 5.562165, acc: 0.390625]\n",
            "354: [D loss: 0.127729, acc: 0.968750]  [G loss: 7.970634, acc: 0.187500]\n",
            "355: [D loss: 0.165968, acc: 0.945312]  [G loss: 9.211864, acc: 0.156250]\n",
            "356: [D loss: 0.347099, acc: 0.882812]  [G loss: 8.024500, acc: 0.250000]\n",
            "357: [D loss: 0.141315, acc: 0.960938]  [G loss: 5.428380, acc: 0.406250]\n",
            "358: [D loss: 0.378031, acc: 0.929688]  [G loss: 5.309274, acc: 0.421875]\n",
            "359: [D loss: 0.167510, acc: 0.968750]  [G loss: 5.982193, acc: 0.421875]\n",
            "360: [D loss: 0.058854, acc: 0.992188]  [G loss: 5.269927, acc: 0.375000]\n",
            "361: [D loss: 0.133822, acc: 0.945312]  [G loss: 5.779008, acc: 0.281250]\n",
            "362: [D loss: 0.109324, acc: 0.960938]  [G loss: 5.560996, acc: 0.296875]\n",
            "363: [D loss: 0.267610, acc: 0.882812]  [G loss: 5.010059, acc: 0.312500]\n",
            "364: [D loss: 0.051101, acc: 0.976562]  [G loss: 3.809560, acc: 0.500000]\n",
            "365: [D loss: 0.053295, acc: 0.984375]  [G loss: 3.377050, acc: 0.703125]\n",
            "366: [D loss: 0.027066, acc: 0.984375]  [G loss: 2.546529, acc: 0.750000]\n",
            "367: [D loss: 0.054303, acc: 0.976562]  [G loss: 2.417122, acc: 0.765625]\n",
            "368: [D loss: 0.050479, acc: 0.992188]  [G loss: 2.407343, acc: 0.796875]\n",
            "369: [D loss: 0.074732, acc: 0.968750]  [G loss: 2.735245, acc: 0.765625]\n",
            "370: [D loss: 0.017483, acc: 0.992188]  [G loss: 2.335393, acc: 0.812500]\n",
            "371: [D loss: 0.019280, acc: 0.992188]  [G loss: 2.499941, acc: 0.765625]\n",
            "372: [D loss: 0.016808, acc: 0.992188]  [G loss: 2.024345, acc: 0.781250]\n",
            "373: [D loss: 0.034604, acc: 0.992188]  [G loss: 2.013699, acc: 0.765625]\n",
            "374: [D loss: 0.056023, acc: 0.976562]  [G loss: 1.892081, acc: 0.718750]\n",
            "375: [D loss: 0.017329, acc: 1.000000]  [G loss: 1.402825, acc: 0.734375]\n",
            "376: [D loss: 0.046973, acc: 0.976562]  [G loss: 0.901812, acc: 0.890625]\n",
            "377: [D loss: 0.038310, acc: 0.992188]  [G loss: 1.256944, acc: 0.812500]\n",
            "378: [D loss: 0.144757, acc: 0.953125]  [G loss: 1.279177, acc: 0.718750]\n",
            "379: [D loss: 0.018775, acc: 1.000000]  [G loss: 0.977080, acc: 0.765625]\n",
            "380: [D loss: 0.039307, acc: 0.984375]  [G loss: 0.899562, acc: 0.734375]\n",
            "381: [D loss: 0.073520, acc: 0.976562]  [G loss: 0.676883, acc: 0.671875]\n",
            "382: [D loss: 0.025536, acc: 1.000000]  [G loss: 0.588855, acc: 0.718750]\n",
            "383: [D loss: 0.021334, acc: 0.992188]  [G loss: 0.383026, acc: 0.828125]\n",
            "384: [D loss: 0.046226, acc: 0.992188]  [G loss: 0.481184, acc: 0.765625]\n",
            "385: [D loss: 0.057004, acc: 0.992188]  [G loss: 0.900793, acc: 0.687500]\n",
            "386: [D loss: 0.053406, acc: 0.984375]  [G loss: 0.907618, acc: 0.656250]\n",
            "387: [D loss: 0.049878, acc: 0.984375]  [G loss: 1.542250, acc: 0.484375]\n",
            "388: [D loss: 0.113238, acc: 0.945312]  [G loss: 1.516122, acc: 0.484375]\n",
            "389: [D loss: 0.089999, acc: 0.953125]  [G loss: 0.866164, acc: 0.625000]\n",
            "390: [D loss: 0.061850, acc: 0.984375]  [G loss: 0.462966, acc: 0.765625]\n",
            "391: [D loss: 0.061775, acc: 0.984375]  [G loss: 0.513979, acc: 0.718750]\n",
            "392: [D loss: 0.125602, acc: 0.976562]  [G loss: 0.973308, acc: 0.671875]\n",
            "393: [D loss: 0.152520, acc: 0.937500]  [G loss: 1.068122, acc: 0.593750]\n",
            "394: [D loss: 0.257035, acc: 0.929688]  [G loss: 0.812269, acc: 0.718750]\n",
            "395: [D loss: 0.102309, acc: 0.968750]  [G loss: 0.589895, acc: 0.718750]\n",
            "396: [D loss: 0.194364, acc: 0.921875]  [G loss: 2.465086, acc: 0.437500]\n",
            "397: [D loss: 0.477031, acc: 0.843750]  [G loss: 1.259456, acc: 0.640625]\n",
            "398: [D loss: 0.515516, acc: 0.828125]  [G loss: 0.318652, acc: 0.890625]\n",
            "399: [D loss: 0.793945, acc: 0.718750]  [G loss: 2.755825, acc: 0.250000]\n",
            "400: [D loss: 0.202885, acc: 0.914062]  [G loss: 5.167727, acc: 0.093750]\n",
            "401: [D loss: 0.434972, acc: 0.851562]  [G loss: 4.197935, acc: 0.078125]\n",
            "402: [D loss: 0.055772, acc: 0.976562]  [G loss: 2.384272, acc: 0.281250]\n",
            "403: [D loss: 0.058348, acc: 0.976562]  [G loss: 1.678024, acc: 0.453125]\n",
            "404: [D loss: 0.207809, acc: 0.937500]  [G loss: 2.660771, acc: 0.234375]\n",
            "405: [D loss: 0.071221, acc: 0.992188]  [G loss: 4.714815, acc: 0.062500]\n",
            "406: [D loss: 0.081021, acc: 0.960938]  [G loss: 6.006083, acc: 0.015625]\n",
            "407: [D loss: 0.191714, acc: 0.921875]  [G loss: 4.720396, acc: 0.062500]\n",
            "408: [D loss: 0.285873, acc: 0.898438]  [G loss: 1.631092, acc: 0.515625]\n",
            "409: [D loss: 0.940745, acc: 0.671875]  [G loss: 4.577083, acc: 0.015625]\n",
            "410: [D loss: 1.441284, acc: 0.656250]  [G loss: 1.024167, acc: 0.703125]\n",
            "411: [D loss: 1.411215, acc: 0.515625]  [G loss: 1.357359, acc: 0.453125]\n",
            "412: [D loss: 1.246823, acc: 0.554688]  [G loss: 1.813513, acc: 0.296875]\n",
            "413: [D loss: 1.104401, acc: 0.546875]  [G loss: 1.870348, acc: 0.125000]\n",
            "414: [D loss: 0.973680, acc: 0.601562]  [G loss: 1.193156, acc: 0.343750]\n",
            "415: [D loss: 0.570017, acc: 0.734375]  [G loss: 1.137785, acc: 0.421875]\n",
            "416: [D loss: 0.544502, acc: 0.742188]  [G loss: 1.741518, acc: 0.140625]\n",
            "417: [D loss: 0.239782, acc: 0.921875]  [G loss: 2.373765, acc: 0.031250]\n",
            "418: [D loss: 0.173366, acc: 0.937500]  [G loss: 2.681298, acc: 0.015625]\n",
            "419: [D loss: 0.125970, acc: 0.960938]  [G loss: 2.986386, acc: 0.015625]\n",
            "420: [D loss: 0.067661, acc: 0.984375]  [G loss: 2.989668, acc: 0.031250]\n",
            "421: [D loss: 0.045057, acc: 1.000000]  [G loss: 2.911485, acc: 0.046875]\n",
            "422: [D loss: 0.026619, acc: 1.000000]  [G loss: 2.710678, acc: 0.078125]\n",
            "423: [D loss: 0.048119, acc: 0.992188]  [G loss: 2.953889, acc: 0.093750]\n",
            "424: [D loss: 0.040446, acc: 0.984375]  [G loss: 2.897484, acc: 0.109375]\n",
            "425: [D loss: 0.028356, acc: 0.992188]  [G loss: 3.459327, acc: 0.000000]\n",
            "426: [D loss: 0.050990, acc: 0.984375]  [G loss: 2.688748, acc: 0.125000]\n",
            "427: [D loss: 0.084564, acc: 0.976562]  [G loss: 2.237339, acc: 0.203125]\n",
            "428: [D loss: 0.057321, acc: 0.976562]  [G loss: 1.727028, acc: 0.281250]\n",
            "429: [D loss: 0.280287, acc: 0.867188]  [G loss: 1.830904, acc: 0.312500]\n",
            "430: [D loss: 0.470405, acc: 0.851562]  [G loss: 0.633778, acc: 0.687500]\n",
            "431: [D loss: 0.973896, acc: 0.695312]  [G loss: 2.966757, acc: 0.203125]\n",
            "432: [D loss: 2.237906, acc: 0.570312]  [G loss: 0.166662, acc: 0.953125]\n",
            "433: [D loss: 1.817019, acc: 0.593750]  [G loss: 0.553374, acc: 0.765625]\n",
            "434: [D loss: 0.645642, acc: 0.710938]  [G loss: 0.907737, acc: 0.625000]\n",
            "435: [D loss: 1.111543, acc: 0.679688]  [G loss: 0.405373, acc: 0.859375]\n",
            "436: [D loss: 0.594581, acc: 0.718750]  [G loss: 0.420531, acc: 0.781250]\n",
            "437: [D loss: 0.444619, acc: 0.789062]  [G loss: 1.124155, acc: 0.546875]\n",
            "438: [D loss: 0.393243, acc: 0.828125]  [G loss: 1.304890, acc: 0.609375]\n",
            "439: [D loss: 0.450353, acc: 0.820312]  [G loss: 1.260649, acc: 0.531250]\n",
            "440: [D loss: 0.283533, acc: 0.890625]  [G loss: 1.240000, acc: 0.484375]\n",
            "441: [D loss: 0.151021, acc: 0.929688]  [G loss: 1.183229, acc: 0.578125]\n",
            "442: [D loss: 0.145225, acc: 0.937500]  [G loss: 1.290985, acc: 0.328125]\n",
            "443: [D loss: 0.077158, acc: 0.992188]  [G loss: 1.481724, acc: 0.203125]\n",
            "444: [D loss: 0.034616, acc: 1.000000]  [G loss: 1.635162, acc: 0.062500]\n",
            "445: [D loss: 0.027897, acc: 1.000000]  [G loss: 1.882725, acc: 0.000000]\n",
            "446: [D loss: 0.027531, acc: 0.984375]  [G loss: 1.989071, acc: 0.000000]\n",
            "447: [D loss: 0.017321, acc: 0.992188]  [G loss: 2.162126, acc: 0.000000]\n",
            "448: [D loss: 0.007388, acc: 1.000000]  [G loss: 2.187622, acc: 0.000000]\n",
            "449: [D loss: 0.012325, acc: 0.992188]  [G loss: 2.225253, acc: 0.000000]\n",
            "450: [D loss: 0.004589, acc: 1.000000]  [G loss: 2.209345, acc: 0.000000]\n",
            "451: [D loss: 0.032852, acc: 0.984375]  [G loss: 2.185416, acc: 0.000000]\n",
            "452: [D loss: 0.005246, acc: 1.000000]  [G loss: 2.122535, acc: 0.000000]\n",
            "453: [D loss: 0.012405, acc: 1.000000]  [G loss: 2.048503, acc: 0.000000]\n",
            "454: [D loss: 0.008172, acc: 1.000000]  [G loss: 1.910029, acc: 0.000000]\n",
            "455: [D loss: 0.010038, acc: 1.000000]  [G loss: 1.876895, acc: 0.000000]\n",
            "456: [D loss: 0.013910, acc: 1.000000]  [G loss: 1.862903, acc: 0.000000]\n",
            "457: [D loss: 0.023391, acc: 0.992188]  [G loss: 1.881351, acc: 0.000000]\n",
            "458: [D loss: 0.021729, acc: 1.000000]  [G loss: 1.873447, acc: 0.000000]\n",
            "459: [D loss: 0.025565, acc: 1.000000]  [G loss: 1.939951, acc: 0.000000]\n",
            "460: [D loss: 0.026181, acc: 1.000000]  [G loss: 1.974994, acc: 0.000000]\n",
            "461: [D loss: 0.029211, acc: 1.000000]  [G loss: 2.061539, acc: 0.000000]\n",
            "462: [D loss: 0.027806, acc: 1.000000]  [G loss: 2.259445, acc: 0.000000]\n",
            "463: [D loss: 0.056503, acc: 0.992188]  [G loss: 2.351101, acc: 0.000000]\n",
            "464: [D loss: 0.027940, acc: 1.000000]  [G loss: 2.610955, acc: 0.000000]\n",
            "465: [D loss: 0.054262, acc: 0.992188]  [G loss: 2.774224, acc: 0.000000]\n",
            "466: [D loss: 0.020302, acc: 1.000000]  [G loss: 3.038445, acc: 0.000000]\n",
            "467: [D loss: 0.018814, acc: 1.000000]  [G loss: 3.359386, acc: 0.000000]\n",
            "468: [D loss: 0.013906, acc: 1.000000]  [G loss: 3.565489, acc: 0.000000]\n",
            "469: [D loss: 0.011638, acc: 1.000000]  [G loss: 3.893016, acc: 0.000000]\n",
            "470: [D loss: 0.017785, acc: 0.992188]  [G loss: 4.153572, acc: 0.000000]\n",
            "471: [D loss: 0.008500, acc: 1.000000]  [G loss: 4.388368, acc: 0.000000]\n",
            "472: [D loss: 0.007904, acc: 1.000000]  [G loss: 4.640396, acc: 0.000000]\n",
            "473: [D loss: 0.005113, acc: 1.000000]  [G loss: 4.951412, acc: 0.000000]\n",
            "474: [D loss: 0.003714, acc: 1.000000]  [G loss: 5.215980, acc: 0.000000]\n",
            "475: [D loss: 0.023881, acc: 0.984375]  [G loss: 5.283642, acc: 0.000000]\n",
            "476: [D loss: 0.002512, acc: 1.000000]  [G loss: 5.440738, acc: 0.000000]\n",
            "477: [D loss: 0.002683, acc: 1.000000]  [G loss: 5.658633, acc: 0.000000]\n",
            "478: [D loss: 0.042044, acc: 0.992188]  [G loss: 5.739554, acc: 0.000000]\n",
            "479: [D loss: 0.008170, acc: 0.992188]  [G loss: 5.752946, acc: 0.000000]\n",
            "480: [D loss: 0.001506, acc: 1.000000]  [G loss: 5.796318, acc: 0.000000]\n",
            "481: [D loss: 0.001473, acc: 1.000000]  [G loss: 5.993945, acc: 0.000000]\n",
            "482: [D loss: 0.001353, acc: 1.000000]  [G loss: 6.005489, acc: 0.000000]\n",
            "483: [D loss: 0.001229, acc: 1.000000]  [G loss: 6.050341, acc: 0.000000]\n",
            "484: [D loss: 0.001192, acc: 1.000000]  [G loss: 6.139360, acc: 0.000000]\n",
            "485: [D loss: 0.000996, acc: 1.000000]  [G loss: 6.209891, acc: 0.000000]\n",
            "486: [D loss: 0.001000, acc: 1.000000]  [G loss: 6.317388, acc: 0.000000]\n",
            "487: [D loss: 0.001006, acc: 1.000000]  [G loss: 6.296472, acc: 0.000000]\n",
            "488: [D loss: 0.000849, acc: 1.000000]  [G loss: 6.441632, acc: 0.000000]\n",
            "489: [D loss: 0.000870, acc: 1.000000]  [G loss: 6.532463, acc: 0.000000]\n",
            "490: [D loss: 0.000791, acc: 1.000000]  [G loss: 6.626538, acc: 0.000000]\n",
            "491: [D loss: 0.000799, acc: 1.000000]  [G loss: 6.699134, acc: 0.000000]\n",
            "492: [D loss: 0.000698, acc: 1.000000]  [G loss: 6.729649, acc: 0.000000]\n",
            "493: [D loss: 0.000595, acc: 1.000000]  [G loss: 6.761271, acc: 0.000000]\n",
            "494: [D loss: 0.000622, acc: 1.000000]  [G loss: 6.866934, acc: 0.000000]\n",
            "495: [D loss: 0.000596, acc: 1.000000]  [G loss: 6.888355, acc: 0.000000]\n",
            "496: [D loss: 0.000551, acc: 1.000000]  [G loss: 6.952681, acc: 0.000000]\n",
            "497: [D loss: 0.018415, acc: 0.992188]  [G loss: 6.915945, acc: 0.000000]\n",
            "498: [D loss: 0.000564, acc: 1.000000]  [G loss: 6.940077, acc: 0.000000]\n",
            "499: [D loss: 0.007614, acc: 0.992188]  [G loss: 6.959498, acc: 0.000000]\n",
            "500: [D loss: 0.000498, acc: 1.000000]  [G loss: 6.948388, acc: 0.000000]\n",
            "501: [D loss: 0.000527, acc: 1.000000]  [G loss: 6.859159, acc: 0.000000]\n",
            "502: [D loss: 0.000571, acc: 1.000000]  [G loss: 6.870271, acc: 0.000000]\n",
            "503: [D loss: 0.000860, acc: 1.000000]  [G loss: 6.940068, acc: 0.000000]\n",
            "504: [D loss: 0.000601, acc: 1.000000]  [G loss: 6.842133, acc: 0.000000]\n",
            "505: [D loss: 0.000523, acc: 1.000000]  [G loss: 6.833602, acc: 0.000000]\n",
            "506: [D loss: 0.039559, acc: 0.992188]  [G loss: 6.843742, acc: 0.000000]\n",
            "507: [D loss: 0.000596, acc: 1.000000]  [G loss: 6.752139, acc: 0.000000]\n",
            "508: [D loss: 0.000633, acc: 1.000000]  [G loss: 6.739163, acc: 0.000000]\n",
            "509: [D loss: 0.000583, acc: 1.000000]  [G loss: 6.712160, acc: 0.000000]\n",
            "510: [D loss: 0.000639, acc: 1.000000]  [G loss: 6.626358, acc: 0.000000]\n",
            "511: [D loss: 0.000633, acc: 1.000000]  [G loss: 6.679959, acc: 0.000000]\n",
            "512: [D loss: 0.000636, acc: 1.000000]  [G loss: 6.729773, acc: 0.000000]\n",
            "513: [D loss: 0.000695, acc: 1.000000]  [G loss: 6.586287, acc: 0.000000]\n",
            "514: [D loss: 0.000628, acc: 1.000000]  [G loss: 6.768560, acc: 0.000000]\n",
            "515: [D loss: 0.000666, acc: 1.000000]  [G loss: 6.811980, acc: 0.000000]\n",
            "516: [D loss: 0.000788, acc: 1.000000]  [G loss: 6.726419, acc: 0.000000]\n",
            "517: [D loss: 0.000704, acc: 1.000000]  [G loss: 6.763015, acc: 0.000000]\n",
            "518: [D loss: 0.007414, acc: 0.992188]  [G loss: 6.681980, acc: 0.000000]\n",
            "519: [D loss: 0.000612, acc: 1.000000]  [G loss: 6.792204, acc: 0.000000]\n",
            "520: [D loss: 0.000559, acc: 1.000000]  [G loss: 6.756340, acc: 0.000000]\n",
            "521: [D loss: 0.000598, acc: 1.000000]  [G loss: 6.753132, acc: 0.000000]\n",
            "522: [D loss: 0.000663, acc: 1.000000]  [G loss: 6.725397, acc: 0.000000]\n",
            "523: [D loss: 0.000656, acc: 1.000000]  [G loss: 6.680407, acc: 0.000000]\n",
            "524: [D loss: 0.000635, acc: 1.000000]  [G loss: 6.738796, acc: 0.000000]\n",
            "525: [D loss: 0.000574, acc: 1.000000]  [G loss: 6.857945, acc: 0.000000]\n",
            "526: [D loss: 0.000632, acc: 1.000000]  [G loss: 6.795068, acc: 0.000000]\n",
            "527: [D loss: 0.000578, acc: 1.000000]  [G loss: 6.747485, acc: 0.000000]\n",
            "528: [D loss: 0.000650, acc: 1.000000]  [G loss: 6.816775, acc: 0.000000]\n",
            "529: [D loss: 0.000541, acc: 1.000000]  [G loss: 6.800358, acc: 0.000000]\n",
            "530: [D loss: 0.000580, acc: 1.000000]  [G loss: 6.946027, acc: 0.000000]\n",
            "531: [D loss: 0.000568, acc: 1.000000]  [G loss: 6.930318, acc: 0.000000]\n",
            "532: [D loss: 0.000551, acc: 1.000000]  [G loss: 6.962275, acc: 0.000000]\n",
            "533: [D loss: 0.000499, acc: 1.000000]  [G loss: 6.942951, acc: 0.000000]\n",
            "534: [D loss: 0.000469, acc: 1.000000]  [G loss: 6.963213, acc: 0.000000]\n",
            "535: [D loss: 0.000518, acc: 1.000000]  [G loss: 7.022789, acc: 0.000000]\n",
            "536: [D loss: 0.000469, acc: 1.000000]  [G loss: 7.002514, acc: 0.000000]\n",
            "537: [D loss: 0.000509, acc: 1.000000]  [G loss: 7.024719, acc: 0.000000]\n",
            "538: [D loss: 0.000477, acc: 1.000000]  [G loss: 7.192124, acc: 0.000000]\n",
            "539: [D loss: 0.000405, acc: 1.000000]  [G loss: 7.155953, acc: 0.000000]\n",
            "540: [D loss: 0.000416, acc: 1.000000]  [G loss: 7.098102, acc: 0.000000]\n",
            "541: [D loss: 0.000387, acc: 1.000000]  [G loss: 7.270098, acc: 0.000000]\n",
            "542: [D loss: 0.000380, acc: 1.000000]  [G loss: 7.258755, acc: 0.000000]\n",
            "543: [D loss: 0.001246, acc: 1.000000]  [G loss: 7.232495, acc: 0.000000]\n",
            "544: [D loss: 0.000356, acc: 1.000000]  [G loss: 7.365043, acc: 0.000000]\n",
            "545: [D loss: 0.000353, acc: 1.000000]  [G loss: 7.338906, acc: 0.000000]\n",
            "546: [D loss: 0.000350, acc: 1.000000]  [G loss: 7.355437, acc: 0.000000]\n",
            "547: [D loss: 0.000348, acc: 1.000000]  [G loss: 7.311436, acc: 0.000000]\n",
            "548: [D loss: 0.000303, acc: 1.000000]  [G loss: 7.372338, acc: 0.000000]\n",
            "549: [D loss: 0.000365, acc: 1.000000]  [G loss: 7.422115, acc: 0.000000]\n",
            "550: [D loss: 0.000277, acc: 1.000000]  [G loss: 7.455011, acc: 0.000000]\n",
            "551: [D loss: 0.000303, acc: 1.000000]  [G loss: 7.512881, acc: 0.000000]\n",
            "552: [D loss: 0.000330, acc: 1.000000]  [G loss: 7.545325, acc: 0.000000]\n",
            "553: [D loss: 0.000311, acc: 1.000000]  [G loss: 7.539462, acc: 0.000000]\n",
            "554: [D loss: 0.000254, acc: 1.000000]  [G loss: 7.522889, acc: 0.000000]\n",
            "555: [D loss: 0.000301, acc: 1.000000]  [G loss: 7.537557, acc: 0.000000]\n",
            "556: [D loss: 0.000288, acc: 1.000000]  [G loss: 7.630189, acc: 0.000000]\n",
            "557: [D loss: 0.000279, acc: 1.000000]  [G loss: 7.584037, acc: 0.000000]\n",
            "558: [D loss: 0.000284, acc: 1.000000]  [G loss: 7.611702, acc: 0.000000]\n",
            "559: [D loss: 0.000454, acc: 1.000000]  [G loss: 7.678335, acc: 0.000000]\n",
            "560: [D loss: 0.000254, acc: 1.000000]  [G loss: 7.697351, acc: 0.000000]\n",
            "561: [D loss: 0.000264, acc: 1.000000]  [G loss: 7.630002, acc: 0.000000]\n",
            "562: [D loss: 0.000245, acc: 1.000000]  [G loss: 7.682128, acc: 0.000000]\n",
            "563: [D loss: 0.000243, acc: 1.000000]  [G loss: 7.752320, acc: 0.000000]\n",
            "564: [D loss: 0.000252, acc: 1.000000]  [G loss: 7.612052, acc: 0.000000]\n",
            "565: [D loss: 0.000213, acc: 1.000000]  [G loss: 7.699993, acc: 0.000000]\n",
            "566: [D loss: 0.000213, acc: 1.000000]  [G loss: 7.762780, acc: 0.000000]\n",
            "567: [D loss: 0.000256, acc: 1.000000]  [G loss: 7.744349, acc: 0.000000]\n",
            "568: [D loss: 0.000219, acc: 1.000000]  [G loss: 7.636585, acc: 0.000000]\n",
            "569: [D loss: 0.000217, acc: 1.000000]  [G loss: 7.783370, acc: 0.000000]\n",
            "570: [D loss: 0.000220, acc: 1.000000]  [G loss: 7.893559, acc: 0.000000]\n",
            "571: [D loss: 0.002014, acc: 1.000000]  [G loss: 7.799625, acc: 0.000000]\n",
            "572: [D loss: 0.000205, acc: 1.000000]  [G loss: 7.736850, acc: 0.000000]\n",
            "573: [D loss: 0.000208, acc: 1.000000]  [G loss: 7.740779, acc: 0.000000]\n",
            "574: [D loss: 0.000179, acc: 1.000000]  [G loss: 7.751288, acc: 0.000000]\n",
            "575: [D loss: 0.000210, acc: 1.000000]  [G loss: 7.731549, acc: 0.000000]\n",
            "576: [D loss: 0.000185, acc: 1.000000]  [G loss: 7.691100, acc: 0.000000]\n",
            "577: [D loss: 0.000187, acc: 1.000000]  [G loss: 7.881796, acc: 0.000000]\n",
            "578: [D loss: 0.000188, acc: 1.000000]  [G loss: 7.698227, acc: 0.000000]\n",
            "579: [D loss: 0.000172, acc: 1.000000]  [G loss: 7.767355, acc: 0.000000]\n",
            "580: [D loss: 0.000222, acc: 1.000000]  [G loss: 7.574670, acc: 0.000000]\n",
            "581: [D loss: 0.000202, acc: 1.000000]  [G loss: 7.707776, acc: 0.000000]\n",
            "582: [D loss: 0.000187, acc: 1.000000]  [G loss: 7.559387, acc: 0.000000]\n",
            "583: [D loss: 0.000209, acc: 1.000000]  [G loss: 7.434031, acc: 0.000000]\n",
            "584: [D loss: 0.000184, acc: 1.000000]  [G loss: 7.593149, acc: 0.000000]\n",
            "585: [D loss: 0.000188, acc: 1.000000]  [G loss: 7.371327, acc: 0.000000]\n",
            "586: [D loss: 0.000193, acc: 1.000000]  [G loss: 7.046867, acc: 0.000000]\n",
            "587: [D loss: 0.000178, acc: 1.000000]  [G loss: 6.967450, acc: 0.000000]\n",
            "588: [D loss: 0.000184, acc: 1.000000]  [G loss: 6.885482, acc: 0.000000]\n",
            "589: [D loss: 0.000172, acc: 1.000000]  [G loss: 6.704610, acc: 0.031250]\n",
            "590: [D loss: 0.000191, acc: 1.000000]  [G loss: 6.441464, acc: 0.046875]\n",
            "591: [D loss: 0.000187, acc: 1.000000]  [G loss: 5.837657, acc: 0.062500]\n",
            "592: [D loss: 0.000158, acc: 1.000000]  [G loss: 5.172621, acc: 0.156250]\n",
            "593: [D loss: 0.000164, acc: 1.000000]  [G loss: 5.042851, acc: 0.171875]\n",
            "594: [D loss: 0.000179, acc: 1.000000]  [G loss: 3.939515, acc: 0.390625]\n",
            "595: [D loss: 0.000183, acc: 1.000000]  [G loss: 4.205996, acc: 0.375000]\n",
            "596: [D loss: 0.000176, acc: 1.000000]  [G loss: 3.457363, acc: 0.406250]\n",
            "597: [D loss: 0.000175, acc: 1.000000]  [G loss: 3.336622, acc: 0.484375]\n",
            "598: [D loss: 0.000177, acc: 1.000000]  [G loss: 2.804812, acc: 0.546875]\n",
            "599: [D loss: 0.000171, acc: 1.000000]  [G loss: 2.396383, acc: 0.593750]\n",
            "600: [D loss: 0.000218, acc: 1.000000]  [G loss: 2.212146, acc: 0.593750]\n",
            "601: [D loss: 0.000227, acc: 1.000000]  [G loss: 2.256047, acc: 0.656250]\n",
            "602: [D loss: 0.000228, acc: 1.000000]  [G loss: 1.819586, acc: 0.750000]\n",
            "603: [D loss: 0.000218, acc: 1.000000]  [G loss: 1.679812, acc: 0.750000]\n",
            "604: [D loss: 0.000238, acc: 1.000000]  [G loss: 1.385255, acc: 0.750000]\n",
            "605: [D loss: 0.000305, acc: 1.000000]  [G loss: 1.320179, acc: 0.796875]\n",
            "606: [D loss: 0.000444, acc: 1.000000]  [G loss: 1.192896, acc: 0.781250]\n",
            "607: [D loss: 0.000396, acc: 1.000000]  [G loss: 1.193250, acc: 0.796875]\n",
            "608: [D loss: 0.000644, acc: 1.000000]  [G loss: 1.065603, acc: 0.812500]\n",
            "609: [D loss: 0.003199, acc: 1.000000]  [G loss: 1.031713, acc: 0.843750]\n",
            "610: [D loss: 0.000930, acc: 1.000000]  [G loss: 0.714679, acc: 0.890625]\n",
            "611: [D loss: 0.001736, acc: 1.000000]  [G loss: 0.734615, acc: 0.890625]\n",
            "612: [D loss: 0.004573, acc: 1.000000]  [G loss: 0.655791, acc: 0.890625]\n",
            "613: [D loss: 0.002648, acc: 1.000000]  [G loss: 0.508941, acc: 0.921875]\n",
            "614: [D loss: 0.009647, acc: 1.000000]  [G loss: 0.724182, acc: 0.890625]\n",
            "615: [D loss: 0.034900, acc: 0.984375]  [G loss: 0.551891, acc: 0.906250]\n",
            "616: [D loss: 0.046877, acc: 0.992188]  [G loss: 0.518108, acc: 0.890625]\n",
            "617: [D loss: 0.092284, acc: 0.968750]  [G loss: 0.483108, acc: 0.921875]\n",
            "618: [D loss: 0.030956, acc: 0.984375]  [G loss: 0.637090, acc: 0.890625]\n",
            "619: [D loss: 0.019603, acc: 0.992188]  [G loss: 0.782346, acc: 0.890625]\n",
            "620: [D loss: 0.009081, acc: 1.000000]  [G loss: 0.617771, acc: 0.890625]\n",
            "621: [D loss: 0.013776, acc: 0.992188]  [G loss: 0.815272, acc: 0.890625]\n",
            "622: [D loss: 0.011252, acc: 0.992188]  [G loss: 0.853855, acc: 0.828125]\n",
            "623: [D loss: 0.016774, acc: 0.992188]  [G loss: 0.973847, acc: 0.828125]\n",
            "624: [D loss: 0.118293, acc: 0.960938]  [G loss: 0.638666, acc: 0.906250]\n",
            "625: [D loss: 0.021763, acc: 0.992188]  [G loss: 0.914889, acc: 0.843750]\n",
            "626: [D loss: 0.075450, acc: 0.968750]  [G loss: 0.817571, acc: 0.890625]\n",
            "627: [D loss: 0.066786, acc: 0.976562]  [G loss: 0.632579, acc: 0.890625]\n",
            "628: [D loss: 0.043922, acc: 0.984375]  [G loss: 0.476703, acc: 0.890625]\n",
            "629: [D loss: 0.086918, acc: 0.968750]  [G loss: 0.537760, acc: 0.875000]\n",
            "630: [D loss: 0.145171, acc: 0.937500]  [G loss: 0.672615, acc: 0.828125]\n",
            "631: [D loss: 0.002668, acc: 1.000000]  [G loss: 1.072446, acc: 0.812500]\n",
            "632: [D loss: 0.012981, acc: 0.992188]  [G loss: 1.238182, acc: 0.765625]\n",
            "633: [D loss: 0.093073, acc: 0.960938]  [G loss: 2.253844, acc: 0.609375]\n",
            "634: [D loss: 0.034485, acc: 0.976562]  [G loss: 3.000318, acc: 0.375000]\n",
            "635: [D loss: 0.021415, acc: 0.992188]  [G loss: 2.832484, acc: 0.343750]\n",
            "636: [D loss: 0.003316, acc: 1.000000]  [G loss: 1.866443, acc: 0.515625]\n",
            "637: [D loss: 0.000291, acc: 1.000000]  [G loss: 0.859857, acc: 0.750000]\n",
            "638: [D loss: 0.000439, acc: 1.000000]  [G loss: 0.756445, acc: 0.765625]\n",
            "639: [D loss: 0.029475, acc: 0.984375]  [G loss: 0.124190, acc: 0.937500]\n",
            "640: [D loss: 0.005403, acc: 1.000000]  [G loss: 0.071424, acc: 0.984375]\n",
            "641: [D loss: 0.007811, acc: 1.000000]  [G loss: 0.003247, acc: 1.000000]\n",
            "642: [D loss: 0.008935, acc: 1.000000]  [G loss: 0.023474, acc: 0.984375]\n",
            "643: [D loss: 0.035268, acc: 0.984375]  [G loss: 0.004385, acc: 1.000000]\n",
            "644: [D loss: 0.020410, acc: 0.992188]  [G loss: 0.125675, acc: 0.937500]\n",
            "645: [D loss: 0.007237, acc: 1.000000]  [G loss: 0.148039, acc: 0.937500]\n",
            "646: [D loss: 0.002162, acc: 1.000000]  [G loss: 0.269228, acc: 0.921875]\n",
            "647: [D loss: 0.001528, acc: 1.000000]  [G loss: 0.477640, acc: 0.859375]\n",
            "648: [D loss: 0.000437, acc: 1.000000]  [G loss: 0.560321, acc: 0.828125]\n",
            "649: [D loss: 0.001171, acc: 1.000000]  [G loss: 0.557104, acc: 0.859375]\n",
            "650: [D loss: 0.000321, acc: 1.000000]  [G loss: 0.276334, acc: 0.890625]\n",
            "651: [D loss: 0.000409, acc: 1.000000]  [G loss: 0.051542, acc: 0.968750]\n",
            "652: [D loss: 0.000208, acc: 1.000000]  [G loss: 0.127925, acc: 0.953125]\n",
            "653: [D loss: 0.013770, acc: 0.992188]  [G loss: 0.008016, acc: 1.000000]\n",
            "654: [D loss: 0.009500, acc: 0.992188]  [G loss: 0.009972, acc: 1.000000]\n",
            "655: [D loss: 0.027289, acc: 0.984375]  [G loss: 0.000782, acc: 1.000000]\n",
            "656: [D loss: 0.011684, acc: 1.000000]  [G loss: 0.000012, acc: 1.000000]\n",
            "657: [D loss: 0.042604, acc: 0.984375]  [G loss: 0.066836, acc: 0.968750]\n",
            "658: [D loss: 0.006551, acc: 1.000000]  [G loss: 0.078941, acc: 0.984375]\n",
            "659: [D loss: 0.011801, acc: 1.000000]  [G loss: 0.022053, acc: 1.000000]\n",
            "660: [D loss: 0.013171, acc: 0.992188]  [G loss: 0.331961, acc: 0.968750]\n",
            "661: [D loss: 0.063834, acc: 0.984375]  [G loss: 0.493183, acc: 0.921875]\n",
            "662: [D loss: 0.082853, acc: 0.984375]  [G loss: 0.117651, acc: 0.984375]\n",
            "663: [D loss: 0.010877, acc: 1.000000]  [G loss: 0.078409, acc: 0.953125]\n",
            "664: [D loss: 0.021449, acc: 0.992188]  [G loss: 0.035447, acc: 0.984375]\n",
            "665: [D loss: 0.046871, acc: 0.984375]  [G loss: 0.071515, acc: 0.937500]\n",
            "666: [D loss: 0.033596, acc: 0.992188]  [G loss: 0.132324, acc: 0.953125]\n",
            "667: [D loss: 0.006313, acc: 1.000000]  [G loss: 0.015004, acc: 0.984375]\n",
            "668: [D loss: 0.003493, acc: 1.000000]  [G loss: 0.095945, acc: 0.937500]\n",
            "669: [D loss: 0.006485, acc: 1.000000]  [G loss: 0.259970, acc: 0.906250]\n",
            "670: [D loss: 0.021636, acc: 0.992188]  [G loss: 0.450337, acc: 0.859375]\n",
            "671: [D loss: 0.005720, acc: 1.000000]  [G loss: 0.203620, acc: 0.906250]\n",
            "672: [D loss: 0.011093, acc: 1.000000]  [G loss: 0.129132, acc: 0.937500]\n",
            "673: [D loss: 0.069683, acc: 0.984375]  [G loss: 0.151548, acc: 0.921875]\n",
            "674: [D loss: 0.052403, acc: 0.976562]  [G loss: 0.341035, acc: 0.859375]\n",
            "675: [D loss: 0.073737, acc: 0.968750]  [G loss: 1.235708, acc: 0.578125]\n",
            "676: [D loss: 0.075780, acc: 0.984375]  [G loss: 2.859939, acc: 0.296875]\n",
            "677: [D loss: 0.045543, acc: 0.984375]  [G loss: 4.748083, acc: 0.062500]\n",
            "678: [D loss: 0.189852, acc: 0.929688]  [G loss: 5.857149, acc: 0.046875]\n",
            "679: [D loss: 0.495396, acc: 0.835938]  [G loss: 4.313758, acc: 0.265625]\n",
            "680: [D loss: 0.362747, acc: 0.851562]  [G loss: 7.916350, acc: 0.000000]\n",
            "681: [D loss: 0.105549, acc: 0.945312]  [G loss: 11.479916, acc: 0.000000]\n",
            "682: [D loss: 0.013938, acc: 0.992188]  [G loss: 14.215191, acc: 0.000000]\n",
            "683: [D loss: 0.000991, acc: 1.000000]  [G loss: 15.196147, acc: 0.000000]\n",
            "684: [D loss: 0.000189, acc: 1.000000]  [G loss: 15.469431, acc: 0.000000]\n",
            "685: [D loss: 0.000136, acc: 1.000000]  [G loss: 15.794546, acc: 0.000000]\n",
            "686: [D loss: 0.000604, acc: 1.000000]  [G loss: 15.764241, acc: 0.000000]\n",
            "687: [D loss: 0.013068, acc: 0.992188]  [G loss: 15.460883, acc: 0.000000]\n",
            "688: [D loss: 0.000071, acc: 1.000000]  [G loss: 15.427758, acc: 0.000000]\n",
            "689: [D loss: 0.000147, acc: 1.000000]  [G loss: 15.255140, acc: 0.000000]\n",
            "690: [D loss: 0.002952, acc: 1.000000]  [G loss: 14.921047, acc: 0.000000]\n",
            "691: [D loss: 0.001718, acc: 1.000000]  [G loss: 14.941405, acc: 0.000000]\n",
            "692: [D loss: 0.054759, acc: 0.960938]  [G loss: 14.942200, acc: 0.000000]\n",
            "693: [D loss: 0.014066, acc: 0.992188]  [G loss: 14.954674, acc: 0.000000]\n",
            "694: [D loss: 0.002532, acc: 1.000000]  [G loss: 15.475049, acc: 0.000000]\n",
            "695: [D loss: 0.004272, acc: 1.000000]  [G loss: 15.897303, acc: 0.000000]\n",
            "696: [D loss: 0.011856, acc: 0.992188]  [G loss: 15.808989, acc: 0.000000]\n",
            "697: [D loss: 0.075904, acc: 0.984375]  [G loss: 15.791297, acc: 0.000000]\n",
            "698: [D loss: 0.009630, acc: 1.000000]  [G loss: 15.527727, acc: 0.000000]\n",
            "699: [D loss: 0.011253, acc: 0.992188]  [G loss: 15.233019, acc: 0.000000]\n",
            "700: [D loss: 0.067792, acc: 0.976562]  [G loss: 14.630205, acc: 0.015625]\n",
            "701: [D loss: 0.037244, acc: 0.984375]  [G loss: 15.400196, acc: 0.015625]\n",
            "702: [D loss: 0.069769, acc: 0.992188]  [G loss: 15.634583, acc: 0.000000]\n",
            "703: [D loss: 0.126965, acc: 0.968750]  [G loss: 14.006380, acc: 0.000000]\n",
            "704: [D loss: 0.118373, acc: 0.968750]  [G loss: 11.405809, acc: 0.031250]\n",
            "705: [D loss: 0.245755, acc: 0.945312]  [G loss: 10.657414, acc: 0.062500]\n",
            "706: [D loss: 0.521821, acc: 0.882812]  [G loss: 11.791821, acc: 0.046875]\n",
            "707: [D loss: 0.363845, acc: 0.898438]  [G loss: 9.590551, acc: 0.140625]\n",
            "708: [D loss: 0.253428, acc: 0.937500]  [G loss: 7.014720, acc: 0.343750]\n",
            "709: [D loss: 0.274840, acc: 0.921875]  [G loss: 4.398394, acc: 0.531250]\n",
            "710: [D loss: 0.172870, acc: 0.968750]  [G loss: 2.903220, acc: 0.687500]\n",
            "711: [D loss: 0.084453, acc: 0.968750]  [G loss: 2.387723, acc: 0.703125]\n",
            "712: [D loss: 0.002775, acc: 1.000000]  [G loss: 1.152621, acc: 0.812500]\n",
            "713: [D loss: 0.023613, acc: 0.984375]  [G loss: 0.817817, acc: 0.796875]\n",
            "714: [D loss: 0.025083, acc: 0.984375]  [G loss: 0.494043, acc: 0.890625]\n",
            "715: [D loss: 0.044849, acc: 0.984375]  [G loss: 0.839982, acc: 0.812500]\n",
            "716: [D loss: 0.003674, acc: 1.000000]  [G loss: 1.505557, acc: 0.609375]\n",
            "717: [D loss: 0.000864, acc: 1.000000]  [G loss: 2.050494, acc: 0.562500]\n",
            "718: [D loss: 0.162427, acc: 0.960938]  [G loss: 1.335624, acc: 0.656250]\n",
            "719: [D loss: 0.012548, acc: 0.992188]  [G loss: 0.961176, acc: 0.765625]\n",
            "720: [D loss: 0.073832, acc: 0.968750]  [G loss: 0.398395, acc: 0.796875]\n",
            "721: [D loss: 0.082399, acc: 0.960938]  [G loss: 1.206667, acc: 0.703125]\n",
            "722: [D loss: 0.034430, acc: 0.992188]  [G loss: 1.715424, acc: 0.593750]\n",
            "723: [D loss: 0.462683, acc: 0.921875]  [G loss: 0.399571, acc: 0.875000]\n",
            "724: [D loss: 0.167880, acc: 0.929688]  [G loss: 0.241986, acc: 0.906250]\n",
            "725: [D loss: 0.101633, acc: 0.984375]  [G loss: 0.248928, acc: 0.859375]\n",
            "726: [D loss: 0.051844, acc: 0.984375]  [G loss: 0.569549, acc: 0.828125]\n",
            "727: [D loss: 0.101052, acc: 0.984375]  [G loss: 1.108225, acc: 0.750000]\n",
            "728: [D loss: 0.082011, acc: 0.968750]  [G loss: 0.459498, acc: 0.812500]\n",
            "729: [D loss: 0.096638, acc: 0.968750]  [G loss: 0.701380, acc: 0.828125]\n",
            "730: [D loss: 0.061987, acc: 0.984375]  [G loss: 1.258632, acc: 0.671875]\n",
            "731: [D loss: 0.081936, acc: 0.968750]  [G loss: 1.874776, acc: 0.578125]\n",
            "732: [D loss: 0.141774, acc: 0.953125]  [G loss: 1.127639, acc: 0.687500]\n",
            "733: [D loss: 0.111349, acc: 0.953125]  [G loss: 0.467101, acc: 0.843750]\n",
            "734: [D loss: 0.340821, acc: 0.867188]  [G loss: 1.045887, acc: 0.687500]\n",
            "735: [D loss: 0.049537, acc: 0.984375]  [G loss: 2.558750, acc: 0.484375]\n",
            "736: [D loss: 0.170721, acc: 0.953125]  [G loss: 2.694649, acc: 0.531250]\n",
            "737: [D loss: 0.230569, acc: 0.929688]  [G loss: 0.527228, acc: 0.828125]\n",
            "738: [D loss: 0.217069, acc: 0.906250]  [G loss: 0.350657, acc: 0.812500]\n",
            "739: [D loss: 0.292555, acc: 0.921875]  [G loss: 2.566648, acc: 0.375000]\n",
            "740: [D loss: 0.231772, acc: 0.929688]  [G loss: 2.532465, acc: 0.421875]\n",
            "741: [D loss: 0.296981, acc: 0.890625]  [G loss: 0.599580, acc: 0.781250]\n",
            "742: [D loss: 0.216441, acc: 0.898438]  [G loss: 0.037700, acc: 1.000000]\n",
            "743: [D loss: 0.783825, acc: 0.703125]  [G loss: 2.541069, acc: 0.390625]\n",
            "744: [D loss: 0.206348, acc: 0.937500]  [G loss: 4.274379, acc: 0.203125]\n",
            "745: [D loss: 0.595145, acc: 0.828125]  [G loss: 2.566265, acc: 0.468750]\n",
            "746: [D loss: 0.161817, acc: 0.921875]  [G loss: 0.952933, acc: 0.703125]\n",
            "747: [D loss: 0.130245, acc: 0.945312]  [G loss: 0.338470, acc: 0.859375]\n",
            "748: [D loss: 0.419813, acc: 0.812500]  [G loss: 0.713409, acc: 0.718750]\n",
            "749: [D loss: 0.184661, acc: 0.914062]  [G loss: 2.633882, acc: 0.234375]\n",
            "750: [D loss: 0.147538, acc: 0.968750]  [G loss: 4.074996, acc: 0.109375]\n",
            "751: [D loss: 0.296881, acc: 0.875000]  [G loss: 4.451818, acc: 0.078125]\n",
            "752: [D loss: 0.240111, acc: 0.898438]  [G loss: 2.879727, acc: 0.140625]\n",
            "753: [D loss: 0.107291, acc: 0.968750]  [G loss: 1.928472, acc: 0.296875]\n",
            "754: [D loss: 0.079338, acc: 0.992188]  [G loss: 1.033816, acc: 0.562500]\n",
            "755: [D loss: 0.163433, acc: 0.914062]  [G loss: 1.318020, acc: 0.437500]\n",
            "756: [D loss: 0.260878, acc: 0.906250]  [G loss: 1.915055, acc: 0.281250]\n",
            "757: [D loss: 0.086302, acc: 0.984375]  [G loss: 3.162170, acc: 0.171875]\n",
            "758: [D loss: 0.152528, acc: 0.953125]  [G loss: 4.218204, acc: 0.093750]\n",
            "759: [D loss: 0.258702, acc: 0.898438]  [G loss: 5.019108, acc: 0.015625]\n",
            "760: [D loss: 0.251674, acc: 0.945312]  [G loss: 4.565264, acc: 0.109375]\n",
            "761: [D loss: 0.293191, acc: 0.882812]  [G loss: 3.953793, acc: 0.031250]\n",
            "762: [D loss: 0.249005, acc: 0.898438]  [G loss: 3.169126, acc: 0.078125]\n",
            "763: [D loss: 0.169844, acc: 0.945312]  [G loss: 2.374037, acc: 0.375000]\n",
            "764: [D loss: 0.366023, acc: 0.843750]  [G loss: 3.274970, acc: 0.093750]\n",
            "765: [D loss: 0.243858, acc: 0.906250]  [G loss: 3.249151, acc: 0.078125]\n",
            "766: [D loss: 0.212744, acc: 0.937500]  [G loss: 3.761969, acc: 0.046875]\n",
            "767: [D loss: 0.176719, acc: 0.921875]  [G loss: 4.513103, acc: 0.015625]\n",
            "768: [D loss: 0.198253, acc: 0.929688]  [G loss: 4.248041, acc: 0.000000]\n",
            "769: [D loss: 0.138644, acc: 0.953125]  [G loss: 3.878630, acc: 0.031250]\n",
            "770: [D loss: 0.208187, acc: 0.914062]  [G loss: 3.830977, acc: 0.000000]\n",
            "771: [D loss: 0.157821, acc: 0.921875]  [G loss: 2.838507, acc: 0.125000]\n",
            "772: [D loss: 0.136877, acc: 0.968750]  [G loss: 2.424330, acc: 0.125000]\n",
            "773: [D loss: 0.136678, acc: 0.953125]  [G loss: 2.271688, acc: 0.171875]\n",
            "774: [D loss: 0.122124, acc: 0.960938]  [G loss: 2.862307, acc: 0.125000]\n",
            "775: [D loss: 0.115652, acc: 0.968750]  [G loss: 2.568512, acc: 0.250000]\n",
            "776: [D loss: 0.097679, acc: 0.984375]  [G loss: 1.977876, acc: 0.437500]\n",
            "777: [D loss: 0.170293, acc: 0.921875]  [G loss: 1.613691, acc: 0.515625]\n",
            "778: [D loss: 0.157802, acc: 0.953125]  [G loss: 1.623032, acc: 0.515625]\n",
            "779: [D loss: 0.210942, acc: 0.921875]  [G loss: 2.223451, acc: 0.406250]\n",
            "780: [D loss: 0.201165, acc: 0.921875]  [G loss: 1.732504, acc: 0.546875]\n",
            "781: [D loss: 0.139087, acc: 0.960938]  [G loss: 1.308597, acc: 0.562500]\n",
            "782: [D loss: 0.174458, acc: 0.898438]  [G loss: 0.930093, acc: 0.687500]\n",
            "783: [D loss: 0.251444, acc: 0.921875]  [G loss: 1.701385, acc: 0.500000]\n",
            "784: [D loss: 0.180232, acc: 0.929688]  [G loss: 1.473804, acc: 0.593750]\n",
            "785: [D loss: 0.127473, acc: 0.945312]  [G loss: 1.568008, acc: 0.531250]\n",
            "786: [D loss: 0.216697, acc: 0.898438]  [G loss: 1.573497, acc: 0.468750]\n",
            "787: [D loss: 0.138310, acc: 0.937500]  [G loss: 1.290509, acc: 0.515625]\n",
            "788: [D loss: 0.203363, acc: 0.945312]  [G loss: 1.003471, acc: 0.687500]\n",
            "789: [D loss: 0.253326, acc: 0.921875]  [G loss: 1.250347, acc: 0.562500]\n",
            "790: [D loss: 0.273091, acc: 0.851562]  [G loss: 1.054141, acc: 0.546875]\n",
            "791: [D loss: 0.224884, acc: 0.906250]  [G loss: 2.234787, acc: 0.375000]\n",
            "792: [D loss: 0.138469, acc: 0.945312]  [G loss: 3.068779, acc: 0.265625]\n",
            "793: [D loss: 0.189674, acc: 0.945312]  [G loss: 2.756361, acc: 0.281250]\n",
            "794: [D loss: 0.276731, acc: 0.890625]  [G loss: 1.683072, acc: 0.531250]\n",
            "795: [D loss: 0.187203, acc: 0.906250]  [G loss: 0.623901, acc: 0.703125]\n",
            "796: [D loss: 0.275371, acc: 0.867188]  [G loss: 2.135885, acc: 0.343750]\n",
            "797: [D loss: 0.161642, acc: 0.929688]  [G loss: 3.032136, acc: 0.203125]\n",
            "798: [D loss: 0.110565, acc: 0.960938]  [G loss: 4.932527, acc: 0.046875]\n",
            "799: [D loss: 0.112293, acc: 0.945312]  [G loss: 4.658983, acc: 0.140625]\n",
            "800: [D loss: 0.094532, acc: 0.945312]  [G loss: 3.731467, acc: 0.078125]\n",
            "801: [D loss: 0.128085, acc: 0.945312]  [G loss: 2.641243, acc: 0.265625]\n",
            "802: [D loss: 0.076098, acc: 0.960938]  [G loss: 1.264387, acc: 0.546875]\n",
            "803: [D loss: 0.120636, acc: 0.937500]  [G loss: 1.719132, acc: 0.531250]\n",
            "804: [D loss: 0.122913, acc: 0.945312]  [G loss: 2.824143, acc: 0.343750]\n",
            "805: [D loss: 0.164598, acc: 0.953125]  [G loss: 5.220434, acc: 0.203125]\n",
            "806: [D loss: 0.045722, acc: 0.992188]  [G loss: 5.419903, acc: 0.218750]\n",
            "807: [D loss: 0.178225, acc: 0.945312]  [G loss: 6.694525, acc: 0.171875]\n",
            "808: [D loss: 0.095769, acc: 0.968750]  [G loss: 6.035571, acc: 0.234375]\n",
            "809: [D loss: 0.107068, acc: 0.960938]  [G loss: 4.785357, acc: 0.296875]\n",
            "810: [D loss: 0.147045, acc: 0.945312]  [G loss: 4.110586, acc: 0.375000]\n",
            "811: [D loss: 0.129168, acc: 0.945312]  [G loss: 4.834861, acc: 0.265625]\n",
            "812: [D loss: 0.078557, acc: 0.976562]  [G loss: 4.899980, acc: 0.312500]\n",
            "813: [D loss: 0.190346, acc: 0.960938]  [G loss: 4.425992, acc: 0.250000]\n",
            "814: [D loss: 0.096454, acc: 0.976562]  [G loss: 4.032216, acc: 0.296875]\n",
            "815: [D loss: 0.085928, acc: 0.968750]  [G loss: 3.136614, acc: 0.250000]\n",
            "816: [D loss: 0.131457, acc: 0.921875]  [G loss: 3.296766, acc: 0.265625]\n",
            "817: [D loss: 0.211099, acc: 0.890625]  [G loss: 4.732889, acc: 0.125000]\n",
            "818: [D loss: 0.218033, acc: 0.953125]  [G loss: 5.439877, acc: 0.078125]\n",
            "819: [D loss: 0.154936, acc: 0.937500]  [G loss: 3.183081, acc: 0.203125]\n",
            "820: [D loss: 0.194323, acc: 0.945312]  [G loss: 4.097587, acc: 0.156250]\n",
            "821: [D loss: 0.186646, acc: 0.937500]  [G loss: 3.511466, acc: 0.203125]\n",
            "822: [D loss: 0.148844, acc: 0.937500]  [G loss: 4.054089, acc: 0.265625]\n",
            "823: [D loss: 0.176272, acc: 0.953125]  [G loss: 5.013432, acc: 0.218750]\n",
            "824: [D loss: 0.108944, acc: 0.937500]  [G loss: 5.691136, acc: 0.140625]\n",
            "825: [D loss: 0.122778, acc: 0.984375]  [G loss: 4.321148, acc: 0.250000]\n",
            "826: [D loss: 0.308155, acc: 0.906250]  [G loss: 4.146634, acc: 0.343750]\n",
            "827: [D loss: 0.143439, acc: 0.921875]  [G loss: 5.410539, acc: 0.156250]\n",
            "828: [D loss: 0.260108, acc: 0.921875]  [G loss: 4.509791, acc: 0.203125]\n",
            "829: [D loss: 0.154144, acc: 0.945312]  [G loss: 3.845747, acc: 0.234375]\n",
            "830: [D loss: 0.234458, acc: 0.953125]  [G loss: 2.161876, acc: 0.390625]\n",
            "831: [D loss: 0.116726, acc: 0.976562]  [G loss: 2.279087, acc: 0.359375]\n",
            "832: [D loss: 0.090392, acc: 0.960938]  [G loss: 3.623902, acc: 0.187500]\n",
            "833: [D loss: 0.168851, acc: 0.976562]  [G loss: 3.780551, acc: 0.187500]\n",
            "834: [D loss: 0.110131, acc: 0.976562]  [G loss: 3.024019, acc: 0.296875]\n",
            "835: [D loss: 0.037294, acc: 0.992188]  [G loss: 3.206559, acc: 0.343750]\n",
            "836: [D loss: 0.126204, acc: 0.937500]  [G loss: 1.668765, acc: 0.406250]\n",
            "837: [D loss: 0.084022, acc: 0.976562]  [G loss: 1.150995, acc: 0.593750]\n",
            "838: [D loss: 0.123044, acc: 0.929688]  [G loss: 2.942070, acc: 0.218750]\n",
            "839: [D loss: 0.087267, acc: 0.960938]  [G loss: 3.864736, acc: 0.250000]\n",
            "840: [D loss: 0.174134, acc: 0.960938]  [G loss: 2.932766, acc: 0.296875]\n",
            "841: [D loss: 0.101987, acc: 0.960938]  [G loss: 1.697735, acc: 0.562500]\n",
            "842: [D loss: 0.076931, acc: 0.976562]  [G loss: 0.904215, acc: 0.671875]\n",
            "843: [D loss: 0.104306, acc: 0.968750]  [G loss: 1.318378, acc: 0.484375]\n",
            "844: [D loss: 0.089511, acc: 0.960938]  [G loss: 1.614839, acc: 0.515625]\n",
            "845: [D loss: 0.084534, acc: 0.984375]  [G loss: 1.816129, acc: 0.437500]\n",
            "846: [D loss: 0.191440, acc: 0.929688]  [G loss: 1.329249, acc: 0.593750]\n",
            "847: [D loss: 0.115653, acc: 0.945312]  [G loss: 1.088281, acc: 0.593750]\n",
            "848: [D loss: 0.068519, acc: 0.960938]  [G loss: 1.862043, acc: 0.468750]\n",
            "849: [D loss: 0.163201, acc: 0.945312]  [G loss: 1.666684, acc: 0.484375]\n",
            "850: [D loss: 0.203060, acc: 0.945312]  [G loss: 1.340669, acc: 0.546875]\n",
            "851: [D loss: 0.127098, acc: 0.945312]  [G loss: 1.767862, acc: 0.593750]\n",
            "852: [D loss: 0.137477, acc: 0.953125]  [G loss: 1.920959, acc: 0.593750]\n",
            "853: [D loss: 0.158175, acc: 0.929688]  [G loss: 2.028864, acc: 0.468750]\n",
            "854: [D loss: 0.145484, acc: 0.953125]  [G loss: 1.371100, acc: 0.578125]\n",
            "855: [D loss: 0.276332, acc: 0.906250]  [G loss: 0.679830, acc: 0.781250]\n",
            "856: [D loss: 0.200682, acc: 0.914062]  [G loss: 1.149828, acc: 0.734375]\n",
            "857: [D loss: 0.158952, acc: 0.937500]  [G loss: 1.844031, acc: 0.562500]\n",
            "858: [D loss: 0.292411, acc: 0.898438]  [G loss: 1.255368, acc: 0.609375]\n",
            "859: [D loss: 0.287991, acc: 0.914062]  [G loss: 1.435488, acc: 0.546875]\n",
            "860: [D loss: 0.078224, acc: 0.968750]  [G loss: 1.200858, acc: 0.609375]\n",
            "861: [D loss: 0.322970, acc: 0.898438]  [G loss: 1.387076, acc: 0.562500]\n",
            "862: [D loss: 0.132641, acc: 0.960938]  [G loss: 1.143116, acc: 0.609375]\n",
            "863: [D loss: 0.204620, acc: 0.937500]  [G loss: 1.298515, acc: 0.531250]\n",
            "864: [D loss: 0.083201, acc: 0.976562]  [G loss: 1.497755, acc: 0.515625]\n",
            "865: [D loss: 0.113377, acc: 0.953125]  [G loss: 1.837122, acc: 0.500000]\n",
            "866: [D loss: 0.115013, acc: 0.968750]  [G loss: 1.854438, acc: 0.453125]\n",
            "867: [D loss: 0.189209, acc: 0.953125]  [G loss: 1.252079, acc: 0.609375]\n",
            "868: [D loss: 0.135233, acc: 0.953125]  [G loss: 1.502088, acc: 0.578125]\n",
            "869: [D loss: 0.155484, acc: 0.937500]  [G loss: 1.210536, acc: 0.625000]\n",
            "870: [D loss: 0.153517, acc: 0.953125]  [G loss: 1.664306, acc: 0.484375]\n",
            "871: [D loss: 0.078559, acc: 0.968750]  [G loss: 3.530969, acc: 0.265625]\n",
            "872: [D loss: 0.115284, acc: 0.976562]  [G loss: 4.191599, acc: 0.218750]\n",
            "873: [D loss: 0.088315, acc: 0.968750]  [G loss: 3.576679, acc: 0.234375]\n",
            "874: [D loss: 0.065550, acc: 0.984375]  [G loss: 3.015937, acc: 0.343750]\n",
            "875: [D loss: 0.107416, acc: 0.937500]  [G loss: 2.153880, acc: 0.468750]\n",
            "876: [D loss: 0.056652, acc: 0.984375]  [G loss: 0.993984, acc: 0.671875]\n",
            "877: [D loss: 0.219147, acc: 0.929688]  [G loss: 1.443466, acc: 0.578125]\n",
            "878: [D loss: 0.102708, acc: 0.953125]  [G loss: 2.117387, acc: 0.484375]\n",
            "879: [D loss: 0.118748, acc: 0.945312]  [G loss: 2.875724, acc: 0.359375]\n",
            "880: [D loss: 0.160495, acc: 0.929688]  [G loss: 2.357946, acc: 0.437500]\n",
            "881: [D loss: 0.258335, acc: 0.937500]  [G loss: 0.841180, acc: 0.687500]\n",
            "882: [D loss: 0.428764, acc: 0.882812]  [G loss: 1.519430, acc: 0.515625]\n",
            "883: [D loss: 0.261185, acc: 0.937500]  [G loss: 2.524568, acc: 0.390625]\n",
            "884: [D loss: 0.202796, acc: 0.921875]  [G loss: 1.981798, acc: 0.562500]\n",
            "885: [D loss: 0.353228, acc: 0.867188]  [G loss: 0.854533, acc: 0.609375]\n",
            "886: [D loss: 0.405950, acc: 0.843750]  [G loss: 1.576850, acc: 0.406250]\n",
            "887: [D loss: 0.207985, acc: 0.937500]  [G loss: 2.982553, acc: 0.328125]\n",
            "888: [D loss: 0.379403, acc: 0.890625]  [G loss: 1.304539, acc: 0.531250]\n",
            "889: [D loss: 0.163058, acc: 0.945312]  [G loss: 0.658835, acc: 0.734375]\n",
            "890: [D loss: 0.225037, acc: 0.890625]  [G loss: 1.304200, acc: 0.468750]\n",
            "891: [D loss: 0.265474, acc: 0.921875]  [G loss: 3.013150, acc: 0.171875]\n",
            "892: [D loss: 0.289462, acc: 0.898438]  [G loss: 1.976685, acc: 0.359375]\n",
            "893: [D loss: 0.210475, acc: 0.929688]  [G loss: 0.563047, acc: 0.734375]\n",
            "894: [D loss: 0.270661, acc: 0.867188]  [G loss: 0.708085, acc: 0.750000]\n",
            "895: [D loss: 0.178650, acc: 0.929688]  [G loss: 1.782030, acc: 0.406250]\n",
            "896: [D loss: 0.214409, acc: 0.914062]  [G loss: 2.314225, acc: 0.359375]\n",
            "897: [D loss: 0.229575, acc: 0.906250]  [G loss: 1.572927, acc: 0.546875]\n",
            "898: [D loss: 0.121802, acc: 0.953125]  [G loss: 0.512901, acc: 0.718750]\n",
            "899: [D loss: 0.129396, acc: 0.937500]  [G loss: 0.633070, acc: 0.734375]\n",
            "900: [D loss: 0.151465, acc: 0.937500]  [G loss: 1.229507, acc: 0.609375]\n",
            "901: [D loss: 0.122439, acc: 0.953125]  [G loss: 1.843057, acc: 0.500000]\n",
            "902: [D loss: 0.203604, acc: 0.937500]  [G loss: 1.802481, acc: 0.531250]\n",
            "903: [D loss: 0.093228, acc: 0.960938]  [G loss: 1.200370, acc: 0.593750]\n",
            "904: [D loss: 0.208265, acc: 0.914062]  [G loss: 0.900375, acc: 0.687500]\n",
            "905: [D loss: 0.152637, acc: 0.953125]  [G loss: 1.134032, acc: 0.562500]\n",
            "906: [D loss: 0.064170, acc: 0.984375]  [G loss: 1.378077, acc: 0.562500]\n",
            "907: [D loss: 0.147958, acc: 0.968750]  [G loss: 1.900133, acc: 0.484375]\n",
            "908: [D loss: 0.081571, acc: 0.960938]  [G loss: 0.935835, acc: 0.656250]\n",
            "909: [D loss: 0.042501, acc: 1.000000]  [G loss: 1.331698, acc: 0.609375]\n",
            "910: [D loss: 0.084781, acc: 0.968750]  [G loss: 1.915777, acc: 0.406250]\n",
            "911: [D loss: 0.045249, acc: 0.984375]  [G loss: 2.099916, acc: 0.421875]\n",
            "912: [D loss: 0.217554, acc: 0.937500]  [G loss: 1.459580, acc: 0.437500]\n",
            "913: [D loss: 0.187792, acc: 0.929688]  [G loss: 1.302313, acc: 0.578125]\n",
            "914: [D loss: 0.083343, acc: 0.984375]  [G loss: 1.339630, acc: 0.625000]\n",
            "915: [D loss: 0.185607, acc: 0.929688]  [G loss: 1.727196, acc: 0.437500]\n",
            "916: [D loss: 0.146873, acc: 0.968750]  [G loss: 1.922394, acc: 0.406250]\n",
            "917: [D loss: 0.082821, acc: 0.960938]  [G loss: 1.919377, acc: 0.421875]\n",
            "918: [D loss: 0.132050, acc: 0.960938]  [G loss: 1.608948, acc: 0.500000]\n",
            "919: [D loss: 0.210398, acc: 0.906250]  [G loss: 1.548998, acc: 0.500000]\n",
            "920: [D loss: 0.119112, acc: 0.960938]  [G loss: 1.416022, acc: 0.593750]\n",
            "921: [D loss: 0.253338, acc: 0.937500]  [G loss: 0.926463, acc: 0.750000]\n",
            "922: [D loss: 0.048006, acc: 0.992188]  [G loss: 0.725419, acc: 0.828125]\n",
            "923: [D loss: 0.193057, acc: 0.945312]  [G loss: 0.832193, acc: 0.703125]\n",
            "924: [D loss: 0.244917, acc: 0.945312]  [G loss: 1.058185, acc: 0.609375]\n",
            "925: [D loss: 0.234199, acc: 0.937500]  [G loss: 0.995926, acc: 0.609375]\n",
            "926: [D loss: 0.111953, acc: 0.976562]  [G loss: 0.839029, acc: 0.625000]\n",
            "927: [D loss: 0.144554, acc: 0.937500]  [G loss: 1.463656, acc: 0.437500]\n",
            "928: [D loss: 0.176711, acc: 0.953125]  [G loss: 2.933431, acc: 0.250000]\n",
            "929: [D loss: 0.058425, acc: 0.976562]  [G loss: 3.114754, acc: 0.203125]\n",
            "930: [D loss: 0.156342, acc: 0.960938]  [G loss: 2.696886, acc: 0.296875]\n",
            "931: [D loss: 0.099503, acc: 0.960938]  [G loss: 2.266456, acc: 0.343750]\n",
            "932: [D loss: 0.145423, acc: 0.945312]  [G loss: 1.356962, acc: 0.500000]\n",
            "933: [D loss: 0.121839, acc: 0.929688]  [G loss: 1.394529, acc: 0.421875]\n",
            "934: [D loss: 0.162595, acc: 0.945312]  [G loss: 2.800171, acc: 0.343750]\n",
            "935: [D loss: 0.116493, acc: 0.960938]  [G loss: 2.620188, acc: 0.343750]\n",
            "936: [D loss: 0.303727, acc: 0.898438]  [G loss: 1.526282, acc: 0.531250]\n",
            "937: [D loss: 0.140871, acc: 0.960938]  [G loss: 0.838271, acc: 0.671875]\n",
            "938: [D loss: 0.166560, acc: 0.937500]  [G loss: 0.473904, acc: 0.812500]\n",
            "939: [D loss: 0.244187, acc: 0.898438]  [G loss: 0.960750, acc: 0.640625]\n",
            "940: [D loss: 0.100308, acc: 0.953125]  [G loss: 1.843116, acc: 0.375000]\n",
            "941: [D loss: 0.137927, acc: 0.953125]  [G loss: 2.681470, acc: 0.359375]\n",
            "942: [D loss: 0.260892, acc: 0.953125]  [G loss: 1.536306, acc: 0.515625]\n",
            "943: [D loss: 0.195706, acc: 0.937500]  [G loss: 0.540168, acc: 0.812500]\n",
            "944: [D loss: 0.172730, acc: 0.914062]  [G loss: 0.411149, acc: 0.859375]\n",
            "945: [D loss: 0.208611, acc: 0.898438]  [G loss: 0.468000, acc: 0.812500]\n",
            "946: [D loss: 0.132794, acc: 0.953125]  [G loss: 0.819154, acc: 0.734375]\n",
            "947: [D loss: 0.085902, acc: 0.976562]  [G loss: 1.223037, acc: 0.781250]\n",
            "948: [D loss: 0.159169, acc: 0.937500]  [G loss: 0.703129, acc: 0.890625]\n",
            "949: [D loss: 0.109802, acc: 0.953125]  [G loss: 0.370521, acc: 0.859375]\n",
            "950: [D loss: 0.168239, acc: 0.945312]  [G loss: 0.196304, acc: 0.937500]\n",
            "951: [D loss: 0.137889, acc: 0.937500]  [G loss: 0.189373, acc: 0.906250]\n",
            "952: [D loss: 0.119630, acc: 0.960938]  [G loss: 0.369284, acc: 0.812500]\n",
            "953: [D loss: 0.073486, acc: 0.976562]  [G loss: 1.142122, acc: 0.406250]\n",
            "954: [D loss: 0.199218, acc: 0.921875]  [G loss: 2.386973, acc: 0.187500]\n",
            "955: [D loss: 0.249763, acc: 0.929688]  [G loss: 2.830188, acc: 0.125000]\n",
            "956: [D loss: 0.382273, acc: 0.882812]  [G loss: 1.394869, acc: 0.406250]\n",
            "957: [D loss: 0.124596, acc: 0.937500]  [G loss: 1.099762, acc: 0.453125]\n",
            "958: [D loss: 0.223093, acc: 0.906250]  [G loss: 1.188810, acc: 0.484375]\n",
            "959: [D loss: 0.236152, acc: 0.914062]  [G loss: 2.330164, acc: 0.218750]\n",
            "960: [D loss: 0.183972, acc: 0.960938]  [G loss: 2.648252, acc: 0.125000]\n",
            "961: [D loss: 0.117769, acc: 0.953125]  [G loss: 3.179207, acc: 0.031250]\n",
            "962: [D loss: 0.071693, acc: 0.984375]  [G loss: 2.984485, acc: 0.093750]\n",
            "963: [D loss: 0.176041, acc: 0.953125]  [G loss: 2.923176, acc: 0.109375]\n",
            "964: [D loss: 0.186809, acc: 0.937500]  [G loss: 2.335307, acc: 0.218750]\n",
            "965: [D loss: 0.107385, acc: 0.968750]  [G loss: 2.703116, acc: 0.093750]\n",
            "966: [D loss: 0.128084, acc: 0.953125]  [G loss: 2.405742, acc: 0.203125]\n",
            "967: [D loss: 0.059372, acc: 0.976562]  [G loss: 2.924336, acc: 0.156250]\n",
            "968: [D loss: 0.085458, acc: 0.976562]  [G loss: 2.852481, acc: 0.171875]\n",
            "969: [D loss: 0.182440, acc: 0.937500]  [G loss: 2.888820, acc: 0.203125]\n",
            "970: [D loss: 0.196663, acc: 0.921875]  [G loss: 3.286300, acc: 0.140625]\n",
            "971: [D loss: 0.070838, acc: 0.984375]  [G loss: 2.707057, acc: 0.203125]\n",
            "972: [D loss: 0.115029, acc: 0.960938]  [G loss: 2.982357, acc: 0.171875]\n",
            "973: [D loss: 0.024538, acc: 0.992188]  [G loss: 2.687225, acc: 0.203125]\n",
            "974: [D loss: 0.090702, acc: 0.984375]  [G loss: 2.956130, acc: 0.250000]\n",
            "975: [D loss: 0.071381, acc: 0.968750]  [G loss: 3.234072, acc: 0.234375]\n",
            "976: [D loss: 0.093944, acc: 0.945312]  [G loss: 3.260855, acc: 0.218750]\n",
            "977: [D loss: 0.156195, acc: 0.953125]  [G loss: 3.233324, acc: 0.203125]\n",
            "978: [D loss: 0.092837, acc: 0.976562]  [G loss: 3.406344, acc: 0.234375]\n",
            "979: [D loss: 0.077038, acc: 0.984375]  [G loss: 3.075869, acc: 0.296875]\n",
            "980: [D loss: 0.210816, acc: 0.953125]  [G loss: 2.915463, acc: 0.328125]\n",
            "981: [D loss: 0.189064, acc: 0.960938]  [G loss: 2.075777, acc: 0.421875]\n",
            "982: [D loss: 0.042216, acc: 0.984375]  [G loss: 2.357718, acc: 0.437500]\n",
            "983: [D loss: 0.171735, acc: 0.945312]  [G loss: 1.948397, acc: 0.484375]\n",
            "984: [D loss: 0.104666, acc: 0.968750]  [G loss: 2.171993, acc: 0.500000]\n",
            "985: [D loss: 0.185912, acc: 0.953125]  [G loss: 3.046575, acc: 0.296875]\n",
            "986: [D loss: 0.342381, acc: 0.890625]  [G loss: 2.734112, acc: 0.328125]\n",
            "987: [D loss: 0.112117, acc: 0.968750]  [G loss: 2.050557, acc: 0.390625]\n",
            "988: [D loss: 0.203225, acc: 0.929688]  [G loss: 2.218740, acc: 0.359375]\n",
            "989: [D loss: 0.459721, acc: 0.851562]  [G loss: 2.203604, acc: 0.359375]\n",
            "990: [D loss: 0.245688, acc: 0.921875]  [G loss: 1.380809, acc: 0.562500]\n",
            "991: [D loss: 0.341183, acc: 0.851562]  [G loss: 1.007813, acc: 0.671875]\n",
            "992: [D loss: 0.333522, acc: 0.875000]  [G loss: 0.930197, acc: 0.609375]\n",
            "993: [D loss: 0.549498, acc: 0.812500]  [G loss: 1.648418, acc: 0.531250]\n",
            "994: [D loss: 0.406109, acc: 0.851562]  [G loss: 1.523194, acc: 0.515625]\n",
            "995: [D loss: 0.326110, acc: 0.875000]  [G loss: 1.178758, acc: 0.562500]\n",
            "996: [D loss: 0.485562, acc: 0.820312]  [G loss: 0.454448, acc: 0.828125]\n",
            "997: [D loss: 0.693839, acc: 0.742188]  [G loss: 0.581958, acc: 0.765625]\n",
            "998: [D loss: 0.725093, acc: 0.734375]  [G loss: 0.916510, acc: 0.593750]\n",
            "999: [D loss: 0.575560, acc: 0.742188]  [G loss: 0.948134, acc: 0.609375]\n",
            "1000: [D loss: 0.436757, acc: 0.812500]  [G loss: 1.162450, acc: 0.515625]\n",
            "1001: [D loss: 0.796901, acc: 0.765625]  [G loss: 0.751790, acc: 0.703125]\n",
            "1002: [D loss: 0.709545, acc: 0.757812]  [G loss: 0.443417, acc: 0.828125]\n",
            "1003: [D loss: 0.565928, acc: 0.789062]  [G loss: 0.411265, acc: 0.812500]\n",
            "1004: [D loss: 0.605628, acc: 0.742188]  [G loss: 0.912393, acc: 0.656250]\n",
            "1005: [D loss: 0.467976, acc: 0.796875]  [G loss: 1.100978, acc: 0.578125]\n",
            "1006: [D loss: 0.533730, acc: 0.781250]  [G loss: 0.393070, acc: 0.796875]\n",
            "1007: [D loss: 0.394455, acc: 0.796875]  [G loss: 0.260958, acc: 0.875000]\n",
            "1008: [D loss: 0.330499, acc: 0.867188]  [G loss: 0.281922, acc: 0.890625]\n",
            "1009: [D loss: 0.380125, acc: 0.781250]  [G loss: 0.414011, acc: 0.796875]\n",
            "1010: [D loss: 0.333501, acc: 0.867188]  [G loss: 0.796900, acc: 0.656250]\n",
            "1011: [D loss: 0.320826, acc: 0.875000]  [G loss: 1.047748, acc: 0.546875]\n",
            "1012: [D loss: 0.355072, acc: 0.843750]  [G loss: 0.772936, acc: 0.671875]\n",
            "1013: [D loss: 0.257872, acc: 0.929688]  [G loss: 0.665310, acc: 0.765625]\n",
            "1014: [D loss: 0.294488, acc: 0.867188]  [G loss: 0.462916, acc: 0.843750]\n",
            "1015: [D loss: 0.298156, acc: 0.851562]  [G loss: 0.603252, acc: 0.796875]\n",
            "1016: [D loss: 0.191995, acc: 0.929688]  [G loss: 1.034898, acc: 0.640625]\n",
            "1017: [D loss: 0.248644, acc: 0.898438]  [G loss: 1.353314, acc: 0.390625]\n",
            "1018: [D loss: 0.252851, acc: 0.890625]  [G loss: 1.469064, acc: 0.359375]\n",
            "1019: [D loss: 0.229764, acc: 0.906250]  [G loss: 1.650104, acc: 0.359375]\n",
            "1020: [D loss: 0.201521, acc: 0.945312]  [G loss: 0.908330, acc: 0.593750]\n",
            "1021: [D loss: 0.161976, acc: 0.937500]  [G loss: 1.029024, acc: 0.578125]\n",
            "1022: [D loss: 0.132257, acc: 0.976562]  [G loss: 1.245043, acc: 0.468750]\n",
            "1023: [D loss: 0.189155, acc: 0.937500]  [G loss: 1.893069, acc: 0.234375]\n",
            "1024: [D loss: 0.130252, acc: 0.953125]  [G loss: 2.082839, acc: 0.171875]\n",
            "1025: [D loss: 0.194138, acc: 0.929688]  [G loss: 2.575004, acc: 0.234375]\n",
            "1026: [D loss: 0.127054, acc: 0.960938]  [G loss: 2.375285, acc: 0.187500]\n",
            "1027: [D loss: 0.141719, acc: 0.960938]  [G loss: 1.996351, acc: 0.296875]\n",
            "1028: [D loss: 0.122416, acc: 0.960938]  [G loss: 1.540360, acc: 0.312500]\n",
            "1029: [D loss: 0.121274, acc: 0.976562]  [G loss: 1.109414, acc: 0.515625]\n",
            "1030: [D loss: 0.223336, acc: 0.921875]  [G loss: 1.549952, acc: 0.343750]\n",
            "1031: [D loss: 0.237011, acc: 0.906250]  [G loss: 1.829875, acc: 0.281250]\n",
            "1032: [D loss: 0.211780, acc: 0.921875]  [G loss: 2.271193, acc: 0.296875]\n",
            "1033: [D loss: 0.111614, acc: 0.960938]  [G loss: 3.475148, acc: 0.187500]\n",
            "1034: [D loss: 0.087174, acc: 0.968750]  [G loss: 3.493893, acc: 0.187500]\n",
            "1035: [D loss: 0.091775, acc: 0.960938]  [G loss: 3.112502, acc: 0.203125]\n",
            "1036: [D loss: 0.134495, acc: 0.937500]  [G loss: 2.550826, acc: 0.265625]\n",
            "1037: [D loss: 0.151600, acc: 0.929688]  [G loss: 2.429112, acc: 0.343750]\n",
            "1038: [D loss: 0.172036, acc: 0.929688]  [G loss: 3.123376, acc: 0.250000]\n",
            "1039: [D loss: 0.112361, acc: 0.968750]  [G loss: 2.829662, acc: 0.296875]\n",
            "1040: [D loss: 0.115947, acc: 0.960938]  [G loss: 3.909117, acc: 0.156250]\n",
            "1041: [D loss: 0.114496, acc: 0.968750]  [G loss: 4.302166, acc: 0.109375]\n",
            "1042: [D loss: 0.120321, acc: 0.953125]  [G loss: 3.700389, acc: 0.234375]\n",
            "1043: [D loss: 0.178449, acc: 0.945312]  [G loss: 3.336025, acc: 0.250000]\n",
            "1044: [D loss: 0.117171, acc: 0.953125]  [G loss: 3.047214, acc: 0.250000]\n",
            "1045: [D loss: 0.152318, acc: 0.937500]  [G loss: 2.691417, acc: 0.250000]\n",
            "1046: [D loss: 0.197916, acc: 0.953125]  [G loss: 4.368444, acc: 0.078125]\n",
            "1047: [D loss: 0.079463, acc: 0.976562]  [G loss: 5.609627, acc: 0.093750]\n",
            "1048: [D loss: 0.153449, acc: 0.960938]  [G loss: 5.640539, acc: 0.015625]\n",
            "1049: [D loss: 0.251007, acc: 0.906250]  [G loss: 5.229486, acc: 0.046875]\n",
            "1050: [D loss: 0.211694, acc: 0.921875]  [G loss: 4.390808, acc: 0.078125]\n",
            "1051: [D loss: 0.163035, acc: 0.921875]  [G loss: 3.320008, acc: 0.218750]\n",
            "1052: [D loss: 0.187842, acc: 0.921875]  [G loss: 3.893524, acc: 0.109375]\n",
            "1053: [D loss: 0.094193, acc: 0.976562]  [G loss: 4.082914, acc: 0.203125]\n",
            "1054: [D loss: 0.146646, acc: 0.945312]  [G loss: 4.881549, acc: 0.109375]\n",
            "1055: [D loss: 0.073700, acc: 0.960938]  [G loss: 5.112983, acc: 0.062500]\n",
            "1056: [D loss: 0.153196, acc: 0.960938]  [G loss: 4.525502, acc: 0.125000]\n",
            "1057: [D loss: 0.063221, acc: 0.976562]  [G loss: 4.976135, acc: 0.078125]\n",
            "1058: [D loss: 0.128337, acc: 0.953125]  [G loss: 4.863391, acc: 0.156250]\n",
            "1059: [D loss: 0.115593, acc: 0.937500]  [G loss: 5.130098, acc: 0.140625]\n",
            "1060: [D loss: 0.091296, acc: 0.976562]  [G loss: 4.899951, acc: 0.093750]\n",
            "1061: [D loss: 0.096574, acc: 0.953125]  [G loss: 4.309969, acc: 0.093750]\n",
            "1062: [D loss: 0.272924, acc: 0.960938]  [G loss: 3.242446, acc: 0.281250]\n",
            "1063: [D loss: 0.187249, acc: 0.921875]  [G loss: 3.087260, acc: 0.234375]\n",
            "1064: [D loss: 0.278104, acc: 0.898438]  [G loss: 3.645057, acc: 0.171875]\n",
            "1065: [D loss: 0.148076, acc: 0.945312]  [G loss: 3.711230, acc: 0.140625]\n",
            "1066: [D loss: 0.260739, acc: 0.937500]  [G loss: 3.201802, acc: 0.187500]\n",
            "1067: [D loss: 0.147175, acc: 0.960938]  [G loss: 2.517647, acc: 0.281250]\n",
            "1068: [D loss: 0.296806, acc: 0.921875]  [G loss: 2.267146, acc: 0.250000]\n",
            "1069: [D loss: 0.183707, acc: 0.929688]  [G loss: 2.173179, acc: 0.328125]\n",
            "1070: [D loss: 0.183475, acc: 0.929688]  [G loss: 1.952416, acc: 0.312500]\n",
            "1071: [D loss: 0.089903, acc: 0.953125]  [G loss: 2.808623, acc: 0.250000]\n",
            "1072: [D loss: 0.096362, acc: 0.968750]  [G loss: 3.322680, acc: 0.265625]\n",
            "1073: [D loss: 0.100874, acc: 0.968750]  [G loss: 4.430832, acc: 0.109375]\n",
            "1074: [D loss: 0.085262, acc: 0.968750]  [G loss: 4.040053, acc: 0.171875]\n",
            "1075: [D loss: 0.092190, acc: 0.976562]  [G loss: 3.773199, acc: 0.281250]\n",
            "1076: [D loss: 0.154768, acc: 0.929688]  [G loss: 3.396718, acc: 0.281250]\n",
            "1077: [D loss: 0.376285, acc: 0.882812]  [G loss: 2.796232, acc: 0.406250]\n",
            "1078: [D loss: 0.227257, acc: 0.898438]  [G loss: 2.430007, acc: 0.281250]\n",
            "1079: [D loss: 0.233524, acc: 0.906250]  [G loss: 2.967015, acc: 0.265625]\n",
            "1080: [D loss: 0.146436, acc: 0.921875]  [G loss: 3.525115, acc: 0.093750]\n",
            "1081: [D loss: 0.337999, acc: 0.898438]  [G loss: 2.910859, acc: 0.156250]\n",
            "1082: [D loss: 0.281165, acc: 0.882812]  [G loss: 2.711518, acc: 0.203125]\n",
            "1083: [D loss: 0.126401, acc: 0.976562]  [G loss: 2.467284, acc: 0.250000]\n",
            "1084: [D loss: 0.314339, acc: 0.867188]  [G loss: 1.981084, acc: 0.343750]\n",
            "1085: [D loss: 0.251668, acc: 0.875000]  [G loss: 3.174145, acc: 0.187500]\n",
            "1086: [D loss: 0.229295, acc: 0.906250]  [G loss: 2.958344, acc: 0.218750]\n",
            "1087: [D loss: 0.256708, acc: 0.898438]  [G loss: 2.326279, acc: 0.390625]\n",
            "1088: [D loss: 0.132214, acc: 0.945312]  [G loss: 2.364472, acc: 0.281250]\n",
            "1089: [D loss: 0.206415, acc: 0.914062]  [G loss: 1.855249, acc: 0.468750]\n",
            "1090: [D loss: 0.186887, acc: 0.937500]  [G loss: 1.899811, acc: 0.421875]\n",
            "1091: [D loss: 0.142733, acc: 0.953125]  [G loss: 2.215188, acc: 0.328125]\n",
            "1092: [D loss: 0.135881, acc: 0.953125]  [G loss: 2.292207, acc: 0.359375]\n",
            "1093: [D loss: 0.108891, acc: 0.937500]  [G loss: 2.926492, acc: 0.390625]\n",
            "1094: [D loss: 0.167609, acc: 0.937500]  [G loss: 2.866059, acc: 0.296875]\n",
            "1095: [D loss: 0.096563, acc: 0.953125]  [G loss: 2.845384, acc: 0.234375]\n",
            "1096: [D loss: 0.090519, acc: 0.968750]  [G loss: 2.426714, acc: 0.406250]\n",
            "1097: [D loss: 0.173299, acc: 0.953125]  [G loss: 2.022201, acc: 0.390625]\n",
            "1098: [D loss: 0.155855, acc: 0.937500]  [G loss: 2.216441, acc: 0.234375]\n",
            "1099: [D loss: 0.168705, acc: 0.960938]  [G loss: 2.627289, acc: 0.218750]\n",
            "1100: [D loss: 0.109900, acc: 0.976562]  [G loss: 3.233420, acc: 0.203125]\n",
            "1101: [D loss: 0.061751, acc: 0.984375]  [G loss: 3.681108, acc: 0.078125]\n",
            "1102: [D loss: 0.081387, acc: 0.968750]  [G loss: 3.670526, acc: 0.031250]\n",
            "1103: [D loss: 0.036376, acc: 0.984375]  [G loss: 3.960858, acc: 0.015625]\n",
            "1104: [D loss: 0.064190, acc: 0.992188]  [G loss: 4.217905, acc: 0.078125]\n",
            "1105: [D loss: 0.061792, acc: 0.984375]  [G loss: 4.705052, acc: 0.046875]\n",
            "1106: [D loss: 0.051378, acc: 0.992188]  [G loss: 4.141651, acc: 0.078125]\n",
            "1107: [D loss: 0.059624, acc: 0.992188]  [G loss: 4.036237, acc: 0.171875]\n",
            "1108: [D loss: 0.044751, acc: 0.984375]  [G loss: 4.037361, acc: 0.078125]\n",
            "1109: [D loss: 0.057762, acc: 0.992188]  [G loss: 4.415648, acc: 0.046875]\n",
            "1110: [D loss: 0.016360, acc: 1.000000]  [G loss: 4.866648, acc: 0.109375]\n",
            "1111: [D loss: 0.026516, acc: 0.992188]  [G loss: 5.111942, acc: 0.062500]\n",
            "1112: [D loss: 0.017293, acc: 1.000000]  [G loss: 5.002000, acc: 0.078125]\n",
            "1113: [D loss: 0.062827, acc: 0.976562]  [G loss: 5.701537, acc: 0.078125]\n",
            "1114: [D loss: 0.185048, acc: 0.953125]  [G loss: 5.294178, acc: 0.109375]\n",
            "1115: [D loss: 0.128929, acc: 0.968750]  [G loss: 4.791944, acc: 0.234375]\n",
            "1116: [D loss: 0.117940, acc: 0.953125]  [G loss: 3.928833, acc: 0.234375]\n",
            "1117: [D loss: 0.211742, acc: 0.968750]  [G loss: 3.599202, acc: 0.312500]\n",
            "1118: [D loss: 0.095729, acc: 0.968750]  [G loss: 4.013654, acc: 0.250000]\n",
            "1119: [D loss: 0.031200, acc: 0.992188]  [G loss: 4.774712, acc: 0.171875]\n",
            "1120: [D loss: 0.122684, acc: 0.960938]  [G loss: 3.989264, acc: 0.187500]\n",
            "1121: [D loss: 0.106990, acc: 0.976562]  [G loss: 4.303373, acc: 0.265625]\n",
            "1122: [D loss: 0.029837, acc: 1.000000]  [G loss: 3.810059, acc: 0.343750]\n",
            "1123: [D loss: 0.261056, acc: 0.906250]  [G loss: 2.848076, acc: 0.390625]\n",
            "1124: [D loss: 0.114859, acc: 0.960938]  [G loss: 2.008017, acc: 0.546875]\n",
            "1125: [D loss: 0.327048, acc: 0.898438]  [G loss: 2.626495, acc: 0.375000]\n",
            "1126: [D loss: 0.253687, acc: 0.921875]  [G loss: 4.324768, acc: 0.328125]\n",
            "1127: [D loss: 0.169647, acc: 0.953125]  [G loss: 5.264007, acc: 0.140625]\n",
            "1128: [D loss: 0.265773, acc: 0.937500]  [G loss: 5.055878, acc: 0.265625]\n",
            "1129: [D loss: 0.223706, acc: 0.929688]  [G loss: 2.613756, acc: 0.390625]\n",
            "1130: [D loss: 0.379380, acc: 0.898438]  [G loss: 1.522648, acc: 0.578125]\n",
            "1131: [D loss: 0.263385, acc: 0.898438]  [G loss: 1.505729, acc: 0.625000]\n",
            "1132: [D loss: 0.341016, acc: 0.914062]  [G loss: 3.581471, acc: 0.265625]\n",
            "1133: [D loss: 0.166556, acc: 0.914062]  [G loss: 3.709357, acc: 0.187500]\n",
            "1134: [D loss: 0.161059, acc: 0.937500]  [G loss: 4.122759, acc: 0.218750]\n",
            "1135: [D loss: 0.270309, acc: 0.898438]  [G loss: 2.521613, acc: 0.375000]\n",
            "1136: [D loss: 0.119784, acc: 0.960938]  [G loss: 1.871097, acc: 0.468750]\n",
            "1137: [D loss: 0.196776, acc: 0.921875]  [G loss: 1.435139, acc: 0.531250]\n",
            "1138: [D loss: 0.183543, acc: 0.921875]  [G loss: 1.217929, acc: 0.593750]\n",
            "1139: [D loss: 0.245178, acc: 0.914062]  [G loss: 2.508466, acc: 0.375000]\n",
            "1140: [D loss: 0.206352, acc: 0.921875]  [G loss: 3.062186, acc: 0.250000]\n",
            "1141: [D loss: 0.319756, acc: 0.914062]  [G loss: 3.203771, acc: 0.156250]\n",
            "1142: [D loss: 0.293069, acc: 0.929688]  [G loss: 3.903147, acc: 0.187500]\n",
            "1143: [D loss: 0.236388, acc: 0.914062]  [G loss: 3.135273, acc: 0.281250]\n",
            "1144: [D loss: 0.252322, acc: 0.906250]  [G loss: 4.571568, acc: 0.109375]\n",
            "1145: [D loss: 0.122978, acc: 0.945312]  [G loss: 3.969626, acc: 0.140625]\n",
            "1146: [D loss: 0.108282, acc: 0.976562]  [G loss: 3.172666, acc: 0.250000]\n",
            "1147: [D loss: 0.111004, acc: 0.953125]  [G loss: 3.304084, acc: 0.265625]\n",
            "1148: [D loss: 0.164931, acc: 0.937500]  [G loss: 2.651877, acc: 0.312500]\n",
            "1149: [D loss: 0.118776, acc: 0.937500]  [G loss: 2.922733, acc: 0.296875]\n",
            "1150: [D loss: 0.149238, acc: 0.953125]  [G loss: 2.656158, acc: 0.187500]\n",
            "1151: [D loss: 0.090771, acc: 0.976562]  [G loss: 3.490705, acc: 0.203125]\n",
            "1152: [D loss: 0.158477, acc: 0.953125]  [G loss: 3.374407, acc: 0.265625]\n",
            "1153: [D loss: 0.141863, acc: 0.937500]  [G loss: 3.982899, acc: 0.281250]\n",
            "1154: [D loss: 0.131114, acc: 0.968750]  [G loss: 3.980298, acc: 0.328125]\n",
            "1155: [D loss: 0.139965, acc: 0.945312]  [G loss: 3.849074, acc: 0.171875]\n",
            "1156: [D loss: 0.101879, acc: 0.960938]  [G loss: 3.535381, acc: 0.296875]\n",
            "1157: [D loss: 0.306781, acc: 0.906250]  [G loss: 3.225507, acc: 0.343750]\n",
            "1158: [D loss: 0.310868, acc: 0.937500]  [G loss: 2.602936, acc: 0.156250]\n",
            "1159: [D loss: 0.113566, acc: 0.945312]  [G loss: 3.573672, acc: 0.156250]\n",
            "1160: [D loss: 0.118657, acc: 0.945312]  [G loss: 2.930018, acc: 0.093750]\n",
            "1161: [D loss: 0.108963, acc: 0.953125]  [G loss: 3.021884, acc: 0.187500]\n",
            "1162: [D loss: 0.106791, acc: 0.968750]  [G loss: 3.510046, acc: 0.140625]\n",
            "1163: [D loss: 0.121258, acc: 0.945312]  [G loss: 3.353519, acc: 0.125000]\n",
            "1164: [D loss: 0.053212, acc: 1.000000]  [G loss: 3.286049, acc: 0.156250]\n",
            "1165: [D loss: 0.217314, acc: 0.937500]  [G loss: 3.605001, acc: 0.171875]\n",
            "1166: [D loss: 0.109418, acc: 0.953125]  [G loss: 2.540354, acc: 0.234375]\n",
            "1167: [D loss: 0.126913, acc: 0.937500]  [G loss: 2.456321, acc: 0.250000]\n",
            "1168: [D loss: 0.173759, acc: 0.945312]  [G loss: 2.634900, acc: 0.234375]\n",
            "1169: [D loss: 0.069339, acc: 0.992188]  [G loss: 3.641086, acc: 0.156250]\n",
            "1170: [D loss: 0.120384, acc: 0.960938]  [G loss: 4.375060, acc: 0.093750]\n",
            "1171: [D loss: 0.067880, acc: 0.968750]  [G loss: 4.375021, acc: 0.125000]\n",
            "1172: [D loss: 0.100867, acc: 0.976562]  [G loss: 4.757247, acc: 0.062500]\n",
            "1173: [D loss: 0.093968, acc: 0.945312]  [G loss: 4.210705, acc: 0.109375]\n",
            "1174: [D loss: 0.072232, acc: 0.960938]  [G loss: 3.611722, acc: 0.171875]\n",
            "1175: [D loss: 0.210154, acc: 0.906250]  [G loss: 3.103041, acc: 0.171875]\n",
            "1176: [D loss: 0.164111, acc: 0.937500]  [G loss: 2.706909, acc: 0.203125]\n",
            "1177: [D loss: 0.155465, acc: 0.937500]  [G loss: 3.241652, acc: 0.234375]\n",
            "1178: [D loss: 0.178176, acc: 0.937500]  [G loss: 4.291698, acc: 0.046875]\n",
            "1179: [D loss: 0.146991, acc: 0.945312]  [G loss: 4.852164, acc: 0.046875]\n",
            "1180: [D loss: 0.155674, acc: 0.945312]  [G loss: 4.531657, acc: 0.015625]\n",
            "1181: [D loss: 0.179314, acc: 0.937500]  [G loss: 3.699355, acc: 0.093750]\n",
            "1182: [D loss: 0.211568, acc: 0.914062]  [G loss: 2.754182, acc: 0.234375]\n",
            "1183: [D loss: 0.099117, acc: 0.953125]  [G loss: 2.066701, acc: 0.265625]\n",
            "1184: [D loss: 0.135993, acc: 0.960938]  [G loss: 2.551680, acc: 0.234375]\n",
            "1185: [D loss: 0.180467, acc: 0.945312]  [G loss: 3.365928, acc: 0.171875]\n",
            "1186: [D loss: 0.105484, acc: 0.953125]  [G loss: 3.771459, acc: 0.140625]\n",
            "1187: [D loss: 0.093297, acc: 0.976562]  [G loss: 3.667679, acc: 0.062500]\n",
            "1188: [D loss: 0.079396, acc: 0.984375]  [G loss: 3.376923, acc: 0.218750]\n",
            "1189: [D loss: 0.159916, acc: 0.945312]  [G loss: 2.468847, acc: 0.312500]\n",
            "1190: [D loss: 0.089131, acc: 0.968750]  [G loss: 1.231698, acc: 0.593750]\n",
            "1191: [D loss: 0.108704, acc: 0.945312]  [G loss: 1.023832, acc: 0.609375]\n",
            "1192: [D loss: 0.153132, acc: 0.929688]  [G loss: 1.268198, acc: 0.546875]\n",
            "1193: [D loss: 0.090057, acc: 0.953125]  [G loss: 1.610425, acc: 0.421875]\n",
            "1194: [D loss: 0.062448, acc: 0.976562]  [G loss: 3.148729, acc: 0.406250]\n",
            "1195: [D loss: 0.145098, acc: 0.960938]  [G loss: 3.406653, acc: 0.359375]\n",
            "1196: [D loss: 0.116806, acc: 0.984375]  [G loss: 3.377966, acc: 0.343750]\n",
            "1197: [D loss: 0.062157, acc: 0.976562]  [G loss: 2.794506, acc: 0.421875]\n",
            "1198: [D loss: 0.043994, acc: 0.984375]  [G loss: 2.263288, acc: 0.515625]\n",
            "1199: [D loss: 0.064347, acc: 0.992188]  [G loss: 2.419154, acc: 0.453125]\n",
            "1200: [D loss: 0.097281, acc: 0.976562]  [G loss: 1.833995, acc: 0.593750]\n",
            "1201: [D loss: 0.041767, acc: 0.984375]  [G loss: 2.266293, acc: 0.437500]\n",
            "1202: [D loss: 0.118354, acc: 0.992188]  [G loss: 1.584416, acc: 0.718750]\n",
            "1203: [D loss: 0.087312, acc: 0.968750]  [G loss: 1.594111, acc: 0.625000]\n",
            "1204: [D loss: 0.038815, acc: 0.984375]  [G loss: 1.778185, acc: 0.609375]\n",
            "1205: [D loss: 0.160914, acc: 0.960938]  [G loss: 1.145873, acc: 0.718750]\n",
            "1206: [D loss: 0.080770, acc: 0.960938]  [G loss: 0.911606, acc: 0.750000]\n",
            "1207: [D loss: 0.089065, acc: 0.960938]  [G loss: 0.429760, acc: 0.859375]\n",
            "1208: [D loss: 0.109688, acc: 0.937500]  [G loss: 0.793887, acc: 0.750000]\n",
            "1209: [D loss: 0.102063, acc: 0.953125]  [G loss: 0.885661, acc: 0.703125]\n",
            "1210: [D loss: 0.249444, acc: 0.906250]  [G loss: 0.790767, acc: 0.781250]\n",
            "1211: [D loss: 0.151886, acc: 0.953125]  [G loss: 0.748471, acc: 0.750000]\n",
            "1212: [D loss: 0.208025, acc: 0.953125]  [G loss: 0.754605, acc: 0.781250]\n",
            "1213: [D loss: 0.115258, acc: 0.945312]  [G loss: 0.850373, acc: 0.734375]\n",
            "1214: [D loss: 0.238341, acc: 0.906250]  [G loss: 0.587350, acc: 0.843750]\n",
            "1215: [D loss: 0.129633, acc: 0.937500]  [G loss: 0.958324, acc: 0.718750]\n",
            "1216: [D loss: 0.134667, acc: 0.960938]  [G loss: 0.911977, acc: 0.734375]\n",
            "1217: [D loss: 0.202607, acc: 0.921875]  [G loss: 0.691810, acc: 0.765625]\n",
            "1218: [D loss: 0.075488, acc: 0.968750]  [G loss: 0.774546, acc: 0.796875]\n",
            "1219: [D loss: 0.052280, acc: 0.968750]  [G loss: 1.001336, acc: 0.718750]\n",
            "1220: [D loss: 0.123039, acc: 0.968750]  [G loss: 0.944886, acc: 0.687500]\n",
            "1221: [D loss: 0.063580, acc: 0.984375]  [G loss: 0.652361, acc: 0.812500]\n",
            "1222: [D loss: 0.116380, acc: 0.968750]  [G loss: 0.147735, acc: 0.937500]\n",
            "1223: [D loss: 0.065914, acc: 0.984375]  [G loss: 0.101478, acc: 0.968750]\n",
            "1224: [D loss: 0.108699, acc: 0.968750]  [G loss: 0.102537, acc: 0.984375]\n",
            "1225: [D loss: 0.099226, acc: 0.960938]  [G loss: 0.342182, acc: 0.859375]\n",
            "1226: [D loss: 0.048152, acc: 0.984375]  [G loss: 0.264614, acc: 0.875000]\n",
            "1227: [D loss: 0.085443, acc: 0.960938]  [G loss: 0.732826, acc: 0.656250]\n",
            "1228: [D loss: 0.158287, acc: 0.960938]  [G loss: 0.610329, acc: 0.687500]\n",
            "1229: [D loss: 0.031108, acc: 0.992188]  [G loss: 0.870660, acc: 0.640625]\n",
            "1230: [D loss: 0.067307, acc: 0.984375]  [G loss: 0.820032, acc: 0.593750]\n",
            "1231: [D loss: 0.016303, acc: 1.000000]  [G loss: 0.640304, acc: 0.750000]\n",
            "1232: [D loss: 0.050144, acc: 0.976562]  [G loss: 0.753780, acc: 0.718750]\n",
            "1233: [D loss: 0.071223, acc: 0.984375]  [G loss: 0.666110, acc: 0.734375]\n",
            "1234: [D loss: 0.037287, acc: 0.984375]  [G loss: 0.505725, acc: 0.750000]\n",
            "1235: [D loss: 0.041496, acc: 0.984375]  [G loss: 1.305244, acc: 0.609375]\n",
            "1236: [D loss: 0.033298, acc: 0.992188]  [G loss: 1.607723, acc: 0.437500]\n",
            "1237: [D loss: 0.030976, acc: 0.984375]  [G loss: 2.278643, acc: 0.312500]\n",
            "1238: [D loss: 0.162998, acc: 0.945312]  [G loss: 2.022713, acc: 0.359375]\n",
            "1239: [D loss: 0.039291, acc: 0.984375]  [G loss: 1.508306, acc: 0.531250]\n",
            "1240: [D loss: 0.078627, acc: 0.992188]  [G loss: 1.215804, acc: 0.500000]\n",
            "1241: [D loss: 0.112209, acc: 0.953125]  [G loss: 1.219555, acc: 0.515625]\n",
            "1242: [D loss: 0.086883, acc: 0.968750]  [G loss: 0.738841, acc: 0.656250]\n",
            "1243: [D loss: 0.100701, acc: 0.968750]  [G loss: 0.804936, acc: 0.718750]\n",
            "1244: [D loss: 0.104994, acc: 0.976562]  [G loss: 1.017599, acc: 0.562500]\n",
            "1245: [D loss: 0.101262, acc: 0.968750]  [G loss: 1.500433, acc: 0.421875]\n",
            "1246: [D loss: 0.027923, acc: 0.992188]  [G loss: 2.025177, acc: 0.359375]\n",
            "1247: [D loss: 0.115627, acc: 0.968750]  [G loss: 2.908620, acc: 0.234375]\n",
            "1248: [D loss: 0.094253, acc: 0.968750]  [G loss: 2.660988, acc: 0.281250]\n",
            "1249: [D loss: 0.100422, acc: 0.976562]  [G loss: 1.669788, acc: 0.375000]\n",
            "1250: [D loss: 0.041336, acc: 0.984375]  [G loss: 1.444715, acc: 0.562500]\n",
            "1251: [D loss: 0.077170, acc: 0.968750]  [G loss: 1.220078, acc: 0.515625]\n",
            "1252: [D loss: 0.110744, acc: 0.953125]  [G loss: 2.383769, acc: 0.312500]\n",
            "1253: [D loss: 0.092105, acc: 0.976562]  [G loss: 2.952257, acc: 0.234375]\n",
            "1254: [D loss: 0.096051, acc: 0.968750]  [G loss: 4.177373, acc: 0.187500]\n",
            "1255: [D loss: 0.031907, acc: 0.984375]  [G loss: 3.737896, acc: 0.171875]\n",
            "1256: [D loss: 0.051304, acc: 0.984375]  [G loss: 3.704383, acc: 0.109375]\n",
            "1257: [D loss: 0.072768, acc: 0.968750]  [G loss: 3.468398, acc: 0.203125]\n",
            "1258: [D loss: 0.147558, acc: 0.937500]  [G loss: 4.068130, acc: 0.171875]\n",
            "1259: [D loss: 0.059490, acc: 0.976562]  [G loss: 3.819944, acc: 0.218750]\n",
            "1260: [D loss: 0.040966, acc: 0.992188]  [G loss: 4.189004, acc: 0.125000]\n",
            "1261: [D loss: 0.069238, acc: 0.984375]  [G loss: 4.846311, acc: 0.140625]\n",
            "1262: [D loss: 0.133726, acc: 0.960938]  [G loss: 3.835002, acc: 0.218750]\n",
            "1263: [D loss: 0.049290, acc: 0.984375]  [G loss: 3.518086, acc: 0.281250]\n",
            "1264: [D loss: 0.112613, acc: 0.976562]  [G loss: 4.037379, acc: 0.187500]\n",
            "1265: [D loss: 0.090794, acc: 0.960938]  [G loss: 4.446323, acc: 0.187500]\n",
            "1266: [D loss: 0.084581, acc: 0.992188]  [G loss: 5.738511, acc: 0.109375]\n",
            "1267: [D loss: 0.135133, acc: 0.960938]  [G loss: 6.547847, acc: 0.078125]\n",
            "1268: [D loss: 0.071351, acc: 0.968750]  [G loss: 6.669012, acc: 0.046875]\n",
            "1269: [D loss: 0.291209, acc: 0.953125]  [G loss: 5.022362, acc: 0.093750]\n",
            "1270: [D loss: 0.119775, acc: 0.929688]  [G loss: 4.051637, acc: 0.218750]\n",
            "1271: [D loss: 0.228024, acc: 0.937500]  [G loss: 3.953286, acc: 0.203125]\n",
            "1272: [D loss: 0.122415, acc: 0.953125]  [G loss: 5.561048, acc: 0.140625]\n",
            "1273: [D loss: 0.075380, acc: 0.968750]  [G loss: 6.964556, acc: 0.109375]\n",
            "1274: [D loss: 0.116576, acc: 0.929688]  [G loss: 7.928530, acc: 0.046875]\n",
            "1275: [D loss: 0.163134, acc: 0.960938]  [G loss: 6.015036, acc: 0.093750]\n",
            "1276: [D loss: 0.077082, acc: 0.968750]  [G loss: 3.534518, acc: 0.265625]\n",
            "1277: [D loss: 0.191171, acc: 0.937500]  [G loss: 3.492053, acc: 0.250000]\n",
            "1278: [D loss: 0.221152, acc: 0.945312]  [G loss: 4.492935, acc: 0.187500]\n",
            "1279: [D loss: 0.054641, acc: 0.976562]  [G loss: 5.946648, acc: 0.078125]\n",
            "1280: [D loss: 0.130746, acc: 0.953125]  [G loss: 5.655076, acc: 0.093750]\n",
            "1281: [D loss: 0.038572, acc: 0.984375]  [G loss: 5.856169, acc: 0.093750]\n",
            "1282: [D loss: 0.206546, acc: 0.945312]  [G loss: 4.353093, acc: 0.187500]\n",
            "1283: [D loss: 0.062366, acc: 0.968750]  [G loss: 2.926842, acc: 0.234375]\n",
            "1284: [D loss: 0.282100, acc: 0.914062]  [G loss: 4.958868, acc: 0.156250]\n",
            "1285: [D loss: 0.125104, acc: 0.960938]  [G loss: 6.526125, acc: 0.093750]\n",
            "1286: [D loss: 0.127569, acc: 0.968750]  [G loss: 6.735621, acc: 0.125000]\n",
            "1287: [D loss: 0.207579, acc: 0.945312]  [G loss: 5.259320, acc: 0.187500]\n",
            "1288: [D loss: 0.093089, acc: 0.960938]  [G loss: 4.080143, acc: 0.343750]\n",
            "1289: [D loss: 0.264264, acc: 0.906250]  [G loss: 2.299964, acc: 0.484375]\n",
            "1290: [D loss: 0.252802, acc: 0.929688]  [G loss: 1.673783, acc: 0.515625]\n",
            "1291: [D loss: 0.231434, acc: 0.914062]  [G loss: 2.580216, acc: 0.359375]\n",
            "1292: [D loss: 0.204617, acc: 0.937500]  [G loss: 3.631270, acc: 0.234375]\n",
            "1293: [D loss: 0.156487, acc: 0.937500]  [G loss: 4.106091, acc: 0.156250]\n",
            "1294: [D loss: 0.137717, acc: 0.945312]  [G loss: 3.548697, acc: 0.234375]\n",
            "1295: [D loss: 0.228965, acc: 0.906250]  [G loss: 2.013621, acc: 0.406250]\n",
            "1296: [D loss: 0.197468, acc: 0.929688]  [G loss: 1.357772, acc: 0.562500]\n",
            "1297: [D loss: 0.192202, acc: 0.906250]  [G loss: 1.418557, acc: 0.484375]\n",
            "1298: [D loss: 0.233349, acc: 0.937500]  [G loss: 2.680556, acc: 0.265625]\n",
            "1299: [D loss: 0.392525, acc: 0.890625]  [G loss: 2.757060, acc: 0.234375]\n",
            "1300: [D loss: 0.159259, acc: 0.921875]  [G loss: 2.820793, acc: 0.203125]\n",
            "1301: [D loss: 0.251255, acc: 0.898438]  [G loss: 2.099357, acc: 0.375000]\n",
            "1302: [D loss: 0.212569, acc: 0.921875]  [G loss: 1.739697, acc: 0.421875]\n",
            "1303: [D loss: 0.215943, acc: 0.921875]  [G loss: 1.970370, acc: 0.390625]\n",
            "1304: [D loss: 0.105078, acc: 0.968750]  [G loss: 2.154678, acc: 0.265625]\n",
            "1305: [D loss: 0.095269, acc: 0.953125]  [G loss: 2.342560, acc: 0.218750]\n",
            "1306: [D loss: 0.097551, acc: 0.960938]  [G loss: 2.589191, acc: 0.281250]\n",
            "1307: [D loss: 0.158647, acc: 0.945312]  [G loss: 2.920300, acc: 0.218750]\n",
            "1308: [D loss: 0.079666, acc: 0.968750]  [G loss: 3.047566, acc: 0.156250]\n",
            "1309: [D loss: 0.103258, acc: 0.945312]  [G loss: 3.127523, acc: 0.250000]\n",
            "1310: [D loss: 0.057978, acc: 0.968750]  [G loss: 2.396999, acc: 0.328125]\n",
            "1311: [D loss: 0.038262, acc: 1.000000]  [G loss: 2.141473, acc: 0.343750]\n",
            "1312: [D loss: 0.126349, acc: 0.953125]  [G loss: 2.546022, acc: 0.421875]\n",
            "1313: [D loss: 0.087177, acc: 0.960938]  [G loss: 2.678171, acc: 0.375000]\n",
            "1314: [D loss: 0.069727, acc: 0.976562]  [G loss: 3.018953, acc: 0.390625]\n",
            "1315: [D loss: 0.047173, acc: 0.984375]  [G loss: 2.970371, acc: 0.390625]\n",
            "1316: [D loss: 0.121866, acc: 0.945312]  [G loss: 3.250876, acc: 0.328125]\n",
            "1317: [D loss: 0.116564, acc: 0.960938]  [G loss: 3.117486, acc: 0.390625]\n",
            "1318: [D loss: 0.049593, acc: 0.992188]  [G loss: 2.583028, acc: 0.359375]\n",
            "1319: [D loss: 0.132833, acc: 0.945312]  [G loss: 2.105532, acc: 0.468750]\n",
            "1320: [D loss: 0.123955, acc: 0.976562]  [G loss: 2.016282, acc: 0.453125]\n",
            "1321: [D loss: 0.112977, acc: 0.945312]  [G loss: 2.176638, acc: 0.390625]\n",
            "1322: [D loss: 0.214655, acc: 0.937500]  [G loss: 2.003964, acc: 0.328125]\n",
            "1323: [D loss: 0.062582, acc: 0.976562]  [G loss: 2.045726, acc: 0.265625]\n",
            "1324: [D loss: 0.175763, acc: 0.929688]  [G loss: 1.737042, acc: 0.453125]\n",
            "1325: [D loss: 0.179896, acc: 0.953125]  [G loss: 1.744244, acc: 0.406250]\n",
            "1326: [D loss: 0.110889, acc: 0.960938]  [G loss: 1.930922, acc: 0.406250]\n",
            "1327: [D loss: 0.133163, acc: 0.960938]  [G loss: 2.808436, acc: 0.281250]\n",
            "1328: [D loss: 0.109987, acc: 0.960938]  [G loss: 2.587544, acc: 0.375000]\n",
            "1329: [D loss: 0.110599, acc: 0.960938]  [G loss: 2.209635, acc: 0.390625]\n",
            "1330: [D loss: 0.109041, acc: 0.968750]  [G loss: 1.940222, acc: 0.468750]\n",
            "1331: [D loss: 0.167581, acc: 0.921875]  [G loss: 2.688927, acc: 0.406250]\n",
            "1332: [D loss: 0.145855, acc: 0.937500]  [G loss: 2.233834, acc: 0.359375]\n",
            "1333: [D loss: 0.305476, acc: 0.890625]  [G loss: 1.771022, acc: 0.484375]\n",
            "1334: [D loss: 0.130040, acc: 0.960938]  [G loss: 1.584117, acc: 0.562500]\n",
            "1335: [D loss: 0.311721, acc: 0.898438]  [G loss: 1.415595, acc: 0.593750]\n",
            "1336: [D loss: 0.310310, acc: 0.921875]  [G loss: 1.386571, acc: 0.578125]\n",
            "1337: [D loss: 0.158500, acc: 0.921875]  [G loss: 2.318295, acc: 0.515625]\n",
            "1338: [D loss: 0.136685, acc: 0.937500]  [G loss: 1.448506, acc: 0.671875]\n",
            "1339: [D loss: 0.267006, acc: 0.890625]  [G loss: 1.305941, acc: 0.703125]\n",
            "1340: [D loss: 0.218805, acc: 0.890625]  [G loss: 1.352350, acc: 0.703125]\n",
            "1341: [D loss: 0.248057, acc: 0.929688]  [G loss: 0.995867, acc: 0.796875]\n",
            "1342: [D loss: 0.151768, acc: 0.929688]  [G loss: 1.182274, acc: 0.578125]\n",
            "1343: [D loss: 0.154650, acc: 0.937500]  [G loss: 1.273889, acc: 0.546875]\n",
            "1344: [D loss: 0.109667, acc: 0.968750]  [G loss: 0.758727, acc: 0.671875]\n",
            "1345: [D loss: 0.166090, acc: 0.937500]  [G loss: 0.955569, acc: 0.593750]\n",
            "1346: [D loss: 0.160560, acc: 0.953125]  [G loss: 1.027582, acc: 0.578125]\n",
            "1347: [D loss: 0.140304, acc: 0.968750]  [G loss: 1.489174, acc: 0.406250]\n",
            "1348: [D loss: 0.061450, acc: 0.976562]  [G loss: 1.279436, acc: 0.531250]\n",
            "1349: [D loss: 0.053100, acc: 0.976562]  [G loss: 1.927022, acc: 0.421875]\n",
            "1350: [D loss: 0.064505, acc: 0.992188]  [G loss: 1.547952, acc: 0.500000]\n",
            "1351: [D loss: 0.138231, acc: 0.953125]  [G loss: 1.238507, acc: 0.687500]\n",
            "1352: [D loss: 0.111890, acc: 0.960938]  [G loss: 1.099235, acc: 0.593750]\n",
            "1353: [D loss: 0.074674, acc: 0.984375]  [G loss: 1.487102, acc: 0.484375]\n",
            "1354: [D loss: 0.083781, acc: 0.960938]  [G loss: 1.836060, acc: 0.531250]\n",
            "1355: [D loss: 0.137666, acc: 0.968750]  [G loss: 1.665414, acc: 0.484375]\n",
            "1356: [D loss: 0.147656, acc: 0.929688]  [G loss: 1.226253, acc: 0.546875]\n",
            "1357: [D loss: 0.091156, acc: 0.960938]  [G loss: 0.847546, acc: 0.656250]\n",
            "1358: [D loss: 0.114933, acc: 0.953125]  [G loss: 1.234700, acc: 0.515625]\n",
            "1359: [D loss: 0.154500, acc: 0.953125]  [G loss: 2.793322, acc: 0.296875]\n",
            "1360: [D loss: 0.104790, acc: 0.953125]  [G loss: 2.910060, acc: 0.265625]\n",
            "1361: [D loss: 0.110777, acc: 0.976562]  [G loss: 2.893646, acc: 0.156250]\n",
            "1362: [D loss: 0.096041, acc: 0.960938]  [G loss: 2.173811, acc: 0.359375]\n",
            "1363: [D loss: 0.055084, acc: 0.984375]  [G loss: 1.438740, acc: 0.437500]\n",
            "1364: [D loss: 0.089224, acc: 0.960938]  [G loss: 1.728930, acc: 0.390625]\n",
            "1365: [D loss: 0.097884, acc: 0.960938]  [G loss: 2.310287, acc: 0.234375]\n",
            "1366: [D loss: 0.039210, acc: 0.992188]  [G loss: 3.201108, acc: 0.171875]\n",
            "1367: [D loss: 0.137088, acc: 0.929688]  [G loss: 3.083492, acc: 0.171875]\n",
            "1368: [D loss: 0.098950, acc: 0.960938]  [G loss: 2.687660, acc: 0.156250]\n",
            "1369: [D loss: 0.075859, acc: 0.968750]  [G loss: 2.747141, acc: 0.140625]\n",
            "1370: [D loss: 0.110490, acc: 0.953125]  [G loss: 3.047647, acc: 0.156250]\n",
            "1371: [D loss: 0.101813, acc: 0.937500]  [G loss: 4.059894, acc: 0.062500]\n",
            "1372: [D loss: 0.073001, acc: 0.968750]  [G loss: 4.575353, acc: 0.031250]\n",
            "1373: [D loss: 0.168856, acc: 0.921875]  [G loss: 4.315166, acc: 0.015625]\n",
            "1374: [D loss: 0.096796, acc: 0.976562]  [G loss: 3.609409, acc: 0.046875]\n",
            "1375: [D loss: 0.055719, acc: 0.968750]  [G loss: 3.374565, acc: 0.156250]\n",
            "1376: [D loss: 0.086474, acc: 0.960938]  [G loss: 3.593153, acc: 0.125000]\n",
            "1377: [D loss: 0.060941, acc: 0.960938]  [G loss: 4.045338, acc: 0.062500]\n",
            "1378: [D loss: 0.054590, acc: 0.992188]  [G loss: 4.873610, acc: 0.078125]\n",
            "1379: [D loss: 0.066317, acc: 0.968750]  [G loss: 6.358630, acc: 0.031250]\n",
            "1380: [D loss: 0.062301, acc: 0.968750]  [G loss: 6.193806, acc: 0.031250]\n",
            "1381: [D loss: 0.162859, acc: 0.937500]  [G loss: 6.408617, acc: 0.031250]\n",
            "1382: [D loss: 0.035128, acc: 0.992188]  [G loss: 5.724851, acc: 0.015625]\n",
            "1383: [D loss: 0.087878, acc: 0.968750]  [G loss: 4.695802, acc: 0.093750]\n",
            "1384: [D loss: 0.115997, acc: 0.953125]  [G loss: 4.340429, acc: 0.109375]\n",
            "1385: [D loss: 0.169915, acc: 0.921875]  [G loss: 4.938391, acc: 0.015625]\n",
            "1386: [D loss: 0.119610, acc: 0.960938]  [G loss: 6.375775, acc: 0.015625]\n",
            "1387: [D loss: 0.150827, acc: 0.968750]  [G loss: 8.477041, acc: 0.000000]\n",
            "1388: [D loss: 0.137788, acc: 0.953125]  [G loss: 8.180515, acc: 0.000000]\n",
            "1389: [D loss: 0.131229, acc: 0.937500]  [G loss: 6.292126, acc: 0.000000]\n",
            "1390: [D loss: 0.127947, acc: 0.976562]  [G loss: 4.256597, acc: 0.109375]\n",
            "1391: [D loss: 0.098603, acc: 0.960938]  [G loss: 3.941194, acc: 0.203125]\n",
            "1392: [D loss: 0.123553, acc: 0.945312]  [G loss: 4.049933, acc: 0.140625]\n",
            "1393: [D loss: 0.144044, acc: 0.945312]  [G loss: 4.622611, acc: 0.171875]\n",
            "1394: [D loss: 0.197327, acc: 0.929688]  [G loss: 3.540772, acc: 0.234375]\n",
            "1395: [D loss: 0.238160, acc: 0.921875]  [G loss: 3.920221, acc: 0.281250]\n",
            "1396: [D loss: 0.269369, acc: 0.906250]  [G loss: 3.111700, acc: 0.328125]\n",
            "1397: [D loss: 0.303420, acc: 0.906250]  [G loss: 3.131588, acc: 0.218750]\n",
            "1398: [D loss: 0.264341, acc: 0.937500]  [G loss: 3.263975, acc: 0.234375]\n",
            "1399: [D loss: 0.213226, acc: 0.921875]  [G loss: 2.872268, acc: 0.171875]\n",
            "1400: [D loss: 0.269706, acc: 0.906250]  [G loss: 2.316006, acc: 0.296875]\n",
            "1401: [D loss: 0.360744, acc: 0.875000]  [G loss: 3.308453, acc: 0.234375]\n",
            "1402: [D loss: 0.149722, acc: 0.945312]  [G loss: 3.112395, acc: 0.234375]\n",
            "1403: [D loss: 0.350598, acc: 0.898438]  [G loss: 2.378326, acc: 0.343750]\n",
            "1404: [D loss: 0.158316, acc: 0.960938]  [G loss: 1.986457, acc: 0.421875]\n",
            "1405: [D loss: 0.284339, acc: 0.867188]  [G loss: 2.885152, acc: 0.296875]\n",
            "1406: [D loss: 0.127791, acc: 0.960938]  [G loss: 4.643208, acc: 0.156250]\n",
            "1407: [D loss: 0.227816, acc: 0.906250]  [G loss: 3.511692, acc: 0.343750]\n",
            "1408: [D loss: 0.249034, acc: 0.906250]  [G loss: 1.993139, acc: 0.484375]\n",
            "1409: [D loss: 0.381687, acc: 0.851562]  [G loss: 2.010031, acc: 0.453125]\n",
            "1410: [D loss: 0.167750, acc: 0.937500]  [G loss: 3.507092, acc: 0.312500]\n",
            "1411: [D loss: 0.192319, acc: 0.929688]  [G loss: 4.940549, acc: 0.140625]\n",
            "1412: [D loss: 0.197237, acc: 0.929688]  [G loss: 3.456904, acc: 0.281250]\n",
            "1413: [D loss: 0.105795, acc: 0.929688]  [G loss: 2.438743, acc: 0.359375]\n",
            "1414: [D loss: 0.127522, acc: 0.968750]  [G loss: 2.238057, acc: 0.500000]\n",
            "1415: [D loss: 0.112114, acc: 0.953125]  [G loss: 2.278831, acc: 0.421875]\n",
            "1416: [D loss: 0.075630, acc: 0.968750]  [G loss: 1.892122, acc: 0.406250]\n",
            "1417: [D loss: 0.159747, acc: 0.929688]  [G loss: 2.256297, acc: 0.421875]\n",
            "1418: [D loss: 0.094758, acc: 0.953125]  [G loss: 1.863298, acc: 0.421875]\n",
            "1419: [D loss: 0.060494, acc: 0.976562]  [G loss: 1.314354, acc: 0.562500]\n",
            "1420: [D loss: 0.081596, acc: 0.976562]  [G loss: 0.830053, acc: 0.718750]\n",
            "1421: [D loss: 0.211353, acc: 0.906250]  [G loss: 0.914591, acc: 0.671875]\n",
            "1422: [D loss: 0.199670, acc: 0.921875]  [G loss: 1.834614, acc: 0.500000]\n",
            "1423: [D loss: 0.162041, acc: 0.960938]  [G loss: 2.746797, acc: 0.312500]\n",
            "1424: [D loss: 0.079485, acc: 0.976562]  [G loss: 2.372204, acc: 0.375000]\n",
            "1425: [D loss: 0.153825, acc: 0.968750]  [G loss: 1.713538, acc: 0.546875]\n",
            "1426: [D loss: 0.093342, acc: 0.945312]  [G loss: 1.228309, acc: 0.671875]\n",
            "1427: [D loss: 0.240979, acc: 0.898438]  [G loss: 1.712666, acc: 0.515625]\n",
            "1428: [D loss: 0.115134, acc: 0.960938]  [G loss: 2.717741, acc: 0.453125]\n",
            "1429: [D loss: 0.126877, acc: 0.945312]  [G loss: 3.924378, acc: 0.343750]\n",
            "1430: [D loss: 0.129349, acc: 0.953125]  [G loss: 3.235107, acc: 0.375000]\n",
            "1431: [D loss: 0.280925, acc: 0.914062]  [G loss: 2.081961, acc: 0.468750]\n",
            "1432: [D loss: 0.163741, acc: 0.929688]  [G loss: 1.125421, acc: 0.625000]\n",
            "1433: [D loss: 0.281566, acc: 0.882812]  [G loss: 1.218931, acc: 0.578125]\n",
            "1434: [D loss: 0.176964, acc: 0.937500]  [G loss: 2.244227, acc: 0.468750]\n",
            "1435: [D loss: 0.092632, acc: 0.968750]  [G loss: 1.791626, acc: 0.484375]\n",
            "1436: [D loss: 0.118672, acc: 0.968750]  [G loss: 3.132639, acc: 0.375000]\n",
            "1437: [D loss: 0.091292, acc: 0.960938]  [G loss: 2.303250, acc: 0.578125]\n",
            "1438: [D loss: 0.125226, acc: 0.953125]  [G loss: 1.954909, acc: 0.484375]\n",
            "1439: [D loss: 0.142236, acc: 0.937500]  [G loss: 1.924533, acc: 0.468750]\n",
            "1440: [D loss: 0.239333, acc: 0.929688]  [G loss: 2.036313, acc: 0.578125]\n",
            "1441: [D loss: 0.133747, acc: 0.953125]  [G loss: 1.352815, acc: 0.593750]\n",
            "1442: [D loss: 0.096396, acc: 0.945312]  [G loss: 2.102071, acc: 0.421875]\n",
            "1443: [D loss: 0.102290, acc: 0.937500]  [G loss: 1.936932, acc: 0.437500]\n",
            "1444: [D loss: 0.143871, acc: 0.937500]  [G loss: 2.679364, acc: 0.343750]\n",
            "1445: [D loss: 0.096724, acc: 0.960938]  [G loss: 2.743814, acc: 0.281250]\n",
            "1446: [D loss: 0.147541, acc: 0.945312]  [G loss: 2.723095, acc: 0.390625]\n",
            "1447: [D loss: 0.230959, acc: 0.906250]  [G loss: 2.892820, acc: 0.296875]\n",
            "1448: [D loss: 0.201793, acc: 0.929688]  [G loss: 3.044816, acc: 0.328125]\n",
            "1449: [D loss: 0.077020, acc: 0.968750]  [G loss: 3.291662, acc: 0.312500]\n",
            "1450: [D loss: 0.279245, acc: 0.914062]  [G loss: 3.034256, acc: 0.312500]\n",
            "1451: [D loss: 0.153382, acc: 0.953125]  [G loss: 3.869050, acc: 0.296875]\n",
            "1452: [D loss: 0.131749, acc: 0.960938]  [G loss: 3.180065, acc: 0.343750]\n",
            "1453: [D loss: 0.175213, acc: 0.953125]  [G loss: 2.528349, acc: 0.390625]\n",
            "1454: [D loss: 0.289800, acc: 0.921875]  [G loss: 2.164136, acc: 0.468750]\n",
            "1455: [D loss: 0.341769, acc: 0.867188]  [G loss: 2.451407, acc: 0.453125]\n",
            "1456: [D loss: 0.173805, acc: 0.937500]  [G loss: 2.694376, acc: 0.421875]\n",
            "1457: [D loss: 0.300294, acc: 0.906250]  [G loss: 3.364488, acc: 0.250000]\n",
            "1458: [D loss: 0.289043, acc: 0.921875]  [G loss: 2.671021, acc: 0.359375]\n",
            "1459: [D loss: 0.134486, acc: 0.968750]  [G loss: 2.126390, acc: 0.468750]\n",
            "1460: [D loss: 0.155245, acc: 0.937500]  [G loss: 1.805275, acc: 0.500000]\n",
            "1461: [D loss: 0.152576, acc: 0.937500]  [G loss: 2.614359, acc: 0.390625]\n",
            "1462: [D loss: 0.074717, acc: 0.976562]  [G loss: 2.665560, acc: 0.359375]\n",
            "1463: [D loss: 0.092370, acc: 0.984375]  [G loss: 3.527322, acc: 0.218750]\n",
            "1464: [D loss: 0.127659, acc: 0.953125]  [G loss: 3.996938, acc: 0.218750]\n",
            "1465: [D loss: 0.165697, acc: 0.945312]  [G loss: 3.473964, acc: 0.343750]\n",
            "1466: [D loss: 0.052503, acc: 0.992188]  [G loss: 2.925093, acc: 0.265625]\n",
            "1467: [D loss: 0.093105, acc: 0.953125]  [G loss: 3.072684, acc: 0.359375]\n",
            "1468: [D loss: 0.093591, acc: 0.960938]  [G loss: 3.879307, acc: 0.218750]\n",
            "1469: [D loss: 0.074498, acc: 0.968750]  [G loss: 4.039700, acc: 0.218750]\n",
            "1470: [D loss: 0.078261, acc: 0.968750]  [G loss: 4.123715, acc: 0.265625]\n",
            "1471: [D loss: 0.102232, acc: 0.960938]  [G loss: 4.333239, acc: 0.203125]\n",
            "1472: [D loss: 0.224022, acc: 0.945312]  [G loss: 3.251380, acc: 0.312500]\n",
            "1473: [D loss: 0.138756, acc: 0.953125]  [G loss: 3.095847, acc: 0.296875]\n",
            "1474: [D loss: 0.208614, acc: 0.937500]  [G loss: 2.474293, acc: 0.421875]\n",
            "1475: [D loss: 0.213859, acc: 0.882812]  [G loss: 2.877654, acc: 0.421875]\n",
            "1476: [D loss: 0.162909, acc: 0.953125]  [G loss: 3.355961, acc: 0.296875]\n",
            "1477: [D loss: 0.114781, acc: 0.953125]  [G loss: 4.239952, acc: 0.234375]\n",
            "1478: [D loss: 0.162813, acc: 0.953125]  [G loss: 4.219427, acc: 0.218750]\n",
            "1479: [D loss: 0.068191, acc: 0.976562]  [G loss: 3.849332, acc: 0.328125]\n",
            "1480: [D loss: 0.137454, acc: 0.953125]  [G loss: 2.961047, acc: 0.375000]\n",
            "1481: [D loss: 0.055516, acc: 0.968750]  [G loss: 2.948643, acc: 0.265625]\n",
            "1482: [D loss: 0.169909, acc: 0.937500]  [G loss: 3.015314, acc: 0.328125]\n",
            "1483: [D loss: 0.156570, acc: 0.937500]  [G loss: 3.632537, acc: 0.281250]\n",
            "1484: [D loss: 0.170859, acc: 0.937500]  [G loss: 3.246998, acc: 0.312500]\n",
            "1485: [D loss: 0.206537, acc: 0.921875]  [G loss: 2.020738, acc: 0.406250]\n",
            "1486: [D loss: 0.118378, acc: 0.953125]  [G loss: 1.375843, acc: 0.609375]\n",
            "1487: [D loss: 0.060020, acc: 0.992188]  [G loss: 1.195509, acc: 0.640625]\n",
            "1488: [D loss: 0.163042, acc: 0.937500]  [G loss: 1.473729, acc: 0.531250]\n",
            "1489: [D loss: 0.125663, acc: 0.960938]  [G loss: 1.531331, acc: 0.609375]\n",
            "1490: [D loss: 0.061210, acc: 0.976562]  [G loss: 1.973413, acc: 0.500000]\n",
            "1491: [D loss: 0.013497, acc: 1.000000]  [G loss: 1.593670, acc: 0.640625]\n",
            "1492: [D loss: 0.033103, acc: 0.976562]  [G loss: 1.207904, acc: 0.609375]\n",
            "1493: [D loss: 0.030811, acc: 0.984375]  [G loss: 1.261284, acc: 0.625000]\n",
            "1494: [D loss: 0.057390, acc: 0.976562]  [G loss: 1.231906, acc: 0.656250]\n",
            "1495: [D loss: 0.062406, acc: 0.976562]  [G loss: 0.994202, acc: 0.703125]\n",
            "1496: [D loss: 0.080030, acc: 0.976562]  [G loss: 0.743127, acc: 0.812500]\n",
            "1497: [D loss: 0.102187, acc: 0.984375]  [G loss: 0.682251, acc: 0.765625]\n",
            "1498: [D loss: 0.022835, acc: 0.992188]  [G loss: 0.808112, acc: 0.765625]\n",
            "1499: [D loss: 0.027324, acc: 0.984375]  [G loss: 0.884085, acc: 0.781250]\n",
            "1500: [D loss: 0.034571, acc: 0.992188]  [G loss: 0.607655, acc: 0.828125]\n",
            "1501: [D loss: 0.010280, acc: 1.000000]  [G loss: 0.932432, acc: 0.765625]\n",
            "1502: [D loss: 0.012583, acc: 1.000000]  [G loss: 1.368637, acc: 0.703125]\n",
            "1503: [D loss: 0.035133, acc: 0.992188]  [G loss: 0.964885, acc: 0.718750]\n",
            "1504: [D loss: 0.025912, acc: 0.992188]  [G loss: 0.848116, acc: 0.812500]\n",
            "1505: [D loss: 0.006268, acc: 1.000000]  [G loss: 0.569479, acc: 0.828125]\n",
            "1506: [D loss: 0.029540, acc: 0.992188]  [G loss: 0.774069, acc: 0.828125]\n",
            "1507: [D loss: 0.020296, acc: 0.992188]  [G loss: 0.339319, acc: 0.906250]\n",
            "1508: [D loss: 0.062303, acc: 0.984375]  [G loss: 0.537701, acc: 0.843750]\n",
            "1509: [D loss: 0.039694, acc: 0.984375]  [G loss: 0.699145, acc: 0.843750]\n",
            "1510: [D loss: 0.025478, acc: 0.984375]  [G loss: 0.699387, acc: 0.812500]\n",
            "1511: [D loss: 0.018974, acc: 0.992188]  [G loss: 0.772541, acc: 0.765625]\n",
            "1512: [D loss: 0.033565, acc: 0.992188]  [G loss: 0.980720, acc: 0.781250]\n",
            "1513: [D loss: 0.116964, acc: 0.960938]  [G loss: 0.683556, acc: 0.796875]\n",
            "1514: [D loss: 0.019068, acc: 0.992188]  [G loss: 0.523409, acc: 0.828125]\n",
            "1515: [D loss: 0.062265, acc: 0.960938]  [G loss: 0.348053, acc: 0.859375]\n",
            "1516: [D loss: 0.069176, acc: 0.992188]  [G loss: 0.536247, acc: 0.781250]\n",
            "1517: [D loss: 0.081887, acc: 0.953125]  [G loss: 1.101818, acc: 0.703125]\n",
            "1518: [D loss: 0.039518, acc: 0.992188]  [G loss: 1.598566, acc: 0.562500]\n",
            "1519: [D loss: 0.065245, acc: 0.976562]  [G loss: 1.984084, acc: 0.437500]\n",
            "1520: [D loss: 0.104776, acc: 0.960938]  [G loss: 2.293458, acc: 0.343750]\n",
            "1521: [D loss: 0.058166, acc: 0.984375]  [G loss: 2.603078, acc: 0.359375]\n",
            "1522: [D loss: 0.042262, acc: 0.984375]  [G loss: 2.109082, acc: 0.515625]\n",
            "1523: [D loss: 0.040477, acc: 0.984375]  [G loss: 2.188407, acc: 0.375000]\n",
            "1524: [D loss: 0.065515, acc: 0.976562]  [G loss: 2.856934, acc: 0.296875]\n",
            "1525: [D loss: 0.070967, acc: 0.968750]  [G loss: 3.159881, acc: 0.234375]\n",
            "1526: [D loss: 0.004851, acc: 1.000000]  [G loss: 3.621584, acc: 0.265625]\n",
            "1527: [D loss: 0.014231, acc: 0.992188]  [G loss: 3.847272, acc: 0.250000]\n",
            "1528: [D loss: 0.036070, acc: 0.976562]  [G loss: 4.008400, acc: 0.234375]\n",
            "1529: [D loss: 0.041767, acc: 0.984375]  [G loss: 3.949301, acc: 0.312500]\n",
            "1530: [D loss: 0.020918, acc: 0.992188]  [G loss: 3.782710, acc: 0.359375]\n",
            "1531: [D loss: 0.030952, acc: 0.992188]  [G loss: 3.684721, acc: 0.234375]\n",
            "1532: [D loss: 0.031813, acc: 0.984375]  [G loss: 3.222659, acc: 0.421875]\n",
            "1533: [D loss: 0.119233, acc: 0.984375]  [G loss: 3.004303, acc: 0.343750]\n",
            "1534: [D loss: 0.028390, acc: 0.992188]  [G loss: 3.255949, acc: 0.437500]\n",
            "1535: [D loss: 0.019265, acc: 0.992188]  [G loss: 3.503074, acc: 0.359375]\n",
            "1536: [D loss: 0.073000, acc: 0.968750]  [G loss: 3.678832, acc: 0.437500]\n",
            "1537: [D loss: 0.081266, acc: 0.953125]  [G loss: 3.217481, acc: 0.421875]\n",
            "1538: [D loss: 0.056822, acc: 0.976562]  [G loss: 2.958268, acc: 0.406250]\n",
            "1539: [D loss: 0.059346, acc: 0.976562]  [G loss: 3.496852, acc: 0.406250]\n",
            "1540: [D loss: 0.179834, acc: 0.960938]  [G loss: 3.568618, acc: 0.421875]\n",
            "1541: [D loss: 0.034759, acc: 0.984375]  [G loss: 3.741400, acc: 0.406250]\n",
            "1542: [D loss: 0.067029, acc: 0.968750]  [G loss: 3.129216, acc: 0.437500]\n",
            "1543: [D loss: 0.072258, acc: 0.976562]  [G loss: 4.314016, acc: 0.343750]\n",
            "1544: [D loss: 0.049826, acc: 0.976562]  [G loss: 4.385664, acc: 0.343750]\n",
            "1545: [D loss: 0.021544, acc: 0.992188]  [G loss: 5.016578, acc: 0.281250]\n",
            "1546: [D loss: 0.113508, acc: 0.968750]  [G loss: 3.835561, acc: 0.281250]\n",
            "1547: [D loss: 0.049765, acc: 0.984375]  [G loss: 3.108713, acc: 0.312500]\n",
            "1548: [D loss: 0.099566, acc: 0.945312]  [G loss: 2.424496, acc: 0.453125]\n",
            "1549: [D loss: 0.120261, acc: 0.953125]  [G loss: 1.804107, acc: 0.531250]\n",
            "1550: [D loss: 0.149994, acc: 0.953125]  [G loss: 2.293742, acc: 0.515625]\n",
            "1551: [D loss: 0.058019, acc: 0.984375]  [G loss: 2.678519, acc: 0.359375]\n",
            "1552: [D loss: 0.073133, acc: 0.960938]  [G loss: 3.405988, acc: 0.250000]\n",
            "1553: [D loss: 0.096087, acc: 0.984375]  [G loss: 4.263508, acc: 0.171875]\n",
            "1554: [D loss: 0.315097, acc: 0.937500]  [G loss: 3.354847, acc: 0.187500]\n",
            "1555: [D loss: 0.127080, acc: 0.968750]  [G loss: 1.918445, acc: 0.390625]\n",
            "1556: [D loss: 0.162781, acc: 0.945312]  [G loss: 1.555348, acc: 0.593750]\n",
            "1557: [D loss: 0.294366, acc: 0.906250]  [G loss: 1.095853, acc: 0.625000]\n",
            "1558: [D loss: 0.274337, acc: 0.890625]  [G loss: 2.694749, acc: 0.359375]\n",
            "1559: [D loss: 0.165603, acc: 0.945312]  [G loss: 3.879832, acc: 0.234375]\n",
            "1560: [D loss: 0.263287, acc: 0.929688]  [G loss: 3.519323, acc: 0.281250]\n",
            "1561: [D loss: 0.501075, acc: 0.867188]  [G loss: 1.175899, acc: 0.609375]\n",
            "1562: [D loss: 0.488521, acc: 0.812500]  [G loss: 0.846338, acc: 0.718750]\n",
            "1563: [D loss: 0.403159, acc: 0.859375]  [G loss: 2.116033, acc: 0.312500]\n",
            "1564: [D loss: 0.497284, acc: 0.851562]  [G loss: 3.119825, acc: 0.203125]\n",
            "1565: [D loss: 0.427646, acc: 0.890625]  [G loss: 1.962379, acc: 0.312500]\n",
            "1566: [D loss: 0.269678, acc: 0.921875]  [G loss: 1.578707, acc: 0.500000]\n",
            "1567: [D loss: 0.190728, acc: 0.921875]  [G loss: 0.969855, acc: 0.609375]\n",
            "1568: [D loss: 0.254486, acc: 0.875000]  [G loss: 1.104856, acc: 0.546875]\n",
            "1569: [D loss: 0.206569, acc: 0.906250]  [G loss: 2.077231, acc: 0.328125]\n",
            "1570: [D loss: 0.188348, acc: 0.929688]  [G loss: 2.854603, acc: 0.156250]\n",
            "1571: [D loss: 0.136845, acc: 0.929688]  [G loss: 2.780707, acc: 0.234375]\n",
            "1572: [D loss: 0.253810, acc: 0.890625]  [G loss: 1.119630, acc: 0.609375]\n",
            "1573: [D loss: 0.202821, acc: 0.937500]  [G loss: 0.899709, acc: 0.640625]\n",
            "1574: [D loss: 0.130293, acc: 0.953125]  [G loss: 1.142352, acc: 0.562500]\n",
            "1575: [D loss: 0.151749, acc: 0.945312]  [G loss: 1.289111, acc: 0.468750]\n",
            "1576: [D loss: 0.124228, acc: 0.968750]  [G loss: 1.734255, acc: 0.453125]\n",
            "1577: [D loss: 0.116315, acc: 0.953125]  [G loss: 2.006252, acc: 0.406250]\n",
            "1578: [D loss: 0.134697, acc: 0.953125]  [G loss: 1.791794, acc: 0.421875]\n",
            "1579: [D loss: 0.193695, acc: 0.929688]  [G loss: 1.409213, acc: 0.515625]\n",
            "1580: [D loss: 0.154761, acc: 0.953125]  [G loss: 1.489640, acc: 0.468750]\n",
            "1581: [D loss: 0.111416, acc: 0.953125]  [G loss: 1.744553, acc: 0.437500]\n",
            "1582: [D loss: 0.100316, acc: 0.953125]  [G loss: 2.331478, acc: 0.343750]\n",
            "1583: [D loss: 0.184933, acc: 0.945312]  [G loss: 1.915077, acc: 0.421875]\n",
            "1584: [D loss: 0.257828, acc: 0.929688]  [G loss: 1.124601, acc: 0.593750]\n",
            "1585: [D loss: 0.158778, acc: 0.937500]  [G loss: 1.375669, acc: 0.500000]\n",
            "1586: [D loss: 0.196839, acc: 0.906250]  [G loss: 1.799598, acc: 0.421875]\n",
            "1587: [D loss: 0.106928, acc: 0.953125]  [G loss: 2.893402, acc: 0.328125]\n",
            "1588: [D loss: 0.182053, acc: 0.945312]  [G loss: 2.165620, acc: 0.390625]\n",
            "1589: [D loss: 0.221768, acc: 0.945312]  [G loss: 1.339510, acc: 0.562500]\n",
            "1590: [D loss: 0.063588, acc: 0.984375]  [G loss: 2.102703, acc: 0.453125]\n",
            "1591: [D loss: 0.131387, acc: 0.937500]  [G loss: 2.537144, acc: 0.343750]\n",
            "1592: [D loss: 0.119631, acc: 0.968750]  [G loss: 2.578118, acc: 0.250000]\n",
            "1593: [D loss: 0.100742, acc: 0.953125]  [G loss: 2.961071, acc: 0.250000]\n",
            "1594: [D loss: 0.107257, acc: 0.945312]  [G loss: 2.417542, acc: 0.234375]\n",
            "1595: [D loss: 0.079250, acc: 0.984375]  [G loss: 2.210315, acc: 0.343750]\n",
            "1596: [D loss: 0.119804, acc: 0.960938]  [G loss: 2.312731, acc: 0.312500]\n",
            "1597: [D loss: 0.062312, acc: 0.984375]  [G loss: 3.518348, acc: 0.265625]\n",
            "1598: [D loss: 0.145779, acc: 0.953125]  [G loss: 3.465723, acc: 0.328125]\n",
            "1599: [D loss: 0.148084, acc: 0.945312]  [G loss: 3.136021, acc: 0.234375]\n",
            "1600: [D loss: 0.202562, acc: 0.960938]  [G loss: 2.523882, acc: 0.328125]\n",
            "1601: [D loss: 0.104329, acc: 0.960938]  [G loss: 1.938993, acc: 0.390625]\n",
            "1602: [D loss: 0.233258, acc: 0.921875]  [G loss: 2.591875, acc: 0.343750]\n",
            "1603: [D loss: 0.181611, acc: 0.921875]  [G loss: 4.798470, acc: 0.078125]\n",
            "1604: [D loss: 0.159378, acc: 0.960938]  [G loss: 7.355427, acc: 0.062500]\n",
            "1605: [D loss: 0.215093, acc: 0.953125]  [G loss: 6.625787, acc: 0.078125]\n",
            "1606: [D loss: 0.345807, acc: 0.914062]  [G loss: 3.306950, acc: 0.234375]\n",
            "1607: [D loss: 0.072366, acc: 0.968750]  [G loss: 1.899290, acc: 0.453125]\n",
            "1608: [D loss: 0.277710, acc: 0.898438]  [G loss: 2.957723, acc: 0.296875]\n",
            "1609: [D loss: 0.078899, acc: 0.968750]  [G loss: 3.576594, acc: 0.218750]\n",
            "1610: [D loss: 0.169051, acc: 0.929688]  [G loss: 4.723185, acc: 0.125000]\n",
            "1611: [D loss: 0.073349, acc: 0.976562]  [G loss: 5.125562, acc: 0.078125]\n",
            "1612: [D loss: 0.163291, acc: 0.960938]  [G loss: 4.880755, acc: 0.187500]\n",
            "1613: [D loss: 0.165061, acc: 0.945312]  [G loss: 3.365510, acc: 0.250000]\n",
            "1614: [D loss: 0.082322, acc: 0.976562]  [G loss: 1.410593, acc: 0.609375]\n",
            "1615: [D loss: 0.179978, acc: 0.929688]  [G loss: 1.600011, acc: 0.531250]\n",
            "1616: [D loss: 0.180527, acc: 0.914062]  [G loss: 2.429878, acc: 0.375000]\n",
            "1617: [D loss: 0.203342, acc: 0.945312]  [G loss: 3.883994, acc: 0.203125]\n",
            "1618: [D loss: 0.162111, acc: 0.945312]  [G loss: 4.497112, acc: 0.171875]\n",
            "1619: [D loss: 0.174202, acc: 0.953125]  [G loss: 4.114313, acc: 0.203125]\n",
            "1620: [D loss: 0.165463, acc: 0.953125]  [G loss: 3.362140, acc: 0.281250]\n",
            "1621: [D loss: 0.127970, acc: 0.953125]  [G loss: 1.975011, acc: 0.437500]\n",
            "1622: [D loss: 0.187793, acc: 0.945312]  [G loss: 1.515810, acc: 0.515625]\n",
            "1623: [D loss: 0.221356, acc: 0.914062]  [G loss: 2.752144, acc: 0.328125]\n",
            "1624: [D loss: 0.077851, acc: 0.976562]  [G loss: 3.589814, acc: 0.218750]\n",
            "1625: [D loss: 0.193594, acc: 0.968750]  [G loss: 3.937945, acc: 0.218750]\n",
            "1626: [D loss: 0.192333, acc: 0.953125]  [G loss: 3.674638, acc: 0.296875]\n",
            "1627: [D loss: 0.173204, acc: 0.960938]  [G loss: 2.713584, acc: 0.328125]\n",
            "1628: [D loss: 0.068451, acc: 0.992188]  [G loss: 1.494163, acc: 0.500000]\n",
            "1629: [D loss: 0.162079, acc: 0.945312]  [G loss: 1.174820, acc: 0.593750]\n",
            "1630: [D loss: 0.086049, acc: 0.976562]  [G loss: 1.365024, acc: 0.546875]\n",
            "1631: [D loss: 0.140701, acc: 0.937500]  [G loss: 1.774968, acc: 0.437500]\n",
            "1632: [D loss: 0.121830, acc: 0.953125]  [G loss: 2.118730, acc: 0.375000]\n",
            "1633: [D loss: 0.125336, acc: 0.960938]  [G loss: 1.879972, acc: 0.484375]\n",
            "1634: [D loss: 0.082035, acc: 0.953125]  [G loss: 2.456468, acc: 0.453125]\n",
            "1635: [D loss: 0.115735, acc: 0.960938]  [G loss: 1.852942, acc: 0.468750]\n",
            "1636: [D loss: 0.086917, acc: 0.960938]  [G loss: 1.708516, acc: 0.468750]\n",
            "1637: [D loss: 0.038385, acc: 0.984375]  [G loss: 1.476630, acc: 0.546875]\n",
            "1638: [D loss: 0.055588, acc: 0.976562]  [G loss: 1.000991, acc: 0.609375]\n",
            "1639: [D loss: 0.034557, acc: 0.992188]  [G loss: 1.063891, acc: 0.687500]\n",
            "1640: [D loss: 0.039448, acc: 0.984375]  [G loss: 0.821211, acc: 0.703125]\n",
            "1641: [D loss: 0.099162, acc: 0.953125]  [G loss: 0.818734, acc: 0.734375]\n",
            "1642: [D loss: 0.080600, acc: 0.984375]  [G loss: 1.069224, acc: 0.546875]\n",
            "1643: [D loss: 0.087902, acc: 0.976562]  [G loss: 1.340671, acc: 0.453125]\n",
            "1644: [D loss: 0.066186, acc: 0.976562]  [G loss: 1.146359, acc: 0.546875]\n",
            "1645: [D loss: 0.129336, acc: 0.945312]  [G loss: 1.123523, acc: 0.593750]\n",
            "1646: [D loss: 0.057950, acc: 0.968750]  [G loss: 0.640539, acc: 0.703125]\n",
            "1647: [D loss: 0.068422, acc: 0.968750]  [G loss: 0.593894, acc: 0.718750]\n",
            "1648: [D loss: 0.085703, acc: 0.953125]  [G loss: 0.763379, acc: 0.609375]\n",
            "1649: [D loss: 0.166616, acc: 0.976562]  [G loss: 0.734208, acc: 0.765625]\n",
            "1650: [D loss: 0.112062, acc: 0.945312]  [G loss: 0.820827, acc: 0.765625]\n",
            "1651: [D loss: 0.041892, acc: 0.992188]  [G loss: 1.234979, acc: 0.625000]\n",
            "1652: [D loss: 0.049556, acc: 0.976562]  [G loss: 1.389965, acc: 0.531250]\n",
            "1653: [D loss: 0.072770, acc: 0.984375]  [G loss: 1.525841, acc: 0.546875]\n",
            "1654: [D loss: 0.053879, acc: 0.976562]  [G loss: 1.375838, acc: 0.656250]\n",
            "1655: [D loss: 0.068038, acc: 0.968750]  [G loss: 1.075318, acc: 0.703125]\n",
            "1656: [D loss: 0.036085, acc: 1.000000]  [G loss: 1.230780, acc: 0.671875]\n",
            "1657: [D loss: 0.036216, acc: 1.000000]  [G loss: 1.557008, acc: 0.625000]\n",
            "1658: [D loss: 0.054088, acc: 0.976562]  [G loss: 1.139350, acc: 0.687500]\n",
            "1659: [D loss: 0.183429, acc: 0.937500]  [G loss: 0.889655, acc: 0.796875]\n",
            "1660: [D loss: 0.061646, acc: 0.976562]  [G loss: 0.865042, acc: 0.703125]\n",
            "1661: [D loss: 0.067163, acc: 0.960938]  [G loss: 0.810028, acc: 0.734375]\n",
            "1662: [D loss: 0.065769, acc: 0.984375]  [G loss: 1.039531, acc: 0.656250]\n",
            "1663: [D loss: 0.088380, acc: 0.945312]  [G loss: 0.982656, acc: 0.703125]\n",
            "1664: [D loss: 0.075504, acc: 0.976562]  [G loss: 0.801447, acc: 0.640625]\n",
            "1665: [D loss: 0.032526, acc: 0.992188]  [G loss: 0.765172, acc: 0.750000]\n",
            "1666: [D loss: 0.046046, acc: 0.976562]  [G loss: 0.750016, acc: 0.687500]\n",
            "1667: [D loss: 0.037236, acc: 0.984375]  [G loss: 1.074173, acc: 0.609375]\n",
            "1668: [D loss: 0.081624, acc: 0.976562]  [G loss: 0.887039, acc: 0.687500]\n",
            "1669: [D loss: 0.103566, acc: 0.960938]  [G loss: 1.441088, acc: 0.500000]\n",
            "1670: [D loss: 0.084594, acc: 0.968750]  [G loss: 2.196280, acc: 0.375000]\n",
            "1671: [D loss: 0.048973, acc: 0.984375]  [G loss: 2.729637, acc: 0.296875]\n",
            "1672: [D loss: 0.119295, acc: 0.953125]  [G loss: 2.256715, acc: 0.406250]\n",
            "1673: [D loss: 0.059874, acc: 0.976562]  [G loss: 1.483987, acc: 0.515625]\n",
            "1674: [D loss: 0.108152, acc: 0.953125]  [G loss: 1.444274, acc: 0.578125]\n",
            "1675: [D loss: 0.136334, acc: 0.960938]  [G loss: 1.804298, acc: 0.390625]\n",
            "1676: [D loss: 0.115575, acc: 0.937500]  [G loss: 3.778021, acc: 0.187500]\n",
            "1677: [D loss: 0.058717, acc: 0.968750]  [G loss: 4.776857, acc: 0.125000]\n",
            "1678: [D loss: 0.060921, acc: 0.976562]  [G loss: 4.226233, acc: 0.156250]\n",
            "1679: [D loss: 0.151138, acc: 0.945312]  [G loss: 3.561103, acc: 0.218750]\n",
            "1680: [D loss: 0.175866, acc: 0.960938]  [G loss: 2.225913, acc: 0.437500]\n",
            "1681: [D loss: 0.244471, acc: 0.890625]  [G loss: 3.219773, acc: 0.203125]\n",
            "1682: [D loss: 0.068833, acc: 0.968750]  [G loss: 6.285307, acc: 0.062500]\n",
            "1683: [D loss: 0.221669, acc: 0.914062]  [G loss: 4.876811, acc: 0.109375]\n",
            "1684: [D loss: 0.104329, acc: 0.960938]  [G loss: 3.502388, acc: 0.218750]\n",
            "1685: [D loss: 0.128600, acc: 0.953125]  [G loss: 2.435108, acc: 0.328125]\n",
            "1686: [D loss: 0.133109, acc: 0.953125]  [G loss: 2.481482, acc: 0.437500]\n",
            "1687: [D loss: 0.272080, acc: 0.890625]  [G loss: 3.892291, acc: 0.250000]\n",
            "1688: [D loss: 0.090176, acc: 0.945312]  [G loss: 5.171664, acc: 0.125000]\n",
            "1689: [D loss: 0.181654, acc: 0.953125]  [G loss: 4.671528, acc: 0.281250]\n",
            "1690: [D loss: 0.077689, acc: 0.976562]  [G loss: 3.746421, acc: 0.453125]\n",
            "1691: [D loss: 0.122340, acc: 0.953125]  [G loss: 3.002632, acc: 0.515625]\n",
            "1692: [D loss: 0.217913, acc: 0.945312]  [G loss: 3.455568, acc: 0.531250]\n",
            "1693: [D loss: 0.196489, acc: 0.953125]  [G loss: 4.253392, acc: 0.421875]\n",
            "1694: [D loss: 0.062314, acc: 0.976562]  [G loss: 4.281692, acc: 0.343750]\n",
            "1695: [D loss: 0.085021, acc: 0.976562]  [G loss: 4.204578, acc: 0.343750]\n",
            "1696: [D loss: 0.042199, acc: 0.984375]  [G loss: 4.441207, acc: 0.296875]\n",
            "1697: [D loss: 0.146937, acc: 0.945312]  [G loss: 3.662058, acc: 0.343750]\n",
            "1698: [D loss: 0.202190, acc: 0.953125]  [G loss: 3.738886, acc: 0.312500]\n",
            "1699: [D loss: 0.223703, acc: 0.937500]  [G loss: 3.999164, acc: 0.312500]\n",
            "1700: [D loss: 0.192348, acc: 0.945312]  [G loss: 3.065158, acc: 0.421875]\n",
            "1701: [D loss: 0.103882, acc: 0.976562]  [G loss: 2.841425, acc: 0.468750]\n",
            "1702: [D loss: 0.153538, acc: 0.929688]  [G loss: 2.934964, acc: 0.359375]\n",
            "1703: [D loss: 0.088253, acc: 0.976562]  [G loss: 4.182239, acc: 0.359375]\n",
            "1704: [D loss: 0.091266, acc: 0.960938]  [G loss: 5.143277, acc: 0.281250]\n",
            "1705: [D loss: 0.187151, acc: 0.929688]  [G loss: 4.812949, acc: 0.312500]\n",
            "1706: [D loss: 0.048440, acc: 0.984375]  [G loss: 4.675811, acc: 0.281250]\n",
            "1707: [D loss: 0.098927, acc: 0.968750]  [G loss: 4.581295, acc: 0.312500]\n",
            "1708: [D loss: 0.051912, acc: 0.976562]  [G loss: 3.786004, acc: 0.312500]\n",
            "1709: [D loss: 0.128656, acc: 0.968750]  [G loss: 4.285367, acc: 0.265625]\n",
            "1710: [D loss: 0.060502, acc: 0.984375]  [G loss: 3.153786, acc: 0.328125]\n",
            "1711: [D loss: 0.104070, acc: 0.945312]  [G loss: 3.476611, acc: 0.203125]\n",
            "1712: [D loss: 0.323003, acc: 0.929688]  [G loss: 2.101684, acc: 0.421875]\n",
            "1713: [D loss: 0.108139, acc: 0.945312]  [G loss: 1.724937, acc: 0.406250]\n",
            "1714: [D loss: 0.096498, acc: 0.953125]  [G loss: 2.063819, acc: 0.437500]\n",
            "1715: [D loss: 0.082425, acc: 0.976562]  [G loss: 2.577361, acc: 0.343750]\n",
            "1716: [D loss: 0.111259, acc: 0.976562]  [G loss: 2.708741, acc: 0.375000]\n",
            "1717: [D loss: 0.119111, acc: 0.976562]  [G loss: 2.085912, acc: 0.468750]\n",
            "1718: [D loss: 0.185522, acc: 0.960938]  [G loss: 0.951882, acc: 0.687500]\n",
            "1719: [D loss: 0.195158, acc: 0.945312]  [G loss: 0.745037, acc: 0.734375]\n",
            "1720: [D loss: 0.259386, acc: 0.882812]  [G loss: 0.376078, acc: 0.828125]\n",
            "1721: [D loss: 0.230855, acc: 0.906250]  [G loss: 0.915964, acc: 0.609375]\n",
            "1722: [D loss: 0.240854, acc: 0.921875]  [G loss: 0.735858, acc: 0.625000]\n",
            "1723: [D loss: 0.152772, acc: 0.929688]  [G loss: 1.054544, acc: 0.531250]\n",
            "1724: [D loss: 0.288853, acc: 0.921875]  [G loss: 0.609690, acc: 0.703125]\n",
            "1725: [D loss: 0.188684, acc: 0.929688]  [G loss: 0.250717, acc: 0.875000]\n",
            "1726: [D loss: 0.216589, acc: 0.898438]  [G loss: 0.313258, acc: 0.859375]\n",
            "1727: [D loss: 0.099534, acc: 0.968750]  [G loss: 0.456829, acc: 0.796875]\n",
            "1728: [D loss: 0.144130, acc: 0.960938]  [G loss: 0.590707, acc: 0.843750]\n",
            "1729: [D loss: 0.157991, acc: 0.953125]  [G loss: 0.831615, acc: 0.781250]\n",
            "1730: [D loss: 0.087552, acc: 0.960938]  [G loss: 0.768554, acc: 0.718750]\n",
            "1731: [D loss: 0.134604, acc: 0.968750]  [G loss: 0.999974, acc: 0.718750]\n",
            "1732: [D loss: 0.165049, acc: 0.960938]  [G loss: 0.839136, acc: 0.718750]\n",
            "1733: [D loss: 0.151873, acc: 0.960938]  [G loss: 0.663009, acc: 0.703125]\n",
            "1734: [D loss: 0.169444, acc: 0.960938]  [G loss: 0.486143, acc: 0.765625]\n",
            "1735: [D loss: 0.062651, acc: 0.984375]  [G loss: 0.703547, acc: 0.703125]\n",
            "1736: [D loss: 0.107047, acc: 0.960938]  [G loss: 1.003146, acc: 0.593750]\n",
            "1737: [D loss: 0.048996, acc: 0.976562]  [G loss: 1.007146, acc: 0.531250]\n",
            "1738: [D loss: 0.060898, acc: 0.992188]  [G loss: 1.231907, acc: 0.453125]\n",
            "1739: [D loss: 0.079921, acc: 0.976562]  [G loss: 1.195446, acc: 0.421875]\n",
            "1740: [D loss: 0.033678, acc: 0.992188]  [G loss: 1.164831, acc: 0.406250]\n",
            "1741: [D loss: 0.055991, acc: 0.984375]  [G loss: 1.435921, acc: 0.421875]\n",
            "1742: [D loss: 0.014026, acc: 1.000000]  [G loss: 0.959128, acc: 0.578125]\n",
            "1743: [D loss: 0.033280, acc: 0.992188]  [G loss: 0.756424, acc: 0.656250]\n",
            "1744: [D loss: 0.075062, acc: 0.976562]  [G loss: 0.729781, acc: 0.656250]\n",
            "1745: [D loss: 0.051798, acc: 0.984375]  [G loss: 0.478695, acc: 0.765625]\n",
            "1746: [D loss: 0.039451, acc: 0.976562]  [G loss: 0.591569, acc: 0.781250]\n",
            "1747: [D loss: 0.053451, acc: 0.984375]  [G loss: 0.455302, acc: 0.828125]\n",
            "1748: [D loss: 0.052219, acc: 0.976562]  [G loss: 0.345834, acc: 0.875000]\n",
            "1749: [D loss: 0.053845, acc: 0.968750]  [G loss: 0.658368, acc: 0.750000]\n",
            "1750: [D loss: 0.024265, acc: 1.000000]  [G loss: 0.558605, acc: 0.781250]\n",
            "1751: [D loss: 0.009217, acc: 1.000000]  [G loss: 0.449345, acc: 0.734375]\n",
            "1752: [D loss: 0.071661, acc: 0.976562]  [G loss: 0.634650, acc: 0.781250]\n",
            "1753: [D loss: 0.037666, acc: 0.984375]  [G loss: 0.631615, acc: 0.796875]\n",
            "1754: [D loss: 0.082806, acc: 0.992188]  [G loss: 0.520444, acc: 0.828125]\n",
            "1755: [D loss: 0.094996, acc: 0.960938]  [G loss: 0.462571, acc: 0.828125]\n",
            "1756: [D loss: 0.080797, acc: 0.953125]  [G loss: 0.440283, acc: 0.828125]\n",
            "1757: [D loss: 0.105219, acc: 0.953125]  [G loss: 0.422065, acc: 0.812500]\n",
            "1758: [D loss: 0.050762, acc: 0.984375]  [G loss: 0.728748, acc: 0.734375]\n",
            "1759: [D loss: 0.070335, acc: 0.992188]  [G loss: 0.936050, acc: 0.656250]\n",
            "1760: [D loss: 0.098459, acc: 0.960938]  [G loss: 0.861913, acc: 0.718750]\n",
            "1761: [D loss: 0.083903, acc: 0.976562]  [G loss: 0.876233, acc: 0.718750]\n",
            "1762: [D loss: 0.051101, acc: 0.976562]  [G loss: 0.683699, acc: 0.843750]\n",
            "1763: [D loss: 0.028115, acc: 1.000000]  [G loss: 0.773607, acc: 0.765625]\n",
            "1764: [D loss: 0.101249, acc: 0.953125]  [G loss: 0.857970, acc: 0.718750]\n",
            "1765: [D loss: 0.017050, acc: 1.000000]  [G loss: 2.037097, acc: 0.562500]\n",
            "1766: [D loss: 0.029328, acc: 0.976562]  [G loss: 2.004382, acc: 0.468750]\n",
            "1767: [D loss: 0.103809, acc: 0.945312]  [G loss: 1.739928, acc: 0.578125]\n",
            "1768: [D loss: 0.054414, acc: 0.984375]  [G loss: 1.104290, acc: 0.703125]\n",
            "1769: [D loss: 0.126286, acc: 0.945312]  [G loss: 0.782301, acc: 0.812500]\n",
            "1770: [D loss: 0.148436, acc: 0.937500]  [G loss: 0.653450, acc: 0.781250]\n",
            "1771: [D loss: 0.101550, acc: 0.968750]  [G loss: 1.503120, acc: 0.562500]\n",
            "1772: [D loss: 0.161847, acc: 0.960938]  [G loss: 0.867336, acc: 0.687500]\n",
            "1773: [D loss: 0.152142, acc: 0.945312]  [G loss: 0.789323, acc: 0.765625]\n",
            "1774: [D loss: 0.198218, acc: 0.921875]  [G loss: 0.579862, acc: 0.859375]\n",
            "1775: [D loss: 0.191394, acc: 0.921875]  [G loss: 0.726541, acc: 0.765625]\n",
            "1776: [D loss: 0.314208, acc: 0.906250]  [G loss: 0.696944, acc: 0.765625]\n",
            "1777: [D loss: 0.112336, acc: 0.960938]  [G loss: 0.665410, acc: 0.750000]\n",
            "1778: [D loss: 0.248864, acc: 0.929688]  [G loss: 0.429484, acc: 0.859375]\n",
            "1779: [D loss: 0.126901, acc: 0.937500]  [G loss: 1.055354, acc: 0.671875]\n",
            "1780: [D loss: 0.360052, acc: 0.875000]  [G loss: 0.621186, acc: 0.796875]\n",
            "1781: [D loss: 0.369236, acc: 0.859375]  [G loss: 0.486414, acc: 0.890625]\n",
            "1782: [D loss: 0.227897, acc: 0.906250]  [G loss: 0.890852, acc: 0.734375]\n",
            "1783: [D loss: 0.085235, acc: 0.968750]  [G loss: 1.699306, acc: 0.500000]\n",
            "1784: [D loss: 0.127777, acc: 0.937500]  [G loss: 2.552675, acc: 0.390625]\n",
            "1785: [D loss: 0.105157, acc: 0.960938]  [G loss: 3.362414, acc: 0.312500]\n",
            "1786: [D loss: 0.111347, acc: 0.953125]  [G loss: 1.997617, acc: 0.437500]\n",
            "1787: [D loss: 0.152098, acc: 0.968750]  [G loss: 2.059452, acc: 0.515625]\n",
            "1788: [D loss: 0.111226, acc: 0.953125]  [G loss: 2.031549, acc: 0.406250]\n",
            "1789: [D loss: 0.158395, acc: 0.945312]  [G loss: 2.711591, acc: 0.375000]\n",
            "1790: [D loss: 0.058507, acc: 0.976562]  [G loss: 2.726309, acc: 0.343750]\n",
            "1791: [D loss: 0.111247, acc: 0.953125]  [G loss: 2.789711, acc: 0.484375]\n",
            "1792: [D loss: 0.108186, acc: 0.984375]  [G loss: 2.176357, acc: 0.593750]\n",
            "1793: [D loss: 0.227501, acc: 0.921875]  [G loss: 1.877613, acc: 0.625000]\n",
            "1794: [D loss: 0.130836, acc: 0.945312]  [G loss: 2.371378, acc: 0.531250]\n",
            "1795: [D loss: 0.028712, acc: 0.984375]  [G loss: 2.896748, acc: 0.515625]\n",
            "1796: [D loss: 0.098272, acc: 0.968750]  [G loss: 1.595813, acc: 0.578125]\n",
            "1797: [D loss: 0.076207, acc: 0.984375]  [G loss: 1.195667, acc: 0.578125]\n",
            "1798: [D loss: 0.115239, acc: 0.960938]  [G loss: 0.787982, acc: 0.671875]\n",
            "1799: [D loss: 0.030801, acc: 0.984375]  [G loss: 0.725423, acc: 0.750000]\n",
            "1800: [D loss: 0.147578, acc: 0.968750]  [G loss: 1.139000, acc: 0.687500]\n",
            "1801: [D loss: 0.040186, acc: 0.984375]  [G loss: 1.556834, acc: 0.593750]\n",
            "1802: [D loss: 0.183918, acc: 0.968750]  [G loss: 1.452294, acc: 0.578125]\n",
            "1803: [D loss: 0.082057, acc: 0.984375]  [G loss: 1.303310, acc: 0.703125]\n",
            "1804: [D loss: 0.159875, acc: 0.968750]  [G loss: 0.816028, acc: 0.734375]\n",
            "1805: [D loss: 0.054675, acc: 0.984375]  [G loss: 0.765107, acc: 0.750000]\n",
            "1806: [D loss: 0.056820, acc: 0.984375]  [G loss: 0.584857, acc: 0.796875]\n",
            "1807: [D loss: 0.080697, acc: 0.968750]  [G loss: 1.447616, acc: 0.578125]\n",
            "1808: [D loss: 0.053312, acc: 0.984375]  [G loss: 1.431605, acc: 0.656250]\n",
            "1809: [D loss: 0.054189, acc: 0.968750]  [G loss: 1.220445, acc: 0.593750]\n",
            "1810: [D loss: 0.118194, acc: 0.984375]  [G loss: 0.975499, acc: 0.750000]\n",
            "1811: [D loss: 0.200751, acc: 0.937500]  [G loss: 0.901673, acc: 0.734375]\n",
            "1812: [D loss: 0.267494, acc: 0.929688]  [G loss: 1.048023, acc: 0.734375]\n",
            "1813: [D loss: 0.169420, acc: 0.929688]  [G loss: 0.885377, acc: 0.734375]\n",
            "1814: [D loss: 0.279387, acc: 0.937500]  [G loss: 0.652129, acc: 0.781250]\n",
            "1815: [D loss: 0.239064, acc: 0.914062]  [G loss: 0.589434, acc: 0.765625]\n",
            "1816: [D loss: 0.270282, acc: 0.937500]  [G loss: 0.983617, acc: 0.703125]\n",
            "1817: [D loss: 0.355946, acc: 0.906250]  [G loss: 0.451323, acc: 0.796875]\n",
            "1818: [D loss: 0.289122, acc: 0.882812]  [G loss: 0.644218, acc: 0.765625]\n",
            "1819: [D loss: 0.335940, acc: 0.882812]  [G loss: 0.839966, acc: 0.640625]\n",
            "1820: [D loss: 0.256257, acc: 0.906250]  [G loss: 0.928402, acc: 0.515625]\n",
            "1821: [D loss: 0.172271, acc: 0.953125]  [G loss: 0.993211, acc: 0.562500]\n",
            "1822: [D loss: 0.268570, acc: 0.921875]  [G loss: 1.202938, acc: 0.468750]\n",
            "1823: [D loss: 0.298939, acc: 0.875000]  [G loss: 0.947213, acc: 0.562500]\n",
            "1824: [D loss: 0.229052, acc: 0.906250]  [G loss: 1.276430, acc: 0.468750]\n",
            "1825: [D loss: 0.107142, acc: 0.984375]  [G loss: 1.429078, acc: 0.343750]\n",
            "1826: [D loss: 0.175075, acc: 0.937500]  [G loss: 1.670599, acc: 0.343750]\n",
            "1827: [D loss: 0.129133, acc: 0.945312]  [G loss: 2.218622, acc: 0.250000]\n",
            "1828: [D loss: 0.127407, acc: 0.968750]  [G loss: 1.871756, acc: 0.265625]\n",
            "1829: [D loss: 0.190875, acc: 0.937500]  [G loss: 1.929453, acc: 0.250000]\n",
            "1830: [D loss: 0.106100, acc: 0.984375]  [G loss: 1.420524, acc: 0.406250]\n",
            "1831: [D loss: 0.147659, acc: 0.960938]  [G loss: 1.518029, acc: 0.359375]\n",
            "1832: [D loss: 0.108952, acc: 1.000000]  [G loss: 1.734989, acc: 0.281250]\n",
            "1833: [D loss: 0.153072, acc: 0.953125]  [G loss: 1.591956, acc: 0.328125]\n",
            "1834: [D loss: 0.142569, acc: 0.953125]  [G loss: 1.529161, acc: 0.250000]\n",
            "1835: [D loss: 0.155398, acc: 0.968750]  [G loss: 1.549611, acc: 0.359375]\n",
            "1836: [D loss: 0.140056, acc: 0.968750]  [G loss: 1.175296, acc: 0.468750]\n",
            "1837: [D loss: 0.167590, acc: 0.953125]  [G loss: 0.955481, acc: 0.593750]\n",
            "1838: [D loss: 0.140464, acc: 0.937500]  [G loss: 0.823545, acc: 0.609375]\n",
            "1839: [D loss: 0.190031, acc: 0.898438]  [G loss: 0.849309, acc: 0.609375]\n",
            "1840: [D loss: 0.131057, acc: 0.937500]  [G loss: 1.001994, acc: 0.515625]\n",
            "1841: [D loss: 0.084320, acc: 0.992188]  [G loss: 1.426290, acc: 0.328125]\n",
            "1842: [D loss: 0.064546, acc: 0.976562]  [G loss: 1.886207, acc: 0.406250]\n",
            "1843: [D loss: 0.077505, acc: 0.960938]  [G loss: 1.518435, acc: 0.453125]\n",
            "1844: [D loss: 0.069113, acc: 0.976562]  [G loss: 1.222906, acc: 0.515625]\n",
            "1845: [D loss: 0.152786, acc: 0.921875]  [G loss: 1.545626, acc: 0.531250]\n",
            "1846: [D loss: 0.155116, acc: 0.921875]  [G loss: 1.445885, acc: 0.515625]\n",
            "1847: [D loss: 0.178279, acc: 0.937500]  [G loss: 1.529333, acc: 0.515625]\n",
            "1848: [D loss: 0.140073, acc: 0.945312]  [G loss: 1.031630, acc: 0.656250]\n",
            "1849: [D loss: 0.127828, acc: 0.953125]  [G loss: 0.881003, acc: 0.718750]\n",
            "1850: [D loss: 0.104986, acc: 0.968750]  [G loss: 0.938694, acc: 0.718750]\n",
            "1851: [D loss: 0.205508, acc: 0.929688]  [G loss: 0.915082, acc: 0.687500]\n",
            "1852: [D loss: 0.478102, acc: 0.843750]  [G loss: 1.376835, acc: 0.625000]\n",
            "1853: [D loss: 0.429771, acc: 0.859375]  [G loss: 0.575902, acc: 0.796875]\n",
            "1854: [D loss: 0.424921, acc: 0.859375]  [G loss: 0.576996, acc: 0.781250]\n",
            "1855: [D loss: 0.676210, acc: 0.781250]  [G loss: 0.386718, acc: 0.843750]\n",
            "1856: [D loss: 0.517675, acc: 0.843750]  [G loss: 0.157429, acc: 0.968750]\n",
            "1857: [D loss: 0.576359, acc: 0.796875]  [G loss: 0.354617, acc: 0.812500]\n",
            "1858: [D loss: 0.358689, acc: 0.875000]  [G loss: 0.133552, acc: 0.921875]\n",
            "1859: [D loss: 0.419108, acc: 0.812500]  [G loss: 0.339856, acc: 0.859375]\n",
            "1860: [D loss: 0.316458, acc: 0.859375]  [G loss: 0.435514, acc: 0.796875]\n",
            "1861: [D loss: 0.233365, acc: 0.906250]  [G loss: 0.475249, acc: 0.828125]\n",
            "1862: [D loss: 0.175300, acc: 0.921875]  [G loss: 0.183172, acc: 0.906250]\n",
            "1863: [D loss: 0.219807, acc: 0.921875]  [G loss: 0.115637, acc: 0.937500]\n",
            "1864: [D loss: 0.263588, acc: 0.875000]  [G loss: 0.988358, acc: 0.671875]\n",
            "1865: [D loss: 0.273506, acc: 0.914062]  [G loss: 0.927590, acc: 0.656250]\n",
            "1866: [D loss: 0.075182, acc: 0.968750]  [G loss: 0.664073, acc: 0.781250]\n",
            "1867: [D loss: 0.169268, acc: 0.929688]  [G loss: 0.254815, acc: 0.937500]\n",
            "1868: [D loss: 0.080968, acc: 0.968750]  [G loss: 0.130700, acc: 0.937500]\n",
            "1869: [D loss: 0.319568, acc: 0.867188]  [G loss: 0.119237, acc: 0.953125]\n",
            "1870: [D loss: 0.127834, acc: 0.953125]  [G loss: 0.162146, acc: 0.937500]\n",
            "1871: [D loss: 0.138934, acc: 0.960938]  [G loss: 0.317233, acc: 0.875000]\n",
            "1872: [D loss: 0.083020, acc: 0.968750]  [G loss: 0.431305, acc: 0.843750]\n",
            "1873: [D loss: 0.114536, acc: 0.968750]  [G loss: 0.402914, acc: 0.890625]\n",
            "1874: [D loss: 0.169375, acc: 0.953125]  [G loss: 0.226485, acc: 0.921875]\n",
            "1875: [D loss: 0.101023, acc: 0.968750]  [G loss: 0.580212, acc: 0.812500]\n",
            "1876: [D loss: 0.091462, acc: 0.968750]  [G loss: 1.547879, acc: 0.562500]\n",
            "1877: [D loss: 0.089911, acc: 0.984375]  [G loss: 1.937916, acc: 0.531250]\n",
            "1878: [D loss: 0.170366, acc: 0.937500]  [G loss: 1.381043, acc: 0.609375]\n",
            "1879: [D loss: 0.078606, acc: 0.976562]  [G loss: 0.561062, acc: 0.812500]\n",
            "1880: [D loss: 0.058571, acc: 0.976562]  [G loss: 0.273105, acc: 0.875000]\n",
            "1881: [D loss: 0.126974, acc: 0.937500]  [G loss: 0.711170, acc: 0.671875]\n",
            "1882: [D loss: 0.145992, acc: 0.953125]  [G loss: 2.382922, acc: 0.375000]\n",
            "1883: [D loss: 0.201974, acc: 0.937500]  [G loss: 2.006114, acc: 0.437500]\n",
            "1884: [D loss: 0.077839, acc: 0.968750]  [G loss: 1.448443, acc: 0.500000]\n",
            "1885: [D loss: 0.043585, acc: 0.992188]  [G loss: 1.548912, acc: 0.515625]\n",
            "1886: [D loss: 0.053159, acc: 0.984375]  [G loss: 1.384721, acc: 0.453125]\n",
            "1887: [D loss: 0.054874, acc: 0.976562]  [G loss: 2.621801, acc: 0.296875]\n",
            "1888: [D loss: 0.028395, acc: 0.992188]  [G loss: 3.585210, acc: 0.281250]\n",
            "1889: [D loss: 0.037682, acc: 0.984375]  [G loss: 5.621984, acc: 0.171875]\n",
            "1890: [D loss: 0.010677, acc: 1.000000]  [G loss: 6.549301, acc: 0.109375]\n",
            "1891: [D loss: 0.213453, acc: 0.937500]  [G loss: 3.207153, acc: 0.296875]\n",
            "1892: [D loss: 0.121501, acc: 0.960938]  [G loss: 1.038908, acc: 0.687500]\n",
            "1893: [D loss: 0.404389, acc: 0.820312]  [G loss: 6.575966, acc: 0.046875]\n",
            "1894: [D loss: 0.184822, acc: 0.953125]  [G loss: 12.173765, acc: 0.015625]\n",
            "1895: [D loss: 0.314459, acc: 0.929688]  [G loss: 11.149549, acc: 0.000000]\n",
            "1896: [D loss: 0.145830, acc: 0.953125]  [G loss: 6.625555, acc: 0.109375]\n",
            "1897: [D loss: 0.146682, acc: 0.960938]  [G loss: 2.971284, acc: 0.359375]\n",
            "1898: [D loss: 0.270556, acc: 0.882812]  [G loss: 4.723798, acc: 0.171875]\n",
            "1899: [D loss: 0.078778, acc: 0.960938]  [G loss: 8.653802, acc: 0.062500]\n",
            "1900: [D loss: 0.023248, acc: 0.992188]  [G loss: 10.435265, acc: 0.093750]\n",
            "1901: [D loss: 0.151869, acc: 0.937500]  [G loss: 10.060890, acc: 0.093750]\n",
            "1902: [D loss: 0.049080, acc: 0.992188]  [G loss: 8.682663, acc: 0.078125]\n",
            "1903: [D loss: 0.094093, acc: 0.968750]  [G loss: 6.096991, acc: 0.218750]\n",
            "1904: [D loss: 0.043837, acc: 0.976562]  [G loss: 4.576558, acc: 0.312500]\n",
            "1905: [D loss: 0.031444, acc: 0.992188]  [G loss: 3.483947, acc: 0.390625]\n",
            "1906: [D loss: 0.127412, acc: 0.953125]  [G loss: 5.407944, acc: 0.234375]\n",
            "1907: [D loss: 0.009030, acc: 1.000000]  [G loss: 5.924021, acc: 0.171875]\n",
            "1908: [D loss: 0.104167, acc: 0.960938]  [G loss: 6.363607, acc: 0.187500]\n",
            "1909: [D loss: 0.059242, acc: 0.976562]  [G loss: 6.213418, acc: 0.093750]\n",
            "1910: [D loss: 0.205758, acc: 0.960938]  [G loss: 5.221993, acc: 0.140625]\n",
            "1911: [D loss: 0.048111, acc: 0.976562]  [G loss: 3.732645, acc: 0.171875]\n",
            "1912: [D loss: 0.048769, acc: 0.984375]  [G loss: 3.342670, acc: 0.359375]\n",
            "1913: [D loss: 0.024318, acc: 0.992188]  [G loss: 2.242157, acc: 0.421875]\n",
            "1914: [D loss: 0.100252, acc: 0.968750]  [G loss: 1.656229, acc: 0.546875]\n",
            "1915: [D loss: 0.046840, acc: 0.984375]  [G loss: 1.098835, acc: 0.625000]\n",
            "1916: [D loss: 0.059827, acc: 0.984375]  [G loss: 1.146183, acc: 0.671875]\n",
            "1917: [D loss: 0.033003, acc: 0.984375]  [G loss: 1.183018, acc: 0.609375]\n",
            "1918: [D loss: 0.091378, acc: 0.984375]  [G loss: 1.107437, acc: 0.671875]\n",
            "1919: [D loss: 0.112015, acc: 0.968750]  [G loss: 1.428862, acc: 0.671875]\n",
            "1920: [D loss: 0.153830, acc: 0.968750]  [G loss: 1.739234, acc: 0.562500]\n",
            "1921: [D loss: 0.020570, acc: 0.992188]  [G loss: 1.439713, acc: 0.640625]\n",
            "1922: [D loss: 0.047299, acc: 0.976562]  [G loss: 1.298925, acc: 0.593750]\n",
            "1923: [D loss: 0.076759, acc: 0.992188]  [G loss: 0.678637, acc: 0.796875]\n",
            "1924: [D loss: 0.010980, acc: 1.000000]  [G loss: 0.474458, acc: 0.812500]\n",
            "1925: [D loss: 0.045340, acc: 0.992188]  [G loss: 0.336613, acc: 0.859375]\n",
            "1926: [D loss: 0.028583, acc: 0.984375]  [G loss: 0.359941, acc: 0.828125]\n",
            "1927: [D loss: 0.095399, acc: 0.945312]  [G loss: 0.629104, acc: 0.703125]\n",
            "1928: [D loss: 0.038607, acc: 0.984375]  [G loss: 1.071741, acc: 0.671875]\n",
            "1929: [D loss: 0.006389, acc: 1.000000]  [G loss: 1.341132, acc: 0.500000]\n",
            "1930: [D loss: 0.185322, acc: 0.945312]  [G loss: 1.431038, acc: 0.656250]\n",
            "1931: [D loss: 0.044379, acc: 0.976562]  [G loss: 1.045394, acc: 0.734375]\n",
            "1932: [D loss: 0.047144, acc: 0.968750]  [G loss: 0.849227, acc: 0.625000]\n",
            "1933: [D loss: 0.157617, acc: 0.953125]  [G loss: 0.966222, acc: 0.656250]\n",
            "1934: [D loss: 0.034761, acc: 0.984375]  [G loss: 1.342322, acc: 0.437500]\n",
            "1935: [D loss: 0.010643, acc: 1.000000]  [G loss: 1.795241, acc: 0.453125]\n",
            "1936: [D loss: 0.129311, acc: 0.968750]  [G loss: 1.743455, acc: 0.328125]\n",
            "1937: [D loss: 0.045717, acc: 0.992188]  [G loss: 1.267601, acc: 0.500000]\n",
            "1938: [D loss: 0.028576, acc: 0.992188]  [G loss: 0.844660, acc: 0.640625]\n",
            "1939: [D loss: 0.024527, acc: 0.992188]  [G loss: 0.895972, acc: 0.593750]\n",
            "1940: [D loss: 0.079607, acc: 0.976562]  [G loss: 0.503785, acc: 0.828125]\n",
            "1941: [D loss: 0.069001, acc: 0.976562]  [G loss: 0.474276, acc: 0.781250]\n",
            "1942: [D loss: 0.031210, acc: 1.000000]  [G loss: 0.734601, acc: 0.671875]\n",
            "1943: [D loss: 0.069020, acc: 0.968750]  [G loss: 0.898687, acc: 0.593750]\n",
            "1944: [D loss: 0.015696, acc: 1.000000]  [G loss: 1.360835, acc: 0.500000]\n",
            "1945: [D loss: 0.004854, acc: 1.000000]  [G loss: 1.753812, acc: 0.359375]\n",
            "1946: [D loss: 0.007791, acc: 1.000000]  [G loss: 2.011370, acc: 0.203125]\n",
            "1947: [D loss: 0.047003, acc: 0.976562]  [G loss: 2.105333, acc: 0.156250]\n",
            "1948: [D loss: 0.036766, acc: 0.992188]  [G loss: 2.082988, acc: 0.140625]\n",
            "1949: [D loss: 0.005578, acc: 1.000000]  [G loss: 2.112339, acc: 0.093750]\n",
            "1950: [D loss: 0.008705, acc: 1.000000]  [G loss: 1.997701, acc: 0.031250]\n",
            "1951: [D loss: 0.003953, acc: 1.000000]  [G loss: 1.925050, acc: 0.015625]\n",
            "1952: [D loss: 0.004484, acc: 1.000000]  [G loss: 1.952079, acc: 0.015625]\n",
            "1953: [D loss: 0.011192, acc: 1.000000]  [G loss: 1.910492, acc: 0.015625]\n",
            "1954: [D loss: 0.008913, acc: 1.000000]  [G loss: 2.270769, acc: 0.000000]\n",
            "1955: [D loss: 0.009099, acc: 1.000000]  [G loss: 2.562769, acc: 0.000000]\n",
            "1956: [D loss: 0.006944, acc: 1.000000]  [G loss: 2.852327, acc: 0.000000]\n",
            "1957: [D loss: 0.004824, acc: 1.000000]  [G loss: 3.542134, acc: 0.000000]\n",
            "1958: [D loss: 0.003596, acc: 1.000000]  [G loss: 3.973065, acc: 0.000000]\n",
            "1959: [D loss: 0.002979, acc: 1.000000]  [G loss: 4.576794, acc: 0.000000]\n",
            "1960: [D loss: 0.001714, acc: 1.000000]  [G loss: 5.074413, acc: 0.000000]\n",
            "1961: [D loss: 0.001200, acc: 1.000000]  [G loss: 5.604725, acc: 0.000000]\n",
            "1962: [D loss: 0.001004, acc: 1.000000]  [G loss: 6.102760, acc: 0.000000]\n",
            "1963: [D loss: 0.000688, acc: 1.000000]  [G loss: 6.477157, acc: 0.000000]\n",
            "1964: [D loss: 0.000453, acc: 1.000000]  [G loss: 7.040276, acc: 0.000000]\n",
            "1965: [D loss: 0.000314, acc: 1.000000]  [G loss: 7.407877, acc: 0.000000]\n",
            "1966: [D loss: 0.000606, acc: 1.000000]  [G loss: 7.730679, acc: 0.000000]\n",
            "1967: [D loss: 0.000171, acc: 1.000000]  [G loss: 8.062635, acc: 0.000000]\n",
            "1968: [D loss: 0.000138, acc: 1.000000]  [G loss: 8.224878, acc: 0.000000]\n",
            "1969: [D loss: 0.000181, acc: 1.000000]  [G loss: 8.646012, acc: 0.000000]\n",
            "1970: [D loss: 0.000086, acc: 1.000000]  [G loss: 8.749218, acc: 0.000000]\n",
            "1971: [D loss: 0.000054, acc: 1.000000]  [G loss: 8.890753, acc: 0.000000]\n",
            "1972: [D loss: 0.000052, acc: 1.000000]  [G loss: 9.028797, acc: 0.000000]\n",
            "1973: [D loss: 0.000109, acc: 1.000000]  [G loss: 9.604520, acc: 0.000000]\n",
            "1974: [D loss: 0.000044, acc: 1.000000]  [G loss: 9.581917, acc: 0.000000]\n",
            "1975: [D loss: 0.000041, acc: 1.000000]  [G loss: 9.789800, acc: 0.000000]\n",
            "1976: [D loss: 0.000030, acc: 1.000000]  [G loss: 9.869458, acc: 0.000000]\n",
            "1977: [D loss: 0.000028, acc: 1.000000]  [G loss: 9.950580, acc: 0.000000]\n",
            "1978: [D loss: 0.000028, acc: 1.000000]  [G loss: 9.997791, acc: 0.000000]\n",
            "1979: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.301154, acc: 0.000000]\n",
            "1980: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.180109, acc: 0.000000]\n",
            "1981: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.466565, acc: 0.000000]\n",
            "1982: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.373255, acc: 0.000000]\n",
            "1983: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.368796, acc: 0.000000]\n",
            "1984: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.685217, acc: 0.000000]\n",
            "1985: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.456121, acc: 0.000000]\n",
            "1986: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.585476, acc: 0.000000]\n",
            "1987: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.571449, acc: 0.000000]\n",
            "1988: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.701166, acc: 0.000000]\n",
            "1989: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.765209, acc: 0.000000]\n",
            "1990: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.865925, acc: 0.000000]\n",
            "1991: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.860371, acc: 0.000000]\n",
            "1992: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.863743, acc: 0.000000]\n",
            "1993: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.765112, acc: 0.000000]\n",
            "1994: [D loss: 0.000400, acc: 1.000000]  [G loss: 10.934057, acc: 0.000000]\n",
            "1995: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.780962, acc: 0.000000]\n",
            "1996: [D loss: 0.001707, acc: 1.000000]  [G loss: 10.935440, acc: 0.000000]\n",
            "1997: [D loss: 0.016449, acc: 0.992188]  [G loss: 10.835159, acc: 0.000000]\n",
            "1998: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.621868, acc: 0.000000]\n",
            "1999: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.501827, acc: 0.000000]\n",
            "2000: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.573875, acc: 0.000000]\n",
            "2001: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.327038, acc: 0.000000]\n",
            "2002: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.289061, acc: 0.000000]\n",
            "2003: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.252107, acc: 0.000000]\n",
            "2004: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.174410, acc: 0.000000]\n",
            "2005: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.282616, acc: 0.000000]\n",
            "2006: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.202070, acc: 0.000000]\n",
            "2007: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.048405, acc: 0.000000]\n",
            "2008: [D loss: 0.000023, acc: 1.000000]  [G loss: 9.982162, acc: 0.000000]\n",
            "2009: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.001493, acc: 0.000000]\n",
            "2010: [D loss: 0.000030, acc: 1.000000]  [G loss: 9.954782, acc: 0.000000]\n",
            "2011: [D loss: 0.000024, acc: 1.000000]  [G loss: 9.964308, acc: 0.000000]\n",
            "2012: [D loss: 0.000028, acc: 1.000000]  [G loss: 9.829664, acc: 0.000000]\n",
            "2013: [D loss: 0.000031, acc: 1.000000]  [G loss: 10.108742, acc: 0.000000]\n",
            "2014: [D loss: 0.000029, acc: 1.000000]  [G loss: 9.878401, acc: 0.000000]\n",
            "2015: [D loss: 0.000032, acc: 1.000000]  [G loss: 9.927019, acc: 0.000000]\n",
            "2016: [D loss: 0.000028, acc: 1.000000]  [G loss: 9.941426, acc: 0.000000]\n",
            "2017: [D loss: 0.000032, acc: 1.000000]  [G loss: 9.836452, acc: 0.000000]\n",
            "2018: [D loss: 0.000028, acc: 1.000000]  [G loss: 9.869162, acc: 0.000000]\n",
            "2019: [D loss: 0.000031, acc: 1.000000]  [G loss: 9.776567, acc: 0.000000]\n",
            "2020: [D loss: 0.000047, acc: 1.000000]  [G loss: 9.811066, acc: 0.000000]\n",
            "2021: [D loss: 0.000032, acc: 1.000000]  [G loss: 9.786535, acc: 0.000000]\n",
            "2022: [D loss: 0.000030, acc: 1.000000]  [G loss: 9.877280, acc: 0.000000]\n",
            "2023: [D loss: 0.000028, acc: 1.000000]  [G loss: 9.815209, acc: 0.000000]\n",
            "2024: [D loss: 0.000033, acc: 1.000000]  [G loss: 9.735025, acc: 0.000000]\n",
            "2025: [D loss: 0.000035, acc: 1.000000]  [G loss: 9.863197, acc: 0.000000]\n",
            "2026: [D loss: 0.000028, acc: 1.000000]  [G loss: 9.777174, acc: 0.000000]\n",
            "2027: [D loss: 0.000035, acc: 1.000000]  [G loss: 9.775414, acc: 0.000000]\n",
            "2028: [D loss: 0.000034, acc: 1.000000]  [G loss: 9.812154, acc: 0.000000]\n",
            "2029: [D loss: 0.000033, acc: 1.000000]  [G loss: 9.866933, acc: 0.000000]\n",
            "2030: [D loss: 0.000029, acc: 1.000000]  [G loss: 9.737383, acc: 0.000000]\n",
            "2031: [D loss: 0.000031, acc: 1.000000]  [G loss: 9.830164, acc: 0.000000]\n",
            "2032: [D loss: 0.000027, acc: 1.000000]  [G loss: 9.859831, acc: 0.000000]\n",
            "2033: [D loss: 0.000029, acc: 1.000000]  [G loss: 9.936619, acc: 0.000000]\n",
            "2034: [D loss: 0.000035, acc: 1.000000]  [G loss: 9.926826, acc: 0.000000]\n",
            "2035: [D loss: 0.000030, acc: 1.000000]  [G loss: 9.907503, acc: 0.000000]\n",
            "2036: [D loss: 0.000031, acc: 1.000000]  [G loss: 9.975501, acc: 0.000000]\n",
            "2037: [D loss: 0.000030, acc: 1.000000]  [G loss: 9.890068, acc: 0.000000]\n",
            "2038: [D loss: 0.000036, acc: 1.000000]  [G loss: 9.873947, acc: 0.000000]\n",
            "2039: [D loss: 0.000032, acc: 1.000000]  [G loss: 9.927237, acc: 0.000000]\n",
            "2040: [D loss: 0.000028, acc: 1.000000]  [G loss: 10.019702, acc: 0.000000]\n",
            "2041: [D loss: 0.000031, acc: 1.000000]  [G loss: 9.908852, acc: 0.000000]\n",
            "2042: [D loss: 0.000030, acc: 1.000000]  [G loss: 9.980719, acc: 0.000000]\n",
            "2043: [D loss: 0.000027, acc: 1.000000]  [G loss: 9.853601, acc: 0.000000]\n",
            "2044: [D loss: 0.000035, acc: 1.000000]  [G loss: 9.983950, acc: 0.000000]\n",
            "2045: [D loss: 0.000029, acc: 1.000000]  [G loss: 9.911186, acc: 0.000000]\n",
            "2046: [D loss: 0.000032, acc: 1.000000]  [G loss: 9.883045, acc: 0.000000]\n",
            "2047: [D loss: 0.000029, acc: 1.000000]  [G loss: 10.098783, acc: 0.000000]\n",
            "2048: [D loss: 0.000030, acc: 1.000000]  [G loss: 9.839133, acc: 0.000000]\n",
            "2049: [D loss: 0.000027, acc: 1.000000]  [G loss: 9.731976, acc: 0.000000]\n",
            "2050: [D loss: 0.000028, acc: 1.000000]  [G loss: 10.002222, acc: 0.000000]\n",
            "2051: [D loss: 0.000028, acc: 1.000000]  [G loss: 9.912629, acc: 0.000000]\n",
            "2052: [D loss: 0.000029, acc: 1.000000]  [G loss: 10.045000, acc: 0.000000]\n",
            "2053: [D loss: 0.000026, acc: 1.000000]  [G loss: 10.084118, acc: 0.000000]\n",
            "2054: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.086136, acc: 0.000000]\n",
            "2055: [D loss: 0.000029, acc: 1.000000]  [G loss: 9.941521, acc: 0.000000]\n",
            "2056: [D loss: 0.000029, acc: 1.000000]  [G loss: 10.037905, acc: 0.000000]\n",
            "2057: [D loss: 0.000027, acc: 1.000000]  [G loss: 9.951687, acc: 0.000000]\n",
            "2058: [D loss: 0.000027, acc: 1.000000]  [G loss: 9.953941, acc: 0.000000]\n",
            "2059: [D loss: 0.000027, acc: 1.000000]  [G loss: 10.047893, acc: 0.000000]\n",
            "2060: [D loss: 0.000028, acc: 1.000000]  [G loss: 10.003190, acc: 0.000000]\n",
            "2061: [D loss: 0.000027, acc: 1.000000]  [G loss: 10.009024, acc: 0.000000]\n",
            "2062: [D loss: 0.000026, acc: 1.000000]  [G loss: 10.070259, acc: 0.000000]\n",
            "2063: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.148202, acc: 0.000000]\n",
            "2064: [D loss: 0.000029, acc: 1.000000]  [G loss: 9.942234, acc: 0.000000]\n",
            "2065: [D loss: 0.000026, acc: 1.000000]  [G loss: 10.161039, acc: 0.000000]\n",
            "2066: [D loss: 0.000029, acc: 1.000000]  [G loss: 10.148742, acc: 0.000000]\n",
            "2067: [D loss: 0.000023, acc: 1.000000]  [G loss: 9.991401, acc: 0.000000]\n",
            "2068: [D loss: 0.000022, acc: 1.000000]  [G loss: 9.897181, acc: 0.000000]\n",
            "2069: [D loss: 0.000026, acc: 1.000000]  [G loss: 9.910967, acc: 0.000000]\n",
            "2070: [D loss: 0.000026, acc: 1.000000]  [G loss: 10.120531, acc: 0.000000]\n",
            "2071: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.112253, acc: 0.000000]\n",
            "2072: [D loss: 0.000026, acc: 1.000000]  [G loss: 9.986535, acc: 0.000000]\n",
            "2073: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.158051, acc: 0.000000]\n",
            "2074: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.122478, acc: 0.000000]\n",
            "2075: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.085426, acc: 0.000000]\n",
            "2076: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.080002, acc: 0.000000]\n",
            "2077: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.022585, acc: 0.000000]\n",
            "2078: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.174153, acc: 0.000000]\n",
            "2079: [D loss: 0.000028, acc: 1.000000]  [G loss: 10.246673, acc: 0.000000]\n",
            "2080: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.188223, acc: 0.000000]\n",
            "2081: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.237604, acc: 0.000000]\n",
            "2082: [D loss: 0.000023, acc: 1.000000]  [G loss: 10.091719, acc: 0.000000]\n",
            "2083: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.136658, acc: 0.000000]\n",
            "2084: [D loss: 0.000026, acc: 1.000000]  [G loss: 10.034340, acc: 0.000000]\n",
            "2085: [D loss: 0.000023, acc: 1.000000]  [G loss: 10.175352, acc: 0.000000]\n",
            "2086: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.246166, acc: 0.000000]\n",
            "2087: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.192994, acc: 0.000000]\n",
            "2088: [D loss: 0.000023, acc: 1.000000]  [G loss: 10.278174, acc: 0.000000]\n",
            "2089: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.138396, acc: 0.000000]\n",
            "2090: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.074545, acc: 0.000000]\n",
            "2091: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.128550, acc: 0.000000]\n",
            "2092: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.248436, acc: 0.000000]\n",
            "2093: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.303152, acc: 0.000000]\n",
            "2094: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.231771, acc: 0.000000]\n",
            "2095: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.293978, acc: 0.000000]\n",
            "2096: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.216423, acc: 0.000000]\n",
            "2097: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.291192, acc: 0.000000]\n",
            "2098: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.316715, acc: 0.000000]\n",
            "2099: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.270632, acc: 0.000000]\n",
            "2100: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.327550, acc: 0.000000]\n",
            "2101: [D loss: 0.000023, acc: 1.000000]  [G loss: 10.378392, acc: 0.000000]\n",
            "2102: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.145577, acc: 0.000000]\n",
            "2103: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.216480, acc: 0.000000]\n",
            "2104: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.334044, acc: 0.000000]\n",
            "2105: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.398542, acc: 0.000000]\n",
            "2106: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.201558, acc: 0.000000]\n",
            "2107: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.379099, acc: 0.000000]\n",
            "2108: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.340757, acc: 0.000000]\n",
            "2109: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.413576, acc: 0.000000]\n",
            "2110: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.190432, acc: 0.000000]\n",
            "2111: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.324631, acc: 0.000000]\n",
            "2112: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.402531, acc: 0.000000]\n",
            "2113: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.322771, acc: 0.000000]\n",
            "2114: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.398933, acc: 0.000000]\n",
            "2115: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.298490, acc: 0.000000]\n",
            "2116: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.305757, acc: 0.000000]\n",
            "2117: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.407915, acc: 0.000000]\n",
            "2118: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.138288, acc: 0.000000]\n",
            "2119: [D loss: 0.000060, acc: 1.000000]  [G loss: 10.290575, acc: 0.000000]\n",
            "2120: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.274848, acc: 0.000000]\n",
            "2121: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.374228, acc: 0.000000]\n",
            "2122: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.308489, acc: 0.000000]\n",
            "2123: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.374102, acc: 0.000000]\n",
            "2124: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.398970, acc: 0.000000]\n",
            "2125: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.358546, acc: 0.000000]\n",
            "2126: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.440848, acc: 0.000000]\n",
            "2127: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.448217, acc: 0.000000]\n",
            "2128: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.464890, acc: 0.000000]\n",
            "2129: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.573423, acc: 0.000000]\n",
            "2130: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.367279, acc: 0.000000]\n",
            "2131: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.499898, acc: 0.000000]\n",
            "2132: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.307940, acc: 0.000000]\n",
            "2133: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.503130, acc: 0.000000]\n",
            "2134: [D loss: 0.000051, acc: 1.000000]  [G loss: 10.388882, acc: 0.000000]\n",
            "2135: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.408474, acc: 0.000000]\n",
            "2136: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.461293, acc: 0.000000]\n",
            "2137: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.498497, acc: 0.000000]\n",
            "2138: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.367894, acc: 0.000000]\n",
            "2139: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.373867, acc: 0.000000]\n",
            "2140: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.409710, acc: 0.000000]\n",
            "2141: [D loss: 0.000498, acc: 1.000000]  [G loss: 10.411552, acc: 0.000000]\n",
            "2142: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.418811, acc: 0.000000]\n",
            "2143: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.575413, acc: 0.000000]\n",
            "2144: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.487749, acc: 0.000000]\n",
            "2145: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.454748, acc: 0.000000]\n",
            "2146: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.358297, acc: 0.000000]\n",
            "2147: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.421549, acc: 0.000000]\n",
            "2148: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.546791, acc: 0.000000]\n",
            "2149: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.478579, acc: 0.000000]\n",
            "2150: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.368739, acc: 0.000000]\n",
            "2151: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.413085, acc: 0.000000]\n",
            "2152: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.375402, acc: 0.000000]\n",
            "2153: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.474627, acc: 0.000000]\n",
            "2154: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.431529, acc: 0.000000]\n",
            "2155: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.431288, acc: 0.000000]\n",
            "2156: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.365786, acc: 0.000000]\n",
            "2157: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.329178, acc: 0.000000]\n",
            "2158: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.581087, acc: 0.000000]\n",
            "2159: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.509467, acc: 0.000000]\n",
            "2160: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.359797, acc: 0.000000]\n",
            "2161: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.547050, acc: 0.000000]\n",
            "2162: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.315901, acc: 0.000000]\n",
            "2163: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.507615, acc: 0.000000]\n",
            "2164: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.295649, acc: 0.000000]\n",
            "2165: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.479778, acc: 0.000000]\n",
            "2166: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.328382, acc: 0.000000]\n",
            "2167: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.417740, acc: 0.000000]\n",
            "2168: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.590010, acc: 0.000000]\n",
            "2169: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.559820, acc: 0.000000]\n",
            "2170: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.556250, acc: 0.000000]\n",
            "2171: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.580482, acc: 0.000000]\n",
            "2172: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.537905, acc: 0.000000]\n",
            "2173: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.540503, acc: 0.000000]\n",
            "2174: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.539494, acc: 0.000000]\n",
            "2175: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.457282, acc: 0.000000]\n",
            "2176: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.579170, acc: 0.000000]\n",
            "2177: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.641181, acc: 0.000000]\n",
            "2178: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.391027, acc: 0.000000]\n",
            "2179: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.586833, acc: 0.000000]\n",
            "2180: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.576757, acc: 0.000000]\n",
            "2181: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.581864, acc: 0.000000]\n",
            "2182: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.501648, acc: 0.000000]\n",
            "2183: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.319384, acc: 0.000000]\n",
            "2184: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.616865, acc: 0.000000]\n",
            "2185: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.534444, acc: 0.000000]\n",
            "2186: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.545799, acc: 0.000000]\n",
            "2187: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.545830, acc: 0.000000]\n",
            "2188: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.674522, acc: 0.000000]\n",
            "2189: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.617377, acc: 0.000000]\n",
            "2190: [D loss: 0.019870, acc: 0.992188]  [G loss: 10.495092, acc: 0.000000]\n",
            "2191: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.278234, acc: 0.000000]\n",
            "2192: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.340087, acc: 0.000000]\n",
            "2193: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.135670, acc: 0.000000]\n",
            "2194: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.163198, acc: 0.000000]\n",
            "2195: [D loss: 0.000019, acc: 1.000000]  [G loss: 9.983290, acc: 0.000000]\n",
            "2196: [D loss: 0.000026, acc: 1.000000]  [G loss: 10.144232, acc: 0.000000]\n",
            "2197: [D loss: 0.000025, acc: 1.000000]  [G loss: 9.996522, acc: 0.000000]\n",
            "2198: [D loss: 0.000028, acc: 1.000000]  [G loss: 10.055806, acc: 0.000000]\n",
            "2199: [D loss: 0.000024, acc: 1.000000]  [G loss: 9.716843, acc: 0.000000]\n",
            "2200: [D loss: 0.000033, acc: 1.000000]  [G loss: 9.850736, acc: 0.000000]\n",
            "2201: [D loss: 0.000032, acc: 1.000000]  [G loss: 9.752334, acc: 0.000000]\n",
            "2202: [D loss: 0.000033, acc: 1.000000]  [G loss: 9.834046, acc: 0.000000]\n",
            "2203: [D loss: 0.000031, acc: 1.000000]  [G loss: 9.764318, acc: 0.000000]\n",
            "2204: [D loss: 0.000034, acc: 1.000000]  [G loss: 9.630052, acc: 0.000000]\n",
            "2205: [D loss: 0.000031, acc: 1.000000]  [G loss: 9.673461, acc: 0.000000]\n",
            "2206: [D loss: 0.000036, acc: 1.000000]  [G loss: 9.728585, acc: 0.000000]\n",
            "2207: [D loss: 0.000033, acc: 1.000000]  [G loss: 9.783293, acc: 0.000000]\n",
            "2208: [D loss: 0.000036, acc: 1.000000]  [G loss: 9.725877, acc: 0.000000]\n",
            "2209: [D loss: 0.000038, acc: 1.000000]  [G loss: 9.892521, acc: 0.000000]\n",
            "2210: [D loss: 0.000035, acc: 1.000000]  [G loss: 9.721151, acc: 0.000000]\n",
            "2211: [D loss: 0.000032, acc: 1.000000]  [G loss: 9.710671, acc: 0.000000]\n",
            "2212: [D loss: 0.000039, acc: 1.000000]  [G loss: 9.867441, acc: 0.000000]\n",
            "2213: [D loss: 0.000040, acc: 1.000000]  [G loss: 9.634696, acc: 0.000000]\n",
            "2214: [D loss: 0.000033, acc: 1.000000]  [G loss: 9.717729, acc: 0.000000]\n",
            "2215: [D loss: 0.000038, acc: 1.000000]  [G loss: 9.640152, acc: 0.000000]\n",
            "2216: [D loss: 0.000034, acc: 1.000000]  [G loss: 9.662369, acc: 0.000000]\n",
            "2217: [D loss: 0.000037, acc: 1.000000]  [G loss: 9.694769, acc: 0.000000]\n",
            "2218: [D loss: 0.000033, acc: 1.000000]  [G loss: 9.691655, acc: 0.000000]\n",
            "2219: [D loss: 0.000028, acc: 1.000000]  [G loss: 9.724588, acc: 0.000000]\n",
            "2220: [D loss: 0.000037, acc: 1.000000]  [G loss: 9.808336, acc: 0.000000]\n",
            "2221: [D loss: 0.000032, acc: 1.000000]  [G loss: 9.665337, acc: 0.000000]\n",
            "2222: [D loss: 0.000040, acc: 1.000000]  [G loss: 9.783981, acc: 0.000000]\n",
            "2223: [D loss: 0.000036, acc: 1.000000]  [G loss: 9.712263, acc: 0.000000]\n",
            "2224: [D loss: 0.000033, acc: 1.000000]  [G loss: 9.821228, acc: 0.000000]\n",
            "2225: [D loss: 0.000034, acc: 1.000000]  [G loss: 9.717104, acc: 0.000000]\n",
            "2226: [D loss: 0.000033, acc: 1.000000]  [G loss: 9.680145, acc: 0.000000]\n",
            "2227: [D loss: 0.000034, acc: 1.000000]  [G loss: 9.767462, acc: 0.000000]\n",
            "2228: [D loss: 0.000035, acc: 1.000000]  [G loss: 9.725279, acc: 0.000000]\n",
            "2229: [D loss: 0.000038, acc: 1.000000]  [G loss: 9.804609, acc: 0.000000]\n",
            "2230: [D loss: 0.000037, acc: 1.000000]  [G loss: 9.753448, acc: 0.000000]\n",
            "2231: [D loss: 0.000034, acc: 1.000000]  [G loss: 9.653231, acc: 0.000000]\n",
            "2232: [D loss: 0.000035, acc: 1.000000]  [G loss: 9.838303, acc: 0.000000]\n",
            "2233: [D loss: 0.000038, acc: 1.000000]  [G loss: 9.808225, acc: 0.000000]\n",
            "2234: [D loss: 0.000039, acc: 1.000000]  [G loss: 9.707777, acc: 0.000000]\n",
            "2235: [D loss: 0.000038, acc: 1.000000]  [G loss: 9.666350, acc: 0.000000]\n",
            "2236: [D loss: 0.000036, acc: 1.000000]  [G loss: 9.807164, acc: 0.000000]\n",
            "2237: [D loss: 0.000036, acc: 1.000000]  [G loss: 9.815765, acc: 0.000000]\n",
            "2238: [D loss: 0.000031, acc: 1.000000]  [G loss: 9.790108, acc: 0.000000]\n",
            "2239: [D loss: 0.000030, acc: 1.000000]  [G loss: 9.941961, acc: 0.000000]\n",
            "2240: [D loss: 0.000036, acc: 1.000000]  [G loss: 9.766085, acc: 0.000000]\n",
            "2241: [D loss: 0.000032, acc: 1.000000]  [G loss: 9.597836, acc: 0.000000]\n",
            "2242: [D loss: 0.000036, acc: 1.000000]  [G loss: 9.937793, acc: 0.000000]\n",
            "2243: [D loss: 0.000033, acc: 1.000000]  [G loss: 9.819022, acc: 0.000000]\n",
            "2244: [D loss: 0.000037, acc: 1.000000]  [G loss: 9.943672, acc: 0.000000]\n",
            "2245: [D loss: 0.000029, acc: 1.000000]  [G loss: 9.875389, acc: 0.000000]\n",
            "2246: [D loss: 0.000034, acc: 1.000000]  [G loss: 9.865370, acc: 0.000000]\n",
            "2247: [D loss: 0.000035, acc: 1.000000]  [G loss: 9.911139, acc: 0.000000]\n",
            "2248: [D loss: 0.000034, acc: 1.000000]  [G loss: 9.968788, acc: 0.000000]\n",
            "2249: [D loss: 0.000031, acc: 1.000000]  [G loss: 9.954252, acc: 0.000000]\n",
            "2250: [D loss: 0.000032, acc: 1.000000]  [G loss: 10.021995, acc: 0.000000]\n",
            "2251: [D loss: 0.000029, acc: 1.000000]  [G loss: 9.922916, acc: 0.000000]\n",
            "2252: [D loss: 0.000033, acc: 1.000000]  [G loss: 9.955275, acc: 0.000000]\n",
            "2253: [D loss: 0.000030, acc: 1.000000]  [G loss: 9.866600, acc: 0.000000]\n",
            "2254: [D loss: 0.000023, acc: 1.000000]  [G loss: 10.009118, acc: 0.000000]\n",
            "2255: [D loss: 0.000028, acc: 1.000000]  [G loss: 9.891872, acc: 0.000000]\n",
            "2256: [D loss: 0.000028, acc: 1.000000]  [G loss: 9.900614, acc: 0.000000]\n",
            "2257: [D loss: 0.000033, acc: 1.000000]  [G loss: 9.951696, acc: 0.000000]\n",
            "2258: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.200029, acc: 0.000000]\n",
            "2259: [D loss: 0.000024, acc: 1.000000]  [G loss: 9.971021, acc: 0.000000]\n",
            "2260: [D loss: 0.000028, acc: 1.000000]  [G loss: 10.139105, acc: 0.000000]\n",
            "2261: [D loss: 0.000027, acc: 1.000000]  [G loss: 9.880758, acc: 0.000000]\n",
            "2262: [D loss: 0.000025, acc: 1.000000]  [G loss: 9.996864, acc: 0.000000]\n",
            "2263: [D loss: 0.000023, acc: 1.000000]  [G loss: 10.181086, acc: 0.000000]\n",
            "2264: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.089849, acc: 0.000000]\n",
            "2265: [D loss: 0.000027, acc: 1.000000]  [G loss: 9.977207, acc: 0.000000]\n",
            "2266: [D loss: 0.000026, acc: 1.000000]  [G loss: 10.125401, acc: 0.000000]\n",
            "2267: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.208530, acc: 0.000000]\n",
            "2268: [D loss: 0.000027, acc: 1.000000]  [G loss: 9.887715, acc: 0.000000]\n",
            "2269: [D loss: 0.000030, acc: 1.000000]  [G loss: 10.163158, acc: 0.000000]\n",
            "2270: [D loss: 0.000026, acc: 1.000000]  [G loss: 10.038972, acc: 0.000000]\n",
            "2271: [D loss: 0.000029, acc: 1.000000]  [G loss: 10.140678, acc: 0.000000]\n",
            "2272: [D loss: 0.000019, acc: 1.000000]  [G loss: 9.935913, acc: 0.000000]\n",
            "2273: [D loss: 0.000029, acc: 1.000000]  [G loss: 10.124580, acc: 0.000000]\n",
            "2274: [D loss: 0.000027, acc: 1.000000]  [G loss: 9.991100, acc: 0.000000]\n",
            "2275: [D loss: 0.000029, acc: 1.000000]  [G loss: 10.239797, acc: 0.000000]\n",
            "2276: [D loss: 0.000026, acc: 1.000000]  [G loss: 10.049085, acc: 0.000000]\n",
            "2277: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.166847, acc: 0.000000]\n",
            "2278: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.270025, acc: 0.000000]\n",
            "2279: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.199625, acc: 0.000000]\n",
            "2280: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.105019, acc: 0.000000]\n",
            "2281: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.088371, acc: 0.000000]\n",
            "2282: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.203066, acc: 0.000000]\n",
            "2283: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.197422, acc: 0.000000]\n",
            "2284: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.174549, acc: 0.000000]\n",
            "2285: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.255110, acc: 0.000000]\n",
            "2286: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.027394, acc: 0.000000]\n",
            "2287: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.150475, acc: 0.000000]\n",
            "2288: [D loss: 0.000023, acc: 1.000000]  [G loss: 10.275764, acc: 0.000000]\n",
            "2289: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.242868, acc: 0.000000]\n",
            "2290: [D loss: 0.000023, acc: 1.000000]  [G loss: 10.235913, acc: 0.000000]\n",
            "2291: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.278958, acc: 0.000000]\n",
            "2292: [D loss: 0.000024, acc: 1.000000]  [G loss: 10.274587, acc: 0.000000]\n",
            "2293: [D loss: 0.000026, acc: 1.000000]  [G loss: 10.118686, acc: 0.000000]\n",
            "2294: [D loss: 0.000025, acc: 1.000000]  [G loss: 10.218307, acc: 0.000000]\n",
            "2295: [D loss: 0.000023, acc: 1.000000]  [G loss: 10.200134, acc: 0.000000]\n",
            "2296: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.222289, acc: 0.000000]\n",
            "2297: [D loss: 0.000023, acc: 1.000000]  [G loss: 10.278183, acc: 0.000000]\n",
            "2298: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.077170, acc: 0.000000]\n",
            "2299: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.196611, acc: 0.000000]\n",
            "2300: [D loss: 0.000023, acc: 1.000000]  [G loss: 10.268613, acc: 0.000000]\n",
            "2301: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.298546, acc: 0.000000]\n",
            "2302: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.273315, acc: 0.000000]\n",
            "2303: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.188549, acc: 0.000000]\n",
            "2304: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.209184, acc: 0.000000]\n",
            "2305: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.436438, acc: 0.000000]\n",
            "2306: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.311571, acc: 0.000000]\n",
            "2307: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.372086, acc: 0.000000]\n",
            "2308: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.117874, acc: 0.000000]\n",
            "2309: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.310382, acc: 0.000000]\n",
            "2310: [D loss: 0.000023, acc: 1.000000]  [G loss: 10.308679, acc: 0.000000]\n",
            "2311: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.380864, acc: 0.000000]\n",
            "2312: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.413118, acc: 0.000000]\n",
            "2313: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.510980, acc: 0.000000]\n",
            "2314: [D loss: 0.000022, acc: 1.000000]  [G loss: 10.263800, acc: 0.000000]\n",
            "2315: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.242915, acc: 0.000000]\n",
            "2316: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.363060, acc: 0.000000]\n",
            "2317: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.335686, acc: 0.000000]\n",
            "2318: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.296352, acc: 0.000000]\n",
            "2319: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.372179, acc: 0.000000]\n",
            "2320: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.415108, acc: 0.000000]\n",
            "2321: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.450026, acc: 0.000000]\n",
            "2322: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.415993, acc: 0.000000]\n",
            "2323: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.313108, acc: 0.000000]\n",
            "2324: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.331139, acc: 0.000000]\n",
            "2325: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.319361, acc: 0.000000]\n",
            "2326: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.436134, acc: 0.000000]\n",
            "2327: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.425315, acc: 0.000000]\n",
            "2328: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.255775, acc: 0.000000]\n",
            "2329: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.464073, acc: 0.000000]\n",
            "2330: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.450127, acc: 0.000000]\n",
            "2331: [D loss: 0.000020, acc: 1.000000]  [G loss: 10.415281, acc: 0.000000]\n",
            "2332: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.418261, acc: 0.000000]\n",
            "2333: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.431705, acc: 0.000000]\n",
            "2334: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.425886, acc: 0.000000]\n",
            "2335: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.610016, acc: 0.000000]\n",
            "2336: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.343258, acc: 0.000000]\n",
            "2337: [D loss: 0.000026, acc: 1.000000]  [G loss: 10.423174, acc: 0.000000]\n",
            "2338: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.604628, acc: 0.000000]\n",
            "2339: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.490503, acc: 0.000000]\n",
            "2340: [D loss: 0.000021, acc: 1.000000]  [G loss: 10.389265, acc: 0.000000]\n",
            "2341: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.594017, acc: 0.000000]\n",
            "2342: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.502461, acc: 0.000000]\n",
            "2343: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.631163, acc: 0.000000]\n",
            "2344: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.508528, acc: 0.000000]\n",
            "2345: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.476465, acc: 0.000000]\n",
            "2346: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.392220, acc: 0.000000]\n",
            "2347: [D loss: 0.000018, acc: 1.000000]  [G loss: 10.633514, acc: 0.000000]\n",
            "2348: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.566055, acc: 0.000000]\n",
            "2349: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.504136, acc: 0.000000]\n",
            "2350: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.495934, acc: 0.000000]\n",
            "2351: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.481474, acc: 0.000000]\n",
            "2352: [D loss: 0.000019, acc: 1.000000]  [G loss: 10.420622, acc: 0.000000]\n",
            "2353: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.370636, acc: 0.000000]\n",
            "2354: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.582197, acc: 0.000000]\n",
            "2355: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.539866, acc: 0.000000]\n",
            "2356: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.528392, acc: 0.000000]\n",
            "2357: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.450642, acc: 0.000000]\n",
            "2358: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.521703, acc: 0.000000]\n",
            "2359: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.571758, acc: 0.000000]\n",
            "2360: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.474874, acc: 0.000000]\n",
            "2361: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.550372, acc: 0.000000]\n",
            "2362: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.617641, acc: 0.000000]\n",
            "2363: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.492826, acc: 0.000000]\n",
            "2364: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.572542, acc: 0.000000]\n",
            "2365: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.616612, acc: 0.000000]\n",
            "2366: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.530160, acc: 0.000000]\n",
            "2367: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.701492, acc: 0.000000]\n",
            "2368: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.446520, acc: 0.000000]\n",
            "2369: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.520024, acc: 0.000000]\n",
            "2370: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.436096, acc: 0.000000]\n",
            "2371: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.590681, acc: 0.000000]\n",
            "2372: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.751079, acc: 0.000000]\n",
            "2373: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.605263, acc: 0.000000]\n",
            "2374: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.692032, acc: 0.000000]\n",
            "2375: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.540110, acc: 0.000000]\n",
            "2376: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.733677, acc: 0.000000]\n",
            "2377: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.651505, acc: 0.000000]\n",
            "2378: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.584291, acc: 0.000000]\n",
            "2379: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.597918, acc: 0.000000]\n",
            "2380: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.690317, acc: 0.000000]\n",
            "2381: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.569315, acc: 0.000000]\n",
            "2382: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.731079, acc: 0.000000]\n",
            "2383: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.542961, acc: 0.000000]\n",
            "2384: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.587299, acc: 0.000000]\n",
            "2385: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.689326, acc: 0.000000]\n",
            "2386: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.635801, acc: 0.000000]\n",
            "2387: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.710605, acc: 0.000000]\n",
            "2388: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.644491, acc: 0.000000]\n",
            "2389: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.752831, acc: 0.000000]\n",
            "2390: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.660161, acc: 0.000000]\n",
            "2391: [D loss: 0.000016, acc: 1.000000]  [G loss: 10.879129, acc: 0.000000]\n",
            "2392: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.776748, acc: 0.000000]\n",
            "2393: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.629492, acc: 0.000000]\n",
            "2394: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.837754, acc: 0.000000]\n",
            "2395: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.721112, acc: 0.000000]\n",
            "2396: [D loss: 0.000017, acc: 1.000000]  [G loss: 10.619190, acc: 0.000000]\n",
            "2397: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.653416, acc: 0.000000]\n",
            "2398: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.713505, acc: 0.000000]\n",
            "2399: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.718558, acc: 0.000000]\n",
            "2400: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.632355, acc: 0.000000]\n",
            "2401: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.725355, acc: 0.000000]\n",
            "2402: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.713322, acc: 0.000000]\n",
            "2403: [D loss: 0.000015, acc: 1.000000]  [G loss: 10.717243, acc: 0.000000]\n",
            "2404: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.742358, acc: 0.000000]\n",
            "2405: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.693847, acc: 0.000000]\n",
            "2406: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.710188, acc: 0.000000]\n",
            "2407: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.711437, acc: 0.000000]\n",
            "2408: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.783514, acc: 0.000000]\n",
            "2409: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.857358, acc: 0.000000]\n",
            "2410: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.703114, acc: 0.000000]\n",
            "2411: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.594767, acc: 0.000000]\n",
            "2412: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.775909, acc: 0.000000]\n",
            "2413: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.774609, acc: 0.000000]\n",
            "2414: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.720089, acc: 0.000000]\n",
            "2415: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.845154, acc: 0.000000]\n",
            "2416: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.834606, acc: 0.000000]\n",
            "2417: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.683982, acc: 0.000000]\n",
            "2418: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.835421, acc: 0.000000]\n",
            "2419: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.914957, acc: 0.000000]\n",
            "2420: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.751936, acc: 0.000000]\n",
            "2421: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.819585, acc: 0.000000]\n",
            "2422: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.895207, acc: 0.000000]\n",
            "2423: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.782253, acc: 0.000000]\n",
            "2424: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.822461, acc: 0.000000]\n",
            "2425: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.762764, acc: 0.000000]\n",
            "2426: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.802942, acc: 0.000000]\n",
            "2427: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.818331, acc: 0.000000]\n",
            "2428: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.752654, acc: 0.000000]\n",
            "2429: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.992700, acc: 0.000000]\n",
            "2430: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.824202, acc: 0.000000]\n",
            "2431: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.814113, acc: 0.000000]\n",
            "2432: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.834305, acc: 0.000000]\n",
            "2433: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.788610, acc: 0.000000]\n",
            "2434: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.712128, acc: 0.000000]\n",
            "2435: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.896515, acc: 0.000000]\n",
            "2436: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.801325, acc: 0.000000]\n",
            "2437: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.857954, acc: 0.000000]\n",
            "2438: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.891636, acc: 0.000000]\n",
            "2439: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.730145, acc: 0.000000]\n",
            "2440: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.966129, acc: 0.000000]\n",
            "2441: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.829998, acc: 0.000000]\n",
            "2442: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.935396, acc: 0.000000]\n",
            "2443: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.946714, acc: 0.000000]\n",
            "2444: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.846395, acc: 0.000000]\n",
            "2445: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.921139, acc: 0.000000]\n",
            "2446: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.851954, acc: 0.000000]\n",
            "2447: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.866148, acc: 0.000000]\n",
            "2448: [D loss: 0.000014, acc: 1.000000]  [G loss: 10.837895, acc: 0.000000]\n",
            "2449: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.989831, acc: 0.000000]\n",
            "2450: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.992107, acc: 0.000000]\n",
            "2451: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.816987, acc: 0.000000]\n",
            "2452: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.036268, acc: 0.000000]\n",
            "2453: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.914782, acc: 0.000000]\n",
            "2454: [D loss: 0.000013, acc: 1.000000]  [G loss: 10.873514, acc: 0.000000]\n",
            "2455: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.924599, acc: 0.000000]\n",
            "2456: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.872123, acc: 0.000000]\n",
            "2457: [D loss: 0.000011, acc: 1.000000]  [G loss: 11.137496, acc: 0.000000]\n",
            "2458: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.933782, acc: 0.000000]\n",
            "2459: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.864717, acc: 0.000000]\n",
            "2460: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.962385, acc: 0.000000]\n",
            "2461: [D loss: 0.000012, acc: 1.000000]  [G loss: 10.911730, acc: 0.000000]\n",
            "2462: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.907112, acc: 0.000000]\n",
            "2463: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.932804, acc: 0.000000]\n",
            "2464: [D loss: 0.000009, acc: 1.000000]  [G loss: 10.936798, acc: 0.000000]\n",
            "2465: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.948693, acc: 0.000000]\n",
            "2466: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.922798, acc: 0.000000]\n",
            "2467: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.984787, acc: 0.000000]\n",
            "2468: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.105181, acc: 0.000000]\n",
            "2469: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.942730, acc: 0.000000]\n",
            "2470: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.937687, acc: 0.000000]\n",
            "2471: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.929512, acc: 0.000000]\n",
            "2472: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.893036, acc: 0.000000]\n",
            "2473: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.914578, acc: 0.000000]\n",
            "2474: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.918407, acc: 0.000000]\n",
            "2475: [D loss: 0.000011, acc: 1.000000]  [G loss: 11.167166, acc: 0.000000]\n",
            "2476: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.927146, acc: 0.000000]\n",
            "2477: [D loss: 0.000012, acc: 1.000000]  [G loss: 11.020950, acc: 0.000000]\n",
            "2478: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.849350, acc: 0.000000]\n",
            "2479: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.044711, acc: 0.000000]\n",
            "2480: [D loss: 0.000011, acc: 1.000000]  [G loss: 11.032509, acc: 0.000000]\n",
            "2481: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.150623, acc: 0.000000]\n",
            "2482: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.014584, acc: 0.000000]\n",
            "2483: [D loss: 0.000011, acc: 1.000000]  [G loss: 11.122056, acc: 0.000000]\n",
            "2484: [D loss: 0.000011, acc: 1.000000]  [G loss: 11.107329, acc: 0.000000]\n",
            "2485: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.059174, acc: 0.000000]\n",
            "2486: [D loss: 0.000011, acc: 1.000000]  [G loss: 11.089468, acc: 0.000000]\n",
            "2487: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.068476, acc: 0.000000]\n",
            "2488: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.972833, acc: 0.000000]\n",
            "2489: [D loss: 0.000011, acc: 1.000000]  [G loss: 10.887209, acc: 0.000000]\n",
            "2490: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.862339, acc: 0.000000]\n",
            "2491: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.053489, acc: 0.000000]\n",
            "2492: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.968413, acc: 0.000000]\n",
            "2493: [D loss: 0.000011, acc: 1.000000]  [G loss: 11.000765, acc: 0.000000]\n",
            "2494: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.113918, acc: 0.000000]\n",
            "2495: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.024179, acc: 0.000000]\n",
            "2496: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.167767, acc: 0.000000]\n",
            "2497: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.090866, acc: 0.000000]\n",
            "2498: [D loss: 0.000010, acc: 1.000000]  [G loss: 10.884371, acc: 0.000000]\n",
            "2499: [D loss: 0.000012, acc: 1.000000]  [G loss: 11.063024, acc: 0.000000]\n",
            "2500: [D loss: 0.000012, acc: 1.000000]  [G loss: 11.217010, acc: 0.000000]\n",
            "2501: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.206581, acc: 0.000000]\n",
            "2502: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.106455, acc: 0.000000]\n",
            "2503: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.181905, acc: 0.000000]\n",
            "2504: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.053947, acc: 0.000000]\n",
            "2505: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.245202, acc: 0.000000]\n",
            "2506: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.051820, acc: 0.000000]\n",
            "2507: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.098082, acc: 0.000000]\n",
            "2508: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.130371, acc: 0.000000]\n",
            "2509: [D loss: 0.000011, acc: 1.000000]  [G loss: 11.235250, acc: 0.000000]\n",
            "2510: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.201636, acc: 0.000000]\n",
            "2511: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.289986, acc: 0.000000]\n",
            "2512: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.076125, acc: 0.000000]\n",
            "2513: [D loss: 0.000011, acc: 1.000000]  [G loss: 11.183418, acc: 0.000000]\n",
            "2514: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.282287, acc: 0.000000]\n",
            "2515: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.110188, acc: 0.000000]\n",
            "2516: [D loss: 0.000008, acc: 1.000000]  [G loss: 10.948427, acc: 0.000000]\n",
            "2517: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.138686, acc: 0.000000]\n",
            "2518: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.264858, acc: 0.000000]\n",
            "2519: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.074305, acc: 0.000000]\n",
            "2520: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.134064, acc: 0.000000]\n",
            "2521: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.111008, acc: 0.000000]\n",
            "2522: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.121071, acc: 0.000000]\n",
            "2523: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.175754, acc: 0.000000]\n",
            "2524: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.147957, acc: 0.000000]\n",
            "2525: [D loss: 0.000009, acc: 1.000000]  [G loss: 10.986446, acc: 0.000000]\n",
            "2526: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.165145, acc: 0.000000]\n",
            "2527: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.142170, acc: 0.000000]\n",
            "2528: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.074442, acc: 0.000000]\n",
            "2529: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.163980, acc: 0.000000]\n",
            "2530: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.223364, acc: 0.000000]\n",
            "2531: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.272688, acc: 0.000000]\n",
            "2532: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.174412, acc: 0.000000]\n",
            "2533: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.134434, acc: 0.000000]\n",
            "2534: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.187010, acc: 0.000000]\n",
            "2535: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.207220, acc: 0.000000]\n",
            "2536: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.363852, acc: 0.000000]\n",
            "2537: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.207205, acc: 0.000000]\n",
            "2538: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.420877, acc: 0.000000]\n",
            "2539: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.169136, acc: 0.000000]\n",
            "2540: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.325848, acc: 0.000000]\n",
            "2541: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.221119, acc: 0.000000]\n",
            "2542: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.197384, acc: 0.000000]\n",
            "2543: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.176772, acc: 0.000000]\n",
            "2544: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.168271, acc: 0.000000]\n",
            "2545: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.311110, acc: 0.000000]\n",
            "2546: [D loss: 0.000171, acc: 1.000000]  [G loss: 11.084659, acc: 0.000000]\n",
            "2547: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.304279, acc: 0.000000]\n",
            "2548: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.340780, acc: 0.000000]\n",
            "2549: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.248795, acc: 0.000000]\n",
            "2550: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.050152, acc: 0.000000]\n",
            "2551: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.327653, acc: 0.000000]\n",
            "2552: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.121950, acc: 0.000000]\n",
            "2553: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.260837, acc: 0.000000]\n",
            "2554: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.189379, acc: 0.000000]\n",
            "2555: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.230163, acc: 0.000000]\n",
            "2556: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.209721, acc: 0.000000]\n",
            "2557: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.080211, acc: 0.000000]\n",
            "2558: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.215994, acc: 0.000000]\n",
            "2559: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.278440, acc: 0.000000]\n",
            "2560: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.279528, acc: 0.000000]\n",
            "2561: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.226052, acc: 0.000000]\n",
            "2562: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.259462, acc: 0.000000]\n",
            "2563: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.228891, acc: 0.000000]\n",
            "2564: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.189025, acc: 0.000000]\n",
            "2565: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.134228, acc: 0.000000]\n",
            "2566: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.316148, acc: 0.000000]\n",
            "2567: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.362703, acc: 0.000000]\n",
            "2568: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.200623, acc: 0.000000]\n",
            "2569: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.217705, acc: 0.000000]\n",
            "2570: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.373419, acc: 0.000000]\n",
            "2571: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.263269, acc: 0.000000]\n",
            "2572: [D loss: 0.000010, acc: 1.000000]  [G loss: 11.292751, acc: 0.000000]\n",
            "2573: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.104749, acc: 0.000000]\n",
            "2574: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.182916, acc: 0.000000]\n",
            "2575: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.304630, acc: 0.000000]\n",
            "2576: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.257152, acc: 0.000000]\n",
            "2577: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.216083, acc: 0.000000]\n",
            "2578: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.270308, acc: 0.000000]\n",
            "2579: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.141607, acc: 0.000000]\n",
            "2580: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.083962, acc: 0.000000]\n",
            "2581: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.270868, acc: 0.000000]\n",
            "2582: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.263487, acc: 0.000000]\n",
            "2583: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.311966, acc: 0.000000]\n",
            "2584: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.206116, acc: 0.000000]\n",
            "2585: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.313147, acc: 0.000000]\n",
            "2586: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.284767, acc: 0.000000]\n",
            "2587: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.332716, acc: 0.000000]\n",
            "2588: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.353514, acc: 0.000000]\n",
            "2589: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.284023, acc: 0.000000]\n",
            "2590: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.290697, acc: 0.000000]\n",
            "2591: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.301422, acc: 0.000000]\n",
            "2592: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.234417, acc: 0.000000]\n",
            "2593: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.247015, acc: 0.000000]\n",
            "2594: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.310883, acc: 0.000000]\n",
            "2595: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.488831, acc: 0.000000]\n",
            "2596: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.377877, acc: 0.000000]\n",
            "2597: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.370934, acc: 0.000000]\n",
            "2598: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.329794, acc: 0.000000]\n",
            "2599: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.212666, acc: 0.000000]\n",
            "2600: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.365366, acc: 0.000000]\n",
            "2601: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.351718, acc: 0.000000]\n",
            "2602: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.338846, acc: 0.000000]\n",
            "2603: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.400331, acc: 0.000000]\n",
            "2604: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.282325, acc: 0.000000]\n",
            "2605: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.231913, acc: 0.000000]\n",
            "2606: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.333475, acc: 0.000000]\n",
            "2607: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.298674, acc: 0.000000]\n",
            "2608: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.344341, acc: 0.000000]\n",
            "2609: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.299011, acc: 0.000000]\n",
            "2610: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.361280, acc: 0.000000]\n",
            "2611: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.436726, acc: 0.000000]\n",
            "2612: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.298903, acc: 0.000000]\n",
            "2613: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.363742, acc: 0.000000]\n",
            "2614: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.379787, acc: 0.000000]\n",
            "2615: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.219545, acc: 0.000000]\n",
            "2616: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.372076, acc: 0.000000]\n",
            "2617: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.180266, acc: 0.000000]\n",
            "2618: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.373953, acc: 0.000000]\n",
            "2619: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.447656, acc: 0.000000]\n",
            "2620: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.501181, acc: 0.000000]\n",
            "2621: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.413014, acc: 0.000000]\n",
            "2622: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.296626, acc: 0.000000]\n",
            "2623: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.322243, acc: 0.000000]\n",
            "2624: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.311417, acc: 0.000000]\n",
            "2625: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.282278, acc: 0.000000]\n",
            "2626: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.257532, acc: 0.000000]\n",
            "2627: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.338972, acc: 0.000000]\n",
            "2628: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.433636, acc: 0.000000]\n",
            "2629: [D loss: 0.000011, acc: 1.000000]  [G loss: 11.410294, acc: 0.000000]\n",
            "2630: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.438860, acc: 0.000000]\n",
            "2631: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.608023, acc: 0.000000]\n",
            "2632: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.357026, acc: 0.000000]\n",
            "2633: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.294446, acc: 0.000000]\n",
            "2634: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.423032, acc: 0.000000]\n",
            "2635: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.436501, acc: 0.000000]\n",
            "2636: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.385953, acc: 0.000000]\n",
            "2637: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.271664, acc: 0.000000]\n",
            "2638: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.511843, acc: 0.000000]\n",
            "2639: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.383085, acc: 0.000000]\n",
            "2640: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.447424, acc: 0.000000]\n",
            "2641: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.349292, acc: 0.000000]\n",
            "2642: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.406086, acc: 0.000000]\n",
            "2643: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.218297, acc: 0.000000]\n",
            "2644: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.453644, acc: 0.000000]\n",
            "2645: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.351042, acc: 0.000000]\n",
            "2646: [D loss: 0.000032, acc: 1.000000]  [G loss: 11.459334, acc: 0.000000]\n",
            "2647: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.533213, acc: 0.000000]\n",
            "2648: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.451149, acc: 0.000000]\n",
            "2649: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.343718, acc: 0.000000]\n",
            "2650: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.342134, acc: 0.000000]\n",
            "2651: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.479210, acc: 0.000000]\n",
            "2652: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.560721, acc: 0.000000]\n",
            "2653: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.499413, acc: 0.000000]\n",
            "2654: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.403896, acc: 0.000000]\n",
            "2655: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.371674, acc: 0.000000]\n",
            "2656: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.280159, acc: 0.000000]\n",
            "2657: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.544592, acc: 0.000000]\n",
            "2658: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.256987, acc: 0.000000]\n",
            "2659: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.505339, acc: 0.000000]\n",
            "2660: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.354434, acc: 0.000000]\n",
            "2661: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.458675, acc: 0.000000]\n",
            "2662: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.461433, acc: 0.000000]\n",
            "2663: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.505270, acc: 0.000000]\n",
            "2664: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.553846, acc: 0.000000]\n",
            "2665: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.407713, acc: 0.000000]\n",
            "2666: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.274260, acc: 0.000000]\n",
            "2667: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.545525, acc: 0.000000]\n",
            "2668: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.580984, acc: 0.000000]\n",
            "2669: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.373442, acc: 0.000000]\n",
            "2670: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.374588, acc: 0.000000]\n",
            "2671: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.422809, acc: 0.000000]\n",
            "2672: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.594536, acc: 0.000000]\n",
            "2673: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.525909, acc: 0.000000]\n",
            "2674: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.473864, acc: 0.000000]\n",
            "2675: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.534645, acc: 0.000000]\n",
            "2676: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.420918, acc: 0.000000]\n",
            "2677: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.303514, acc: 0.000000]\n",
            "2678: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.497686, acc: 0.000000]\n",
            "2679: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.598310, acc: 0.000000]\n",
            "2680: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.568208, acc: 0.000000]\n",
            "2681: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.534069, acc: 0.000000]\n",
            "2682: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.322595, acc: 0.000000]\n",
            "2683: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.394320, acc: 0.000000]\n",
            "2684: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.427244, acc: 0.000000]\n",
            "2685: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.419220, acc: 0.000000]\n",
            "2686: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.511641, acc: 0.000000]\n",
            "2687: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.456484, acc: 0.000000]\n",
            "2688: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.525135, acc: 0.000000]\n",
            "2689: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.524629, acc: 0.000000]\n",
            "2690: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.457836, acc: 0.000000]\n",
            "2691: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.541700, acc: 0.000000]\n",
            "2692: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.660646, acc: 0.000000]\n",
            "2693: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.408046, acc: 0.000000]\n",
            "2694: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.491204, acc: 0.000000]\n",
            "2695: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.440325, acc: 0.000000]\n",
            "2696: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.482174, acc: 0.000000]\n",
            "2697: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.566671, acc: 0.000000]\n",
            "2698: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.468943, acc: 0.000000]\n",
            "2699: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.533324, acc: 0.000000]\n",
            "2700: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.586319, acc: 0.000000]\n",
            "2701: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.487417, acc: 0.000000]\n",
            "2702: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.553791, acc: 0.000000]\n",
            "2703: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.572895, acc: 0.000000]\n",
            "2704: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.607448, acc: 0.000000]\n",
            "2705: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.635375, acc: 0.000000]\n",
            "2706: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.520241, acc: 0.000000]\n",
            "2707: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.712268, acc: 0.000000]\n",
            "2708: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.486904, acc: 0.000000]\n",
            "2709: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.558694, acc: 0.000000]\n",
            "2710: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.610659, acc: 0.000000]\n",
            "2711: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.510064, acc: 0.000000]\n",
            "2712: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.505419, acc: 0.000000]\n",
            "2713: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.550983, acc: 0.000000]\n",
            "2714: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.603315, acc: 0.000000]\n",
            "2715: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.496821, acc: 0.000000]\n",
            "2716: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.508859, acc: 0.000000]\n",
            "2717: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.607338, acc: 0.000000]\n",
            "2718: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.531867, acc: 0.000000]\n",
            "2719: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.492323, acc: 0.000000]\n",
            "2720: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.617840, acc: 0.000000]\n",
            "2721: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.636218, acc: 0.000000]\n",
            "2722: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.663737, acc: 0.000000]\n",
            "2723: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.563070, acc: 0.000000]\n",
            "2724: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.639828, acc: 0.000000]\n",
            "2725: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.575909, acc: 0.000000]\n",
            "2726: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.584824, acc: 0.000000]\n",
            "2727: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.680183, acc: 0.000000]\n",
            "2728: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.486559, acc: 0.000000]\n",
            "2729: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.613416, acc: 0.000000]\n",
            "2730: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.577376, acc: 0.000000]\n",
            "2731: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.591375, acc: 0.000000]\n",
            "2732: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.563565, acc: 0.000000]\n",
            "2733: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.612329, acc: 0.000000]\n",
            "2734: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.617022, acc: 0.000000]\n",
            "2735: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.626421, acc: 0.000000]\n",
            "2736: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.550718, acc: 0.000000]\n",
            "2737: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.540511, acc: 0.000000]\n",
            "2738: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.609505, acc: 0.000000]\n",
            "2739: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.622494, acc: 0.000000]\n",
            "2740: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.870515, acc: 0.000000]\n",
            "2741: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.528934, acc: 0.000000]\n",
            "2742: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.772499, acc: 0.000000]\n",
            "2743: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.533535, acc: 0.000000]\n",
            "2744: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.616506, acc: 0.000000]\n",
            "2745: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.718374, acc: 0.000000]\n",
            "2746: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.732599, acc: 0.000000]\n",
            "2747: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.592223, acc: 0.000000]\n",
            "2748: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.607950, acc: 0.000000]\n",
            "2749: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.671666, acc: 0.000000]\n",
            "2750: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.587496, acc: 0.000000]\n",
            "2751: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.665143, acc: 0.000000]\n",
            "2752: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.724433, acc: 0.000000]\n",
            "2753: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.590971, acc: 0.000000]\n",
            "2754: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.679682, acc: 0.000000]\n",
            "2755: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.610014, acc: 0.000000]\n",
            "2756: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.753769, acc: 0.000000]\n",
            "2757: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.558141, acc: 0.000000]\n",
            "2758: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.584997, acc: 0.000000]\n",
            "2759: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.752728, acc: 0.000000]\n",
            "2760: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.660516, acc: 0.000000]\n",
            "2761: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.667420, acc: 0.000000]\n",
            "2762: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.677552, acc: 0.000000]\n",
            "2763: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.581446, acc: 0.000000]\n",
            "2764: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.648992, acc: 0.000000]\n",
            "2765: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.604126, acc: 0.000000]\n",
            "2766: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.631240, acc: 0.000000]\n",
            "2767: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.737049, acc: 0.000000]\n",
            "2768: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.647524, acc: 0.000000]\n",
            "2769: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.784546, acc: 0.000000]\n",
            "2770: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.738234, acc: 0.000000]\n",
            "2771: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.854483, acc: 0.000000]\n",
            "2772: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.692262, acc: 0.000000]\n",
            "2773: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.693317, acc: 0.000000]\n",
            "2774: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.809078, acc: 0.000000]\n",
            "2775: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.806319, acc: 0.000000]\n",
            "2776: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.785257, acc: 0.000000]\n",
            "2777: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.726328, acc: 0.000000]\n",
            "2778: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.638536, acc: 0.000000]\n",
            "2779: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.617100, acc: 0.000000]\n",
            "2780: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.660700, acc: 0.000000]\n",
            "2781: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.730371, acc: 0.000000]\n",
            "2782: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.687985, acc: 0.000000]\n",
            "2783: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.693164, acc: 0.000000]\n",
            "2784: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.688520, acc: 0.000000]\n",
            "2785: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.669180, acc: 0.000000]\n",
            "2786: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.609171, acc: 0.000000]\n",
            "2787: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.749369, acc: 0.000000]\n",
            "2788: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.743464, acc: 0.000000]\n",
            "2789: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.779694, acc: 0.000000]\n",
            "2790: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.688364, acc: 0.000000]\n",
            "2791: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.607603, acc: 0.000000]\n",
            "2792: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.830986, acc: 0.000000]\n",
            "2793: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.831589, acc: 0.000000]\n",
            "2794: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.678492, acc: 0.000000]\n",
            "2795: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.773367, acc: 0.000000]\n",
            "2796: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.760859, acc: 0.000000]\n",
            "2797: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.581331, acc: 0.000000]\n",
            "2798: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.956786, acc: 0.000000]\n",
            "2799: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.718382, acc: 0.000000]\n",
            "2800: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.731823, acc: 0.000000]\n",
            "2801: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.785325, acc: 0.000000]\n",
            "2802: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.700640, acc: 0.000000]\n",
            "2803: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.697168, acc: 0.000000]\n",
            "2804: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.717886, acc: 0.000000]\n",
            "2805: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.740562, acc: 0.000000]\n",
            "2806: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.780904, acc: 0.000000]\n",
            "2807: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.910726, acc: 0.000000]\n",
            "2808: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.809408, acc: 0.000000]\n",
            "2809: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.735814, acc: 0.000000]\n",
            "2810: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.949890, acc: 0.000000]\n",
            "2811: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.880791, acc: 0.000000]\n",
            "2812: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.933707, acc: 0.000000]\n",
            "2813: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.860737, acc: 0.000000]\n",
            "2814: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.674349, acc: 0.000000]\n",
            "2815: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.741052, acc: 0.000000]\n",
            "2816: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.799635, acc: 0.000000]\n",
            "2817: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.764462, acc: 0.000000]\n",
            "2818: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.841272, acc: 0.000000]\n",
            "2819: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.823319, acc: 0.000000]\n",
            "2820: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.783189, acc: 0.000000]\n",
            "2821: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.890701, acc: 0.000000]\n",
            "2822: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.803122, acc: 0.000000]\n",
            "2823: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.750868, acc: 0.000000]\n",
            "2824: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.810692, acc: 0.000000]\n",
            "2825: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.807606, acc: 0.000000]\n",
            "2826: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.909835, acc: 0.000000]\n",
            "2827: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.822309, acc: 0.000000]\n",
            "2828: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.847008, acc: 0.000000]\n",
            "2829: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.038873, acc: 0.000000]\n",
            "2830: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.758251, acc: 0.000000]\n",
            "2831: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.676798, acc: 0.000000]\n",
            "2832: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.905594, acc: 0.000000]\n",
            "2833: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.762334, acc: 0.000000]\n",
            "2834: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.928140, acc: 0.000000]\n",
            "2835: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.031216, acc: 0.000000]\n",
            "2836: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.857781, acc: 0.000000]\n",
            "2837: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.596712, acc: 0.000000]\n",
            "2838: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.687846, acc: 0.000000]\n",
            "2839: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.707575, acc: 0.000000]\n",
            "2840: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.838724, acc: 0.000000]\n",
            "2841: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.822039, acc: 0.000000]\n",
            "2842: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.810507, acc: 0.000000]\n",
            "2843: [D loss: 0.003184, acc: 1.000000]  [G loss: 11.735015, acc: 0.000000]\n",
            "2844: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.653576, acc: 0.000000]\n",
            "2845: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.725296, acc: 0.000000]\n",
            "2846: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.629803, acc: 0.000000]\n",
            "2847: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.644557, acc: 0.000000]\n",
            "2848: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.525561, acc: 0.000000]\n",
            "2849: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.504091, acc: 0.000000]\n",
            "2850: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.613219, acc: 0.000000]\n",
            "2851: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.410410, acc: 0.000000]\n",
            "2852: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.513968, acc: 0.000000]\n",
            "2853: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.528166, acc: 0.000000]\n",
            "2854: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.463448, acc: 0.000000]\n",
            "2855: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.457718, acc: 0.000000]\n",
            "2856: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.584793, acc: 0.000000]\n",
            "2857: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.538360, acc: 0.000000]\n",
            "2858: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.399467, acc: 0.000000]\n",
            "2859: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.578346, acc: 0.000000]\n",
            "2860: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.406878, acc: 0.000000]\n",
            "2861: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.424535, acc: 0.000000]\n",
            "2862: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.365114, acc: 0.000000]\n",
            "2863: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.341639, acc: 0.000000]\n",
            "2864: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.425841, acc: 0.000000]\n",
            "2865: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.434529, acc: 0.000000]\n",
            "2866: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.468442, acc: 0.000000]\n",
            "2867: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.334902, acc: 0.000000]\n",
            "2868: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.338074, acc: 0.000000]\n",
            "2869: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.497688, acc: 0.000000]\n",
            "2870: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.418129, acc: 0.000000]\n",
            "2871: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.248910, acc: 0.000000]\n",
            "2872: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.321982, acc: 0.000000]\n",
            "2873: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.421764, acc: 0.000000]\n",
            "2874: [D loss: 0.000009, acc: 1.000000]  [G loss: 11.452287, acc: 0.000000]\n",
            "2875: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.308596, acc: 0.000000]\n",
            "2876: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.304430, acc: 0.000000]\n",
            "2877: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.306944, acc: 0.000000]\n",
            "2878: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.293636, acc: 0.000000]\n",
            "2879: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.389120, acc: 0.000000]\n",
            "2880: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.414308, acc: 0.000000]\n",
            "2881: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.469618, acc: 0.000000]\n",
            "2882: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.405426, acc: 0.000000]\n",
            "2883: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.254925, acc: 0.000000]\n",
            "2884: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.303411, acc: 0.000000]\n",
            "2885: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.344955, acc: 0.000000]\n",
            "2886: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.433825, acc: 0.000000]\n",
            "2887: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.515736, acc: 0.000000]\n",
            "2888: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.510864, acc: 0.000000]\n",
            "2889: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.420105, acc: 0.000000]\n",
            "2890: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.460073, acc: 0.000000]\n",
            "2891: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.454911, acc: 0.000000]\n",
            "2892: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.530923, acc: 0.000000]\n",
            "2893: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.324442, acc: 0.000000]\n",
            "2894: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.332031, acc: 0.000000]\n",
            "2895: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.536094, acc: 0.000000]\n",
            "2896: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.382049, acc: 0.000000]\n",
            "2897: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.354950, acc: 0.000000]\n",
            "2898: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.391942, acc: 0.000000]\n",
            "2899: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.472748, acc: 0.000000]\n",
            "2900: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.503983, acc: 0.000000]\n",
            "2901: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.384065, acc: 0.000000]\n",
            "2902: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.428570, acc: 0.000000]\n",
            "2903: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.518516, acc: 0.000000]\n",
            "2904: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.451321, acc: 0.000000]\n",
            "2905: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.411606, acc: 0.000000]\n",
            "2906: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.392792, acc: 0.000000]\n",
            "2907: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.307442, acc: 0.000000]\n",
            "2908: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.473037, acc: 0.000000]\n",
            "2909: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.505494, acc: 0.000000]\n",
            "2910: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.563465, acc: 0.000000]\n",
            "2911: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.450332, acc: 0.000000]\n",
            "2912: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.637262, acc: 0.000000]\n",
            "2913: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.447104, acc: 0.000000]\n",
            "2914: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.511580, acc: 0.000000]\n",
            "2915: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.468660, acc: 0.000000]\n",
            "2916: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.381733, acc: 0.000000]\n",
            "2917: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.401127, acc: 0.000000]\n",
            "2918: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.444822, acc: 0.000000]\n",
            "2919: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.377842, acc: 0.000000]\n",
            "2920: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.592472, acc: 0.000000]\n",
            "2921: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.519150, acc: 0.000000]\n",
            "2922: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.573916, acc: 0.000000]\n",
            "2923: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.570881, acc: 0.000000]\n",
            "2924: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.263140, acc: 0.000000]\n",
            "2925: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.490408, acc: 0.000000]\n",
            "2926: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.535902, acc: 0.000000]\n",
            "2927: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.446070, acc: 0.000000]\n",
            "2928: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.478708, acc: 0.000000]\n",
            "2929: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.470604, acc: 0.000000]\n",
            "2930: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.362099, acc: 0.000000]\n",
            "2931: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.464788, acc: 0.000000]\n",
            "2932: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.479773, acc: 0.000000]\n",
            "2933: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.664742, acc: 0.000000]\n",
            "2934: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.489510, acc: 0.000000]\n",
            "2935: [D loss: 0.000059, acc: 1.000000]  [G loss: 11.456403, acc: 0.000000]\n",
            "2936: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.504860, acc: 0.000000]\n",
            "2937: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.453327, acc: 0.000000]\n",
            "2938: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.553019, acc: 0.000000]\n",
            "2939: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.517452, acc: 0.000000]\n",
            "2940: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.485174, acc: 0.000000]\n",
            "2941: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.559237, acc: 0.000000]\n",
            "2942: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.579583, acc: 0.000000]\n",
            "2943: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.571589, acc: 0.000000]\n",
            "2944: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.625296, acc: 0.000000]\n",
            "2945: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.365481, acc: 0.000000]\n",
            "2946: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.535080, acc: 0.000000]\n",
            "2947: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.622560, acc: 0.000000]\n",
            "2948: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.610617, acc: 0.000000]\n",
            "2949: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.424170, acc: 0.000000]\n",
            "2950: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.475627, acc: 0.000000]\n",
            "2951: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.504993, acc: 0.000000]\n",
            "2952: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.589125, acc: 0.000000]\n",
            "2953: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.630846, acc: 0.000000]\n",
            "2954: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.381725, acc: 0.000000]\n",
            "2955: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.522711, acc: 0.000000]\n",
            "2956: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.501505, acc: 0.000000]\n",
            "2957: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.504652, acc: 0.000000]\n",
            "2958: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.543745, acc: 0.000000]\n",
            "2959: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.478584, acc: 0.000000]\n",
            "2960: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.660280, acc: 0.000000]\n",
            "2961: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.590019, acc: 0.000000]\n",
            "2962: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.523733, acc: 0.000000]\n",
            "2963: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.443320, acc: 0.000000]\n",
            "2964: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.419825, acc: 0.000000]\n",
            "2965: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.533253, acc: 0.000000]\n",
            "2966: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.660183, acc: 0.000000]\n",
            "2967: [D loss: 0.000008, acc: 1.000000]  [G loss: 11.652590, acc: 0.000000]\n",
            "2968: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.505251, acc: 0.000000]\n",
            "2969: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.579099, acc: 0.000000]\n",
            "2970: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.550904, acc: 0.000000]\n",
            "2971: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.549686, acc: 0.000000]\n",
            "2972: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.675056, acc: 0.000000]\n",
            "2973: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.591343, acc: 0.000000]\n",
            "2974: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.723072, acc: 0.000000]\n",
            "2975: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.676900, acc: 0.000000]\n",
            "2976: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.643059, acc: 0.000000]\n",
            "2977: [D loss: 0.000014, acc: 1.000000]  [G loss: 11.532120, acc: 0.000000]\n",
            "2978: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.672998, acc: 0.000000]\n",
            "2979: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.507814, acc: 0.000000]\n",
            "2980: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.745358, acc: 0.000000]\n",
            "2981: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.628529, acc: 0.000000]\n",
            "2982: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.631418, acc: 0.000000]\n",
            "2983: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.654867, acc: 0.000000]\n",
            "2984: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.609731, acc: 0.000000]\n",
            "2985: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.546391, acc: 0.000000]\n",
            "2986: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.681667, acc: 0.000000]\n",
            "2987: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.719852, acc: 0.000000]\n",
            "2988: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.672869, acc: 0.000000]\n",
            "2989: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.628839, acc: 0.000000]\n",
            "2990: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.583660, acc: 0.000000]\n",
            "2991: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.614269, acc: 0.000000]\n",
            "2992: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.542988, acc: 0.000000]\n",
            "2993: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.653942, acc: 0.000000]\n",
            "2994: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.747323, acc: 0.000000]\n",
            "2995: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.708923, acc: 0.000000]\n",
            "2996: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.727879, acc: 0.000000]\n",
            "2997: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.597316, acc: 0.000000]\n",
            "2998: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.647036, acc: 0.000000]\n",
            "2999: [D loss: 0.000007, acc: 1.000000]  [G loss: 11.672662, acc: 0.000000]\n",
            "3000: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.634066, acc: 0.000000]\n",
            "3001: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.456432, acc: 0.000000]\n",
            "3002: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.683582, acc: 0.000000]\n",
            "3003: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.688200, acc: 0.000000]\n",
            "3004: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.745668, acc: 0.000000]\n",
            "3005: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.783085, acc: 0.000000]\n",
            "3006: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.682392, acc: 0.000000]\n",
            "3007: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.744864, acc: 0.000000]\n",
            "3008: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.643902, acc: 0.000000]\n",
            "3009: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.647477, acc: 0.000000]\n",
            "3010: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.696560, acc: 0.000000]\n",
            "3011: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.720205, acc: 0.000000]\n",
            "3012: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.678850, acc: 0.000000]\n",
            "3013: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.681832, acc: 0.000000]\n",
            "3014: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.815876, acc: 0.000000]\n",
            "3015: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.739952, acc: 0.000000]\n",
            "3016: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.770212, acc: 0.000000]\n",
            "3017: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.763586, acc: 0.000000]\n",
            "3018: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.657190, acc: 0.000000]\n",
            "3019: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.610278, acc: 0.000000]\n",
            "3020: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.737646, acc: 0.000000]\n",
            "3021: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.682878, acc: 0.000000]\n",
            "3022: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.710073, acc: 0.000000]\n",
            "3023: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.797934, acc: 0.000000]\n",
            "3024: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.628222, acc: 0.000000]\n",
            "3025: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.710634, acc: 0.000000]\n",
            "3026: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.689842, acc: 0.000000]\n",
            "3027: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.744575, acc: 0.000000]\n",
            "3028: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.776556, acc: 0.000000]\n",
            "3029: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.759868, acc: 0.000000]\n",
            "3030: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.929684, acc: 0.000000]\n",
            "3031: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.801366, acc: 0.000000]\n",
            "3032: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.733629, acc: 0.000000]\n",
            "3033: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.662962, acc: 0.000000]\n",
            "3034: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.748938, acc: 0.000000]\n",
            "3035: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.872395, acc: 0.000000]\n",
            "3036: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.653116, acc: 0.000000]\n",
            "3037: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.738314, acc: 0.000000]\n",
            "3038: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.778194, acc: 0.000000]\n",
            "3039: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.673901, acc: 0.000000]\n",
            "3040: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.800314, acc: 0.000000]\n",
            "3041: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.894662, acc: 0.000000]\n",
            "3042: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.739712, acc: 0.000000]\n",
            "3043: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.744226, acc: 0.000000]\n",
            "3044: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.669029, acc: 0.000000]\n",
            "3045: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.828022, acc: 0.000000]\n",
            "3046: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.722315, acc: 0.000000]\n",
            "3047: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.801711, acc: 0.000000]\n",
            "3048: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.730016, acc: 0.000000]\n",
            "3049: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.849225, acc: 0.000000]\n",
            "3050: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.777405, acc: 0.000000]\n",
            "3051: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.811903, acc: 0.000000]\n",
            "3052: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.888348, acc: 0.000000]\n",
            "3053: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.693066, acc: 0.000000]\n",
            "3054: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.675611, acc: 0.000000]\n",
            "3055: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.784125, acc: 0.000000]\n",
            "3056: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.893400, acc: 0.000000]\n",
            "3057: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.779619, acc: 0.000000]\n",
            "3058: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.721586, acc: 0.000000]\n",
            "3059: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.721170, acc: 0.000000]\n",
            "3060: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.935291, acc: 0.000000]\n",
            "3061: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.798923, acc: 0.000000]\n",
            "3062: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.933157, acc: 0.000000]\n",
            "3063: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.841902, acc: 0.000000]\n",
            "3064: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.728723, acc: 0.000000]\n",
            "3065: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.726544, acc: 0.000000]\n",
            "3066: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.856282, acc: 0.000000]\n",
            "3067: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.826897, acc: 0.000000]\n",
            "3068: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.911424, acc: 0.000000]\n",
            "3069: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.738831, acc: 0.000000]\n",
            "3070: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.978343, acc: 0.000000]\n",
            "3071: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.761967, acc: 0.000000]\n",
            "3072: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.827929, acc: 0.000000]\n",
            "3073: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.647781, acc: 0.000000]\n",
            "3074: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.731441, acc: 0.000000]\n",
            "3075: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.883612, acc: 0.000000]\n",
            "3076: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.741497, acc: 0.000000]\n",
            "3077: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.907341, acc: 0.000000]\n",
            "3078: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.733953, acc: 0.000000]\n",
            "3079: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.941680, acc: 0.000000]\n",
            "3080: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.790982, acc: 0.000000]\n",
            "3081: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.645567, acc: 0.000000]\n",
            "3082: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.999533, acc: 0.000000]\n",
            "3083: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.991035, acc: 0.000000]\n",
            "3084: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.901799, acc: 0.000000]\n",
            "3085: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.787158, acc: 0.000000]\n",
            "3086: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.845039, acc: 0.000000]\n",
            "3087: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.927584, acc: 0.000000]\n",
            "3088: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.900291, acc: 0.000000]\n",
            "3089: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.849621, acc: 0.000000]\n",
            "3090: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.999325, acc: 0.000000]\n",
            "3091: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.915179, acc: 0.000000]\n",
            "3092: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.046354, acc: 0.000000]\n",
            "3093: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.761759, acc: 0.000000]\n",
            "3094: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.047330, acc: 0.000000]\n",
            "3095: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.810448, acc: 0.000000]\n",
            "3096: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.920428, acc: 0.000000]\n",
            "3097: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.050055, acc: 0.000000]\n",
            "3098: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.991079, acc: 0.000000]\n",
            "3099: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.747787, acc: 0.000000]\n",
            "3100: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.963215, acc: 0.000000]\n",
            "3101: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.894283, acc: 0.000000]\n",
            "3102: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.933420, acc: 0.000000]\n",
            "3103: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.771633, acc: 0.000000]\n",
            "3104: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.854599, acc: 0.000000]\n",
            "3105: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.915408, acc: 0.000000]\n",
            "3106: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.875364, acc: 0.000000]\n",
            "3107: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.835060, acc: 0.000000]\n",
            "3108: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.946707, acc: 0.000000]\n",
            "3109: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.941403, acc: 0.000000]\n",
            "3110: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.855989, acc: 0.000000]\n",
            "3111: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.848072, acc: 0.000000]\n",
            "3112: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.986311, acc: 0.000000]\n",
            "3113: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.653805, acc: 0.000000]\n",
            "3114: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.834009, acc: 0.000000]\n",
            "3115: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.952419, acc: 0.000000]\n",
            "3116: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.029651, acc: 0.000000]\n",
            "3117: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.984678, acc: 0.000000]\n",
            "3118: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.985568, acc: 0.000000]\n",
            "3119: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.881180, acc: 0.000000]\n",
            "3120: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.879202, acc: 0.000000]\n",
            "3121: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.951194, acc: 0.000000]\n",
            "3122: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.090856, acc: 0.000000]\n",
            "3123: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.862019, acc: 0.000000]\n",
            "3124: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.884668, acc: 0.000000]\n",
            "3125: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.067929, acc: 0.000000]\n",
            "3126: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.740887, acc: 0.000000]\n",
            "3127: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.992746, acc: 0.000000]\n",
            "3128: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.920098, acc: 0.000000]\n",
            "3129: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.916203, acc: 0.000000]\n",
            "3130: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.096332, acc: 0.000000]\n",
            "3131: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.862244, acc: 0.000000]\n",
            "3132: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.075021, acc: 0.000000]\n",
            "3133: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.073917, acc: 0.000000]\n",
            "3134: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.008777, acc: 0.000000]\n",
            "3135: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.743650, acc: 0.000000]\n",
            "3136: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.923487, acc: 0.000000]\n",
            "3137: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.844134, acc: 0.000000]\n",
            "3138: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.018104, acc: 0.000000]\n",
            "3139: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.937923, acc: 0.000000]\n",
            "3140: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.110906, acc: 0.000000]\n",
            "3141: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.926941, acc: 0.000000]\n",
            "3142: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.012756, acc: 0.000000]\n",
            "3143: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.000386, acc: 0.000000]\n",
            "3144: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.878069, acc: 0.000000]\n",
            "3145: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.885259, acc: 0.000000]\n",
            "3146: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.027433, acc: 0.000000]\n",
            "3147: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.930933, acc: 0.000000]\n",
            "3148: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.003562, acc: 0.000000]\n",
            "3149: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.890694, acc: 0.000000]\n",
            "3150: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.083916, acc: 0.000000]\n",
            "3151: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.925415, acc: 0.000000]\n",
            "3152: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.990725, acc: 0.000000]\n",
            "3153: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.989401, acc: 0.000000]\n",
            "3154: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.876520, acc: 0.000000]\n",
            "3155: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.988665, acc: 0.000000]\n",
            "3156: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.958246, acc: 0.000000]\n",
            "3157: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.047001, acc: 0.000000]\n",
            "3158: [D loss: 0.000003, acc: 1.000000]  [G loss: 11.987978, acc: 0.000000]\n",
            "3159: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.017921, acc: 0.000000]\n",
            "3160: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.990854, acc: 0.000000]\n",
            "3161: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.894290, acc: 0.000000]\n",
            "3162: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.101321, acc: 0.000000]\n",
            "3163: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.960606, acc: 0.000000]\n",
            "3164: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.933712, acc: 0.000000]\n",
            "3165: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.065745, acc: 0.000000]\n",
            "3166: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.208727, acc: 0.000000]\n",
            "3167: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.060114, acc: 0.000000]\n",
            "3168: [D loss: 0.000003, acc: 1.000000]  [G loss: 11.914664, acc: 0.000000]\n",
            "3169: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.995327, acc: 0.000000]\n",
            "3170: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.059673, acc: 0.000000]\n",
            "3171: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.997033, acc: 0.000000]\n",
            "3172: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.125183, acc: 0.000000]\n",
            "3173: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.059070, acc: 0.000000]\n",
            "3174: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.035401, acc: 0.000000]\n",
            "3175: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.230250, acc: 0.000000]\n",
            "3176: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.747332, acc: 0.000000]\n",
            "3177: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.067349, acc: 0.000000]\n",
            "3178: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.131878, acc: 0.000000]\n",
            "3179: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.969597, acc: 0.000000]\n",
            "3180: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.072725, acc: 0.000000]\n",
            "3181: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.055611, acc: 0.000000]\n",
            "3182: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.996592, acc: 0.000000]\n",
            "3183: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.065388, acc: 0.000000]\n",
            "3184: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.128134, acc: 0.000000]\n",
            "3185: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.075397, acc: 0.000000]\n",
            "3186: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.212591, acc: 0.000000]\n",
            "3187: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.965908, acc: 0.000000]\n",
            "3188: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.039663, acc: 0.000000]\n",
            "3189: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.105278, acc: 0.000000]\n",
            "3190: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.011616, acc: 0.000000]\n",
            "3191: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.080404, acc: 0.000000]\n",
            "3192: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.030348, acc: 0.000000]\n",
            "3193: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.257666, acc: 0.000000]\n",
            "3194: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.990144, acc: 0.000000]\n",
            "3195: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.944368, acc: 0.000000]\n",
            "3196: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.038004, acc: 0.000000]\n",
            "3197: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.049524, acc: 0.000000]\n",
            "3198: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.007931, acc: 0.000000]\n",
            "3199: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.043349, acc: 0.000000]\n",
            "3200: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.969404, acc: 0.000000]\n",
            "3201: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.141455, acc: 0.000000]\n",
            "3202: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.041805, acc: 0.000000]\n",
            "3203: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.043597, acc: 0.000000]\n",
            "3204: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.279203, acc: 0.000000]\n",
            "3205: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.146660, acc: 0.000000]\n",
            "3206: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.063461, acc: 0.000000]\n",
            "3207: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.000784, acc: 0.000000]\n",
            "3208: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.024496, acc: 0.000000]\n",
            "3209: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.026546, acc: 0.000000]\n",
            "3210: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.305408, acc: 0.000000]\n",
            "3211: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.135092, acc: 0.000000]\n",
            "3212: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.185505, acc: 0.000000]\n",
            "3213: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.164514, acc: 0.000000]\n",
            "3214: [D loss: 0.000003, acc: 1.000000]  [G loss: 11.906981, acc: 0.000000]\n",
            "3215: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.093651, acc: 0.000000]\n",
            "3216: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.259681, acc: 0.000000]\n",
            "3217: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.246531, acc: 0.000000]\n",
            "3218: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.047058, acc: 0.000000]\n",
            "3219: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.999804, acc: 0.000000]\n",
            "3220: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.277317, acc: 0.000000]\n",
            "3221: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.091006, acc: 0.000000]\n",
            "3222: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.080853, acc: 0.000000]\n",
            "3223: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.163910, acc: 0.000000]\n",
            "3224: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.077291, acc: 0.000000]\n",
            "3225: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.226685, acc: 0.000000]\n",
            "3226: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.068630, acc: 0.000000]\n",
            "3227: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.173647, acc: 0.000000]\n",
            "3228: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.104383, acc: 0.000000]\n",
            "3229: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.059532, acc: 0.000000]\n",
            "3230: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.202239, acc: 0.000000]\n",
            "3231: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.171139, acc: 0.000000]\n",
            "3232: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.072098, acc: 0.000000]\n",
            "3233: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.030354, acc: 0.000000]\n",
            "3234: [D loss: 0.000008, acc: 1.000000]  [G loss: 12.043493, acc: 0.000000]\n",
            "3235: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.239738, acc: 0.000000]\n",
            "3236: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.071724, acc: 0.000000]\n",
            "3237: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.241919, acc: 0.000000]\n",
            "3238: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.218493, acc: 0.000000]\n",
            "3239: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.241481, acc: 0.000000]\n",
            "3240: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.201765, acc: 0.000000]\n",
            "3241: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.227535, acc: 0.000000]\n",
            "3242: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.139444, acc: 0.000000]\n",
            "3243: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.190529, acc: 0.000000]\n",
            "3244: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.196177, acc: 0.000000]\n",
            "3245: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.078094, acc: 0.000000]\n",
            "3246: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.073088, acc: 0.000000]\n",
            "3247: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.204849, acc: 0.000000]\n",
            "3248: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.053524, acc: 0.000000]\n",
            "3249: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.064154, acc: 0.000000]\n",
            "3250: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.049146, acc: 0.000000]\n",
            "3251: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.198072, acc: 0.000000]\n",
            "3252: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.121399, acc: 0.000000]\n",
            "3253: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.210798, acc: 0.000000]\n",
            "3254: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.328167, acc: 0.000000]\n",
            "3255: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.181946, acc: 0.000000]\n",
            "3256: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.264934, acc: 0.000000]\n",
            "3257: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.188522, acc: 0.000000]\n",
            "3258: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.278989, acc: 0.000000]\n",
            "3259: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.205088, acc: 0.000000]\n",
            "3260: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.057959, acc: 0.000000]\n",
            "3261: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.184336, acc: 0.000000]\n",
            "3262: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.097808, acc: 0.000000]\n",
            "3263: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.131369, acc: 0.000000]\n",
            "3264: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.066388, acc: 0.000000]\n",
            "3265: [D loss: 0.000003, acc: 1.000000]  [G loss: 11.987864, acc: 0.000000]\n",
            "3266: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.159574, acc: 0.000000]\n",
            "3267: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.094913, acc: 0.000000]\n",
            "3268: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.183998, acc: 0.000000]\n",
            "3269: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.358715, acc: 0.000000]\n",
            "3270: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.096226, acc: 0.000000]\n",
            "3271: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.037911, acc: 0.000000]\n",
            "3272: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.240065, acc: 0.000000]\n",
            "3273: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.256464, acc: 0.000000]\n",
            "3274: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.327602, acc: 0.000000]\n",
            "3275: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.246530, acc: 0.000000]\n",
            "3276: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.226238, acc: 0.000000]\n",
            "3277: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.227070, acc: 0.000000]\n",
            "3278: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.115059, acc: 0.000000]\n",
            "3279: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.016714, acc: 0.000000]\n",
            "3280: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.210043, acc: 0.000000]\n",
            "3281: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.245106, acc: 0.000000]\n",
            "3282: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.155720, acc: 0.000000]\n",
            "3283: [D loss: 0.000003, acc: 1.000000]  [G loss: 11.932631, acc: 0.000000]\n",
            "3284: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.245360, acc: 0.000000]\n",
            "3285: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.262062, acc: 0.000000]\n",
            "3286: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.224415, acc: 0.000000]\n",
            "3287: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.214518, acc: 0.000000]\n",
            "3288: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.089560, acc: 0.000000]\n",
            "3289: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.080530, acc: 0.000000]\n",
            "3290: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.120059, acc: 0.000000]\n",
            "3291: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.255754, acc: 0.000000]\n",
            "3292: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.179158, acc: 0.000000]\n",
            "3293: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.063225, acc: 0.000000]\n",
            "3294: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.255977, acc: 0.000000]\n",
            "3295: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.192465, acc: 0.000000]\n",
            "3296: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.146700, acc: 0.000000]\n",
            "3297: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.097843, acc: 0.000000]\n",
            "3298: [D loss: 0.001358, acc: 1.000000]  [G loss: 12.188358, acc: 0.000000]\n",
            "3299: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.085761, acc: 0.000000]\n",
            "3300: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.101794, acc: 0.000000]\n",
            "3301: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.009027, acc: 0.000000]\n",
            "3302: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.007939, acc: 0.000000]\n",
            "3303: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.008112, acc: 0.000000]\n",
            "3304: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.879491, acc: 0.000000]\n",
            "3305: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.066812, acc: 0.000000]\n",
            "3306: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.861969, acc: 0.000000]\n",
            "3307: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.961630, acc: 0.000000]\n",
            "3308: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.790497, acc: 0.000000]\n",
            "3309: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.922747, acc: 0.000000]\n",
            "3310: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.789756, acc: 0.000000]\n",
            "3311: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.890465, acc: 0.000000]\n",
            "3312: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.762510, acc: 0.000000]\n",
            "3313: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.762860, acc: 0.000000]\n",
            "3314: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.835182, acc: 0.000000]\n",
            "3315: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.870255, acc: 0.000000]\n",
            "3316: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.765057, acc: 0.000000]\n",
            "3317: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.809923, acc: 0.000000]\n",
            "3318: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.810426, acc: 0.000000]\n",
            "3319: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.847474, acc: 0.000000]\n",
            "3320: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.888548, acc: 0.000000]\n",
            "3321: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.786201, acc: 0.000000]\n",
            "3322: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.812228, acc: 0.000000]\n",
            "3323: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.741755, acc: 0.000000]\n",
            "3324: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.696958, acc: 0.000000]\n",
            "3325: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.807363, acc: 0.000000]\n",
            "3326: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.879207, acc: 0.000000]\n",
            "3327: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.833421, acc: 0.000000]\n",
            "3328: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.880931, acc: 0.000000]\n",
            "3329: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.665533, acc: 0.000000]\n",
            "3330: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.781881, acc: 0.000000]\n",
            "3331: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.688437, acc: 0.000000]\n",
            "3332: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.770111, acc: 0.000000]\n",
            "3333: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.758679, acc: 0.000000]\n",
            "3334: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.843231, acc: 0.000000]\n",
            "3335: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.754948, acc: 0.000000]\n",
            "3336: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.876822, acc: 0.000000]\n",
            "3337: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.902392, acc: 0.000000]\n",
            "3338: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.758343, acc: 0.000000]\n",
            "3339: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.772879, acc: 0.000000]\n",
            "3340: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.716793, acc: 0.000000]\n",
            "3341: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.677338, acc: 0.000000]\n",
            "3342: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.700775, acc: 0.000000]\n",
            "3343: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.678242, acc: 0.000000]\n",
            "3344: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.964411, acc: 0.000000]\n",
            "3345: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.847163, acc: 0.000000]\n",
            "3346: [D loss: 0.000006, acc: 1.000000]  [G loss: 11.949627, acc: 0.000000]\n",
            "3347: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.711674, acc: 0.000000]\n",
            "3348: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.805855, acc: 0.000000]\n",
            "3349: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.922069, acc: 0.000000]\n",
            "3350: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.791695, acc: 0.000000]\n",
            "3351: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.759502, acc: 0.000000]\n",
            "3352: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.790558, acc: 0.000000]\n",
            "3353: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.834503, acc: 0.000000]\n",
            "3354: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.881552, acc: 0.000000]\n",
            "3355: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.778202, acc: 0.000000]\n",
            "3356: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.798973, acc: 0.000000]\n",
            "3357: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.763317, acc: 0.000000]\n",
            "3358: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.695620, acc: 0.000000]\n",
            "3359: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.910370, acc: 0.000000]\n",
            "3360: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.775085, acc: 0.000000]\n",
            "3361: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.922220, acc: 0.000000]\n",
            "3362: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.768017, acc: 0.000000]\n",
            "3363: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.968819, acc: 0.000000]\n",
            "3364: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.789102, acc: 0.000000]\n",
            "3365: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.852796, acc: 0.000000]\n",
            "3366: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.842813, acc: 0.000000]\n",
            "3367: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.696375, acc: 0.000000]\n",
            "3368: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.870452, acc: 0.000000]\n",
            "3369: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.820330, acc: 0.000000]\n",
            "3370: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.849518, acc: 0.000000]\n",
            "3371: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.845446, acc: 0.000000]\n",
            "3372: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.812717, acc: 0.000000]\n",
            "3373: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.667953, acc: 0.000000]\n",
            "3374: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.887741, acc: 0.000000]\n",
            "3375: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.638289, acc: 0.000000]\n",
            "3376: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.799529, acc: 0.000000]\n",
            "3377: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.931445, acc: 0.000000]\n",
            "3378: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.951829, acc: 0.000000]\n",
            "3379: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.919626, acc: 0.000000]\n",
            "3380: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.829182, acc: 0.000000]\n",
            "3381: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.906896, acc: 0.000000]\n",
            "3382: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.007110, acc: 0.000000]\n",
            "3383: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.905899, acc: 0.000000]\n",
            "3384: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.017597, acc: 0.000000]\n",
            "3385: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.832430, acc: 0.000000]\n",
            "3386: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.851494, acc: 0.000000]\n",
            "3387: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.883403, acc: 0.000000]\n",
            "3388: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.799715, acc: 0.000000]\n",
            "3389: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.810802, acc: 0.000000]\n",
            "3390: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.796768, acc: 0.000000]\n",
            "3391: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.797838, acc: 0.000000]\n",
            "3392: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.782989, acc: 0.000000]\n",
            "3393: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.960762, acc: 0.000000]\n",
            "3394: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.798933, acc: 0.000000]\n",
            "3395: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.922190, acc: 0.000000]\n",
            "3396: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.924688, acc: 0.000000]\n",
            "3397: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.899641, acc: 0.000000]\n",
            "3398: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.916201, acc: 0.000000]\n",
            "3399: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.845172, acc: 0.000000]\n",
            "3400: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.106848, acc: 0.000000]\n",
            "3401: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.909435, acc: 0.000000]\n",
            "3402: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.844548, acc: 0.000000]\n",
            "3403: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.013399, acc: 0.000000]\n",
            "3404: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.757101, acc: 0.000000]\n",
            "3405: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.798482, acc: 0.000000]\n",
            "3406: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.935415, acc: 0.000000]\n",
            "3407: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.797622, acc: 0.000000]\n",
            "3408: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.983171, acc: 0.000000]\n",
            "3409: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.066042, acc: 0.000000]\n",
            "3410: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.812258, acc: 0.000000]\n",
            "3411: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.928100, acc: 0.000000]\n",
            "3412: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.852837, acc: 0.000000]\n",
            "3413: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.926425, acc: 0.000000]\n",
            "3414: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.922836, acc: 0.000000]\n",
            "3415: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.108679, acc: 0.000000]\n",
            "3416: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.945342, acc: 0.000000]\n",
            "3417: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.990822, acc: 0.000000]\n",
            "3418: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.839837, acc: 0.000000]\n",
            "3419: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.999084, acc: 0.000000]\n",
            "3420: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.926413, acc: 0.000000]\n",
            "3421: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.060228, acc: 0.000000]\n",
            "3422: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.768858, acc: 0.000000]\n",
            "3423: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.031353, acc: 0.000000]\n",
            "3424: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.933264, acc: 0.000000]\n",
            "3425: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.990276, acc: 0.000000]\n",
            "3426: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.994679, acc: 0.000000]\n",
            "3427: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.071796, acc: 0.000000]\n",
            "3428: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.070229, acc: 0.000000]\n",
            "3429: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.957039, acc: 0.000000]\n",
            "3430: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.965332, acc: 0.000000]\n",
            "3431: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.882170, acc: 0.000000]\n",
            "3432: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.895839, acc: 0.000000]\n",
            "3433: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.134691, acc: 0.000000]\n",
            "3434: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.024861, acc: 0.000000]\n",
            "3435: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.919741, acc: 0.000000]\n",
            "3436: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.071844, acc: 0.000000]\n",
            "3437: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.137131, acc: 0.000000]\n",
            "3438: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.015270, acc: 0.000000]\n",
            "3439: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.025948, acc: 0.000000]\n",
            "3440: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.048332, acc: 0.000000]\n",
            "3441: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.992455, acc: 0.000000]\n",
            "3442: [D loss: 0.000003, acc: 1.000000]  [G loss: 11.958099, acc: 0.000000]\n",
            "3443: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.069368, acc: 0.000000]\n",
            "3444: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.077400, acc: 0.000000]\n",
            "3445: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.167620, acc: 0.000000]\n",
            "3446: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.251629, acc: 0.000000]\n",
            "3447: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.924810, acc: 0.000000]\n",
            "3448: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.020130, acc: 0.000000]\n",
            "3449: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.053992, acc: 0.000000]\n",
            "3450: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.043859, acc: 0.000000]\n",
            "3451: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.971313, acc: 0.000000]\n",
            "3452: [D loss: 0.000003, acc: 1.000000]  [G loss: 11.832693, acc: 0.000000]\n",
            "3453: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.120916, acc: 0.000000]\n",
            "3454: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.997555, acc: 0.000000]\n",
            "3455: [D loss: 0.000003, acc: 1.000000]  [G loss: 11.928308, acc: 0.000000]\n",
            "3456: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.900204, acc: 0.000000]\n",
            "3457: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.015220, acc: 0.000000]\n",
            "3458: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.070690, acc: 0.000000]\n",
            "3459: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.094595, acc: 0.000000]\n",
            "3460: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.986593, acc: 0.000000]\n",
            "3461: [D loss: 0.000005, acc: 1.000000]  [G loss: 11.922613, acc: 0.000000]\n",
            "3462: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.212808, acc: 0.000000]\n",
            "3463: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.070114, acc: 0.000000]\n",
            "3464: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.223759, acc: 0.000000]\n",
            "3465: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.092897, acc: 0.000000]\n",
            "3466: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.965217, acc: 0.000000]\n",
            "3467: [D loss: 0.000003, acc: 1.000000]  [G loss: 11.990585, acc: 0.000000]\n",
            "3468: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.130335, acc: 0.000000]\n",
            "3469: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.990446, acc: 0.000000]\n",
            "3470: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.002815, acc: 0.000000]\n",
            "3471: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.027251, acc: 0.000000]\n",
            "3472: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.001616, acc: 0.000000]\n",
            "3473: [D loss: 0.000005, acc: 1.000000]  [G loss: 12.104655, acc: 0.000000]\n",
            "3474: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.111863, acc: 0.000000]\n",
            "3475: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.070490, acc: 0.000000]\n",
            "3476: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.126205, acc: 0.000000]\n",
            "3477: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.925993, acc: 0.000000]\n",
            "3478: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.128107, acc: 0.000000]\n",
            "3479: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.104468, acc: 0.000000]\n",
            "3480: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.073167, acc: 0.000000]\n",
            "3481: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.092730, acc: 0.000000]\n",
            "3482: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.038094, acc: 0.000000]\n",
            "3483: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.015639, acc: 0.000000]\n",
            "3484: [D loss: 0.000003, acc: 1.000000]  [G loss: 11.964876, acc: 0.000000]\n",
            "3485: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.179766, acc: 0.000000]\n",
            "3486: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.155558, acc: 0.000000]\n",
            "3487: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.184868, acc: 0.000000]\n",
            "3488: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.061056, acc: 0.000000]\n",
            "3489: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.082126, acc: 0.000000]\n",
            "3490: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.138426, acc: 0.000000]\n",
            "3491: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.050236, acc: 0.000000]\n",
            "3492: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.082160, acc: 0.000000]\n",
            "3493: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.117303, acc: 0.000000]\n",
            "3494: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.111540, acc: 0.000000]\n",
            "3495: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.099775, acc: 0.000000]\n",
            "3496: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.217600, acc: 0.000000]\n",
            "3497: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.050220, acc: 0.000000]\n",
            "3498: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.009825, acc: 0.000000]\n",
            "3499: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.215901, acc: 0.000000]\n",
            "3500: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.180256, acc: 0.000000]\n",
            "3501: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.084413, acc: 0.000000]\n",
            "3502: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.251649, acc: 0.000000]\n",
            "3503: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.942770, acc: 0.000000]\n",
            "3504: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.096659, acc: 0.000000]\n",
            "3505: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.039284, acc: 0.000000]\n",
            "3506: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.282319, acc: 0.000000]\n",
            "3507: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.270241, acc: 0.000000]\n",
            "3508: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.061296, acc: 0.000000]\n",
            "3509: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.260371, acc: 0.000000]\n",
            "3510: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.283144, acc: 0.000000]\n",
            "3511: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.194441, acc: 0.000000]\n",
            "3512: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.189819, acc: 0.000000]\n",
            "3513: [D loss: 0.000003, acc: 1.000000]  [G loss: 11.908524, acc: 0.000000]\n",
            "3514: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.174070, acc: 0.000000]\n",
            "3515: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.056662, acc: 0.000000]\n",
            "3516: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.157670, acc: 0.000000]\n",
            "3517: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.182930, acc: 0.000000]\n",
            "3518: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.217012, acc: 0.000000]\n",
            "3519: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.026028, acc: 0.000000]\n",
            "3520: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.215662, acc: 0.000000]\n",
            "3521: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.212601, acc: 0.000000]\n",
            "3522: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.192519, acc: 0.000000]\n",
            "3523: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.181091, acc: 0.000000]\n",
            "3524: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.168346, acc: 0.000000]\n",
            "3525: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.243799, acc: 0.000000]\n",
            "3526: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.151649, acc: 0.000000]\n",
            "3527: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.144737, acc: 0.000000]\n",
            "3528: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.096680, acc: 0.000000]\n",
            "3529: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.170860, acc: 0.000000]\n",
            "3530: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.140882, acc: 0.000000]\n",
            "3531: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.114880, acc: 0.000000]\n",
            "3532: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.164412, acc: 0.000000]\n",
            "3533: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.218720, acc: 0.000000]\n",
            "3534: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.161860, acc: 0.000000]\n",
            "3535: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.215952, acc: 0.000000]\n",
            "3536: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.227215, acc: 0.000000]\n",
            "3537: [D loss: 0.000004, acc: 1.000000]  [G loss: 11.952188, acc: 0.000000]\n",
            "3538: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.132587, acc: 0.000000]\n",
            "3539: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.073832, acc: 0.000000]\n",
            "3540: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.199359, acc: 0.000000]\n",
            "3541: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.208891, acc: 0.000000]\n",
            "3542: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.290514, acc: 0.000000]\n",
            "3543: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.338200, acc: 0.000000]\n",
            "3544: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.081775, acc: 0.000000]\n",
            "3545: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.188128, acc: 0.000000]\n",
            "3546: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.321787, acc: 0.000000]\n",
            "3547: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.202055, acc: 0.000000]\n",
            "3548: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.058359, acc: 0.000000]\n",
            "3549: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.318687, acc: 0.000000]\n",
            "3550: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.405027, acc: 0.000000]\n",
            "3551: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.373964, acc: 0.000000]\n",
            "3552: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.312429, acc: 0.000000]\n",
            "3553: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.264905, acc: 0.000000]\n",
            "3554: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.378733, acc: 0.000000]\n",
            "3555: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.360897, acc: 0.000000]\n",
            "3556: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.093079, acc: 0.000000]\n",
            "3557: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.187613, acc: 0.000000]\n",
            "3558: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.296661, acc: 0.000000]\n",
            "3559: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.181934, acc: 0.000000]\n",
            "3560: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.240657, acc: 0.000000]\n",
            "3561: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.261538, acc: 0.000000]\n",
            "3562: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.190462, acc: 0.000000]\n",
            "3563: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.312025, acc: 0.000000]\n",
            "3564: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.168315, acc: 0.000000]\n",
            "3565: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.172621, acc: 0.000000]\n",
            "3566: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.472900, acc: 0.000000]\n",
            "3567: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.278464, acc: 0.000000]\n",
            "3568: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.277566, acc: 0.000000]\n",
            "3569: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.309446, acc: 0.000000]\n",
            "3570: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.426541, acc: 0.000000]\n",
            "3571: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.319738, acc: 0.000000]\n",
            "3572: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.215302, acc: 0.000000]\n",
            "3573: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.292765, acc: 0.000000]\n",
            "3574: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.225493, acc: 0.000000]\n",
            "3575: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.299364, acc: 0.000000]\n",
            "3576: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.234404, acc: 0.000000]\n",
            "3577: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.306042, acc: 0.000000]\n",
            "3578: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.563266, acc: 0.000000]\n",
            "3579: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.259662, acc: 0.000000]\n",
            "3580: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.272045, acc: 0.000000]\n",
            "3581: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.437432, acc: 0.000000]\n",
            "3582: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.337709, acc: 0.000000]\n",
            "3583: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.318631, acc: 0.000000]\n",
            "3584: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.207904, acc: 0.000000]\n",
            "3585: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.335851, acc: 0.000000]\n",
            "3586: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.362702, acc: 0.000000]\n",
            "3587: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.161689, acc: 0.000000]\n",
            "3588: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.304617, acc: 0.000000]\n",
            "3589: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.368368, acc: 0.000000]\n",
            "3590: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.272642, acc: 0.000000]\n",
            "3591: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.264645, acc: 0.000000]\n",
            "3592: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.304253, acc: 0.000000]\n",
            "3593: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.420235, acc: 0.000000]\n",
            "3594: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.174498, acc: 0.000000]\n",
            "3595: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.211365, acc: 0.000000]\n",
            "3596: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.353622, acc: 0.000000]\n",
            "3597: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.542873, acc: 0.000000]\n",
            "3598: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.276314, acc: 0.000000]\n",
            "3599: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.343732, acc: 0.000000]\n",
            "3600: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.454756, acc: 0.000000]\n",
            "3601: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.282133, acc: 0.000000]\n",
            "3602: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.316082, acc: 0.000000]\n",
            "3603: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.321895, acc: 0.000000]\n",
            "3604: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.433523, acc: 0.000000]\n",
            "3605: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.392766, acc: 0.000000]\n",
            "3606: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.418742, acc: 0.000000]\n",
            "3607: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.378876, acc: 0.000000]\n",
            "3608: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.352979, acc: 0.000000]\n",
            "3609: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.277754, acc: 0.000000]\n",
            "3610: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.256018, acc: 0.000000]\n",
            "3611: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.439907, acc: 0.000000]\n",
            "3612: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.323702, acc: 0.000000]\n",
            "3613: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.285107, acc: 0.000000]\n",
            "3614: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.330206, acc: 0.000000]\n",
            "3615: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.266218, acc: 0.000000]\n",
            "3616: [D loss: 0.000004, acc: 1.000000]  [G loss: 12.548826, acc: 0.000000]\n",
            "3617: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.454159, acc: 0.000000]\n",
            "3618: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.345175, acc: 0.000000]\n",
            "3619: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.287588, acc: 0.000000]\n",
            "3620: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.306690, acc: 0.000000]\n",
            "3621: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.352287, acc: 0.000000]\n",
            "3622: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.430840, acc: 0.000000]\n",
            "3623: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.468731, acc: 0.000000]\n",
            "3624: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.457724, acc: 0.000000]\n",
            "3625: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.408305, acc: 0.000000]\n",
            "3626: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.353623, acc: 0.000000]\n",
            "3627: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.402893, acc: 0.000000]\n",
            "3628: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.588913, acc: 0.000000]\n",
            "3629: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.225513, acc: 0.000000]\n",
            "3630: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.606225, acc: 0.000000]\n",
            "3631: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.467490, acc: 0.000000]\n",
            "3632: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.410645, acc: 0.000000]\n",
            "3633: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.238523, acc: 0.000000]\n",
            "3634: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.432285, acc: 0.000000]\n",
            "3635: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.546701, acc: 0.000000]\n",
            "3636: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.357550, acc: 0.000000]\n",
            "3637: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.250670, acc: 0.000000]\n",
            "3638: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.669489, acc: 0.000000]\n",
            "3639: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.253395, acc: 0.000000]\n",
            "3640: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.327837, acc: 0.000000]\n",
            "3641: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.526615, acc: 0.000000]\n",
            "3642: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.294930, acc: 0.000000]\n",
            "3643: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.353971, acc: 0.000000]\n",
            "3644: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.181808, acc: 0.000000]\n",
            "3645: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.284320, acc: 0.000000]\n",
            "3646: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.490658, acc: 0.000000]\n",
            "3647: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.695051, acc: 0.000000]\n",
            "3648: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.386489, acc: 0.000000]\n",
            "3649: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.444147, acc: 0.000000]\n",
            "3650: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.530739, acc: 0.000000]\n",
            "3651: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.479486, acc: 0.000000]\n",
            "3652: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.429764, acc: 0.000000]\n",
            "3653: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.485842, acc: 0.000000]\n",
            "3654: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.474422, acc: 0.000000]\n",
            "3655: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.373226, acc: 0.000000]\n",
            "3656: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.344376, acc: 0.000000]\n",
            "3657: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.340813, acc: 0.000000]\n",
            "3658: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.567249, acc: 0.000000]\n",
            "3659: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.307489, acc: 0.000000]\n",
            "3660: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.476647, acc: 0.000000]\n",
            "3661: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.349665, acc: 0.000000]\n",
            "3662: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.329032, acc: 0.000000]\n",
            "3663: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.349950, acc: 0.000000]\n",
            "3664: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.443636, acc: 0.000000]\n",
            "3665: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.423992, acc: 0.000000]\n",
            "3666: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.215210, acc: 0.000000]\n",
            "3667: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.490213, acc: 0.000000]\n",
            "3668: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.374214, acc: 0.000000]\n",
            "3669: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.441124, acc: 0.000000]\n",
            "3670: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.366651, acc: 0.000000]\n",
            "3671: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.357213, acc: 0.000000]\n",
            "3672: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.472878, acc: 0.000000]\n",
            "3673: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.458819, acc: 0.000000]\n",
            "3674: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.519083, acc: 0.000000]\n",
            "3675: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.440820, acc: 0.000000]\n",
            "3676: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.401594, acc: 0.000000]\n",
            "3677: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.377398, acc: 0.000000]\n",
            "3678: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.528584, acc: 0.000000]\n",
            "3679: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.360606, acc: 0.000000]\n",
            "3680: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.511239, acc: 0.000000]\n",
            "3681: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.595205, acc: 0.000000]\n",
            "3682: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.388376, acc: 0.000000]\n",
            "3683: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.407639, acc: 0.000000]\n",
            "3684: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.392868, acc: 0.000000]\n",
            "3685: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.615862, acc: 0.000000]\n",
            "3686: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.436912, acc: 0.000000]\n",
            "3687: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.489934, acc: 0.000000]\n",
            "3688: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.376534, acc: 0.000000]\n",
            "3689: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.319811, acc: 0.000000]\n",
            "3690: [D loss: 0.000032, acc: 1.000000]  [G loss: 12.562046, acc: 0.000000]\n",
            "3691: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.429758, acc: 0.000000]\n",
            "3692: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.467711, acc: 0.000000]\n",
            "3693: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.338802, acc: 0.000000]\n",
            "3694: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.493711, acc: 0.000000]\n",
            "3695: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.408781, acc: 0.000000]\n",
            "3696: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.306108, acc: 0.000000]\n",
            "3697: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.465235, acc: 0.000000]\n",
            "3698: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.495518, acc: 0.000000]\n",
            "3699: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.505655, acc: 0.000000]\n",
            "3700: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.596521, acc: 0.000000]\n",
            "3701: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.695671, acc: 0.000000]\n",
            "3702: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.517642, acc: 0.000000]\n",
            "3703: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.393994, acc: 0.000000]\n",
            "3704: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.458210, acc: 0.000000]\n",
            "3705: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.644527, acc: 0.000000]\n",
            "3706: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.486799, acc: 0.000000]\n",
            "3707: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.400451, acc: 0.000000]\n",
            "3708: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.405958, acc: 0.000000]\n",
            "3709: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.651182, acc: 0.000000]\n",
            "3710: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.338752, acc: 0.000000]\n",
            "3711: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.456278, acc: 0.000000]\n",
            "3712: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.538666, acc: 0.000000]\n",
            "3713: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.505084, acc: 0.000000]\n",
            "3714: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.446699, acc: 0.000000]\n",
            "3715: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.554968, acc: 0.000000]\n",
            "3716: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.477579, acc: 0.000000]\n",
            "3717: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.424238, acc: 0.000000]\n",
            "3718: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.497551, acc: 0.000000]\n",
            "3719: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.520777, acc: 0.000000]\n",
            "3720: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.546984, acc: 0.000000]\n",
            "3721: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.484104, acc: 0.000000]\n",
            "3722: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.658597, acc: 0.000000]\n",
            "3723: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.528922, acc: 0.000000]\n",
            "3724: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.492834, acc: 0.000000]\n",
            "3725: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.418862, acc: 0.000000]\n",
            "3726: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.495625, acc: 0.000000]\n",
            "3727: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.549015, acc: 0.000000]\n",
            "3728: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.506800, acc: 0.000000]\n",
            "3729: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.578291, acc: 0.000000]\n",
            "3730: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.550608, acc: 0.000000]\n",
            "3731: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.513206, acc: 0.000000]\n",
            "3732: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.513424, acc: 0.000000]\n",
            "3733: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.545888, acc: 0.000000]\n",
            "3734: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.722363, acc: 0.000000]\n",
            "3735: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.404721, acc: 0.000000]\n",
            "3736: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.405527, acc: 0.000000]\n",
            "3737: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.406898, acc: 0.000000]\n",
            "3738: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.638749, acc: 0.000000]\n",
            "3739: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.572808, acc: 0.000000]\n",
            "3740: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.470993, acc: 0.000000]\n",
            "3741: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.519837, acc: 0.000000]\n",
            "3742: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.699204, acc: 0.000000]\n",
            "3743: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.578968, acc: 0.000000]\n",
            "3744: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.407986, acc: 0.000000]\n",
            "3745: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.460320, acc: 0.000000]\n",
            "3746: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.599157, acc: 0.000000]\n",
            "3747: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.580902, acc: 0.000000]\n",
            "3748: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.462702, acc: 0.000000]\n",
            "3749: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.635856, acc: 0.000000]\n",
            "3750: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.476976, acc: 0.000000]\n",
            "3751: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.555561, acc: 0.000000]\n",
            "3752: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.434367, acc: 0.000000]\n",
            "3753: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.481333, acc: 0.000000]\n",
            "3754: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.640008, acc: 0.000000]\n",
            "3755: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.547068, acc: 0.000000]\n",
            "3756: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.570805, acc: 0.000000]\n",
            "3757: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.634123, acc: 0.000000]\n",
            "3758: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.522811, acc: 0.000000]\n",
            "3759: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.476620, acc: 0.000000]\n",
            "3760: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.613001, acc: 0.000000]\n",
            "3761: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.654053, acc: 0.000000]\n",
            "3762: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.582630, acc: 0.000000]\n",
            "3763: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.632359, acc: 0.000000]\n",
            "3764: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.387604, acc: 0.000000]\n",
            "3765: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.657562, acc: 0.000000]\n",
            "3766: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.619305, acc: 0.000000]\n",
            "3767: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.655354, acc: 0.000000]\n",
            "3768: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.467560, acc: 0.000000]\n",
            "3769: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.609662, acc: 0.000000]\n",
            "3770: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.610660, acc: 0.000000]\n",
            "3771: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.534050, acc: 0.000000]\n",
            "3772: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.603006, acc: 0.000000]\n",
            "3773: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.634027, acc: 0.000000]\n",
            "3774: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.540252, acc: 0.000000]\n",
            "3775: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.458592, acc: 0.000000]\n",
            "3776: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.688799, acc: 0.000000]\n",
            "3777: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.656799, acc: 0.000000]\n",
            "3778: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.650424, acc: 0.000000]\n",
            "3779: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.432177, acc: 0.000000]\n",
            "3780: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.491667, acc: 0.000000]\n",
            "3781: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.483667, acc: 0.000000]\n",
            "3782: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.618456, acc: 0.000000]\n",
            "3783: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.662437, acc: 0.000000]\n",
            "3784: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.453248, acc: 0.000000]\n",
            "3785: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.610227, acc: 0.000000]\n",
            "3786: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.713468, acc: 0.000000]\n",
            "3787: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.708860, acc: 0.000000]\n",
            "3788: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.523644, acc: 0.000000]\n",
            "3789: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.417404, acc: 0.000000]\n",
            "3790: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.559401, acc: 0.000000]\n",
            "3791: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.591832, acc: 0.000000]\n",
            "3792: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.571941, acc: 0.000000]\n",
            "3793: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.888195, acc: 0.000000]\n",
            "3794: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.496777, acc: 0.000000]\n",
            "3795: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.762920, acc: 0.000000]\n",
            "3796: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.526767, acc: 0.000000]\n",
            "3797: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.552328, acc: 0.000000]\n",
            "3798: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.734961, acc: 0.000000]\n",
            "3799: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.750864, acc: 0.000000]\n",
            "3800: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.589815, acc: 0.000000]\n",
            "3801: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.649734, acc: 0.000000]\n",
            "3802: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.732121, acc: 0.000000]\n",
            "3803: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.625389, acc: 0.000000]\n",
            "3804: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.454866, acc: 0.000000]\n",
            "3805: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.738255, acc: 0.000000]\n",
            "3806: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.763785, acc: 0.000000]\n",
            "3807: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.649460, acc: 0.000000]\n",
            "3808: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.559572, acc: 0.000000]\n",
            "3809: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.651953, acc: 0.000000]\n",
            "3810: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.605433, acc: 0.000000]\n",
            "3811: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.666383, acc: 0.000000]\n",
            "3812: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.813567, acc: 0.000000]\n",
            "3813: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.678007, acc: 0.000000]\n",
            "3814: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.506340, acc: 0.000000]\n",
            "3815: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.513163, acc: 0.000000]\n",
            "3816: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.678783, acc: 0.000000]\n",
            "3817: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.646443, acc: 0.000000]\n",
            "3818: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.587595, acc: 0.000000]\n",
            "3819: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.489901, acc: 0.000000]\n",
            "3820: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.634495, acc: 0.000000]\n",
            "3821: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.597868, acc: 0.000000]\n",
            "3822: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.661560, acc: 0.000000]\n",
            "3823: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.610773, acc: 0.000000]\n",
            "3824: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.616699, acc: 0.000000]\n",
            "3825: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.819495, acc: 0.000000]\n",
            "3826: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.871250, acc: 0.000000]\n",
            "3827: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.684162, acc: 0.000000]\n",
            "3828: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.553537, acc: 0.000000]\n",
            "3829: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.750767, acc: 0.000000]\n",
            "3830: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.646069, acc: 0.000000]\n",
            "3831: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.623375, acc: 0.000000]\n",
            "3832: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.762915, acc: 0.000000]\n",
            "3833: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.779718, acc: 0.000000]\n",
            "3834: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.468863, acc: 0.000000]\n",
            "3835: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.664309, acc: 0.000000]\n",
            "3836: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.624496, acc: 0.000000]\n",
            "3837: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.764709, acc: 0.000000]\n",
            "3838: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.763193, acc: 0.000000]\n",
            "3839: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.736191, acc: 0.000000]\n",
            "3840: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.719967, acc: 0.000000]\n",
            "3841: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.613387, acc: 0.000000]\n",
            "3842: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.797575, acc: 0.000000]\n",
            "3843: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.736118, acc: 0.000000]\n",
            "3844: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.834013, acc: 0.000000]\n",
            "3845: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.771690, acc: 0.000000]\n",
            "3846: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.667897, acc: 0.000000]\n",
            "3847: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.747997, acc: 0.000000]\n",
            "3848: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.717279, acc: 0.000000]\n",
            "3849: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.697946, acc: 0.000000]\n",
            "3850: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.743891, acc: 0.000000]\n",
            "3851: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.829546, acc: 0.000000]\n",
            "3852: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.673246, acc: 0.000000]\n",
            "3853: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.709972, acc: 0.000000]\n",
            "3854: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.694281, acc: 0.000000]\n",
            "3855: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.704433, acc: 0.000000]\n",
            "3856: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.586931, acc: 0.000000]\n",
            "3857: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.759634, acc: 0.000000]\n",
            "3858: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.632962, acc: 0.000000]\n",
            "3859: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.769869, acc: 0.000000]\n",
            "3860: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.776911, acc: 0.000000]\n",
            "3861: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.693953, acc: 0.000000]\n",
            "3862: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.548054, acc: 0.000000]\n",
            "3863: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.697104, acc: 0.000000]\n",
            "3864: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.762239, acc: 0.000000]\n",
            "3865: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.695741, acc: 0.000000]\n",
            "3866: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.672346, acc: 0.000000]\n",
            "3867: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.665378, acc: 0.000000]\n",
            "3868: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.706423, acc: 0.000000]\n",
            "3869: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.771395, acc: 0.000000]\n",
            "3870: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.689593, acc: 0.000000]\n",
            "3871: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.643863, acc: 0.000000]\n",
            "3872: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.805065, acc: 0.000000]\n",
            "3873: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.637850, acc: 0.000000]\n",
            "3874: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.728866, acc: 0.000000]\n",
            "3875: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.641348, acc: 0.000000]\n",
            "3876: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.685755, acc: 0.000000]\n",
            "3877: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.725472, acc: 0.000000]\n",
            "3878: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.771534, acc: 0.000000]\n",
            "3879: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.759470, acc: 0.000000]\n",
            "3880: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.790106, acc: 0.000000]\n",
            "3881: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.619038, acc: 0.000000]\n",
            "3882: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.636448, acc: 0.000000]\n",
            "3883: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.587434, acc: 0.000000]\n",
            "3884: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.672464, acc: 0.000000]\n",
            "3885: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.763147, acc: 0.000000]\n",
            "3886: [D loss: 0.000001, acc: 1.000000]  [G loss: 12.850836, acc: 0.000000]\n",
            "3887: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.786175, acc: 0.000000]\n",
            "3888: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.599966, acc: 0.000000]\n",
            "3889: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.699026, acc: 0.000000]\n",
            "3890: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.856943, acc: 0.000000]\n",
            "3891: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.721916, acc: 0.000000]\n",
            "3892: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.774588, acc: 0.000000]\n",
            "3893: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.717377, acc: 0.000000]\n",
            "3894: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.747581, acc: 0.000000]\n",
            "3895: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.705654, acc: 0.000000]\n",
            "3896: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.571918, acc: 0.000000]\n",
            "3897: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.797940, acc: 0.000000]\n",
            "3898: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.604766, acc: 0.000000]\n",
            "3899: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.690748, acc: 0.000000]\n",
            "3900: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.610346, acc: 0.000000]\n",
            "3901: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.680199, acc: 0.000000]\n",
            "3902: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.819036, acc: 0.000000]\n",
            "3903: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.788716, acc: 0.000000]\n",
            "3904: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.720867, acc: 0.000000]\n",
            "3905: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.862650, acc: 0.000000]\n",
            "3906: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.805931, acc: 0.000000]\n",
            "3907: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.690838, acc: 0.000000]\n",
            "3908: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.536285, acc: 0.000000]\n",
            "3909: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.695713, acc: 0.000000]\n",
            "3910: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.912150, acc: 0.000000]\n",
            "3911: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.882575, acc: 0.000000]\n",
            "3912: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.943798, acc: 0.000000]\n",
            "3913: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.644190, acc: 0.000000]\n",
            "3914: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.721339, acc: 0.000000]\n",
            "3915: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.772011, acc: 0.000000]\n",
            "3916: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.823773, acc: 0.000000]\n",
            "3917: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.790021, acc: 0.000000]\n",
            "3918: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.607195, acc: 0.000000]\n",
            "3919: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.745506, acc: 0.000000]\n",
            "3920: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.689847, acc: 0.000000]\n",
            "3921: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.807745, acc: 0.000000]\n",
            "3922: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.603445, acc: 0.000000]\n",
            "3923: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.609365, acc: 0.000000]\n",
            "3924: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.849318, acc: 0.000000]\n",
            "3925: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.791838, acc: 0.000000]\n",
            "3926: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.930044, acc: 0.000000]\n",
            "3927: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.730498, acc: 0.000000]\n",
            "3928: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.826550, acc: 0.000000]\n",
            "3929: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.878044, acc: 0.000000]\n",
            "3930: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.781544, acc: 0.000000]\n",
            "3931: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.692781, acc: 0.000000]\n",
            "3932: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.938240, acc: 0.000000]\n",
            "3933: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.785963, acc: 0.000000]\n",
            "3934: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.850021, acc: 0.000000]\n",
            "3935: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.935415, acc: 0.000000]\n",
            "3936: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.696560, acc: 0.000000]\n",
            "3937: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.690548, acc: 0.000000]\n",
            "3938: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.745061, acc: 0.000000]\n",
            "3939: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.771130, acc: 0.000000]\n",
            "3940: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.771326, acc: 0.000000]\n",
            "3941: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.856365, acc: 0.000000]\n",
            "3942: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.722746, acc: 0.000000]\n",
            "3943: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.730068, acc: 0.000000]\n",
            "3944: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.814550, acc: 0.000000]\n",
            "3945: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.697044, acc: 0.000000]\n",
            "3946: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.669239, acc: 0.000000]\n",
            "3947: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.758795, acc: 0.000000]\n",
            "3948: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.777958, acc: 0.000000]\n",
            "3949: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.609793, acc: 0.000000]\n",
            "3950: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.812637, acc: 0.000000]\n",
            "3951: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.683501, acc: 0.000000]\n",
            "3952: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.531229, acc: 0.000000]\n",
            "3953: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.749095, acc: 0.000000]\n",
            "3954: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.653399, acc: 0.000000]\n",
            "3955: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.701469, acc: 0.000000]\n",
            "3956: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.972505, acc: 0.000000]\n",
            "3957: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.819370, acc: 0.000000]\n",
            "3958: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.910269, acc: 0.000000]\n",
            "3959: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.851689, acc: 0.000000]\n",
            "3960: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.639143, acc: 0.000000]\n",
            "3961: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.700707, acc: 0.000000]\n",
            "3962: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.397413, acc: 0.000000]\n",
            "3963: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.501139, acc: 0.000000]\n",
            "3964: [D loss: 0.000002, acc: 1.000000]  [G loss: 12.509066, acc: 0.000000]\n",
            "3965: [D loss: 0.000006, acc: 1.000000]  [G loss: 12.456895, acc: 0.000000]\n",
            "3966: [D loss: 0.000003, acc: 1.000000]  [G loss: 12.464769, acc: 0.000000]\n",
            "3967: [D loss: 0.000015, acc: 1.000000]  [G loss: 12.147700, acc: 0.000000]\n",
            "3968: [D loss: 0.001002, acc: 1.000000]  [G loss: 11.574790, acc: 0.000000]\n",
            "3969: [D loss: 0.011376, acc: 0.992188]  [G loss: 11.704538, acc: 0.000000]\n",
            "3970: [D loss: 0.000437, acc: 1.000000]  [G loss: 11.818592, acc: 0.046875]\n",
            "3971: [D loss: 0.006389, acc: 1.000000]  [G loss: 11.866401, acc: 0.062500]\n",
            "3972: [D loss: 0.228539, acc: 0.968750]  [G loss: 13.373104, acc: 0.046875]\n",
            "3973: [D loss: 0.081536, acc: 0.976562]  [G loss: 14.652075, acc: 0.015625]\n",
            "3974: [D loss: 0.000478, acc: 1.000000]  [G loss: 15.762545, acc: 0.000000]\n",
            "3975: [D loss: 0.131292, acc: 0.976562]  [G loss: 15.790334, acc: 0.000000]\n",
            "3976: [D loss: 0.152327, acc: 0.968750]  [G loss: 14.427609, acc: 0.015625]\n",
            "3977: [D loss: 0.092392, acc: 0.984375]  [G loss: 11.053105, acc: 0.062500]\n",
            "3978: [D loss: 0.647273, acc: 0.929688]  [G loss: 12.123400, acc: 0.031250]\n",
            "3979: [D loss: 0.002055, acc: 1.000000]  [G loss: 11.615854, acc: 0.031250]\n",
            "3980: [D loss: 0.029074, acc: 0.992188]  [G loss: 11.566303, acc: 0.015625]\n",
            "3981: [D loss: 0.230754, acc: 0.953125]  [G loss: 9.443748, acc: 0.140625]\n",
            "3982: [D loss: 0.117975, acc: 0.968750]  [G loss: 8.953164, acc: 0.218750]\n",
            "3983: [D loss: 0.345840, acc: 0.898438]  [G loss: 7.953839, acc: 0.296875]\n",
            "3984: [D loss: 0.322473, acc: 0.898438]  [G loss: 6.332030, acc: 0.218750]\n",
            "3985: [D loss: 0.331942, acc: 0.882812]  [G loss: 9.194883, acc: 0.046875]\n",
            "3986: [D loss: 0.623807, acc: 0.789062]  [G loss: 3.273699, acc: 0.359375]\n",
            "3987: [D loss: 0.372379, acc: 0.906250]  [G loss: 1.801422, acc: 0.484375]\n",
            "3988: [D loss: 0.416346, acc: 0.812500]  [G loss: 5.245350, acc: 0.125000]\n",
            "3989: [D loss: 0.181415, acc: 0.945312]  [G loss: 9.015815, acc: 0.000000]\n",
            "3990: [D loss: 0.541945, acc: 0.796875]  [G loss: 6.896217, acc: 0.125000]\n",
            "3991: [D loss: 0.239201, acc: 0.906250]  [G loss: 3.794939, acc: 0.250000]\n",
            "3992: [D loss: 0.169281, acc: 0.914062]  [G loss: 2.809488, acc: 0.312500]\n",
            "3993: [D loss: 0.208805, acc: 0.898438]  [G loss: 5.150265, acc: 0.078125]\n",
            "3994: [D loss: 0.041493, acc: 0.976562]  [G loss: 8.227811, acc: 0.015625]\n",
            "3995: [D loss: 0.135454, acc: 0.968750]  [G loss: 8.004393, acc: 0.000000]\n",
            "3996: [D loss: 0.109837, acc: 0.968750]  [G loss: 6.469329, acc: 0.171875]\n",
            "3997: [D loss: 0.045704, acc: 0.976562]  [G loss: 5.204682, acc: 0.281250]\n",
            "3998: [D loss: 0.029724, acc: 0.984375]  [G loss: 3.745924, acc: 0.468750]\n",
            "3999: [D loss: 0.033722, acc: 0.992188]  [G loss: 3.303066, acc: 0.546875]\n",
            "4000: [D loss: 0.002809, acc: 1.000000]  [G loss: 1.890745, acc: 0.531250]\n",
            "4001: [D loss: 0.005383, acc: 1.000000]  [G loss: 1.455830, acc: 0.718750]\n",
            "4002: [D loss: 0.004577, acc: 1.000000]  [G loss: 0.851776, acc: 0.859375]\n",
            "4003: [D loss: 0.016185, acc: 0.992188]  [G loss: 0.247667, acc: 0.890625]\n",
            "4004: [D loss: 0.004100, acc: 1.000000]  [G loss: 0.117903, acc: 0.953125]\n",
            "4005: [D loss: 0.058041, acc: 0.976562]  [G loss: 0.517949, acc: 0.843750]\n",
            "4006: [D loss: 0.007157, acc: 1.000000]  [G loss: 0.446367, acc: 0.859375]\n",
            "4007: [D loss: 0.007765, acc: 1.000000]  [G loss: 0.448373, acc: 0.781250]\n",
            "4008: [D loss: 0.005394, acc: 1.000000]  [G loss: 0.470454, acc: 0.796875]\n",
            "4009: [D loss: 0.058297, acc: 0.976562]  [G loss: 0.324291, acc: 0.875000]\n",
            "4010: [D loss: 0.002772, acc: 1.000000]  [G loss: 0.373619, acc: 0.921875]\n",
            "4011: [D loss: 0.021497, acc: 0.992188]  [G loss: 0.020003, acc: 0.984375]\n",
            "4012: [D loss: 0.016727, acc: 0.992188]  [G loss: 0.276354, acc: 0.968750]\n",
            "4013: [D loss: 0.000084, acc: 1.000000]  [G loss: 0.001921, acc: 1.000000]\n",
            "4014: [D loss: 0.016378, acc: 0.992188]  [G loss: 0.177498, acc: 0.968750]\n",
            "4015: [D loss: 0.009545, acc: 0.992188]  [G loss: 0.061877, acc: 0.968750]\n",
            "4016: [D loss: 0.020828, acc: 0.992188]  [G loss: 0.046908, acc: 0.984375]\n",
            "4017: [D loss: 0.000143, acc: 1.000000]  [G loss: 0.286130, acc: 0.906250]\n",
            "4018: [D loss: 0.030659, acc: 0.992188]  [G loss: 0.220950, acc: 0.937500]\n",
            "4019: [D loss: 0.002670, acc: 1.000000]  [G loss: 0.284019, acc: 0.875000]\n",
            "4020: [D loss: 0.026138, acc: 0.992188]  [G loss: 0.084733, acc: 0.968750]\n",
            "4021: [D loss: 0.005875, acc: 1.000000]  [G loss: 0.077192, acc: 0.953125]\n",
            "4022: [D loss: 0.063462, acc: 0.984375]  [G loss: 0.186779, acc: 0.921875]\n",
            "4023: [D loss: 0.029641, acc: 0.984375]  [G loss: 1.982785, acc: 0.718750]\n",
            "4024: [D loss: 0.006263, acc: 1.000000]  [G loss: 3.166182, acc: 0.640625]\n",
            "4025: [D loss: 0.201468, acc: 0.960938]  [G loss: 1.794052, acc: 0.781250]\n",
            "4026: [D loss: 0.125911, acc: 0.976562]  [G loss: 0.076256, acc: 0.953125]\n",
            "4027: [D loss: 0.825807, acc: 0.859375]  [G loss: 2.177615, acc: 0.656250]\n",
            "4028: [D loss: 0.125363, acc: 0.968750]  [G loss: 6.462922, acc: 0.390625]\n",
            "4029: [D loss: 0.694918, acc: 0.898438]  [G loss: 3.306946, acc: 0.562500]\n",
            "4030: [D loss: 0.181921, acc: 0.976562]  [G loss: 0.539844, acc: 0.828125]\n",
            "4031: [D loss: 0.363695, acc: 0.867188]  [G loss: 1.704090, acc: 0.703125]\n",
            "4032: [D loss: 0.306024, acc: 0.929688]  [G loss: 5.231187, acc: 0.390625]\n",
            "4033: [D loss: 0.685901, acc: 0.875000]  [G loss: 2.868863, acc: 0.515625]\n",
            "4034: [D loss: 0.487087, acc: 0.867188]  [G loss: 3.449457, acc: 0.421875]\n",
            "4035: [D loss: 0.458580, acc: 0.851562]  [G loss: 5.366730, acc: 0.203125]\n",
            "4036: [D loss: 0.428073, acc: 0.898438]  [G loss: 3.895869, acc: 0.156250]\n",
            "4037: [D loss: 0.746084, acc: 0.820312]  [G loss: 1.312140, acc: 0.515625]\n",
            "4038: [D loss: 1.010834, acc: 0.609375]  [G loss: 0.792626, acc: 0.656250]\n",
            "4039: [D loss: 0.707512, acc: 0.632812]  [G loss: 2.012680, acc: 0.109375]\n",
            "4040: [D loss: 0.274800, acc: 0.953125]  [G loss: 2.992935, acc: 0.000000]\n",
            "4041: [D loss: 0.124739, acc: 0.953125]  [G loss: 3.821246, acc: 0.000000]\n",
            "4042: [D loss: 0.088760, acc: 0.984375]  [G loss: 4.006364, acc: 0.000000]\n",
            "4043: [D loss: 0.025316, acc: 0.992188]  [G loss: 4.176175, acc: 0.000000]\n",
            "4044: [D loss: 0.030644, acc: 0.984375]  [G loss: 4.273746, acc: 0.000000]\n",
            "4045: [D loss: 0.005000, acc: 1.000000]  [G loss: 3.874184, acc: 0.000000]\n",
            "4046: [D loss: 0.007402, acc: 1.000000]  [G loss: 3.497629, acc: 0.000000]\n",
            "4047: [D loss: 0.029398, acc: 0.992188]  [G loss: 3.290373, acc: 0.015625]\n",
            "4048: [D loss: 0.010085, acc: 1.000000]  [G loss: 2.825816, acc: 0.109375]\n",
            "4049: [D loss: 0.016555, acc: 1.000000]  [G loss: 2.855533, acc: 0.171875]\n",
            "4050: [D loss: 0.015084, acc: 1.000000]  [G loss: 2.501170, acc: 0.343750]\n",
            "4051: [D loss: 0.017401, acc: 1.000000]  [G loss: 2.997893, acc: 0.328125]\n",
            "4052: [D loss: 0.038577, acc: 0.984375]  [G loss: 3.453530, acc: 0.390625]\n",
            "4053: [D loss: 0.022260, acc: 0.992188]  [G loss: 4.049710, acc: 0.453125]\n",
            "4054: [D loss: 0.002585, acc: 1.000000]  [G loss: 4.480930, acc: 0.453125]\n",
            "4055: [D loss: 0.007648, acc: 1.000000]  [G loss: 4.043058, acc: 0.531250]\n",
            "4056: [D loss: 0.000955, acc: 1.000000]  [G loss: 4.098800, acc: 0.687500]\n",
            "4057: [D loss: 0.025553, acc: 0.984375]  [G loss: 3.583386, acc: 0.671875]\n",
            "4058: [D loss: 0.003016, acc: 1.000000]  [G loss: 4.421795, acc: 0.562500]\n",
            "4059: [D loss: 0.000311, acc: 1.000000]  [G loss: 3.484117, acc: 0.593750]\n",
            "4060: [D loss: 0.003581, acc: 1.000000]  [G loss: 3.433095, acc: 0.656250]\n",
            "4061: [D loss: 0.022744, acc: 0.992188]  [G loss: 3.221967, acc: 0.734375]\n",
            "4062: [D loss: 0.017624, acc: 0.992188]  [G loss: 2.821959, acc: 0.796875]\n",
            "4063: [D loss: 0.038402, acc: 0.992188]  [G loss: 2.365196, acc: 0.828125]\n",
            "4064: [D loss: 0.173947, acc: 0.960938]  [G loss: 2.503770, acc: 0.781250]\n",
            "4065: [D loss: 0.231816, acc: 0.968750]  [G loss: 2.339299, acc: 0.812500]\n",
            "4066: [D loss: 0.170572, acc: 0.976562]  [G loss: 1.850478, acc: 0.843750]\n",
            "4067: [D loss: 0.152846, acc: 0.976562]  [G loss: 1.219348, acc: 0.921875]\n",
            "4068: [D loss: 0.054190, acc: 0.976562]  [G loss: 1.395225, acc: 0.828125]\n",
            "4069: [D loss: 0.105569, acc: 0.984375]  [G loss: 0.850778, acc: 0.906250]\n",
            "4070: [D loss: 0.085561, acc: 0.984375]  [G loss: 0.787387, acc: 0.906250]\n",
            "4071: [D loss: 0.340538, acc: 0.929688]  [G loss: 2.165428, acc: 0.671875]\n",
            "4072: [D loss: 0.839026, acc: 0.921875]  [G loss: 2.545337, acc: 0.656250]\n",
            "4073: [D loss: 0.668283, acc: 0.929688]  [G loss: 0.092200, acc: 0.968750]\n",
            "4074: [D loss: 0.779051, acc: 0.843750]  [G loss: 0.318447, acc: 0.921875]\n",
            "4075: [D loss: 0.400518, acc: 0.937500]  [G loss: 0.177561, acc: 0.937500]\n",
            "4076: [D loss: 0.532660, acc: 0.937500]  [G loss: 0.021311, acc: 0.984375]\n",
            "4077: [D loss: 0.601326, acc: 0.875000]  [G loss: 0.079688, acc: 0.968750]\n",
            "4078: [D loss: 0.230599, acc: 0.921875]  [G loss: 4.738261, acc: 0.343750]\n",
            "4079: [D loss: 0.923402, acc: 0.867188]  [G loss: 2.560264, acc: 0.484375]\n",
            "4080: [D loss: 0.710831, acc: 0.820312]  [G loss: 0.047796, acc: 0.984375]\n",
            "4081: [D loss: 1.217519, acc: 0.773438]  [G loss: 1.835890, acc: 0.546875]\n",
            "4082: [D loss: 0.122232, acc: 0.953125]  [G loss: 5.832054, acc: 0.218750]\n",
            "4083: [D loss: 0.532650, acc: 0.859375]  [G loss: 5.110377, acc: 0.265625]\n",
            "4084: [D loss: 0.552876, acc: 0.835938]  [G loss: 1.432239, acc: 0.703125]\n",
            "4085: [D loss: 0.730737, acc: 0.789062]  [G loss: 0.297385, acc: 0.890625]\n",
            "4086: [D loss: 0.289525, acc: 0.851562]  [G loss: 1.853082, acc: 0.484375]\n",
            "4087: [D loss: 0.122917, acc: 0.953125]  [G loss: 4.652390, acc: 0.312500]\n",
            "4088: [D loss: 0.265951, acc: 0.906250]  [G loss: 5.212891, acc: 0.265625]\n",
            "4089: [D loss: 0.265117, acc: 0.914062]  [G loss: 2.672020, acc: 0.562500]\n",
            "4090: [D loss: 0.091335, acc: 0.960938]  [G loss: 3.016667, acc: 0.515625]\n",
            "4091: [D loss: 0.390550, acc: 0.906250]  [G loss: 3.383280, acc: 0.312500]\n",
            "4092: [D loss: 0.226111, acc: 0.906250]  [G loss: 8.719133, acc: 0.156250]\n",
            "4093: [D loss: 0.189976, acc: 0.929688]  [G loss: 10.639192, acc: 0.031250]\n",
            "4094: [D loss: 0.294325, acc: 0.929688]  [G loss: 8.244648, acc: 0.234375]\n",
            "4095: [D loss: 0.218551, acc: 0.937500]  [G loss: 6.495873, acc: 0.328125]\n",
            "4096: [D loss: 0.437475, acc: 0.859375]  [G loss: 6.032164, acc: 0.375000]\n",
            "4097: [D loss: 0.243997, acc: 0.937500]  [G loss: 6.668406, acc: 0.187500]\n",
            "4098: [D loss: 0.120915, acc: 0.945312]  [G loss: 8.657770, acc: 0.156250]\n",
            "4099: [D loss: 0.410818, acc: 0.921875]  [G loss: 6.125226, acc: 0.328125]\n",
            "4100: [D loss: 0.143264, acc: 0.960938]  [G loss: 5.536430, acc: 0.468750]\n",
            "4101: [D loss: 0.178694, acc: 0.937500]  [G loss: 5.279077, acc: 0.406250]\n",
            "4102: [D loss: 0.107731, acc: 0.968750]  [G loss: 7.313844, acc: 0.296875]\n",
            "4103: [D loss: 0.157055, acc: 0.976562]  [G loss: 9.794052, acc: 0.171875]\n",
            "4104: [D loss: 0.295396, acc: 0.937500]  [G loss: 7.753797, acc: 0.171875]\n",
            "4105: [D loss: 0.079944, acc: 0.976562]  [G loss: 5.509880, acc: 0.406250]\n",
            "4106: [D loss: 0.126550, acc: 0.953125]  [G loss: 3.506330, acc: 0.500000]\n",
            "4107: [D loss: 0.276559, acc: 0.898438]  [G loss: 7.158109, acc: 0.281250]\n",
            "4108: [D loss: 0.163156, acc: 0.984375]  [G loss: 10.239748, acc: 0.062500]\n",
            "4109: [D loss: 0.088202, acc: 0.984375]  [G loss: 11.821470, acc: 0.015625]\n",
            "4110: [D loss: 0.477307, acc: 0.898438]  [G loss: 6.185973, acc: 0.203125]\n",
            "4111: [D loss: 0.081583, acc: 0.976562]  [G loss: 3.661579, acc: 0.625000]\n",
            "4112: [D loss: 0.554624, acc: 0.867188]  [G loss: 6.223109, acc: 0.281250]\n",
            "4113: [D loss: 0.017797, acc: 0.992188]  [G loss: 8.168018, acc: 0.156250]\n",
            "4114: [D loss: 0.217236, acc: 0.953125]  [G loss: 7.519917, acc: 0.203125]\n",
            "4115: [D loss: 0.243980, acc: 0.953125]  [G loss: 4.853885, acc: 0.437500]\n",
            "4116: [D loss: 0.033749, acc: 0.984375]  [G loss: 2.623133, acc: 0.640625]\n",
            "4117: [D loss: 0.320384, acc: 0.929688]  [G loss: 2.538475, acc: 0.687500]\n",
            "4118: [D loss: 0.106994, acc: 0.968750]  [G loss: 2.199103, acc: 0.687500]\n",
            "4119: [D loss: 0.255704, acc: 0.968750]  [G loss: 2.557337, acc: 0.687500]\n",
            "4120: [D loss: 0.010052, acc: 0.992188]  [G loss: 1.709757, acc: 0.734375]\n",
            "4121: [D loss: 0.031803, acc: 0.976562]  [G loss: 1.831300, acc: 0.734375]\n",
            "4122: [D loss: 0.198471, acc: 0.953125]  [G loss: 0.711762, acc: 0.812500]\n",
            "4123: [D loss: 0.078352, acc: 0.968750]  [G loss: 0.933894, acc: 0.781250]\n",
            "4124: [D loss: 0.041972, acc: 0.992188]  [G loss: 0.545797, acc: 0.843750]\n",
            "4125: [D loss: 0.080728, acc: 0.960938]  [G loss: 0.316110, acc: 0.890625]\n",
            "4126: [D loss: 0.122422, acc: 0.953125]  [G loss: 0.382438, acc: 0.859375]\n",
            "4127: [D loss: 0.060092, acc: 0.976562]  [G loss: 0.390193, acc: 0.859375]\n",
            "4128: [D loss: 0.439790, acc: 0.914062]  [G loss: 0.485675, acc: 0.890625]\n",
            "4129: [D loss: 0.496082, acc: 0.921875]  [G loss: 0.307095, acc: 0.906250]\n",
            "4130: [D loss: 0.161580, acc: 0.960938]  [G loss: 0.682191, acc: 0.765625]\n",
            "4131: [D loss: 0.130799, acc: 0.968750]  [G loss: 0.784442, acc: 0.781250]\n",
            "4132: [D loss: 0.080091, acc: 0.984375]  [G loss: 1.311929, acc: 0.687500]\n",
            "4133: [D loss: 0.111090, acc: 0.976562]  [G loss: 1.361164, acc: 0.734375]\n",
            "4134: [D loss: 0.026267, acc: 0.992188]  [G loss: 1.004897, acc: 0.828125]\n",
            "4135: [D loss: 0.170137, acc: 0.953125]  [G loss: 0.819832, acc: 0.875000]\n",
            "4136: [D loss: 0.233154, acc: 0.898438]  [G loss: 1.380586, acc: 0.546875]\n",
            "4137: [D loss: 0.040593, acc: 0.984375]  [G loss: 2.815361, acc: 0.468750]\n",
            "4138: [D loss: 0.197333, acc: 0.953125]  [G loss: 3.358244, acc: 0.265625]\n",
            "4139: [D loss: 0.290684, acc: 0.914062]  [G loss: 1.097905, acc: 0.734375]\n",
            "4140: [D loss: 0.114808, acc: 0.945312]  [G loss: 0.734985, acc: 0.828125]\n",
            "4141: [D loss: 0.094243, acc: 0.968750]  [G loss: 0.608706, acc: 0.750000]\n",
            "4142: [D loss: 0.075050, acc: 0.968750]  [G loss: 1.110252, acc: 0.765625]\n",
            "4143: [D loss: 0.040609, acc: 0.984375]  [G loss: 1.776287, acc: 0.718750]\n",
            "4144: [D loss: 0.017903, acc: 1.000000]  [G loss: 2.353906, acc: 0.546875]\n",
            "4145: [D loss: 0.210969, acc: 0.937500]  [G loss: 2.505867, acc: 0.531250]\n",
            "4146: [D loss: 0.080546, acc: 0.976562]  [G loss: 1.841449, acc: 0.437500]\n",
            "4147: [D loss: 0.061878, acc: 0.968750]  [G loss: 0.874728, acc: 0.625000]\n",
            "4148: [D loss: 0.146615, acc: 0.945312]  [G loss: 1.398607, acc: 0.484375]\n",
            "4149: [D loss: 0.129737, acc: 0.945312]  [G loss: 2.868012, acc: 0.265625]\n",
            "4150: [D loss: 0.094808, acc: 0.968750]  [G loss: 4.806324, acc: 0.078125]\n",
            "4151: [D loss: 0.208040, acc: 0.945312]  [G loss: 4.295724, acc: 0.203125]\n",
            "4152: [D loss: 0.115218, acc: 0.945312]  [G loss: 3.547792, acc: 0.250000]\n",
            "4153: [D loss: 0.122604, acc: 0.960938]  [G loss: 1.883289, acc: 0.500000]\n",
            "4154: [D loss: 0.077588, acc: 0.976562]  [G loss: 1.988664, acc: 0.453125]\n",
            "4155: [D loss: 0.131851, acc: 0.937500]  [G loss: 3.768881, acc: 0.125000]\n",
            "4156: [D loss: 0.049138, acc: 0.992188]  [G loss: 5.672527, acc: 0.046875]\n",
            "4157: [D loss: 0.084479, acc: 0.968750]  [G loss: 7.698647, acc: 0.000000]\n",
            "4158: [D loss: 0.041963, acc: 0.976562]  [G loss: 8.005342, acc: 0.015625]\n",
            "4159: [D loss: 0.136692, acc: 0.960938]  [G loss: 7.609844, acc: 0.046875]\n",
            "4160: [D loss: 0.051395, acc: 0.984375]  [G loss: 6.034776, acc: 0.046875]\n",
            "4161: [D loss: 0.016491, acc: 0.992188]  [G loss: 4.502421, acc: 0.171875]\n",
            "4162: [D loss: 0.033748, acc: 0.984375]  [G loss: 3.456759, acc: 0.187500]\n",
            "4163: [D loss: 0.243388, acc: 0.929688]  [G loss: 4.731960, acc: 0.140625]\n",
            "4164: [D loss: 0.139035, acc: 0.914062]  [G loss: 8.223948, acc: 0.062500]\n",
            "4165: [D loss: 0.037962, acc: 0.984375]  [G loss: 12.096884, acc: 0.000000]\n",
            "4166: [D loss: 0.197089, acc: 0.976562]  [G loss: 13.973095, acc: 0.000000]\n",
            "4167: [D loss: 0.222045, acc: 0.960938]  [G loss: 13.642490, acc: 0.000000]\n",
            "4168: [D loss: 0.183603, acc: 0.960938]  [G loss: 12.044080, acc: 0.093750]\n",
            "4169: [D loss: 0.405099, acc: 0.906250]  [G loss: 9.828595, acc: 0.093750]\n",
            "4170: [D loss: 0.735114, acc: 0.882812]  [G loss: 11.557903, acc: 0.031250]\n",
            "4171: [D loss: 0.121124, acc: 0.937500]  [G loss: 10.722885, acc: 0.015625]\n",
            "4172: [D loss: 0.576180, acc: 0.906250]  [G loss: 9.058884, acc: 0.031250]\n",
            "4173: [D loss: 0.419327, acc: 0.882812]  [G loss: 7.087288, acc: 0.109375]\n",
            "4174: [D loss: 0.380286, acc: 0.859375]  [G loss: 6.553160, acc: 0.218750]\n",
            "4175: [D loss: 0.627146, acc: 0.843750]  [G loss: 6.251401, acc: 0.187500]\n",
            "4176: [D loss: 0.729697, acc: 0.828125]  [G loss: 3.424321, acc: 0.359375]\n",
            "4177: [D loss: 0.713997, acc: 0.882812]  [G loss: 4.518299, acc: 0.234375]\n",
            "4178: [D loss: 0.294337, acc: 0.906250]  [G loss: 4.124260, acc: 0.265625]\n",
            "4179: [D loss: 0.296232, acc: 0.929688]  [G loss: 4.756412, acc: 0.250000]\n",
            "4180: [D loss: 0.185655, acc: 0.945312]  [G loss: 6.380806, acc: 0.218750]\n",
            "4181: [D loss: 0.084786, acc: 0.960938]  [G loss: 6.379215, acc: 0.265625]\n",
            "4182: [D loss: 0.367163, acc: 0.898438]  [G loss: 4.916512, acc: 0.328125]\n",
            "4183: [D loss: 0.110076, acc: 0.953125]  [G loss: 2.663743, acc: 0.562500]\n",
            "4184: [D loss: 0.286230, acc: 0.898438]  [G loss: 2.376457, acc: 0.578125]\n",
            "4185: [D loss: 0.274947, acc: 0.890625]  [G loss: 4.521059, acc: 0.421875]\n",
            "4186: [D loss: 0.086131, acc: 0.976562]  [G loss: 7.275257, acc: 0.234375]\n",
            "4187: [D loss: 0.360993, acc: 0.937500]  [G loss: 7.190637, acc: 0.140625]\n",
            "4188: [D loss: 0.115085, acc: 0.937500]  [G loss: 7.302245, acc: 0.093750]\n",
            "4189: [D loss: 0.096082, acc: 0.984375]  [G loss: 4.722184, acc: 0.171875]\n",
            "4190: [D loss: 0.199858, acc: 0.960938]  [G loss: 3.171711, acc: 0.406250]\n",
            "4191: [D loss: 0.069575, acc: 0.968750]  [G loss: 1.669046, acc: 0.500000]\n",
            "4192: [D loss: 0.247387, acc: 0.875000]  [G loss: 3.940331, acc: 0.343750]\n",
            "4193: [D loss: 0.022593, acc: 0.992188]  [G loss: 5.523229, acc: 0.234375]\n",
            "4194: [D loss: 0.242870, acc: 0.937500]  [G loss: 5.134892, acc: 0.328125]\n",
            "4195: [D loss: 0.065808, acc: 0.984375]  [G loss: 4.823959, acc: 0.406250]\n",
            "4196: [D loss: 0.054993, acc: 0.976562]  [G loss: 4.507170, acc: 0.437500]\n",
            "4197: [D loss: 0.073843, acc: 0.992188]  [G loss: 4.427837, acc: 0.390625]\n",
            "4198: [D loss: 0.040306, acc: 0.976562]  [G loss: 3.892050, acc: 0.468750]\n",
            "4199: [D loss: 0.086426, acc: 0.960938]  [G loss: 3.387122, acc: 0.531250]\n",
            "4200: [D loss: 0.096312, acc: 0.960938]  [G loss: 3.293420, acc: 0.484375]\n",
            "4201: [D loss: 0.134507, acc: 0.953125]  [G loss: 3.516609, acc: 0.546875]\n",
            "4202: [D loss: 0.074849, acc: 0.992188]  [G loss: 2.403989, acc: 0.609375]\n",
            "4203: [D loss: 0.148353, acc: 0.929688]  [G loss: 2.404681, acc: 0.609375]\n",
            "4204: [D loss: 0.104831, acc: 0.953125]  [G loss: 3.080740, acc: 0.562500]\n",
            "4205: [D loss: 0.163751, acc: 0.945312]  [G loss: 2.556507, acc: 0.546875]\n",
            "4206: [D loss: 0.047570, acc: 0.984375]  [G loss: 2.911371, acc: 0.484375]\n",
            "4207: [D loss: 0.081705, acc: 0.976562]  [G loss: 3.001036, acc: 0.468750]\n",
            "4208: [D loss: 0.125286, acc: 0.976562]  [G loss: 3.107944, acc: 0.484375]\n",
            "4209: [D loss: 0.151461, acc: 0.968750]  [G loss: 2.385043, acc: 0.640625]\n",
            "4210: [D loss: 0.340202, acc: 0.921875]  [G loss: 1.120380, acc: 0.812500]\n",
            "4211: [D loss: 0.284914, acc: 0.921875]  [G loss: 1.155480, acc: 0.687500]\n",
            "4212: [D loss: 0.197570, acc: 0.945312]  [G loss: 1.349626, acc: 0.593750]\n",
            "4213: [D loss: 0.039897, acc: 0.992188]  [G loss: 3.032788, acc: 0.421875]\n",
            "4214: [D loss: 0.231247, acc: 0.945312]  [G loss: 3.355262, acc: 0.296875]\n",
            "4215: [D loss: 0.093526, acc: 0.976562]  [G loss: 2.891373, acc: 0.421875]\n",
            "4216: [D loss: 0.051232, acc: 0.976562]  [G loss: 3.265656, acc: 0.359375]\n",
            "4217: [D loss: 0.117488, acc: 0.953125]  [G loss: 2.182625, acc: 0.468750]\n",
            "4218: [D loss: 0.093355, acc: 0.960938]  [G loss: 1.422925, acc: 0.625000]\n",
            "4219: [D loss: 0.219478, acc: 0.890625]  [G loss: 3.158212, acc: 0.437500]\n",
            "4220: [D loss: 0.074456, acc: 0.976562]  [G loss: 4.965242, acc: 0.281250]\n",
            "4221: [D loss: 0.131895, acc: 0.960938]  [G loss: 5.632879, acc: 0.234375]\n",
            "4222: [D loss: 0.446916, acc: 0.867188]  [G loss: 4.219062, acc: 0.281250]\n",
            "4223: [D loss: 0.217839, acc: 0.890625]  [G loss: 3.819828, acc: 0.343750]\n",
            "4224: [D loss: 0.171963, acc: 0.953125]  [G loss: 3.601162, acc: 0.265625]\n",
            "4225: [D loss: 0.142697, acc: 0.960938]  [G loss: 3.772636, acc: 0.156250]\n",
            "4226: [D loss: 0.087358, acc: 0.968750]  [G loss: 5.283592, acc: 0.046875]\n",
            "4227: [D loss: 0.192117, acc: 0.921875]  [G loss: 6.249535, acc: 0.031250]\n",
            "4228: [D loss: 0.183465, acc: 0.914062]  [G loss: 5.735688, acc: 0.093750]\n",
            "4229: [D loss: 0.128643, acc: 0.945312]  [G loss: 5.390753, acc: 0.140625]\n",
            "4230: [D loss: 0.265130, acc: 0.890625]  [G loss: 4.744084, acc: 0.062500]\n",
            "4231: [D loss: 0.100475, acc: 0.960938]  [G loss: 4.456425, acc: 0.187500]\n",
            "4232: [D loss: 0.142222, acc: 0.937500]  [G loss: 4.726500, acc: 0.125000]\n",
            "4233: [D loss: 0.241547, acc: 0.906250]  [G loss: 6.854824, acc: 0.078125]\n",
            "4234: [D loss: 0.186995, acc: 0.937500]  [G loss: 7.608809, acc: 0.078125]\n",
            "4235: [D loss: 0.127057, acc: 0.960938]  [G loss: 7.080207, acc: 0.093750]\n",
            "4236: [D loss: 0.128242, acc: 0.953125]  [G loss: 5.535527, acc: 0.093750]\n",
            "4237: [D loss: 0.136038, acc: 0.937500]  [G loss: 4.677638, acc: 0.125000]\n",
            "4238: [D loss: 0.115315, acc: 0.937500]  [G loss: 6.147509, acc: 0.078125]\n",
            "4239: [D loss: 0.072661, acc: 0.968750]  [G loss: 5.442877, acc: 0.015625]\n",
            "4240: [D loss: 0.065961, acc: 0.960938]  [G loss: 7.184427, acc: 0.031250]\n",
            "4241: [D loss: 0.014501, acc: 1.000000]  [G loss: 8.145610, acc: 0.000000]\n",
            "4242: [D loss: 0.024616, acc: 0.984375]  [G loss: 7.795924, acc: 0.000000]\n",
            "4243: [D loss: 0.052392, acc: 0.960938]  [G loss: 6.180803, acc: 0.000000]\n",
            "4244: [D loss: 0.067205, acc: 0.992188]  [G loss: 5.447788, acc: 0.046875]\n",
            "4245: [D loss: 0.106911, acc: 0.976562]  [G loss: 5.497210, acc: 0.046875]\n",
            "4246: [D loss: 0.166176, acc: 0.953125]  [G loss: 4.548826, acc: 0.046875]\n",
            "4247: [D loss: 0.075391, acc: 0.976562]  [G loss: 4.682213, acc: 0.125000]\n",
            "4248: [D loss: 0.129895, acc: 0.968750]  [G loss: 4.468979, acc: 0.125000]\n",
            "4249: [D loss: 0.042123, acc: 0.984375]  [G loss: 4.284098, acc: 0.218750]\n",
            "4250: [D loss: 0.033895, acc: 0.984375]  [G loss: 4.770633, acc: 0.187500]\n",
            "4251: [D loss: 0.051663, acc: 0.984375]  [G loss: 3.799355, acc: 0.468750]\n",
            "4252: [D loss: 0.058500, acc: 0.992188]  [G loss: 2.916814, acc: 0.546875]\n",
            "4253: [D loss: 0.138899, acc: 0.960938]  [G loss: 3.790754, acc: 0.500000]\n",
            "4254: [D loss: 0.226899, acc: 0.914062]  [G loss: 3.431910, acc: 0.406250]\n",
            "4255: [D loss: 0.116363, acc: 0.953125]  [G loss: 3.765583, acc: 0.437500]\n",
            "4256: [D loss: 0.072213, acc: 0.976562]  [G loss: 3.289185, acc: 0.484375]\n",
            "4257: [D loss: 0.151249, acc: 0.945312]  [G loss: 2.495999, acc: 0.468750]\n",
            "4258: [D loss: 0.352196, acc: 0.898438]  [G loss: 2.014505, acc: 0.578125]\n",
            "4259: [D loss: 0.630177, acc: 0.875000]  [G loss: 3.688338, acc: 0.453125]\n",
            "4260: [D loss: 0.272147, acc: 0.921875]  [G loss: 3.869474, acc: 0.593750]\n",
            "4261: [D loss: 0.423736, acc: 0.898438]  [G loss: 3.788650, acc: 0.656250]\n",
            "4262: [D loss: 0.427474, acc: 0.859375]  [G loss: 2.970263, acc: 0.718750]\n",
            "4263: [D loss: 0.237039, acc: 0.914062]  [G loss: 3.903975, acc: 0.500000]\n",
            "4264: [D loss: 0.128439, acc: 0.953125]  [G loss: 6.026267, acc: 0.203125]\n",
            "4265: [D loss: 0.269540, acc: 0.937500]  [G loss: 5.923339, acc: 0.250000]\n",
            "4266: [D loss: 0.198950, acc: 0.945312]  [G loss: 5.697621, acc: 0.187500]\n",
            "4267: [D loss: 0.101538, acc: 0.953125]  [G loss: 3.938975, acc: 0.500000]\n",
            "4268: [D loss: 0.186466, acc: 0.953125]  [G loss: 4.736792, acc: 0.390625]\n",
            "4269: [D loss: 0.144827, acc: 0.968750]  [G loss: 5.220114, acc: 0.328125]\n",
            "4270: [D loss: 0.144379, acc: 0.945312]  [G loss: 6.604134, acc: 0.328125]\n",
            "4271: [D loss: 0.028089, acc: 0.992188]  [G loss: 7.547007, acc: 0.125000]\n",
            "4272: [D loss: 0.213342, acc: 0.968750]  [G loss: 7.028720, acc: 0.140625]\n",
            "4273: [D loss: 0.025316, acc: 0.992188]  [G loss: 6.341462, acc: 0.296875]\n",
            "4274: [D loss: 0.198586, acc: 0.968750]  [G loss: 4.527904, acc: 0.421875]\n",
            "4275: [D loss: 0.163926, acc: 0.945312]  [G loss: 3.349117, acc: 0.546875]\n",
            "4276: [D loss: 0.340747, acc: 0.906250]  [G loss: 3.537994, acc: 0.562500]\n",
            "4277: [D loss: 0.175412, acc: 0.921875]  [G loss: 2.781699, acc: 0.562500]\n",
            "4278: [D loss: 0.088584, acc: 0.960938]  [G loss: 2.252915, acc: 0.515625]\n",
            "4279: [D loss: 0.046343, acc: 0.984375]  [G loss: 2.705190, acc: 0.562500]\n",
            "4280: [D loss: 0.114751, acc: 0.960938]  [G loss: 2.246562, acc: 0.578125]\n",
            "4281: [D loss: 0.185172, acc: 0.953125]  [G loss: 1.902533, acc: 0.578125]\n",
            "4282: [D loss: 0.105176, acc: 0.945312]  [G loss: 1.143701, acc: 0.734375]\n",
            "4283: [D loss: 0.073744, acc: 0.968750]  [G loss: 1.717089, acc: 0.656250]\n",
            "4284: [D loss: 0.147349, acc: 0.960938]  [G loss: 1.151302, acc: 0.734375]\n",
            "4285: [D loss: 0.223057, acc: 0.937500]  [G loss: 1.212891, acc: 0.718750]\n",
            "4286: [D loss: 0.111856, acc: 0.945312]  [G loss: 1.634837, acc: 0.578125]\n",
            "4287: [D loss: 0.280758, acc: 0.914062]  [G loss: 1.987677, acc: 0.593750]\n",
            "4288: [D loss: 0.400717, acc: 0.890625]  [G loss: 1.327403, acc: 0.687500]\n",
            "4289: [D loss: 0.058741, acc: 0.968750]  [G loss: 0.740442, acc: 0.781250]\n",
            "4290: [D loss: 0.099281, acc: 0.953125]  [G loss: 0.573054, acc: 0.796875]\n",
            "4291: [D loss: 0.063102, acc: 0.976562]  [G loss: 0.497591, acc: 0.812500]\n",
            "4292: [D loss: 0.153249, acc: 0.945312]  [G loss: 0.726418, acc: 0.656250]\n",
            "4293: [D loss: 0.048715, acc: 0.984375]  [G loss: 0.753029, acc: 0.718750]\n",
            "4294: [D loss: 0.115676, acc: 0.953125]  [G loss: 1.084316, acc: 0.562500]\n",
            "4295: [D loss: 0.093140, acc: 0.968750]  [G loss: 1.255756, acc: 0.578125]\n",
            "4296: [D loss: 0.104727, acc: 0.945312]  [G loss: 1.429749, acc: 0.468750]\n",
            "4297: [D loss: 0.231223, acc: 0.921875]  [G loss: 0.670272, acc: 0.656250]\n",
            "4298: [D loss: 0.136442, acc: 0.953125]  [G loss: 0.425366, acc: 0.812500]\n",
            "4299: [D loss: 0.091517, acc: 0.968750]  [G loss: 0.257701, acc: 0.890625]\n",
            "4300: [D loss: 0.125034, acc: 0.960938]  [G loss: 0.347098, acc: 0.859375]\n",
            "4301: [D loss: 0.061929, acc: 0.976562]  [G loss: 0.640542, acc: 0.703125]\n",
            "4302: [D loss: 0.031740, acc: 0.992188]  [G loss: 1.181132, acc: 0.656250]\n",
            "4303: [D loss: 0.067606, acc: 0.968750]  [G loss: 1.255397, acc: 0.546875]\n",
            "4304: [D loss: 0.102135, acc: 0.968750]  [G loss: 0.521429, acc: 0.796875]\n",
            "4305: [D loss: 0.046124, acc: 0.992188]  [G loss: 0.248936, acc: 0.875000]\n",
            "4306: [D loss: 0.034733, acc: 0.992188]  [G loss: 0.431834, acc: 0.843750]\n",
            "4307: [D loss: 0.097804, acc: 0.968750]  [G loss: 0.479435, acc: 0.765625]\n",
            "4308: [D loss: 0.038189, acc: 0.992188]  [G loss: 0.609945, acc: 0.765625]\n",
            "4309: [D loss: 0.176278, acc: 0.953125]  [G loss: 1.750606, acc: 0.484375]\n",
            "4310: [D loss: 0.034704, acc: 0.992188]  [G loss: 2.472908, acc: 0.187500]\n",
            "4311: [D loss: 0.191539, acc: 0.953125]  [G loss: 2.053144, acc: 0.359375]\n",
            "4312: [D loss: 0.053339, acc: 0.984375]  [G loss: 1.498407, acc: 0.468750]\n",
            "4313: [D loss: 0.025949, acc: 0.992188]  [G loss: 0.901799, acc: 0.562500]\n",
            "4314: [D loss: 0.058159, acc: 0.984375]  [G loss: 1.634533, acc: 0.437500]\n",
            "4315: [D loss: 0.023808, acc: 0.992188]  [G loss: 2.839385, acc: 0.328125]\n",
            "4316: [D loss: 0.075540, acc: 0.968750]  [G loss: 2.843736, acc: 0.328125]\n",
            "4317: [D loss: 0.050188, acc: 0.976562]  [G loss: 2.862048, acc: 0.328125]\n",
            "4318: [D loss: 0.128188, acc: 0.953125]  [G loss: 2.951334, acc: 0.359375]\n",
            "4319: [D loss: 0.034328, acc: 0.984375]  [G loss: 2.416396, acc: 0.453125]\n",
            "4320: [D loss: 0.082325, acc: 0.953125]  [G loss: 3.173563, acc: 0.343750]\n",
            "4321: [D loss: 0.043098, acc: 0.984375]  [G loss: 4.711127, acc: 0.187500]\n",
            "4322: [D loss: 0.243684, acc: 0.945312]  [G loss: 2.616047, acc: 0.296875]\n",
            "4323: [D loss: 0.163409, acc: 0.968750]  [G loss: 1.594955, acc: 0.484375]\n",
            "4324: [D loss: 0.085944, acc: 0.953125]  [G loss: 0.703437, acc: 0.687500]\n",
            "4325: [D loss: 0.063145, acc: 0.984375]  [G loss: 1.218233, acc: 0.625000]\n",
            "4326: [D loss: 0.037567, acc: 0.976562]  [G loss: 2.065574, acc: 0.406250]\n",
            "4327: [D loss: 0.145483, acc: 0.960938]  [G loss: 2.188240, acc: 0.406250]\n",
            "4328: [D loss: 0.210836, acc: 0.960938]  [G loss: 2.171016, acc: 0.484375]\n",
            "4329: [D loss: 0.054758, acc: 0.984375]  [G loss: 1.792060, acc: 0.500000]\n",
            "4330: [D loss: 0.061541, acc: 0.976562]  [G loss: 2.573956, acc: 0.390625]\n",
            "4331: [D loss: 0.100603, acc: 0.968750]  [G loss: 2.261636, acc: 0.437500]\n",
            "4332: [D loss: 0.095273, acc: 0.953125]  [G loss: 1.632538, acc: 0.640625]\n",
            "4333: [D loss: 0.080310, acc: 0.960938]  [G loss: 2.143102, acc: 0.515625]\n",
            "4334: [D loss: 0.046113, acc: 0.992188]  [G loss: 2.345427, acc: 0.546875]\n",
            "4335: [D loss: 0.038132, acc: 0.984375]  [G loss: 3.007936, acc: 0.296875]\n",
            "4336: [D loss: 0.170660, acc: 0.953125]  [G loss: 2.221046, acc: 0.437500]\n",
            "4337: [D loss: 0.282252, acc: 0.937500]  [G loss: 1.280011, acc: 0.531250]\n",
            "4338: [D loss: 0.197719, acc: 0.929688]  [G loss: 1.461257, acc: 0.500000]\n",
            "4339: [D loss: 0.164731, acc: 0.937500]  [G loss: 2.289964, acc: 0.406250]\n",
            "4340: [D loss: 0.046358, acc: 0.984375]  [G loss: 3.934704, acc: 0.203125]\n",
            "4341: [D loss: 0.188646, acc: 0.945312]  [G loss: 3.323897, acc: 0.296875]\n",
            "4342: [D loss: 0.076121, acc: 0.984375]  [G loss: 2.303050, acc: 0.406250]\n",
            "4343: [D loss: 0.093429, acc: 0.976562]  [G loss: 1.259082, acc: 0.687500]\n",
            "4344: [D loss: 0.152205, acc: 0.953125]  [G loss: 1.627038, acc: 0.625000]\n",
            "4345: [D loss: 0.117925, acc: 0.953125]  [G loss: 2.523654, acc: 0.421875]\n",
            "4346: [D loss: 0.136527, acc: 0.960938]  [G loss: 3.769877, acc: 0.343750]\n",
            "4347: [D loss: 0.088614, acc: 0.953125]  [G loss: 5.702973, acc: 0.140625]\n",
            "4348: [D loss: 0.095522, acc: 0.976562]  [G loss: 6.871386, acc: 0.046875]\n",
            "4349: [D loss: 0.144631, acc: 0.960938]  [G loss: 5.897196, acc: 0.156250]\n",
            "4350: [D loss: 0.157526, acc: 0.960938]  [G loss: 3.340739, acc: 0.421875]\n",
            "4351: [D loss: 0.043962, acc: 0.984375]  [G loss: 2.260345, acc: 0.593750]\n",
            "4352: [D loss: 0.044739, acc: 0.992188]  [G loss: 1.829290, acc: 0.578125]\n",
            "4353: [D loss: 0.094364, acc: 0.953125]  [G loss: 3.446424, acc: 0.343750]\n",
            "4354: [D loss: 0.052016, acc: 0.992188]  [G loss: 4.333696, acc: 0.296875]\n",
            "4355: [D loss: 0.063051, acc: 0.984375]  [G loss: 4.814466, acc: 0.250000]\n",
            "4356: [D loss: 0.033807, acc: 0.984375]  [G loss: 4.447485, acc: 0.281250]\n",
            "4357: [D loss: 0.073123, acc: 0.968750]  [G loss: 3.409224, acc: 0.375000]\n",
            "4358: [D loss: 0.034430, acc: 0.992188]  [G loss: 3.508572, acc: 0.390625]\n",
            "4359: [D loss: 0.104696, acc: 0.968750]  [G loss: 1.685231, acc: 0.640625]\n",
            "4360: [D loss: 0.112436, acc: 0.953125]  [G loss: 2.836235, acc: 0.468750]\n",
            "4361: [D loss: 0.046113, acc: 0.976562]  [G loss: 4.622392, acc: 0.281250]\n",
            "4362: [D loss: 0.013391, acc: 1.000000]  [G loss: 4.350318, acc: 0.281250]\n",
            "4363: [D loss: 0.056027, acc: 0.984375]  [G loss: 3.725030, acc: 0.390625]\n",
            "4364: [D loss: 0.060402, acc: 0.976562]  [G loss: 3.758888, acc: 0.390625]\n",
            "4365: [D loss: 0.031322, acc: 0.984375]  [G loss: 4.279383, acc: 0.390625]\n",
            "4366: [D loss: 0.042607, acc: 0.984375]  [G loss: 4.255129, acc: 0.296875]\n",
            "4367: [D loss: 0.099476, acc: 0.976562]  [G loss: 3.498348, acc: 0.328125]\n",
            "4368: [D loss: 0.084314, acc: 0.960938]  [G loss: 3.797050, acc: 0.343750]\n",
            "4369: [D loss: 0.085326, acc: 0.976562]  [G loss: 3.122009, acc: 0.406250]\n",
            "4370: [D loss: 0.179145, acc: 0.968750]  [G loss: 1.798721, acc: 0.640625]\n",
            "4371: [D loss: 0.136397, acc: 0.953125]  [G loss: 1.960660, acc: 0.625000]\n",
            "4372: [D loss: 0.172354, acc: 0.929688]  [G loss: 2.395758, acc: 0.437500]\n",
            "4373: [D loss: 0.070068, acc: 0.976562]  [G loss: 3.655121, acc: 0.312500]\n",
            "4374: [D loss: 0.139680, acc: 0.960938]  [G loss: 3.888052, acc: 0.312500]\n",
            "4375: [D loss: 0.081447, acc: 0.992188]  [G loss: 3.154393, acc: 0.453125]\n",
            "4376: [D loss: 0.024453, acc: 1.000000]  [G loss: 2.120687, acc: 0.578125]\n",
            "4377: [D loss: 0.128196, acc: 0.945312]  [G loss: 2.846180, acc: 0.359375]\n",
            "4378: [D loss: 0.254996, acc: 0.937500]  [G loss: 3.351775, acc: 0.406250]\n",
            "4379: [D loss: 0.090432, acc: 0.953125]  [G loss: 5.185017, acc: 0.265625]\n",
            "4380: [D loss: 0.126255, acc: 0.960938]  [G loss: 5.314068, acc: 0.187500]\n",
            "4381: [D loss: 0.221039, acc: 0.945312]  [G loss: 3.751526, acc: 0.281250]\n",
            "4382: [D loss: 0.240324, acc: 0.921875]  [G loss: 2.403198, acc: 0.421875]\n",
            "4383: [D loss: 0.290332, acc: 0.898438]  [G loss: 6.732457, acc: 0.156250]\n",
            "4384: [D loss: 0.180003, acc: 0.960938]  [G loss: 9.314598, acc: 0.093750]\n",
            "4385: [D loss: 0.213144, acc: 0.921875]  [G loss: 7.926588, acc: 0.156250]\n",
            "4386: [D loss: 0.156132, acc: 0.929688]  [G loss: 7.652562, acc: 0.093750]\n",
            "4387: [D loss: 0.184946, acc: 0.960938]  [G loss: 8.163870, acc: 0.125000]\n",
            "4388: [D loss: 0.137366, acc: 0.953125]  [G loss: 8.523623, acc: 0.093750]\n",
            "4389: [D loss: 0.051114, acc: 0.976562]  [G loss: 8.377978, acc: 0.109375]\n",
            "4390: [D loss: 0.021132, acc: 0.984375]  [G loss: 7.458672, acc: 0.156250]\n",
            "4391: [D loss: 0.057831, acc: 0.976562]  [G loss: 4.954749, acc: 0.281250]\n",
            "4392: [D loss: 0.068813, acc: 0.968750]  [G loss: 7.357613, acc: 0.156250]\n",
            "4393: [D loss: 0.095186, acc: 0.968750]  [G loss: 7.477609, acc: 0.125000]\n",
            "4394: [D loss: 0.216072, acc: 0.953125]  [G loss: 7.512643, acc: 0.109375]\n",
            "4395: [D loss: 0.137813, acc: 0.960938]  [G loss: 6.473202, acc: 0.218750]\n",
            "4396: [D loss: 0.207867, acc: 0.960938]  [G loss: 7.757042, acc: 0.109375]\n",
            "4397: [D loss: 0.255034, acc: 0.953125]  [G loss: 6.347651, acc: 0.218750]\n",
            "4398: [D loss: 0.207358, acc: 0.937500]  [G loss: 6.795996, acc: 0.171875]\n",
            "4399: [D loss: 0.094746, acc: 0.976562]  [G loss: 5.721396, acc: 0.140625]\n",
            "4400: [D loss: 0.126995, acc: 0.976562]  [G loss: 5.194953, acc: 0.140625]\n",
            "4401: [D loss: 0.133275, acc: 0.984375]  [G loss: 4.776233, acc: 0.265625]\n",
            "4402: [D loss: 0.252318, acc: 0.960938]  [G loss: 3.193363, acc: 0.390625]\n",
            "4403: [D loss: 0.075008, acc: 0.968750]  [G loss: 3.507381, acc: 0.312500]\n",
            "4404: [D loss: 0.153504, acc: 0.945312]  [G loss: 3.162894, acc: 0.421875]\n",
            "4405: [D loss: 0.180893, acc: 0.929688]  [G loss: 2.372260, acc: 0.531250]\n",
            "4406: [D loss: 0.411666, acc: 0.843750]  [G loss: 2.934282, acc: 0.265625]\n",
            "4407: [D loss: 0.218171, acc: 0.914062]  [G loss: 2.578237, acc: 0.312500]\n",
            "4408: [D loss: 0.376262, acc: 0.906250]  [G loss: 2.031683, acc: 0.546875]\n",
            "4409: [D loss: 0.372925, acc: 0.882812]  [G loss: 3.468897, acc: 0.359375]\n",
            "4410: [D loss: 0.221483, acc: 0.914062]  [G loss: 4.214161, acc: 0.312500]\n",
            "4411: [D loss: 0.642737, acc: 0.859375]  [G loss: 1.569468, acc: 0.593750]\n",
            "4412: [D loss: 0.373049, acc: 0.898438]  [G loss: 1.564127, acc: 0.593750]\n",
            "4413: [D loss: 0.208256, acc: 0.906250]  [G loss: 3.118342, acc: 0.531250]\n",
            "4414: [D loss: 0.388268, acc: 0.898438]  [G loss: 3.876792, acc: 0.281250]\n",
            "4415: [D loss: 0.184731, acc: 0.937500]  [G loss: 3.572636, acc: 0.281250]\n",
            "4416: [D loss: 0.248816, acc: 0.914062]  [G loss: 2.640687, acc: 0.515625]\n",
            "4417: [D loss: 0.225033, acc: 0.914062]  [G loss: 2.324983, acc: 0.625000]\n",
            "4418: [D loss: 0.289656, acc: 0.859375]  [G loss: 2.784526, acc: 0.406250]\n",
            "4419: [D loss: 0.107914, acc: 0.945312]  [G loss: 4.073593, acc: 0.187500]\n",
            "4420: [D loss: 0.144501, acc: 0.945312]  [G loss: 5.394143, acc: 0.125000]\n",
            "4421: [D loss: 0.130394, acc: 0.960938]  [G loss: 4.541362, acc: 0.171875]\n",
            "4422: [D loss: 0.042046, acc: 0.976562]  [G loss: 3.445751, acc: 0.343750]\n",
            "4423: [D loss: 0.033559, acc: 0.984375]  [G loss: 2.163080, acc: 0.421875]\n",
            "4424: [D loss: 0.061463, acc: 0.976562]  [G loss: 1.500951, acc: 0.531250]\n",
            "4425: [D loss: 0.026862, acc: 1.000000]  [G loss: 0.597606, acc: 0.718750]\n",
            "4426: [D loss: 0.169387, acc: 0.929688]  [G loss: 1.197034, acc: 0.609375]\n",
            "4427: [D loss: 0.072190, acc: 0.976562]  [G loss: 2.137610, acc: 0.500000]\n",
            "4428: [D loss: 0.016364, acc: 1.000000]  [G loss: 3.258542, acc: 0.468750]\n",
            "4429: [D loss: 0.117516, acc: 0.960938]  [G loss: 2.573766, acc: 0.468750]\n",
            "4430: [D loss: 0.051627, acc: 0.984375]  [G loss: 2.539328, acc: 0.437500]\n",
            "4431: [D loss: 0.054719, acc: 0.984375]  [G loss: 1.723319, acc: 0.546875]\n",
            "4432: [D loss: 0.029824, acc: 0.992188]  [G loss: 0.889124, acc: 0.734375]\n",
            "4433: [D loss: 0.072362, acc: 0.976562]  [G loss: 1.238387, acc: 0.687500]\n",
            "4434: [D loss: 0.126381, acc: 0.937500]  [G loss: 0.905514, acc: 0.687500]\n",
            "4435: [D loss: 0.089231, acc: 0.960938]  [G loss: 0.934639, acc: 0.703125]\n",
            "4436: [D loss: 0.172562, acc: 0.921875]  [G loss: 1.416862, acc: 0.531250]\n",
            "4437: [D loss: 0.156693, acc: 0.953125]  [G loss: 2.064750, acc: 0.562500]\n",
            "4438: [D loss: 0.196566, acc: 0.960938]  [G loss: 2.250184, acc: 0.468750]\n",
            "4439: [D loss: 0.303465, acc: 0.937500]  [G loss: 1.228900, acc: 0.671875]\n",
            "4440: [D loss: 0.148447, acc: 0.953125]  [G loss: 0.387588, acc: 0.859375]\n",
            "4441: [D loss: 0.225044, acc: 0.914062]  [G loss: 0.611161, acc: 0.812500]\n",
            "4442: [D loss: 0.176847, acc: 0.945312]  [G loss: 1.024320, acc: 0.703125]\n",
            "4443: [D loss: 0.174657, acc: 0.929688]  [G loss: 1.607939, acc: 0.546875]\n",
            "4444: [D loss: 0.161532, acc: 0.945312]  [G loss: 1.723868, acc: 0.484375]\n",
            "4445: [D loss: 0.164348, acc: 0.937500]  [G loss: 1.369126, acc: 0.531250]\n",
            "4446: [D loss: 0.211404, acc: 0.953125]  [G loss: 1.241571, acc: 0.578125]\n",
            "4447: [D loss: 0.211060, acc: 0.914062]  [G loss: 1.148546, acc: 0.671875]\n",
            "4448: [D loss: 0.216355, acc: 0.937500]  [G loss: 1.293550, acc: 0.671875]\n",
            "4449: [D loss: 0.156734, acc: 0.929688]  [G loss: 1.654206, acc: 0.640625]\n",
            "4450: [D loss: 0.263225, acc: 0.906250]  [G loss: 2.042694, acc: 0.453125]\n",
            "4451: [D loss: 0.087416, acc: 0.968750]  [G loss: 2.706730, acc: 0.390625]\n",
            "4452: [D loss: 0.064546, acc: 0.960938]  [G loss: 2.226151, acc: 0.421875]\n",
            "4453: [D loss: 0.047335, acc: 0.976562]  [G loss: 2.895774, acc: 0.359375]\n",
            "4454: [D loss: 0.148088, acc: 0.968750]  [G loss: 3.765813, acc: 0.281250]\n",
            "4455: [D loss: 0.086425, acc: 0.960938]  [G loss: 4.799704, acc: 0.265625]\n",
            "4456: [D loss: 0.073422, acc: 0.976562]  [G loss: 6.185055, acc: 0.218750]\n",
            "4457: [D loss: 0.144888, acc: 0.953125]  [G loss: 6.302643, acc: 0.171875]\n",
            "4458: [D loss: 0.040570, acc: 0.976562]  [G loss: 6.130716, acc: 0.171875]\n",
            "4459: [D loss: 0.082247, acc: 0.976562]  [G loss: 4.301669, acc: 0.375000]\n",
            "4460: [D loss: 0.140832, acc: 0.953125]  [G loss: 4.408313, acc: 0.265625]\n",
            "4461: [D loss: 0.181684, acc: 0.960938]  [G loss: 4.750876, acc: 0.312500]\n",
            "4462: [D loss: 0.226766, acc: 0.945312]  [G loss: 3.313623, acc: 0.375000]\n",
            "4463: [D loss: 0.092901, acc: 0.937500]  [G loss: 2.537219, acc: 0.328125]\n",
            "4464: [D loss: 0.196580, acc: 0.945312]  [G loss: 5.527569, acc: 0.140625]\n",
            "4465: [D loss: 0.074955, acc: 0.968750]  [G loss: 7.283010, acc: 0.062500]\n",
            "4466: [D loss: 0.279268, acc: 0.921875]  [G loss: 5.795919, acc: 0.187500]\n",
            "4467: [D loss: 0.097284, acc: 0.968750]  [G loss: 3.804614, acc: 0.375000]\n",
            "4468: [D loss: 0.102751, acc: 0.960938]  [G loss: 3.539483, acc: 0.328125]\n",
            "4469: [D loss: 0.140138, acc: 0.937500]  [G loss: 5.411538, acc: 0.125000]\n",
            "4470: [D loss: 0.050993, acc: 0.984375]  [G loss: 7.319001, acc: 0.031250]\n",
            "4471: [D loss: 0.330546, acc: 0.929688]  [G loss: 6.377871, acc: 0.031250]\n",
            "4472: [D loss: 0.094771, acc: 0.953125]  [G loss: 4.342750, acc: 0.203125]\n",
            "4473: [D loss: 0.049561, acc: 0.984375]  [G loss: 3.318152, acc: 0.328125]\n",
            "4474: [D loss: 0.110127, acc: 0.953125]  [G loss: 3.689269, acc: 0.328125]\n",
            "4475: [D loss: 0.197291, acc: 0.929688]  [G loss: 6.329050, acc: 0.078125]\n",
            "4476: [D loss: 0.087937, acc: 0.976562]  [G loss: 7.777756, acc: 0.125000]\n",
            "4477: [D loss: 0.052784, acc: 0.976562]  [G loss: 7.972073, acc: 0.203125]\n",
            "4478: [D loss: 0.055906, acc: 0.960938]  [G loss: 9.185026, acc: 0.109375]\n",
            "4479: [D loss: 0.093941, acc: 0.960938]  [G loss: 8.193451, acc: 0.171875]\n",
            "4480: [D loss: 0.140104, acc: 0.968750]  [G loss: 8.462334, acc: 0.234375]\n",
            "4481: [D loss: 0.202898, acc: 0.953125]  [G loss: 7.414525, acc: 0.390625]\n",
            "4482: [D loss: 0.007161, acc: 1.000000]  [G loss: 6.560894, acc: 0.343750]\n",
            "4483: [D loss: 0.289711, acc: 0.953125]  [G loss: 6.925514, acc: 0.359375]\n",
            "4484: [D loss: 0.067421, acc: 0.976562]  [G loss: 6.409348, acc: 0.359375]\n",
            "4485: [D loss: 0.109918, acc: 0.968750]  [G loss: 6.473789, acc: 0.250000]\n",
            "4486: [D loss: 0.020388, acc: 0.984375]  [G loss: 6.148018, acc: 0.343750]\n",
            "4487: [D loss: 0.015125, acc: 0.992188]  [G loss: 6.036899, acc: 0.312500]\n",
            "4488: [D loss: 0.002252, acc: 1.000000]  [G loss: 6.182396, acc: 0.250000]\n",
            "4489: [D loss: 0.028212, acc: 0.992188]  [G loss: 5.548692, acc: 0.343750]\n",
            "4490: [D loss: 0.027161, acc: 0.992188]  [G loss: 6.515741, acc: 0.218750]\n",
            "4491: [D loss: 0.000234, acc: 1.000000]  [G loss: 6.955960, acc: 0.140625]\n",
            "4492: [D loss: 0.015751, acc: 0.992188]  [G loss: 5.832961, acc: 0.234375]\n",
            "4493: [D loss: 0.036664, acc: 0.992188]  [G loss: 5.471550, acc: 0.187500]\n",
            "4494: [D loss: 0.092468, acc: 0.976562]  [G loss: 5.615451, acc: 0.218750]\n",
            "4495: [D loss: 0.037708, acc: 0.984375]  [G loss: 5.627880, acc: 0.187500]\n",
            "4496: [D loss: 0.085815, acc: 0.960938]  [G loss: 4.804165, acc: 0.203125]\n",
            "4497: [D loss: 0.042131, acc: 0.984375]  [G loss: 3.848516, acc: 0.406250]\n",
            "4498: [D loss: 0.104110, acc: 0.984375]  [G loss: 2.801184, acc: 0.468750]\n",
            "4499: [D loss: 0.040335, acc: 0.976562]  [G loss: 2.256835, acc: 0.500000]\n",
            "4500: [D loss: 0.011885, acc: 0.992188]  [G loss: 2.428700, acc: 0.500000]\n",
            "4501: [D loss: 0.037440, acc: 0.984375]  [G loss: 2.341907, acc: 0.578125]\n",
            "4502: [D loss: 0.060263, acc: 0.992188]  [G loss: 1.764773, acc: 0.656250]\n",
            "4503: [D loss: 0.090095, acc: 0.992188]  [G loss: 1.505565, acc: 0.656250]\n",
            "4504: [D loss: 0.002140, acc: 1.000000]  [G loss: 0.653511, acc: 0.812500]\n",
            "4505: [D loss: 0.037464, acc: 0.992188]  [G loss: 0.746640, acc: 0.765625]\n",
            "4506: [D loss: 0.061905, acc: 0.976562]  [G loss: 1.574744, acc: 0.687500]\n",
            "4507: [D loss: 0.009525, acc: 0.992188]  [G loss: 1.433805, acc: 0.625000]\n",
            "4508: [D loss: 0.025233, acc: 0.984375]  [G loss: 1.444860, acc: 0.671875]\n",
            "4509: [D loss: 0.143273, acc: 0.976562]  [G loss: 0.862921, acc: 0.718750]\n",
            "4510: [D loss: 0.047965, acc: 0.984375]  [G loss: 0.663040, acc: 0.734375]\n",
            "4511: [D loss: 0.015209, acc: 0.992188]  [G loss: 0.340532, acc: 0.875000]\n",
            "4512: [D loss: 0.056711, acc: 0.976562]  [G loss: 0.150007, acc: 0.921875]\n",
            "4513: [D loss: 0.232737, acc: 0.953125]  [G loss: 0.772471, acc: 0.796875]\n",
            "4514: [D loss: 0.016131, acc: 0.992188]  [G loss: 0.816142, acc: 0.734375]\n",
            "4515: [D loss: 0.000240, acc: 1.000000]  [G loss: 1.281147, acc: 0.703125]\n",
            "4516: [D loss: 0.143111, acc: 0.976562]  [G loss: 0.533202, acc: 0.843750]\n",
            "4517: [D loss: 0.022583, acc: 0.992188]  [G loss: 0.248278, acc: 0.890625]\n",
            "4518: [D loss: 0.090035, acc: 0.984375]  [G loss: 0.049678, acc: 0.984375]\n",
            "4519: [D loss: 0.021573, acc: 0.992188]  [G loss: 0.012239, acc: 1.000000]\n",
            "4520: [D loss: 0.257783, acc: 0.945312]  [G loss: 0.812315, acc: 0.812500]\n",
            "4521: [D loss: 0.005807, acc: 1.000000]  [G loss: 2.014221, acc: 0.609375]\n",
            "4522: [D loss: 0.061339, acc: 0.976562]  [G loss: 3.901980, acc: 0.515625]\n",
            "4523: [D loss: 0.101592, acc: 0.976562]  [G loss: 3.267220, acc: 0.593750]\n",
            "4524: [D loss: 0.399327, acc: 0.960938]  [G loss: 1.782368, acc: 0.718750]\n",
            "4525: [D loss: 0.017896, acc: 0.992188]  [G loss: 0.618422, acc: 0.859375]\n",
            "4526: [D loss: 0.010882, acc: 0.992188]  [G loss: 0.138748, acc: 0.937500]\n",
            "4527: [D loss: 0.027491, acc: 0.992188]  [G loss: 0.002392, acc: 1.000000]\n",
            "4528: [D loss: 0.221072, acc: 0.945312]  [G loss: 0.214008, acc: 0.937500]\n",
            "4529: [D loss: 0.240963, acc: 0.976562]  [G loss: 0.617599, acc: 0.828125]\n",
            "4530: [D loss: 0.225335, acc: 0.968750]  [G loss: 0.727575, acc: 0.828125]\n",
            "4531: [D loss: 0.320092, acc: 0.945312]  [G loss: 0.792291, acc: 0.875000]\n",
            "4532: [D loss: 0.274309, acc: 0.953125]  [G loss: 0.482128, acc: 0.890625]\n",
            "4533: [D loss: 0.141253, acc: 0.960938]  [G loss: 0.172001, acc: 0.937500]\n",
            "4534: [D loss: 0.056244, acc: 0.976562]  [G loss: 0.341430, acc: 0.921875]\n",
            "4535: [D loss: 0.094624, acc: 0.984375]  [G loss: 0.326974, acc: 0.890625]\n",
            "4536: [D loss: 0.128543, acc: 0.960938]  [G loss: 0.451261, acc: 0.875000]\n",
            "4537: [D loss: 0.135876, acc: 0.960938]  [G loss: 1.013903, acc: 0.812500]\n",
            "4538: [D loss: 0.067514, acc: 0.968750]  [G loss: 0.982718, acc: 0.843750]\n",
            "4539: [D loss: 0.155848, acc: 0.945312]  [G loss: 1.125291, acc: 0.796875]\n",
            "4540: [D loss: 0.324991, acc: 0.945312]  [G loss: 0.705687, acc: 0.781250]\n",
            "4541: [D loss: 0.291885, acc: 0.898438]  [G loss: 1.083009, acc: 0.765625]\n",
            "4542: [D loss: 0.224915, acc: 0.953125]  [G loss: 0.960819, acc: 0.703125]\n",
            "4543: [D loss: 0.326683, acc: 0.945312]  [G loss: 1.436503, acc: 0.734375]\n",
            "4544: [D loss: 0.133026, acc: 0.976562]  [G loss: 2.635642, acc: 0.500000]\n",
            "4545: [D loss: 0.236256, acc: 0.945312]  [G loss: 2.457533, acc: 0.468750]\n",
            "4546: [D loss: 0.122911, acc: 0.976562]  [G loss: 1.950663, acc: 0.625000]\n",
            "4547: [D loss: 0.028282, acc: 0.992188]  [G loss: 1.497671, acc: 0.656250]\n",
            "4548: [D loss: 0.155107, acc: 0.945312]  [G loss: 1.778714, acc: 0.671875]\n",
            "4549: [D loss: 0.055643, acc: 0.976562]  [G loss: 2.062259, acc: 0.562500]\n",
            "4550: [D loss: 0.071664, acc: 0.968750]  [G loss: 2.149440, acc: 0.515625]\n",
            "4551: [D loss: 0.013791, acc: 0.992188]  [G loss: 3.139313, acc: 0.375000]\n",
            "4552: [D loss: 0.078026, acc: 0.960938]  [G loss: 3.120989, acc: 0.328125]\n",
            "4553: [D loss: 0.051258, acc: 0.976562]  [G loss: 2.715076, acc: 0.359375]\n",
            "4554: [D loss: 0.055284, acc: 0.968750]  [G loss: 2.793651, acc: 0.359375]\n",
            "4555: [D loss: 0.031711, acc: 0.984375]  [G loss: 2.607606, acc: 0.359375]\n",
            "4556: [D loss: 0.053246, acc: 0.976562]  [G loss: 1.705573, acc: 0.515625]\n",
            "4557: [D loss: 0.071551, acc: 0.984375]  [G loss: 2.471513, acc: 0.515625]\n",
            "4558: [D loss: 0.045256, acc: 0.976562]  [G loss: 2.024551, acc: 0.546875]\n",
            "4559: [D loss: 0.032868, acc: 0.984375]  [G loss: 1.665721, acc: 0.625000]\n",
            "4560: [D loss: 0.027483, acc: 0.992188]  [G loss: 1.745032, acc: 0.593750]\n",
            "4561: [D loss: 0.083715, acc: 0.968750]  [G loss: 2.264160, acc: 0.500000]\n",
            "4562: [D loss: 0.034029, acc: 0.984375]  [G loss: 2.292308, acc: 0.546875]\n",
            "4563: [D loss: 0.019544, acc: 0.992188]  [G loss: 2.266432, acc: 0.531250]\n",
            "4564: [D loss: 0.068118, acc: 0.984375]  [G loss: 2.371459, acc: 0.500000]\n",
            "4565: [D loss: 0.007547, acc: 1.000000]  [G loss: 3.206684, acc: 0.500000]\n",
            "4566: [D loss: 0.065613, acc: 0.984375]  [G loss: 2.710996, acc: 0.546875]\n",
            "4567: [D loss: 0.049023, acc: 0.992188]  [G loss: 2.404900, acc: 0.515625]\n",
            "4568: [D loss: 0.044045, acc: 0.984375]  [G loss: 2.791107, acc: 0.531250]\n",
            "4569: [D loss: 0.028818, acc: 0.992188]  [G loss: 2.480784, acc: 0.531250]\n",
            "4570: [D loss: 0.132363, acc: 0.945312]  [G loss: 3.351139, acc: 0.437500]\n",
            "4571: [D loss: 0.015045, acc: 1.000000]  [G loss: 3.710622, acc: 0.359375]\n",
            "4572: [D loss: 0.126554, acc: 0.960938]  [G loss: 3.221194, acc: 0.421875]\n",
            "4573: [D loss: 0.120255, acc: 0.953125]  [G loss: 4.013636, acc: 0.406250]\n",
            "4574: [D loss: 0.074898, acc: 0.976562]  [G loss: 2.720989, acc: 0.453125]\n",
            "4575: [D loss: 0.073227, acc: 0.984375]  [G loss: 2.517263, acc: 0.546875]\n",
            "4576: [D loss: 0.087778, acc: 0.960938]  [G loss: 2.824445, acc: 0.484375]\n",
            "4577: [D loss: 0.054428, acc: 0.976562]  [G loss: 3.780298, acc: 0.406250]\n",
            "4578: [D loss: 0.066274, acc: 0.968750]  [G loss: 5.738218, acc: 0.156250]\n",
            "4579: [D loss: 0.158730, acc: 0.953125]  [G loss: 6.320536, acc: 0.156250]\n",
            "4580: [D loss: 0.130149, acc: 0.960938]  [G loss: 5.387694, acc: 0.234375]\n",
            "4581: [D loss: 0.128496, acc: 0.953125]  [G loss: 3.157453, acc: 0.515625]\n",
            "4582: [D loss: 0.139297, acc: 0.945312]  [G loss: 3.562281, acc: 0.375000]\n",
            "4583: [D loss: 0.045576, acc: 0.976562]  [G loss: 3.571871, acc: 0.421875]\n",
            "4584: [D loss: 0.093309, acc: 0.945312]  [G loss: 4.453353, acc: 0.281250]\n",
            "4585: [D loss: 0.075820, acc: 0.968750]  [G loss: 5.171479, acc: 0.312500]\n",
            "4586: [D loss: 0.060507, acc: 0.976562]  [G loss: 5.843187, acc: 0.187500]\n",
            "4587: [D loss: 0.272855, acc: 0.906250]  [G loss: 7.681263, acc: 0.125000]\n",
            "4588: [D loss: 0.073660, acc: 0.976562]  [G loss: 7.487021, acc: 0.078125]\n",
            "4589: [D loss: 0.176554, acc: 0.914062]  [G loss: 5.438617, acc: 0.156250]\n",
            "4590: [D loss: 0.092453, acc: 0.968750]  [G loss: 4.919901, acc: 0.375000]\n",
            "4591: [D loss: 0.096037, acc: 0.968750]  [G loss: 4.654212, acc: 0.218750]\n",
            "4592: [D loss: 0.146757, acc: 0.937500]  [G loss: 6.661567, acc: 0.078125]\n",
            "4593: [D loss: 0.164489, acc: 0.953125]  [G loss: 7.119746, acc: 0.109375]\n",
            "4594: [D loss: 0.113037, acc: 0.968750]  [G loss: 5.813391, acc: 0.281250]\n",
            "4595: [D loss: 0.027911, acc: 0.984375]  [G loss: 4.459438, acc: 0.265625]\n",
            "4596: [D loss: 0.227216, acc: 0.937500]  [G loss: 5.619507, acc: 0.218750]\n",
            "4597: [D loss: 0.111870, acc: 0.945312]  [G loss: 6.291832, acc: 0.250000]\n",
            "4598: [D loss: 0.354362, acc: 0.921875]  [G loss: 4.317711, acc: 0.421875]\n",
            "4599: [D loss: 0.107319, acc: 0.953125]  [G loss: 4.004799, acc: 0.375000]\n",
            "4600: [D loss: 0.066047, acc: 0.968750]  [G loss: 4.887664, acc: 0.312500]\n",
            "4601: [D loss: 0.304181, acc: 0.921875]  [G loss: 5.348369, acc: 0.359375]\n",
            "4602: [D loss: 0.160127, acc: 0.953125]  [G loss: 3.863986, acc: 0.375000]\n",
            "4603: [D loss: 0.047461, acc: 0.984375]  [G loss: 2.420152, acc: 0.546875]\n",
            "4604: [D loss: 0.031565, acc: 0.992188]  [G loss: 2.706915, acc: 0.484375]\n",
            "4605: [D loss: 0.125119, acc: 0.945312]  [G loss: 3.609632, acc: 0.453125]\n",
            "4606: [D loss: 0.033759, acc: 0.992188]  [G loss: 4.046764, acc: 0.421875]\n",
            "4607: [D loss: 0.026293, acc: 0.992188]  [G loss: 4.744267, acc: 0.390625]\n",
            "4608: [D loss: 0.134893, acc: 0.976562]  [G loss: 3.738028, acc: 0.375000]\n",
            "4609: [D loss: 0.079099, acc: 0.976562]  [G loss: 3.959567, acc: 0.515625]\n",
            "4610: [D loss: 0.042713, acc: 0.984375]  [G loss: 3.117989, acc: 0.531250]\n",
            "4611: [D loss: 0.121239, acc: 0.976562]  [G loss: 2.735712, acc: 0.593750]\n",
            "4612: [D loss: 0.059404, acc: 0.984375]  [G loss: 3.102425, acc: 0.500000]\n",
            "4613: [D loss: 0.074855, acc: 0.976562]  [G loss: 4.300705, acc: 0.531250]\n",
            "4614: [D loss: 0.095381, acc: 0.960938]  [G loss: 2.943382, acc: 0.625000]\n",
            "4615: [D loss: 0.052422, acc: 0.976562]  [G loss: 2.278289, acc: 0.625000]\n",
            "4616: [D loss: 0.033800, acc: 0.984375]  [G loss: 2.035770, acc: 0.671875]\n",
            "4617: [D loss: 0.051241, acc: 0.984375]  [G loss: 2.630740, acc: 0.671875]\n",
            "4618: [D loss: 0.057966, acc: 0.984375]  [G loss: 2.731703, acc: 0.546875]\n",
            "4619: [D loss: 0.076262, acc: 0.976562]  [G loss: 3.673270, acc: 0.484375]\n",
            "4620: [D loss: 0.149045, acc: 0.945312]  [G loss: 2.423400, acc: 0.625000]\n",
            "4621: [D loss: 0.050676, acc: 0.984375]  [G loss: 1.294812, acc: 0.703125]\n",
            "4622: [D loss: 0.159928, acc: 0.945312]  [G loss: 0.661915, acc: 0.828125]\n",
            "4623: [D loss: 0.215455, acc: 0.937500]  [G loss: 2.489125, acc: 0.468750]\n",
            "4624: [D loss: 0.081545, acc: 0.968750]  [G loss: 6.359754, acc: 0.218750]\n",
            "4625: [D loss: 0.104760, acc: 0.976562]  [G loss: 7.053037, acc: 0.187500]\n",
            "4626: [D loss: 0.444354, acc: 0.906250]  [G loss: 3.623966, acc: 0.375000]\n",
            "4627: [D loss: 0.059049, acc: 0.976562]  [G loss: 1.339243, acc: 0.656250]\n",
            "4628: [D loss: 0.048099, acc: 0.976562]  [G loss: 0.516643, acc: 0.906250]\n",
            "4629: [D loss: 0.401527, acc: 0.867188]  [G loss: 1.802975, acc: 0.546875]\n",
            "4630: [D loss: 0.007553, acc: 1.000000]  [G loss: 3.863517, acc: 0.406250]\n",
            "4631: [D loss: 0.265406, acc: 0.945312]  [G loss: 4.406115, acc: 0.359375]\n",
            "4632: [D loss: 0.206526, acc: 0.945312]  [G loss: 3.593408, acc: 0.515625]\n",
            "4633: [D loss: 0.262254, acc: 0.921875]  [G loss: 0.899284, acc: 0.796875]\n",
            "4634: [D loss: 0.220893, acc: 0.929688]  [G loss: 0.571089, acc: 0.875000]\n",
            "4635: [D loss: 0.467422, acc: 0.859375]  [G loss: 1.404843, acc: 0.671875]\n",
            "4636: [D loss: 0.046659, acc: 0.968750]  [G loss: 2.960121, acc: 0.390625]\n",
            "4637: [D loss: 0.214694, acc: 0.945312]  [G loss: 3.591597, acc: 0.375000]\n",
            "4638: [D loss: 0.128140, acc: 0.960938]  [G loss: 3.094807, acc: 0.390625]\n",
            "4639: [D loss: 0.141345, acc: 0.968750]  [G loss: 1.843541, acc: 0.578125]\n",
            "4640: [D loss: 0.017369, acc: 0.992188]  [G loss: 0.818850, acc: 0.640625]\n",
            "4641: [D loss: 0.084285, acc: 0.968750]  [G loss: 0.976967, acc: 0.656250]\n",
            "4642: [D loss: 0.267027, acc: 0.921875]  [G loss: 1.510381, acc: 0.546875]\n",
            "4643: [D loss: 0.086184, acc: 0.976562]  [G loss: 2.908171, acc: 0.328125]\n",
            "4644: [D loss: 0.097045, acc: 0.960938]  [G loss: 3.153739, acc: 0.218750]\n",
            "4645: [D loss: 0.157192, acc: 0.945312]  [G loss: 3.262858, acc: 0.265625]\n",
            "4646: [D loss: 0.434588, acc: 0.906250]  [G loss: 1.502196, acc: 0.531250]\n",
            "4647: [D loss: 0.140684, acc: 0.945312]  [G loss: 0.674776, acc: 0.734375]\n",
            "4648: [D loss: 0.101141, acc: 0.960938]  [G loss: 0.447171, acc: 0.859375]\n",
            "4649: [D loss: 0.209010, acc: 0.921875]  [G loss: 0.581690, acc: 0.765625]\n",
            "4650: [D loss: 0.102158, acc: 0.960938]  [G loss: 1.473082, acc: 0.609375]\n",
            "4651: [D loss: 0.061974, acc: 0.976562]  [G loss: 1.857726, acc: 0.578125]\n",
            "4652: [D loss: 0.071188, acc: 0.976562]  [G loss: 1.909637, acc: 0.578125]\n",
            "4653: [D loss: 0.112959, acc: 0.953125]  [G loss: 1.966345, acc: 0.484375]\n",
            "4654: [D loss: 0.163505, acc: 0.945312]  [G loss: 1.891807, acc: 0.562500]\n",
            "4655: [D loss: 0.101920, acc: 0.953125]  [G loss: 1.360683, acc: 0.609375]\n",
            "4656: [D loss: 0.130402, acc: 0.960938]  [G loss: 0.861648, acc: 0.625000]\n",
            "4657: [D loss: 0.071511, acc: 0.976562]  [G loss: 0.817958, acc: 0.703125]\n",
            "4658: [D loss: 0.037129, acc: 0.992188]  [G loss: 0.703146, acc: 0.687500]\n",
            "4659: [D loss: 0.187995, acc: 0.953125]  [G loss: 0.961839, acc: 0.640625]\n",
            "4660: [D loss: 0.069236, acc: 0.984375]  [G loss: 1.255867, acc: 0.531250]\n",
            "4661: [D loss: 0.020445, acc: 1.000000]  [G loss: 1.677213, acc: 0.484375]\n",
            "4662: [D loss: 0.057943, acc: 0.968750]  [G loss: 2.533480, acc: 0.343750]\n",
            "4663: [D loss: 0.052363, acc: 0.968750]  [G loss: 2.102567, acc: 0.421875]\n",
            "4664: [D loss: 0.050892, acc: 0.968750]  [G loss: 2.224135, acc: 0.437500]\n",
            "4665: [D loss: 0.067205, acc: 0.968750]  [G loss: 1.740832, acc: 0.468750]\n",
            "4666: [D loss: 0.097181, acc: 0.976562]  [G loss: 1.560203, acc: 0.484375]\n",
            "4667: [D loss: 0.064602, acc: 0.976562]  [G loss: 1.309399, acc: 0.531250]\n",
            "4668: [D loss: 0.060112, acc: 0.992188]  [G loss: 1.438223, acc: 0.515625]\n",
            "4669: [D loss: 0.056422, acc: 0.992188]  [G loss: 1.307936, acc: 0.500000]\n",
            "4670: [D loss: 0.023511, acc: 1.000000]  [G loss: 1.432654, acc: 0.484375]\n",
            "4671: [D loss: 0.041228, acc: 0.984375]  [G loss: 1.222408, acc: 0.515625]\n",
            "4672: [D loss: 0.044578, acc: 0.976562]  [G loss: 2.065988, acc: 0.375000]\n",
            "4673: [D loss: 0.018759, acc: 0.992188]  [G loss: 2.524140, acc: 0.343750]\n",
            "4674: [D loss: 0.075402, acc: 0.984375]  [G loss: 2.843919, acc: 0.156250]\n",
            "4675: [D loss: 0.029363, acc: 0.984375]  [G loss: 2.969912, acc: 0.187500]\n",
            "4676: [D loss: 0.019630, acc: 0.984375]  [G loss: 3.064817, acc: 0.125000]\n",
            "4677: [D loss: 0.011708, acc: 1.000000]  [G loss: 3.260731, acc: 0.093750]\n",
            "4678: [D loss: 0.008919, acc: 1.000000]  [G loss: 3.803546, acc: 0.062500]\n",
            "4679: [D loss: 0.035495, acc: 0.984375]  [G loss: 4.670975, acc: 0.000000]\n",
            "4680: [D loss: 0.011341, acc: 1.000000]  [G loss: 5.348089, acc: 0.000000]\n",
            "4681: [D loss: 0.102714, acc: 0.984375]  [G loss: 6.105384, acc: 0.000000]\n",
            "4682: [D loss: 0.013824, acc: 1.000000]  [G loss: 7.134663, acc: 0.000000]\n",
            "4683: [D loss: 0.059772, acc: 0.984375]  [G loss: 8.223597, acc: 0.000000]\n",
            "4684: [D loss: 0.012359, acc: 0.992188]  [G loss: 8.401915, acc: 0.000000]\n",
            "4685: [D loss: 0.020128, acc: 0.984375]  [G loss: 8.820023, acc: 0.000000]\n",
            "4686: [D loss: 0.028208, acc: 0.992188]  [G loss: 9.199305, acc: 0.000000]\n",
            "4687: [D loss: 0.003162, acc: 1.000000]  [G loss: 9.355955, acc: 0.000000]\n",
            "4688: [D loss: 0.002521, acc: 1.000000]  [G loss: 9.590168, acc: 0.000000]\n",
            "4689: [D loss: 0.005895, acc: 1.000000]  [G loss: 9.317280, acc: 0.000000]\n",
            "4690: [D loss: 0.009796, acc: 1.000000]  [G loss: 9.370614, acc: 0.000000]\n",
            "4691: [D loss: 0.021597, acc: 0.992188]  [G loss: 9.551332, acc: 0.000000]\n",
            "4692: [D loss: 0.021659, acc: 0.984375]  [G loss: 8.720760, acc: 0.000000]\n",
            "4693: [D loss: 0.010574, acc: 0.992188]  [G loss: 8.873652, acc: 0.000000]\n",
            "4694: [D loss: 0.039992, acc: 0.976562]  [G loss: 9.383109, acc: 0.000000]\n",
            "4695: [D loss: 0.014154, acc: 0.992188]  [G loss: 9.750355, acc: 0.000000]\n",
            "4696: [D loss: 0.022160, acc: 0.984375]  [G loss: 9.443653, acc: 0.000000]\n",
            "4697: [D loss: 0.037117, acc: 0.984375]  [G loss: 9.509130, acc: 0.000000]\n",
            "4698: [D loss: 0.090350, acc: 0.968750]  [G loss: 8.406200, acc: 0.000000]\n",
            "4699: [D loss: 0.101321, acc: 0.976562]  [G loss: 7.943161, acc: 0.015625]\n",
            "4700: [D loss: 0.048595, acc: 0.984375]  [G loss: 8.254522, acc: 0.000000]\n",
            "4701: [D loss: 0.094149, acc: 0.960938]  [G loss: 8.397664, acc: 0.000000]\n",
            "4702: [D loss: 0.131737, acc: 0.960938]  [G loss: 6.826642, acc: 0.031250]\n",
            "4703: [D loss: 0.111410, acc: 0.953125]  [G loss: 5.675390, acc: 0.093750]\n",
            "4704: [D loss: 0.202624, acc: 0.953125]  [G loss: 5.468084, acc: 0.187500]\n",
            "4705: [D loss: 0.156228, acc: 0.929688]  [G loss: 6.782465, acc: 0.171875]\n",
            "4706: [D loss: 0.045618, acc: 0.984375]  [G loss: 8.037192, acc: 0.109375]\n",
            "4707: [D loss: 0.078127, acc: 0.976562]  [G loss: 8.153194, acc: 0.125000]\n",
            "4708: [D loss: 0.111236, acc: 0.960938]  [G loss: 7.245800, acc: 0.187500]\n",
            "4709: [D loss: 0.134686, acc: 0.960938]  [G loss: 7.345247, acc: 0.218750]\n",
            "4710: [D loss: 0.198159, acc: 0.929688]  [G loss: 5.905512, acc: 0.328125]\n",
            "4711: [D loss: 0.143659, acc: 0.945312]  [G loss: 4.334434, acc: 0.390625]\n",
            "4712: [D loss: 0.115983, acc: 0.968750]  [G loss: 4.495507, acc: 0.390625]\n",
            "4713: [D loss: 0.081900, acc: 0.953125]  [G loss: 6.012137, acc: 0.265625]\n",
            "4714: [D loss: 0.159843, acc: 0.960938]  [G loss: 6.767256, acc: 0.218750]\n",
            "4715: [D loss: 0.204472, acc: 0.921875]  [G loss: 6.010205, acc: 0.171875]\n",
            "4716: [D loss: 0.184410, acc: 0.921875]  [G loss: 6.151013, acc: 0.156250]\n",
            "4717: [D loss: 0.099785, acc: 0.960938]  [G loss: 5.559014, acc: 0.218750]\n",
            "4718: [D loss: 0.098233, acc: 0.953125]  [G loss: 5.517784, acc: 0.250000]\n",
            "4719: [D loss: 0.225614, acc: 0.921875]  [G loss: 5.225714, acc: 0.281250]\n",
            "4720: [D loss: 0.201160, acc: 0.937500]  [G loss: 3.700924, acc: 0.359375]\n",
            "4721: [D loss: 0.255906, acc: 0.929688]  [G loss: 3.242032, acc: 0.328125]\n",
            "4722: [D loss: 0.235733, acc: 0.914062]  [G loss: 4.162694, acc: 0.296875]\n",
            "4723: [D loss: 0.240227, acc: 0.906250]  [G loss: 4.775041, acc: 0.265625]\n",
            "4724: [D loss: 0.360461, acc: 0.921875]  [G loss: 4.599635, acc: 0.187500]\n",
            "4725: [D loss: 0.287385, acc: 0.906250]  [G loss: 4.215036, acc: 0.296875]\n",
            "4726: [D loss: 0.209728, acc: 0.937500]  [G loss: 2.453648, acc: 0.421875]\n",
            "4727: [D loss: 0.185755, acc: 0.929688]  [G loss: 2.407563, acc: 0.437500]\n",
            "4728: [D loss: 0.127893, acc: 0.929688]  [G loss: 2.651488, acc: 0.500000]\n",
            "4729: [D loss: 0.143168, acc: 0.960938]  [G loss: 3.523226, acc: 0.265625]\n",
            "4730: [D loss: 0.103005, acc: 0.976562]  [G loss: 4.180218, acc: 0.234375]\n",
            "4731: [D loss: 0.105929, acc: 0.960938]  [G loss: 4.010943, acc: 0.328125]\n",
            "4732: [D loss: 0.279459, acc: 0.945312]  [G loss: 3.837158, acc: 0.328125]\n",
            "4733: [D loss: 0.326437, acc: 0.890625]  [G loss: 3.152659, acc: 0.343750]\n",
            "4734: [D loss: 0.149928, acc: 0.945312]  [G loss: 4.058022, acc: 0.125000]\n",
            "4735: [D loss: 0.156307, acc: 0.960938]  [G loss: 3.376563, acc: 0.281250]\n",
            "4736: [D loss: 0.224075, acc: 0.953125]  [G loss: 3.764215, acc: 0.203125]\n",
            "4737: [D loss: 0.132799, acc: 0.960938]  [G loss: 3.613724, acc: 0.234375]\n",
            "4738: [D loss: 0.192387, acc: 0.953125]  [G loss: 4.439584, acc: 0.250000]\n",
            "4739: [D loss: 0.239128, acc: 0.945312]  [G loss: 2.889257, acc: 0.312500]\n",
            "4740: [D loss: 0.211597, acc: 0.945312]  [G loss: 1.421859, acc: 0.468750]\n",
            "4741: [D loss: 0.166566, acc: 0.921875]  [G loss: 1.229406, acc: 0.609375]\n",
            "4742: [D loss: 0.224739, acc: 0.906250]  [G loss: 1.684881, acc: 0.500000]\n",
            "4743: [D loss: 0.105975, acc: 0.953125]  [G loss: 2.788418, acc: 0.343750]\n",
            "4744: [D loss: 0.088208, acc: 0.976562]  [G loss: 3.831147, acc: 0.218750]\n",
            "4745: [D loss: 0.203879, acc: 0.953125]  [G loss: 4.525180, acc: 0.218750]\n",
            "4746: [D loss: 0.090378, acc: 0.976562]  [G loss: 4.306553, acc: 0.171875]\n",
            "4747: [D loss: 0.217077, acc: 0.937500]  [G loss: 2.788363, acc: 0.390625]\n",
            "4748: [D loss: 0.168208, acc: 0.945312]  [G loss: 2.116781, acc: 0.593750]\n",
            "4749: [D loss: 0.131807, acc: 0.937500]  [G loss: 2.165308, acc: 0.531250]\n",
            "4750: [D loss: 0.065959, acc: 0.968750]  [G loss: 1.428861, acc: 0.625000]\n",
            "4751: [D loss: 0.144967, acc: 0.960938]  [G loss: 1.898125, acc: 0.500000]\n",
            "4752: [D loss: 0.056311, acc: 0.968750]  [G loss: 2.876913, acc: 0.484375]\n",
            "4753: [D loss: 0.029945, acc: 0.984375]  [G loss: 3.289078, acc: 0.406250]\n",
            "4754: [D loss: 0.099284, acc: 0.976562]  [G loss: 3.719303, acc: 0.328125]\n",
            "4755: [D loss: 0.159896, acc: 0.953125]  [G loss: 2.913570, acc: 0.453125]\n",
            "4756: [D loss: 0.044905, acc: 0.992188]  [G loss: 2.231535, acc: 0.578125]\n",
            "4757: [D loss: 0.053195, acc: 0.976562]  [G loss: 1.813512, acc: 0.687500]\n",
            "4758: [D loss: 0.076936, acc: 0.976562]  [G loss: 1.890352, acc: 0.671875]\n",
            "4759: [D loss: 0.058052, acc: 0.984375]  [G loss: 1.202399, acc: 0.609375]\n",
            "4760: [D loss: 0.178516, acc: 0.945312]  [G loss: 1.611929, acc: 0.640625]\n",
            "4761: [D loss: 0.114135, acc: 0.968750]  [G loss: 1.915231, acc: 0.515625]\n",
            "4762: [D loss: 0.053388, acc: 0.976562]  [G loss: 2.294920, acc: 0.562500]\n",
            "4763: [D loss: 0.151470, acc: 0.953125]  [G loss: 2.284047, acc: 0.343750]\n",
            "4764: [D loss: 0.145340, acc: 0.945312]  [G loss: 1.827962, acc: 0.406250]\n",
            "4765: [D loss: 0.078290, acc: 0.953125]  [G loss: 1.086138, acc: 0.546875]\n",
            "4766: [D loss: 0.108027, acc: 0.968750]  [G loss: 0.698313, acc: 0.703125]\n",
            "4767: [D loss: 0.087682, acc: 0.968750]  [G loss: 0.672420, acc: 0.781250]\n",
            "4768: [D loss: 0.129970, acc: 0.937500]  [G loss: 1.073488, acc: 0.656250]\n",
            "4769: [D loss: 0.068096, acc: 0.976562]  [G loss: 2.311034, acc: 0.421875]\n",
            "4770: [D loss: 0.034001, acc: 0.984375]  [G loss: 2.813951, acc: 0.343750]\n",
            "4771: [D loss: 0.176854, acc: 0.929688]  [G loss: 2.818335, acc: 0.375000]\n",
            "4772: [D loss: 0.115712, acc: 0.968750]  [G loss: 2.522853, acc: 0.312500]\n",
            "4773: [D loss: 0.061234, acc: 0.968750]  [G loss: 1.758768, acc: 0.500000]\n",
            "4774: [D loss: 0.049753, acc: 0.984375]  [G loss: 1.635819, acc: 0.453125]\n",
            "4775: [D loss: 0.052012, acc: 0.968750]  [G loss: 1.325373, acc: 0.484375]\n",
            "4776: [D loss: 0.095877, acc: 0.984375]  [G loss: 1.624736, acc: 0.515625]\n",
            "4777: [D loss: 0.050353, acc: 0.976562]  [G loss: 1.282971, acc: 0.515625]\n",
            "4778: [D loss: 0.145494, acc: 0.953125]  [G loss: 2.024075, acc: 0.468750]\n",
            "4779: [D loss: 0.110152, acc: 0.953125]  [G loss: 2.508626, acc: 0.328125]\n",
            "4780: [D loss: 0.152476, acc: 0.945312]  [G loss: 2.054599, acc: 0.453125]\n",
            "4781: [D loss: 0.084877, acc: 0.984375]  [G loss: 2.067777, acc: 0.406250]\n",
            "4782: [D loss: 0.088891, acc: 0.960938]  [G loss: 1.862821, acc: 0.484375]\n",
            "4783: [D loss: 0.253829, acc: 0.890625]  [G loss: 1.515274, acc: 0.437500]\n",
            "4784: [D loss: 0.273543, acc: 0.906250]  [G loss: 2.222146, acc: 0.375000]\n",
            "4785: [D loss: 0.129118, acc: 0.945312]  [G loss: 3.044105, acc: 0.250000]\n",
            "4786: [D loss: 0.139985, acc: 0.953125]  [G loss: 3.637161, acc: 0.171875]\n",
            "4787: [D loss: 0.163593, acc: 0.906250]  [G loss: 3.278913, acc: 0.250000]\n",
            "4788: [D loss: 0.251244, acc: 0.890625]  [G loss: 3.248756, acc: 0.218750]\n",
            "4789: [D loss: 0.072894, acc: 0.976562]  [G loss: 3.609375, acc: 0.125000]\n",
            "4790: [D loss: 0.101201, acc: 0.968750]  [G loss: 4.226914, acc: 0.078125]\n",
            "4791: [D loss: 0.105994, acc: 0.960938]  [G loss: 4.324166, acc: 0.093750]\n",
            "4792: [D loss: 0.099377, acc: 0.984375]  [G loss: 4.895793, acc: 0.078125]\n",
            "4793: [D loss: 0.090296, acc: 0.960938]  [G loss: 5.859507, acc: 0.062500]\n",
            "4794: [D loss: 0.023515, acc: 0.992188]  [G loss: 6.244752, acc: 0.015625]\n",
            "4795: [D loss: 0.013547, acc: 1.000000]  [G loss: 6.809351, acc: 0.031250]\n",
            "4796: [D loss: 0.009466, acc: 1.000000]  [G loss: 7.533916, acc: 0.000000]\n",
            "4797: [D loss: 0.101959, acc: 0.976562]  [G loss: 6.740074, acc: 0.015625]\n",
            "4798: [D loss: 0.009994, acc: 1.000000]  [G loss: 6.652145, acc: 0.031250]\n",
            "4799: [D loss: 0.049460, acc: 0.984375]  [G loss: 7.055916, acc: 0.078125]\n",
            "4800: [D loss: 0.056934, acc: 0.984375]  [G loss: 6.518845, acc: 0.156250]\n",
            "4801: [D loss: 0.015683, acc: 1.000000]  [G loss: 6.099335, acc: 0.156250]\n",
            "4802: [D loss: 0.019677, acc: 1.000000]  [G loss: 5.457871, acc: 0.156250]\n",
            "4803: [D loss: 0.054677, acc: 0.976562]  [G loss: 6.250119, acc: 0.140625]\n",
            "4804: [D loss: 0.017322, acc: 0.992188]  [G loss: 6.303902, acc: 0.218750]\n",
            "4805: [D loss: 0.078192, acc: 0.968750]  [G loss: 5.763082, acc: 0.187500]\n",
            "4806: [D loss: 0.035849, acc: 0.992188]  [G loss: 6.159847, acc: 0.109375]\n",
            "4807: [D loss: 0.003658, acc: 1.000000]  [G loss: 6.640545, acc: 0.203125]\n",
            "4808: [D loss: 0.014331, acc: 1.000000]  [G loss: 6.622120, acc: 0.203125]\n",
            "4809: [D loss: 0.032626, acc: 0.984375]  [G loss: 6.024713, acc: 0.187500]\n",
            "4810: [D loss: 0.027018, acc: 0.984375]  [G loss: 4.897207, acc: 0.281250]\n",
            "4811: [D loss: 0.012400, acc: 0.992188]  [G loss: 3.685298, acc: 0.484375]\n",
            "4812: [D loss: 0.197444, acc: 0.945312]  [G loss: 4.644625, acc: 0.343750]\n",
            "4813: [D loss: 0.063168, acc: 0.968750]  [G loss: 4.214012, acc: 0.250000]\n",
            "4814: [D loss: 0.100447, acc: 0.976562]  [G loss: 5.622892, acc: 0.203125]\n",
            "4815: [D loss: 0.189484, acc: 0.960938]  [G loss: 4.836192, acc: 0.296875]\n",
            "4816: [D loss: 0.116855, acc: 0.960938]  [G loss: 3.318091, acc: 0.312500]\n",
            "4817: [D loss: 0.223085, acc: 0.960938]  [G loss: 2.605916, acc: 0.437500]\n",
            "4818: [D loss: 0.193010, acc: 0.937500]  [G loss: 2.383803, acc: 0.421875]\n",
            "4819: [D loss: 0.227975, acc: 0.937500]  [G loss: 1.947855, acc: 0.484375]\n",
            "4820: [D loss: 0.137622, acc: 0.953125]  [G loss: 1.980619, acc: 0.562500]\n",
            "4821: [D loss: 0.111123, acc: 0.960938]  [G loss: 1.420208, acc: 0.687500]\n",
            "4822: [D loss: 0.312994, acc: 0.929688]  [G loss: 1.374924, acc: 0.656250]\n",
            "4823: [D loss: 0.412396, acc: 0.898438]  [G loss: 1.083176, acc: 0.687500]\n",
            "4824: [D loss: 0.130217, acc: 0.953125]  [G loss: 1.113041, acc: 0.703125]\n",
            "4825: [D loss: 0.152168, acc: 0.937500]  [G loss: 1.884570, acc: 0.531250]\n",
            "4826: [D loss: 0.086327, acc: 0.960938]  [G loss: 1.347242, acc: 0.625000]\n",
            "4827: [D loss: 0.214237, acc: 0.921875]  [G loss: 0.894053, acc: 0.734375]\n",
            "4828: [D loss: 0.118850, acc: 0.953125]  [G loss: 0.754434, acc: 0.703125]\n",
            "4829: [D loss: 0.142239, acc: 0.945312]  [G loss: 1.957759, acc: 0.500000]\n",
            "4830: [D loss: 0.095086, acc: 0.960938]  [G loss: 2.336647, acc: 0.375000]\n",
            "4831: [D loss: 0.094111, acc: 0.960938]  [G loss: 1.816433, acc: 0.484375]\n",
            "4832: [D loss: 0.044358, acc: 0.992188]  [G loss: 0.956119, acc: 0.640625]\n",
            "4833: [D loss: 0.164831, acc: 0.953125]  [G loss: 1.070088, acc: 0.640625]\n",
            "4834: [D loss: 0.139751, acc: 0.953125]  [G loss: 1.210149, acc: 0.609375]\n",
            "4835: [D loss: 0.097440, acc: 0.960938]  [G loss: 1.299125, acc: 0.578125]\n",
            "4836: [D loss: 0.036550, acc: 0.976562]  [G loss: 1.928680, acc: 0.468750]\n",
            "4837: [D loss: 0.029464, acc: 0.992188]  [G loss: 1.754225, acc: 0.484375]\n",
            "4838: [D loss: 0.035620, acc: 0.976562]  [G loss: 2.321587, acc: 0.421875]\n",
            "4839: [D loss: 0.032452, acc: 0.976562]  [G loss: 1.740773, acc: 0.562500]\n",
            "4840: [D loss: 0.044385, acc: 0.976562]  [G loss: 1.706645, acc: 0.578125]\n",
            "4841: [D loss: 0.014514, acc: 0.992188]  [G loss: 2.206378, acc: 0.453125]\n",
            "4842: [D loss: 0.029798, acc: 0.984375]  [G loss: 2.320356, acc: 0.468750]\n",
            "4843: [D loss: 0.049874, acc: 0.984375]  [G loss: 2.439700, acc: 0.468750]\n",
            "4844: [D loss: 0.031685, acc: 0.976562]  [G loss: 1.728112, acc: 0.562500]\n",
            "4845: [D loss: 0.019992, acc: 0.992188]  [G loss: 1.130466, acc: 0.656250]\n",
            "4846: [D loss: 0.052461, acc: 0.984375]  [G loss: 1.047035, acc: 0.656250]\n",
            "4847: [D loss: 0.162390, acc: 0.945312]  [G loss: 0.652046, acc: 0.703125]\n",
            "4848: [D loss: 0.123379, acc: 0.953125]  [G loss: 0.850710, acc: 0.687500]\n",
            "4849: [D loss: 0.024393, acc: 0.992188]  [G loss: 0.796714, acc: 0.703125]\n",
            "4850: [D loss: 0.121231, acc: 0.953125]  [G loss: 1.925593, acc: 0.453125]\n",
            "4851: [D loss: 0.026228, acc: 0.984375]  [G loss: 1.464835, acc: 0.500000]\n",
            "4852: [D loss: 0.017386, acc: 0.992188]  [G loss: 1.738398, acc: 0.500000]\n",
            "4853: [D loss: 0.020186, acc: 0.992188]  [G loss: 2.154188, acc: 0.343750]\n",
            "4854: [D loss: 0.140234, acc: 0.992188]  [G loss: 1.317294, acc: 0.562500]\n",
            "4855: [D loss: 0.095686, acc: 0.968750]  [G loss: 1.197590, acc: 0.640625]\n",
            "4856: [D loss: 0.012814, acc: 1.000000]  [G loss: 1.188628, acc: 0.593750]\n",
            "4857: [D loss: 0.057983, acc: 0.968750]  [G loss: 0.324542, acc: 0.906250]\n",
            "4858: [D loss: 0.069906, acc: 0.976562]  [G loss: 0.497781, acc: 0.843750]\n",
            "4859: [D loss: 0.046475, acc: 0.984375]  [G loss: 0.633045, acc: 0.734375]\n",
            "4860: [D loss: 0.075618, acc: 0.984375]  [G loss: 0.970889, acc: 0.718750]\n",
            "4861: [D loss: 0.116820, acc: 0.960938]  [G loss: 0.821347, acc: 0.687500]\n",
            "4862: [D loss: 0.086905, acc: 0.992188]  [G loss: 1.129808, acc: 0.656250]\n",
            "4863: [D loss: 0.126790, acc: 0.960938]  [G loss: 1.362689, acc: 0.562500]\n",
            "4864: [D loss: 0.176786, acc: 0.953125]  [G loss: 2.496979, acc: 0.390625]\n",
            "4865: [D loss: 0.062791, acc: 0.984375]  [G loss: 3.258967, acc: 0.296875]\n",
            "4866: [D loss: 0.049394, acc: 0.976562]  [G loss: 3.356962, acc: 0.312500]\n",
            "4867: [D loss: 0.096326, acc: 0.968750]  [G loss: 3.045694, acc: 0.390625]\n",
            "4868: [D loss: 0.050818, acc: 0.992188]  [G loss: 1.624877, acc: 0.531250]\n",
            "4869: [D loss: 0.046018, acc: 0.976562]  [G loss: 1.132194, acc: 0.593750]\n",
            "4870: [D loss: 0.065057, acc: 0.968750]  [G loss: 1.147836, acc: 0.562500]\n",
            "4871: [D loss: 0.159760, acc: 0.960938]  [G loss: 1.554377, acc: 0.531250]\n",
            "4872: [D loss: 0.060733, acc: 0.968750]  [G loss: 2.781170, acc: 0.218750]\n",
            "4873: [D loss: 0.076017, acc: 0.976562]  [G loss: 3.556343, acc: 0.187500]\n",
            "4874: [D loss: 0.130963, acc: 0.960938]  [G loss: 2.892816, acc: 0.359375]\n",
            "4875: [D loss: 0.097087, acc: 0.984375]  [G loss: 2.171461, acc: 0.484375]\n",
            "4876: [D loss: 0.042641, acc: 0.984375]  [G loss: 1.451774, acc: 0.546875]\n",
            "4877: [D loss: 0.108746, acc: 0.960938]  [G loss: 1.138603, acc: 0.578125]\n",
            "4878: [D loss: 0.061826, acc: 0.976562]  [G loss: 2.236426, acc: 0.343750]\n",
            "4879: [D loss: 0.039046, acc: 0.984375]  [G loss: 3.976233, acc: 0.203125]\n",
            "4880: [D loss: 0.111495, acc: 0.976562]  [G loss: 5.695045, acc: 0.109375]\n",
            "4881: [D loss: 0.082529, acc: 0.953125]  [G loss: 5.293187, acc: 0.140625]\n",
            "4882: [D loss: 0.007038, acc: 1.000000]  [G loss: 4.041302, acc: 0.203125]\n",
            "4883: [D loss: 0.018062, acc: 0.992188]  [G loss: 3.706269, acc: 0.281250]\n",
            "4884: [D loss: 0.201706, acc: 0.960938]  [G loss: 2.933629, acc: 0.296875]\n",
            "4885: [D loss: 0.119843, acc: 0.960938]  [G loss: 2.477094, acc: 0.375000]\n",
            "4886: [D loss: 0.141212, acc: 0.921875]  [G loss: 4.195123, acc: 0.140625]\n",
            "4887: [D loss: 0.096492, acc: 0.984375]  [G loss: 5.514817, acc: 0.046875]\n",
            "4888: [D loss: 0.069543, acc: 0.968750]  [G loss: 5.207950, acc: 0.078125]\n",
            "4889: [D loss: 0.156964, acc: 0.953125]  [G loss: 4.357082, acc: 0.140625]\n",
            "4890: [D loss: 0.139134, acc: 0.968750]  [G loss: 2.196915, acc: 0.359375]\n",
            "4891: [D loss: 0.087742, acc: 0.968750]  [G loss: 1.722876, acc: 0.359375]\n",
            "4892: [D loss: 0.113306, acc: 0.945312]  [G loss: 2.188864, acc: 0.328125]\n",
            "4893: [D loss: 0.103067, acc: 0.984375]  [G loss: 3.455334, acc: 0.218750]\n",
            "4894: [D loss: 0.042201, acc: 0.976562]  [G loss: 3.971242, acc: 0.156250]\n",
            "4895: [D loss: 0.044014, acc: 0.984375]  [G loss: 3.904934, acc: 0.250000]\n",
            "4896: [D loss: 0.024782, acc: 1.000000]  [G loss: 4.033056, acc: 0.156250]\n",
            "4897: [D loss: 0.067846, acc: 0.976562]  [G loss: 4.828587, acc: 0.140625]\n",
            "4898: [D loss: 0.022820, acc: 0.984375]  [G loss: 4.925026, acc: 0.156250]\n",
            "4899: [D loss: 0.088337, acc: 0.984375]  [G loss: 4.857523, acc: 0.156250]\n",
            "4900: [D loss: 0.106420, acc: 0.976562]  [G loss: 3.693590, acc: 0.203125]\n",
            "4901: [D loss: 0.148927, acc: 0.945312]  [G loss: 4.046863, acc: 0.171875]\n",
            "4902: [D loss: 0.171223, acc: 0.960938]  [G loss: 3.700380, acc: 0.203125]\n",
            "4903: [D loss: 0.010318, acc: 1.000000]  [G loss: 3.154422, acc: 0.281250]\n",
            "4904: [D loss: 0.064327, acc: 0.960938]  [G loss: 4.754569, acc: 0.171875]\n",
            "4905: [D loss: 0.080455, acc: 0.960938]  [G loss: 7.240088, acc: 0.015625]\n",
            "4906: [D loss: 0.221539, acc: 0.945312]  [G loss: 7.758205, acc: 0.000000]\n",
            "4907: [D loss: 0.135141, acc: 0.960938]  [G loss: 6.848091, acc: 0.015625]\n",
            "4908: [D loss: 0.273838, acc: 0.929688]  [G loss: 4.541870, acc: 0.140625]\n",
            "4909: [D loss: 0.184203, acc: 0.953125]  [G loss: 2.775629, acc: 0.250000]\n",
            "4910: [D loss: 0.311265, acc: 0.906250]  [G loss: 4.342310, acc: 0.125000]\n",
            "4911: [D loss: 0.078243, acc: 0.968750]  [G loss: 7.835222, acc: 0.000000]\n",
            "4912: [D loss: 0.097181, acc: 0.968750]  [G loss: 8.955357, acc: 0.000000]\n",
            "4913: [D loss: 0.228177, acc: 0.953125]  [G loss: 8.499796, acc: 0.000000]\n",
            "4914: [D loss: 0.452010, acc: 0.882812]  [G loss: 5.435430, acc: 0.015625]\n",
            "4915: [D loss: 0.069569, acc: 0.968750]  [G loss: 2.483687, acc: 0.250000]\n",
            "4916: [D loss: 0.176248, acc: 0.937500]  [G loss: 1.765994, acc: 0.312500]\n",
            "4917: [D loss: 0.138622, acc: 0.929688]  [G loss: 2.950415, acc: 0.125000]\n",
            "4918: [D loss: 0.145829, acc: 0.945312]  [G loss: 4.858944, acc: 0.015625]\n",
            "4919: [D loss: 0.110368, acc: 0.976562]  [G loss: 6.508566, acc: 0.000000]\n",
            "4920: [D loss: 0.136104, acc: 0.960938]  [G loss: 6.792912, acc: 0.000000]\n",
            "4921: [D loss: 0.102236, acc: 0.968750]  [G loss: 5.081906, acc: 0.015625]\n",
            "4922: [D loss: 0.140659, acc: 0.984375]  [G loss: 3.805766, acc: 0.156250]\n",
            "4923: [D loss: 0.067566, acc: 0.968750]  [G loss: 2.483837, acc: 0.421875]\n",
            "4924: [D loss: 0.080641, acc: 0.968750]  [G loss: 2.143617, acc: 0.437500]\n",
            "4925: [D loss: 0.033502, acc: 0.992188]  [G loss: 2.240790, acc: 0.390625]\n",
            "4926: [D loss: 0.035042, acc: 0.992188]  [G loss: 1.956508, acc: 0.453125]\n",
            "4927: [D loss: 0.131300, acc: 0.953125]  [G loss: 2.112355, acc: 0.421875]\n",
            "4928: [D loss: 0.056629, acc: 0.992188]  [G loss: 2.637499, acc: 0.375000]\n",
            "4929: [D loss: 0.059687, acc: 0.984375]  [G loss: 3.138105, acc: 0.187500]\n",
            "4930: [D loss: 0.099837, acc: 0.968750]  [G loss: 2.948419, acc: 0.171875]\n",
            "4931: [D loss: 0.067975, acc: 0.984375]  [G loss: 3.569665, acc: 0.203125]\n",
            "4932: [D loss: 0.061588, acc: 0.968750]  [G loss: 3.766605, acc: 0.140625]\n",
            "4933: [D loss: 0.092355, acc: 0.953125]  [G loss: 3.215895, acc: 0.218750]\n",
            "4934: [D loss: 0.068002, acc: 0.984375]  [G loss: 3.177253, acc: 0.187500]\n",
            "4935: [D loss: 0.140044, acc: 0.953125]  [G loss: 2.910142, acc: 0.265625]\n",
            "4936: [D loss: 0.049562, acc: 0.984375]  [G loss: 2.219886, acc: 0.437500]\n",
            "4937: [D loss: 0.047276, acc: 0.984375]  [G loss: 2.424638, acc: 0.421875]\n",
            "4938: [D loss: 0.136461, acc: 0.968750]  [G loss: 2.102167, acc: 0.453125]\n",
            "4939: [D loss: 0.065667, acc: 0.976562]  [G loss: 3.203338, acc: 0.359375]\n",
            "4940: [D loss: 0.087184, acc: 0.960938]  [G loss: 3.277691, acc: 0.312500]\n",
            "4941: [D loss: 0.071653, acc: 0.984375]  [G loss: 3.340664, acc: 0.390625]\n",
            "4942: [D loss: 0.029767, acc: 0.984375]  [G loss: 3.479911, acc: 0.312500]\n",
            "4943: [D loss: 0.044829, acc: 0.984375]  [G loss: 2.458322, acc: 0.453125]\n",
            "4944: [D loss: 0.031624, acc: 1.000000]  [G loss: 2.284688, acc: 0.437500]\n",
            "4945: [D loss: 0.100326, acc: 0.945312]  [G loss: 2.588225, acc: 0.437500]\n",
            "4946: [D loss: 0.047231, acc: 0.984375]  [G loss: 2.279721, acc: 0.515625]\n",
            "4947: [D loss: 0.032744, acc: 0.992188]  [G loss: 2.475114, acc: 0.515625]\n",
            "4948: [D loss: 0.025526, acc: 0.992188]  [G loss: 2.796154, acc: 0.437500]\n",
            "4949: [D loss: 0.012904, acc: 1.000000]  [G loss: 2.571504, acc: 0.453125]\n",
            "4950: [D loss: 0.060729, acc: 0.976562]  [G loss: 2.679326, acc: 0.468750]\n",
            "4951: [D loss: 0.023533, acc: 1.000000]  [G loss: 2.200482, acc: 0.593750]\n",
            "4952: [D loss: 0.038804, acc: 0.992188]  [G loss: 2.035936, acc: 0.468750]\n",
            "4953: [D loss: 0.057314, acc: 0.968750]  [G loss: 1.881627, acc: 0.515625]\n",
            "4954: [D loss: 0.034790, acc: 0.976562]  [G loss: 2.141055, acc: 0.421875]\n",
            "4955: [D loss: 0.064023, acc: 0.976562]  [G loss: 2.349183, acc: 0.437500]\n",
            "4956: [D loss: 0.108169, acc: 0.953125]  [G loss: 1.854046, acc: 0.515625]\n",
            "4957: [D loss: 0.061117, acc: 0.976562]  [G loss: 1.450497, acc: 0.578125]\n",
            "4958: [D loss: 0.102862, acc: 0.960938]  [G loss: 1.269884, acc: 0.703125]\n",
            "4959: [D loss: 0.087670, acc: 0.976562]  [G loss: 1.480061, acc: 0.625000]\n",
            "4960: [D loss: 0.106809, acc: 0.968750]  [G loss: 1.542119, acc: 0.687500]\n",
            "4961: [D loss: 0.114362, acc: 0.960938]  [G loss: 1.468634, acc: 0.687500]\n",
            "4962: [D loss: 0.069374, acc: 0.968750]  [G loss: 1.915302, acc: 0.531250]\n",
            "4963: [D loss: 0.054559, acc: 0.984375]  [G loss: 2.439356, acc: 0.562500]\n",
            "4964: [D loss: 0.072699, acc: 0.968750]  [G loss: 1.972071, acc: 0.515625]\n",
            "4965: [D loss: 0.044800, acc: 0.976562]  [G loss: 1.757138, acc: 0.609375]\n",
            "4966: [D loss: 0.130973, acc: 0.960938]  [G loss: 1.804036, acc: 0.609375]\n",
            "4967: [D loss: 0.109136, acc: 0.968750]  [G loss: 1.971674, acc: 0.593750]\n",
            "4968: [D loss: 0.124429, acc: 0.953125]  [G loss: 2.591492, acc: 0.484375]\n",
            "4969: [D loss: 0.051024, acc: 0.984375]  [G loss: 3.626484, acc: 0.312500]\n",
            "4970: [D loss: 0.044021, acc: 0.976562]  [G loss: 4.330556, acc: 0.218750]\n",
            "4971: [D loss: 0.083028, acc: 0.960938]  [G loss: 4.960308, acc: 0.156250]\n",
            "4972: [D loss: 0.137171, acc: 0.953125]  [G loss: 2.976855, acc: 0.390625]\n",
            "4973: [D loss: 0.050801, acc: 0.984375]  [G loss: 1.867927, acc: 0.593750]\n",
            "4974: [D loss: 0.103216, acc: 0.960938]  [G loss: 1.392436, acc: 0.656250]\n",
            "4975: [D loss: 0.105787, acc: 0.945312]  [G loss: 1.657392, acc: 0.593750]\n",
            "4976: [D loss: 0.097462, acc: 0.976562]  [G loss: 2.491475, acc: 0.406250]\n",
            "4977: [D loss: 0.013542, acc: 1.000000]  [G loss: 4.052514, acc: 0.187500]\n",
            "4978: [D loss: 0.117208, acc: 0.968750]  [G loss: 4.146870, acc: 0.250000]\n",
            "4979: [D loss: 0.079494, acc: 0.960938]  [G loss: 3.581150, acc: 0.312500]\n",
            "4980: [D loss: 0.248999, acc: 0.937500]  [G loss: 2.930854, acc: 0.390625]\n",
            "4981: [D loss: 0.050439, acc: 0.976562]  [G loss: 2.338415, acc: 0.531250]\n",
            "4982: [D loss: 0.183540, acc: 0.929688]  [G loss: 3.436766, acc: 0.343750]\n",
            "4983: [D loss: 0.067585, acc: 0.976562]  [G loss: 4.132267, acc: 0.296875]\n",
            "4984: [D loss: 0.035547, acc: 0.968750]  [G loss: 3.728762, acc: 0.390625]\n",
            "4985: [D loss: 0.084531, acc: 0.984375]  [G loss: 3.862051, acc: 0.390625]\n",
            "4986: [D loss: 0.190567, acc: 0.960938]  [G loss: 3.168159, acc: 0.375000]\n",
            "4987: [D loss: 0.135998, acc: 0.953125]  [G loss: 3.194413, acc: 0.468750]\n",
            "4988: [D loss: 0.125303, acc: 0.968750]  [G loss: 2.191619, acc: 0.671875]\n",
            "4989: [D loss: 0.197532, acc: 0.953125]  [G loss: 2.025425, acc: 0.562500]\n",
            "4990: [D loss: 0.030361, acc: 0.984375]  [G loss: 2.749819, acc: 0.453125]\n",
            "4991: [D loss: 0.033077, acc: 0.984375]  [G loss: 3.249669, acc: 0.359375]\n",
            "4992: [D loss: 0.050736, acc: 0.992188]  [G loss: 2.613268, acc: 0.421875]\n",
            "4993: [D loss: 0.147303, acc: 0.960938]  [G loss: 2.671086, acc: 0.312500]\n",
            "4994: [D loss: 0.240879, acc: 0.906250]  [G loss: 1.740344, acc: 0.546875]\n",
            "4995: [D loss: 0.039119, acc: 0.984375]  [G loss: 1.019868, acc: 0.718750]\n",
            "4996: [D loss: 0.169897, acc: 0.937500]  [G loss: 1.495347, acc: 0.671875]\n",
            "4997: [D loss: 0.087047, acc: 0.976562]  [G loss: 2.938208, acc: 0.375000]\n",
            "4998: [D loss: 0.034710, acc: 0.976562]  [G loss: 4.722331, acc: 0.265625]\n",
            "4999: [D loss: 0.209180, acc: 0.960938]  [G loss: 4.250274, acc: 0.343750]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9VVAy_TBHPGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90073
        },
        "outputId": "5427da78-6a9e-4b90-abdc-6e4b33fb0684"
      },
      "cell_type": "code",
      "source": [
        "train_steps = 5000 #or few if  you want\n",
        "hist = train_on_steps(X_train,DM,AM,G,train_steps,128)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\thena\\Redes Neuronales\\rna\\lib\\site-packages\\keras\\engine\\training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: [D loss: 0.696654, acc: 0.507812]  [G loss: 1.407661, acc: 0.000000]\n",
            "1: [D loss: 0.555570, acc: 0.503906]  [G loss: 3.027910, acc: 0.000000]\n",
            "2: [D loss: 0.343916, acc: 0.886719]  [G loss: 0.170987, acc: 1.000000]\n",
            "3: [D loss: 1.349513, acc: 0.500000]  [G loss: 5.069710, acc: 0.000000]\n",
            "4: [D loss: 0.191190, acc: 0.957031]  [G loss: 7.514878, acc: 0.000000]\n",
            "5: [D loss: 0.514667, acc: 0.675781]  [G loss: 5.292920, acc: 0.000000]\n",
            "6: [D loss: 0.302866, acc: 0.863281]  [G loss: 2.265081, acc: 0.000000]\n",
            "7: [D loss: 0.191296, acc: 0.988281]  [G loss: 1.047423, acc: 0.117188]\n",
            "8: [D loss: 0.360694, acc: 0.769531]  [G loss: 1.959508, acc: 0.000000]\n",
            "9: [D loss: 0.161786, acc: 0.980469]  [G loss: 3.847542, acc: 0.000000]\n",
            "10: [D loss: 0.066495, acc: 0.996094]  [G loss: 5.725089, acc: 0.000000]\n",
            "11: [D loss: 0.068730, acc: 0.988281]  [G loss: 6.770453, acc: 0.000000]\n",
            "12: [D loss: 0.062422, acc: 0.980469]  [G loss: 6.919716, acc: 0.000000]\n",
            "13: [D loss: 0.060280, acc: 0.988281]  [G loss: 5.788957, acc: 0.000000]\n",
            "14: [D loss: 0.019257, acc: 0.992188]  [G loss: 3.384180, acc: 0.007812]\n",
            "15: [D loss: 0.088100, acc: 0.960938]  [G loss: 3.905250, acc: 0.000000]\n",
            "16: [D loss: 0.090461, acc: 0.968750]  [G loss: 7.240628, acc: 0.000000]\n",
            "17: [D loss: 0.046946, acc: 0.996094]  [G loss: 9.433886, acc: 0.000000]\n",
            "18: [D loss: 0.164362, acc: 0.949219]  [G loss: 8.726620, acc: 0.000000]\n",
            "19: [D loss: 0.219936, acc: 0.949219]  [G loss: 5.622446, acc: 0.000000]\n",
            "20: [D loss: 0.144804, acc: 0.941406]  [G loss: 2.947838, acc: 0.015625]\n",
            "21: [D loss: 0.397094, acc: 0.839844]  [G loss: 4.521964, acc: 0.000000]\n",
            "22: [D loss: 0.221570, acc: 0.882812]  [G loss: 6.771463, acc: 0.000000]\n",
            "23: [D loss: 0.417319, acc: 0.878906]  [G loss: 5.456493, acc: 0.000000]\n",
            "24: [D loss: 0.329639, acc: 0.882812]  [G loss: 3.442559, acc: 0.000000]\n",
            "25: [D loss: 0.566561, acc: 0.761719]  [G loss: 2.460136, acc: 0.007812]\n",
            "26: [D loss: 0.787346, acc: 0.707031]  [G loss: 2.202823, acc: 0.000000]\n",
            "27: [D loss: 0.750161, acc: 0.640625]  [G loss: 2.141942, acc: 0.000000]\n",
            "28: [D loss: 0.589323, acc: 0.656250]  [G loss: 1.766870, acc: 0.000000]\n",
            "29: [D loss: 0.484538, acc: 0.714844]  [G loss: 1.378845, acc: 0.039062]\n",
            "30: [D loss: 0.404073, acc: 0.800781]  [G loss: 1.308051, acc: 0.031250]\n",
            "31: [D loss: 0.327511, acc: 0.898438]  [G loss: 1.416716, acc: 0.031250]\n",
            "32: [D loss: 0.241469, acc: 0.976562]  [G loss: 1.829924, acc: 0.000000]\n",
            "33: [D loss: 0.183175, acc: 0.972656]  [G loss: 2.632023, acc: 0.000000]\n",
            "34: [D loss: 0.123808, acc: 0.976562]  [G loss: 3.244783, acc: 0.000000]\n",
            "35: [D loss: 0.173018, acc: 0.933594]  [G loss: 2.916183, acc: 0.000000]\n",
            "36: [D loss: 0.119435, acc: 0.957031]  [G loss: 2.381473, acc: 0.031250]\n",
            "37: [D loss: 0.187181, acc: 0.929688]  [G loss: 2.840250, acc: 0.007812]\n",
            "38: [D loss: 0.157937, acc: 0.945312]  [G loss: 4.309287, acc: 0.000000]\n",
            "39: [D loss: 0.225996, acc: 0.921875]  [G loss: 4.278076, acc: 0.000000]\n",
            "40: [D loss: 0.227573, acc: 0.917969]  [G loss: 2.477097, acc: 0.023438]\n",
            "41: [D loss: 0.537338, acc: 0.746094]  [G loss: 5.044092, acc: 0.000000]\n",
            "42: [D loss: 0.987904, acc: 0.769531]  [G loss: 1.571497, acc: 0.179688]\n",
            "43: [D loss: 0.888198, acc: 0.640625]  [G loss: 2.131426, acc: 0.093750]\n",
            "44: [D loss: 0.844508, acc: 0.738281]  [G loss: 3.719779, acc: 0.000000]\n",
            "45: [D loss: 1.190282, acc: 0.683594]  [G loss: 0.736468, acc: 0.601562]\n",
            "46: [D loss: 1.171700, acc: 0.507812]  [G loss: 0.871498, acc: 0.445312]\n",
            "47: [D loss: 0.771200, acc: 0.671875]  [G loss: 2.160007, acc: 0.093750]\n",
            "48: [D loss: 0.728902, acc: 0.777344]  [G loss: 1.333382, acc: 0.210938]\n",
            "49: [D loss: 0.486991, acc: 0.828125]  [G loss: 0.417154, acc: 0.828125]\n",
            "50: [D loss: 0.491520, acc: 0.714844]  [G loss: 0.780983, acc: 0.484375]\n",
            "51: [D loss: 0.317598, acc: 0.894531]  [G loss: 1.940146, acc: 0.039062]\n",
            "52: [D loss: 0.307881, acc: 0.925781]  [G loss: 2.488365, acc: 0.007812]\n",
            "53: [D loss: 0.249122, acc: 0.910156]  [G loss: 1.709877, acc: 0.171875]\n",
            "54: [D loss: 0.299177, acc: 0.890625]  [G loss: 0.516341, acc: 0.765625]\n",
            "55: [D loss: 0.482901, acc: 0.730469]  [G loss: 1.519111, acc: 0.265625]\n",
            "56: [D loss: 0.381732, acc: 0.859375]  [G loss: 2.171032, acc: 0.054688]\n",
            "57: [D loss: 0.571912, acc: 0.804688]  [G loss: 0.571112, acc: 0.703125]\n",
            "58: [D loss: 0.815016, acc: 0.546875]  [G loss: 0.682089, acc: 0.539062]\n",
            "59: [D loss: 1.051328, acc: 0.464844]  [G loss: 1.315360, acc: 0.156250]\n",
            "60: [D loss: 1.325458, acc: 0.507812]  [G loss: 0.724416, acc: 0.585938]\n",
            "61: [D loss: 1.326515, acc: 0.398438]  [G loss: 0.335015, acc: 0.968750]\n",
            "62: [D loss: 0.716124, acc: 0.500000]  [G loss: 0.449884, acc: 0.945312]\n",
            "63: [D loss: 0.453217, acc: 0.812500]  [G loss: 0.614902, acc: 0.734375]\n",
            "64: [D loss: 0.233777, acc: 0.968750]  [G loss: 0.777800, acc: 0.351562]\n",
            "65: [D loss: 0.165511, acc: 0.976562]  [G loss: 0.741655, acc: 0.453125]\n",
            "66: [D loss: 0.125545, acc: 0.996094]  [G loss: 0.627550, acc: 0.695312]\n",
            "67: [D loss: 0.185596, acc: 0.980469]  [G loss: 0.530439, acc: 0.882812]\n",
            "68: [D loss: 0.254017, acc: 0.941406]  [G loss: 0.535322, acc: 0.859375]\n",
            "69: [D loss: 0.283009, acc: 0.921875]  [G loss: 0.712556, acc: 0.460938]\n",
            "70: [D loss: 0.229409, acc: 0.976562]  [G loss: 1.140457, acc: 0.007812]\n",
            "71: [D loss: 0.174459, acc: 0.953125]  [G loss: 1.461061, acc: 0.000000]\n",
            "72: [D loss: 0.232522, acc: 0.902344]  [G loss: 1.361944, acc: 0.007812]\n",
            "73: [D loss: 0.245359, acc: 0.914062]  [G loss: 0.877059, acc: 0.289062]\n",
            "74: [D loss: 0.118674, acc: 0.988281]  [G loss: 0.534519, acc: 0.828125]\n",
            "75: [D loss: 0.251992, acc: 0.957031]  [G loss: 0.495994, acc: 0.796875]\n",
            "76: [D loss: 0.400036, acc: 0.703125]  [G loss: 1.020494, acc: 0.179688]\n",
            "77: [D loss: 0.412834, acc: 0.710938]  [G loss: 2.675225, acc: 0.000000]\n",
            "78: [D loss: 0.309364, acc: 0.902344]  [G loss: 4.151218, acc: 0.000000]\n",
            "79: [D loss: 0.565364, acc: 0.773438]  [G loss: 3.936795, acc: 0.000000]\n",
            "80: [D loss: 0.496606, acc: 0.800781]  [G loss: 2.537235, acc: 0.007812]\n",
            "81: [D loss: 0.235956, acc: 0.953125]  [G loss: 1.725197, acc: 0.062500]\n",
            "82: [D loss: 0.140854, acc: 0.992188]  [G loss: 1.288509, acc: 0.210938]\n",
            "83: [D loss: 0.126849, acc: 0.968750]  [G loss: 1.093101, acc: 0.414062]\n",
            "84: [D loss: 0.106358, acc: 0.964844]  [G loss: 0.922225, acc: 0.570312]\n",
            "85: [D loss: 0.126100, acc: 0.941406]  [G loss: 0.824102, acc: 0.656250]\n",
            "86: [D loss: 0.173154, acc: 0.914062]  [G loss: 0.721798, acc: 0.757812]\n",
            "87: [D loss: 0.267374, acc: 0.863281]  [G loss: 0.782202, acc: 0.687500]\n",
            "88: [D loss: 0.249640, acc: 0.875000]  [G loss: 0.758035, acc: 0.695312]\n",
            "89: [D loss: 0.350608, acc: 0.843750]  [G loss: 0.729997, acc: 0.632812]\n",
            "90: [D loss: 0.522105, acc: 0.742188]  [G loss: 1.390414, acc: 0.289062]\n",
            "91: [D loss: 0.844525, acc: 0.593750]  [G loss: 3.492825, acc: 0.031250]\n",
            "92: [D loss: 0.766716, acc: 0.683594]  [G loss: 5.903158, acc: 0.000000]\n",
            "93: [D loss: 0.804650, acc: 0.652344]  [G loss: 6.870972, acc: 0.007812]\n",
            "94: [D loss: 1.125953, acc: 0.570312]  [G loss: 4.373070, acc: 0.070312]\n",
            "95: [D loss: 1.229979, acc: 0.617188]  [G loss: 2.033784, acc: 0.421875]\n",
            "96: [D loss: 1.329138, acc: 0.671875]  [G loss: 2.102796, acc: 0.335938]\n",
            "97: [D loss: 0.730661, acc: 0.667969]  [G loss: 3.645640, acc: 0.023438]\n",
            "98: [D loss: 0.912445, acc: 0.703125]  [G loss: 1.428104, acc: 0.273438]\n",
            "99: [D loss: 0.475697, acc: 0.800781]  [G loss: 0.082029, acc: 0.984375]\n",
            "100: [D loss: 0.256428, acc: 0.878906]  [G loss: 0.020947, acc: 1.000000]\n",
            "101: [D loss: 0.402619, acc: 0.855469]  [G loss: 0.110692, acc: 0.968750]\n",
            "102: [D loss: 0.091534, acc: 0.957031]  [G loss: 0.779859, acc: 0.687500]\n",
            "103: [D loss: 0.228277, acc: 0.898438]  [G loss: 1.031909, acc: 0.695312]\n",
            "104: [D loss: 0.292705, acc: 0.886719]  [G loss: 0.371974, acc: 0.843750]\n",
            "105: [D loss: 0.113444, acc: 0.957031]  [G loss: 0.019336, acc: 1.000000]\n",
            "106: [D loss: 0.083482, acc: 0.968750]  [G loss: 0.002706, acc: 1.000000]\n",
            "107: [D loss: 0.243225, acc: 0.910156]  [G loss: 0.023952, acc: 1.000000]\n",
            "108: [D loss: 0.067409, acc: 0.972656]  [G loss: 0.247315, acc: 0.882812]\n",
            "109: [D loss: 0.042275, acc: 0.984375]  [G loss: 0.745332, acc: 0.679688]\n",
            "110: [D loss: 0.149370, acc: 0.960938]  [G loss: 0.885641, acc: 0.648438]\n",
            "111: [D loss: 0.109547, acc: 0.964844]  [G loss: 0.459973, acc: 0.765625]\n",
            "112: [D loss: 0.045992, acc: 0.988281]  [G loss: 0.196903, acc: 0.906250]\n",
            "113: [D loss: 0.188594, acc: 0.917969]  [G loss: 0.279571, acc: 0.906250]\n",
            "114: [D loss: 0.113221, acc: 0.953125]  [G loss: 1.152002, acc: 0.437500]\n",
            "115: [D loss: 0.067701, acc: 0.984375]  [G loss: 2.818454, acc: 0.015625]\n",
            "116: [D loss: 0.075388, acc: 0.972656]  [G loss: 3.288172, acc: 0.007812]\n",
            "117: [D loss: 0.148333, acc: 0.953125]  [G loss: 2.893780, acc: 0.015625]\n",
            "118: [D loss: 0.076412, acc: 0.976562]  [G loss: 1.889402, acc: 0.070312]\n",
            "119: [D loss: 0.097700, acc: 0.972656]  [G loss: 2.713228, acc: 0.015625]\n",
            "120: [D loss: 0.066133, acc: 0.992188]  [G loss: 4.113883, acc: 0.000000]\n",
            "121: [D loss: 0.043362, acc: 0.988281]  [G loss: 5.233531, acc: 0.000000]\n",
            "122: [D loss: 0.021857, acc: 1.000000]  [G loss: 5.490931, acc: 0.000000]\n",
            "123: [D loss: 0.042275, acc: 0.996094]  [G loss: 5.579196, acc: 0.000000]\n",
            "124: [D loss: 0.077803, acc: 0.984375]  [G loss: 5.452421, acc: 0.000000]\n",
            "125: [D loss: 0.155699, acc: 0.937500]  [G loss: 5.834779, acc: 0.000000]\n",
            "126: [D loss: 0.306829, acc: 0.878906]  [G loss: 5.987864, acc: 0.000000]\n",
            "127: [D loss: 0.567934, acc: 0.750000]  [G loss: 4.639863, acc: 0.015625]\n",
            "128: [D loss: 0.759181, acc: 0.714844]  [G loss: 4.476826, acc: 0.007812]\n",
            "129: [D loss: 1.070892, acc: 0.636719]  [G loss: 1.393851, acc: 0.335938]\n",
            "130: [D loss: 1.175663, acc: 0.589844]  [G loss: 3.718473, acc: 0.039062]\n",
            "131: [D loss: 1.473624, acc: 0.667969]  [G loss: 0.316814, acc: 0.859375]\n",
            "132: [D loss: 0.782212, acc: 0.640625]  [G loss: 1.040483, acc: 0.484375]\n",
            "133: [D loss: 0.318791, acc: 0.917969]  [G loss: 2.702408, acc: 0.164062]\n",
            "134: [D loss: 0.415094, acc: 0.863281]  [G loss: 1.966318, acc: 0.265625]\n",
            "135: [D loss: 0.127088, acc: 0.953125]  [G loss: 0.841191, acc: 0.609375]\n",
            "136: [D loss: 0.095881, acc: 0.953125]  [G loss: 0.294128, acc: 0.867188]\n",
            "137: [D loss: 0.103119, acc: 0.957031]  [G loss: 1.134709, acc: 0.546875]\n",
            "138: [D loss: 0.038446, acc: 0.980469]  [G loss: 2.269269, acc: 0.382812]\n",
            "139: [D loss: 0.032096, acc: 0.988281]  [G loss: 3.182716, acc: 0.281250]\n",
            "140: [D loss: 0.059107, acc: 0.980469]  [G loss: 3.153578, acc: 0.296875]\n",
            "141: [D loss: 0.041348, acc: 0.976562]  [G loss: 2.373713, acc: 0.351562]\n",
            "142: [D loss: 0.098180, acc: 0.976562]  [G loss: 2.821491, acc: 0.335938]\n",
            "143: [D loss: 0.144882, acc: 0.957031]  [G loss: 3.796535, acc: 0.218750]\n",
            "144: [D loss: 0.221855, acc: 0.929688]  [G loss: 5.227326, acc: 0.085938]\n",
            "145: [D loss: 0.268949, acc: 0.914062]  [G loss: 3.061225, acc: 0.218750]\n",
            "146: [D loss: 0.326334, acc: 0.875000]  [G loss: 1.142244, acc: 0.585938]\n",
            "147: [D loss: 0.440811, acc: 0.800781]  [G loss: 3.834059, acc: 0.109375]\n",
            "148: [D loss: 0.847211, acc: 0.746094]  [G loss: 0.083379, acc: 0.968750]\n",
            "149: [D loss: 1.527632, acc: 0.566406]  [G loss: 5.300570, acc: 0.054688]\n",
            "150: [D loss: 1.650255, acc: 0.628906]  [G loss: 0.775847, acc: 0.703125]\n",
            "151: [D loss: 0.911386, acc: 0.640625]  [G loss: 0.053908, acc: 0.984375]\n",
            "152: [D loss: 1.856817, acc: 0.460938]  [G loss: 1.492750, acc: 0.414062]\n",
            "153: [D loss: 1.125147, acc: 0.597656]  [G loss: 2.243915, acc: 0.257812]\n",
            "154: [D loss: 1.406358, acc: 0.566406]  [G loss: 0.295806, acc: 0.890625]\n",
            "155: [D loss: 0.860610, acc: 0.625000]  [G loss: 0.215305, acc: 0.914062]\n",
            "156: [D loss: 1.118131, acc: 0.523438]  [G loss: 0.903242, acc: 0.578125]\n",
            "157: [D loss: 0.846688, acc: 0.613281]  [G loss: 1.864912, acc: 0.265625]\n",
            "158: [D loss: 0.875277, acc: 0.558594]  [G loss: 1.512525, acc: 0.343750]\n",
            "159: [D loss: 0.706926, acc: 0.664062]  [G loss: 0.701679, acc: 0.703125]\n",
            "160: [D loss: 0.554751, acc: 0.730469]  [G loss: 0.493066, acc: 0.757812]\n",
            "161: [D loss: 0.737609, acc: 0.644531]  [G loss: 0.857142, acc: 0.523438]\n",
            "162: [D loss: 0.458954, acc: 0.796875]  [G loss: 1.829698, acc: 0.132812]\n",
            "163: [D loss: 0.461258, acc: 0.761719]  [G loss: 2.488984, acc: 0.039062]\n",
            "164: [D loss: 0.514028, acc: 0.738281]  [G loss: 1.898336, acc: 0.054688]\n",
            "165: [D loss: 0.389826, acc: 0.839844]  [G loss: 1.094904, acc: 0.296875]\n",
            "166: [D loss: 0.343939, acc: 0.878906]  [G loss: 0.778849, acc: 0.523438]\n",
            "167: [D loss: 0.415124, acc: 0.812500]  [G loss: 1.057001, acc: 0.359375]\n",
            "168: [D loss: 0.352498, acc: 0.886719]  [G loss: 2.494294, acc: 0.054688]\n",
            "169: [D loss: 0.350053, acc: 0.890625]  [G loss: 3.027307, acc: 0.000000]\n",
            "170: [D loss: 0.300954, acc: 0.867188]  [G loss: 2.155439, acc: 0.031250]\n",
            "171: [D loss: 0.252211, acc: 0.925781]  [G loss: 1.346355, acc: 0.242188]\n",
            "172: [D loss: 0.218759, acc: 0.953125]  [G loss: 1.379519, acc: 0.242188]\n",
            "173: [D loss: 0.237890, acc: 0.933594]  [G loss: 3.002457, acc: 0.078125]\n",
            "174: [D loss: 0.179247, acc: 0.953125]  [G loss: 4.251124, acc: 0.000000]\n",
            "175: [D loss: 0.313370, acc: 0.902344]  [G loss: 1.637115, acc: 0.257812]\n",
            "176: [D loss: 0.197638, acc: 0.945312]  [G loss: 1.019669, acc: 0.453125]\n",
            "177: [D loss: 0.244907, acc: 0.898438]  [G loss: 1.839590, acc: 0.234375]\n",
            "178: [D loss: 0.139249, acc: 0.976562]  [G loss: 3.804920, acc: 0.093750]\n",
            "179: [D loss: 0.174245, acc: 0.945312]  [G loss: 3.465265, acc: 0.109375]\n",
            "180: [D loss: 0.250523, acc: 0.902344]  [G loss: 0.950320, acc: 0.546875]\n",
            "181: [D loss: 0.198550, acc: 0.925781]  [G loss: 0.759419, acc: 0.632812]\n",
            "182: [D loss: 0.299294, acc: 0.843750]  [G loss: 2.775618, acc: 0.226562]\n",
            "183: [D loss: 0.354195, acc: 0.917969]  [G loss: 2.211746, acc: 0.210938]\n",
            "184: [D loss: 0.334899, acc: 0.902344]  [G loss: 0.604185, acc: 0.671875]\n",
            "185: [D loss: 0.340807, acc: 0.832031]  [G loss: 0.889849, acc: 0.562500]\n",
            "186: [D loss: 0.223933, acc: 0.910156]  [G loss: 1.917117, acc: 0.242188]\n",
            "187: [D loss: 0.246936, acc: 0.914062]  [G loss: 1.971311, acc: 0.218750]\n",
            "188: [D loss: 0.306934, acc: 0.894531]  [G loss: 1.013443, acc: 0.437500]\n",
            "189: [D loss: 0.275401, acc: 0.914062]  [G loss: 0.796583, acc: 0.507812]\n",
            "190: [D loss: 0.267938, acc: 0.878906]  [G loss: 1.456829, acc: 0.226562]\n",
            "191: [D loss: 0.218866, acc: 0.945312]  [G loss: 1.986956, acc: 0.140625]\n",
            "192: [D loss: 0.462380, acc: 0.816406]  [G loss: 1.076637, acc: 0.390625]\n",
            "193: [D loss: 0.359913, acc: 0.843750]  [G loss: 0.658326, acc: 0.632812]\n",
            "194: [D loss: 0.432963, acc: 0.734375]  [G loss: 1.477691, acc: 0.218750]\n",
            "195: [D loss: 0.349861, acc: 0.863281]  [G loss: 2.071069, acc: 0.031250]\n",
            "196: [D loss: 0.711682, acc: 0.726562]  [G loss: 0.992796, acc: 0.359375]\n",
            "197: [D loss: 0.543902, acc: 0.703125]  [G loss: 0.539102, acc: 0.734375]\n",
            "198: [D loss: 0.814914, acc: 0.574219]  [G loss: 1.513395, acc: 0.117188]\n",
            "199: [D loss: 0.631362, acc: 0.707031]  [G loss: 1.954677, acc: 0.007812]\n",
            "200: [D loss: 0.800770, acc: 0.625000]  [G loss: 1.190270, acc: 0.164062]\n",
            "201: [D loss: 0.684939, acc: 0.613281]  [G loss: 1.134677, acc: 0.164062]\n",
            "202: [D loss: 0.649347, acc: 0.667969]  [G loss: 1.972828, acc: 0.000000]\n",
            "203: [D loss: 0.592547, acc: 0.707031]  [G loss: 2.434090, acc: 0.000000]\n",
            "204: [D loss: 0.502445, acc: 0.781250]  [G loss: 2.401517, acc: 0.000000]\n",
            "205: [D loss: 0.394118, acc: 0.835938]  [G loss: 2.228282, acc: 0.007812]\n",
            "206: [D loss: 0.351937, acc: 0.859375]  [G loss: 2.167653, acc: 0.015625]\n",
            "207: [D loss: 0.343562, acc: 0.839844]  [G loss: 2.419626, acc: 0.007812]\n",
            "208: [D loss: 0.241150, acc: 0.914062]  [G loss: 2.679049, acc: 0.007812]\n",
            "209: [D loss: 0.200460, acc: 0.937500]  [G loss: 2.974672, acc: 0.015625]\n",
            "210: [D loss: 0.108130, acc: 0.976562]  [G loss: 3.325937, acc: 0.000000]\n",
            "211: [D loss: 0.112698, acc: 0.957031]  [G loss: 3.633316, acc: 0.000000]\n",
            "212: [D loss: 0.101607, acc: 0.972656]  [G loss: 3.548769, acc: 0.000000]\n",
            "213: [D loss: 0.089227, acc: 0.972656]  [G loss: 3.196010, acc: 0.023438]\n",
            "214: [D loss: 0.091750, acc: 0.972656]  [G loss: 2.719970, acc: 0.085938]\n",
            "215: [D loss: 0.132788, acc: 0.960938]  [G loss: 2.716321, acc: 0.164062]\n",
            "216: [D loss: 0.094015, acc: 0.968750]  [G loss: 2.822260, acc: 0.273438]\n",
            "217: [D loss: 0.090123, acc: 0.960938]  [G loss: 3.369313, acc: 0.234375]\n",
            "218: [D loss: 0.150591, acc: 0.953125]  [G loss: 3.681858, acc: 0.203125]\n",
            "219: [D loss: 0.129265, acc: 0.957031]  [G loss: 3.333779, acc: 0.296875]\n",
            "220: [D loss: 0.114332, acc: 0.960938]  [G loss: 2.651704, acc: 0.453125]\n",
            "221: [D loss: 0.127378, acc: 0.964844]  [G loss: 2.150719, acc: 0.531250]\n",
            "222: [D loss: 0.150046, acc: 0.949219]  [G loss: 2.572033, acc: 0.453125]\n",
            "223: [D loss: 0.232438, acc: 0.906250]  [G loss: 2.200368, acc: 0.492188]\n",
            "224: [D loss: 0.271950, acc: 0.914062]  [G loss: 2.401612, acc: 0.492188]\n",
            "225: [D loss: 0.255031, acc: 0.898438]  [G loss: 2.655910, acc: 0.421875]\n",
            "226: [D loss: 0.342630, acc: 0.871094]  [G loss: 1.338909, acc: 0.632812]\n",
            "227: [D loss: 0.348639, acc: 0.847656]  [G loss: 2.727649, acc: 0.312500]\n",
            "228: [D loss: 0.355098, acc: 0.875000]  [G loss: 1.670155, acc: 0.414062]\n",
            "229: [D loss: 0.441021, acc: 0.820312]  [G loss: 2.689781, acc: 0.359375]\n",
            "230: [D loss: 0.373402, acc: 0.851562]  [G loss: 2.887358, acc: 0.242188]\n",
            "231: [D loss: 0.467794, acc: 0.835938]  [G loss: 2.616387, acc: 0.250000]\n",
            "232: [D loss: 0.487423, acc: 0.808594]  [G loss: 3.457019, acc: 0.171875]\n",
            "233: [D loss: 0.641742, acc: 0.726562]  [G loss: 2.166494, acc: 0.375000]\n",
            "234: [D loss: 0.496135, acc: 0.781250]  [G loss: 2.089230, acc: 0.312500]\n",
            "235: [D loss: 0.600273, acc: 0.773438]  [G loss: 3.899945, acc: 0.109375]\n",
            "236: [D loss: 0.854717, acc: 0.699219]  [G loss: 0.285125, acc: 0.875000]\n",
            "237: [D loss: 1.605501, acc: 0.597656]  [G loss: 4.455681, acc: 0.031250]\n",
            "238: [D loss: 1.208928, acc: 0.621094]  [G loss: 3.673925, acc: 0.031250]\n",
            "239: [D loss: 1.074938, acc: 0.621094]  [G loss: 0.247006, acc: 0.890625]\n",
            "240: [D loss: 1.192839, acc: 0.554688]  [G loss: 0.649985, acc: 0.648438]\n",
            "241: [D loss: 0.778684, acc: 0.656250]  [G loss: 2.281646, acc: 0.148438]\n",
            "242: [D loss: 0.756654, acc: 0.699219]  [G loss: 2.416607, acc: 0.109375]\n",
            "243: [D loss: 0.863966, acc: 0.656250]  [G loss: 1.637220, acc: 0.234375]\n",
            "244: [D loss: 0.561391, acc: 0.746094]  [G loss: 0.946749, acc: 0.460938]\n",
            "245: [D loss: 0.483325, acc: 0.777344]  [G loss: 0.522619, acc: 0.718750]\n",
            "246: [D loss: 0.491257, acc: 0.734375]  [G loss: 0.558616, acc: 0.609375]\n",
            "247: [D loss: 0.430922, acc: 0.781250]  [G loss: 1.016299, acc: 0.453125]\n",
            "248: [D loss: 0.317528, acc: 0.882812]  [G loss: 1.878751, acc: 0.218750]\n",
            "249: [D loss: 0.273735, acc: 0.890625]  [G loss: 3.137856, acc: 0.140625]\n",
            "250: [D loss: 0.242171, acc: 0.898438]  [G loss: 3.978428, acc: 0.085938]\n",
            "251: [D loss: 0.335259, acc: 0.824219]  [G loss: 3.951133, acc: 0.093750]\n",
            "252: [D loss: 0.177643, acc: 0.929688]  [G loss: 3.050320, acc: 0.140625]\n",
            "253: [D loss: 0.174818, acc: 0.941406]  [G loss: 1.996690, acc: 0.179688]\n",
            "254: [D loss: 0.141270, acc: 0.976562]  [G loss: 1.510684, acc: 0.328125]\n",
            "255: [D loss: 0.217252, acc: 0.910156]  [G loss: 1.813401, acc: 0.265625]\n",
            "256: [D loss: 0.153220, acc: 0.949219]  [G loss: 2.699274, acc: 0.234375]\n",
            "257: [D loss: 0.090680, acc: 0.972656]  [G loss: 4.473637, acc: 0.101562]\n",
            "258: [D loss: 0.160779, acc: 0.960938]  [G loss: 4.908215, acc: 0.085938]\n",
            "259: [D loss: 0.162352, acc: 0.941406]  [G loss: 3.675298, acc: 0.132812]\n",
            "260: [D loss: 0.120038, acc: 0.960938]  [G loss: 2.446755, acc: 0.218750]\n",
            "261: [D loss: 0.158444, acc: 0.929688]  [G loss: 2.054883, acc: 0.273438]\n",
            "262: [D loss: 0.225652, acc: 0.917969]  [G loss: 2.943185, acc: 0.156250]\n",
            "263: [D loss: 0.193174, acc: 0.921875]  [G loss: 3.677220, acc: 0.125000]\n",
            "264: [D loss: 0.245564, acc: 0.925781]  [G loss: 3.038284, acc: 0.140625]\n",
            "265: [D loss: 0.252068, acc: 0.921875]  [G loss: 1.569030, acc: 0.421875]\n",
            "266: [D loss: 0.266686, acc: 0.875000]  [G loss: 1.656455, acc: 0.437500]\n",
            "267: [D loss: 0.226575, acc: 0.902344]  [G loss: 2.417431, acc: 0.281250]\n",
            "268: [D loss: 0.258131, acc: 0.902344]  [G loss: 2.298151, acc: 0.296875]\n",
            "269: [D loss: 0.158634, acc: 0.941406]  [G loss: 1.569095, acc: 0.406250]\n",
            "270: [D loss: 0.340550, acc: 0.875000]  [G loss: 0.668815, acc: 0.710938]\n",
            "271: [D loss: 0.243851, acc: 0.898438]  [G loss: 0.764134, acc: 0.671875]\n",
            "272: [D loss: 0.209561, acc: 0.902344]  [G loss: 1.192206, acc: 0.523438]\n",
            "273: [D loss: 0.196033, acc: 0.933594]  [G loss: 1.120569, acc: 0.515625]\n",
            "274: [D loss: 0.173656, acc: 0.941406]  [G loss: 1.124405, acc: 0.492188]\n",
            "275: [D loss: 0.205147, acc: 0.925781]  [G loss: 0.577150, acc: 0.687500]\n",
            "276: [D loss: 0.198409, acc: 0.910156]  [G loss: 0.690533, acc: 0.687500]\n",
            "277: [D loss: 0.139428, acc: 0.945312]  [G loss: 0.884573, acc: 0.601562]\n",
            "278: [D loss: 0.154534, acc: 0.957031]  [G loss: 1.179048, acc: 0.578125]\n",
            "279: [D loss: 0.140389, acc: 0.964844]  [G loss: 1.215888, acc: 0.500000]\n",
            "280: [D loss: 0.102785, acc: 0.964844]  [G loss: 0.998353, acc: 0.578125]\n",
            "281: [D loss: 0.170698, acc: 0.945312]  [G loss: 0.886696, acc: 0.578125]\n",
            "282: [D loss: 0.158416, acc: 0.933594]  [G loss: 0.885502, acc: 0.601562]\n",
            "283: [D loss: 0.195883, acc: 0.917969]  [G loss: 1.240497, acc: 0.507812]\n",
            "284: [D loss: 0.190390, acc: 0.949219]  [G loss: 1.399289, acc: 0.500000]\n",
            "285: [D loss: 0.104198, acc: 0.972656]  [G loss: 1.826896, acc: 0.390625]\n",
            "286: [D loss: 0.057183, acc: 0.984375]  [G loss: 2.108207, acc: 0.351562]\n",
            "287: [D loss: 0.179323, acc: 0.937500]  [G loss: 1.510378, acc: 0.445312]\n",
            "288: [D loss: 0.227419, acc: 0.921875]  [G loss: 0.658658, acc: 0.703125]\n",
            "289: [D loss: 0.182646, acc: 0.925781]  [G loss: 0.432936, acc: 0.796875]\n",
            "290: [D loss: 0.194970, acc: 0.914062]  [G loss: 0.681460, acc: 0.710938]\n",
            "291: [D loss: 0.178363, acc: 0.925781]  [G loss: 1.242227, acc: 0.570312]\n",
            "292: [D loss: 0.164498, acc: 0.941406]  [G loss: 1.332268, acc: 0.492188]\n",
            "293: [D loss: 0.131440, acc: 0.953125]  [G loss: 1.448415, acc: 0.515625]\n",
            "294: [D loss: 0.092681, acc: 0.972656]  [G loss: 0.925504, acc: 0.640625]\n",
            "295: [D loss: 0.102751, acc: 0.964844]  [G loss: 0.873332, acc: 0.632812]\n",
            "296: [D loss: 0.057477, acc: 0.988281]  [G loss: 0.802639, acc: 0.664062]\n",
            "297: [D loss: 0.149827, acc: 0.929688]  [G loss: 1.271405, acc: 0.515625]\n",
            "298: [D loss: 0.088307, acc: 0.968750]  [G loss: 1.347090, acc: 0.500000]\n",
            "299: [D loss: 0.118517, acc: 0.968750]  [G loss: 0.882699, acc: 0.617188]\n",
            "300: [D loss: 0.132082, acc: 0.960938]  [G loss: 1.058576, acc: 0.609375]\n",
            "301: [D loss: 0.116087, acc: 0.953125]  [G loss: 1.316433, acc: 0.593750]\n",
            "302: [D loss: 0.121862, acc: 0.945312]  [G loss: 1.525532, acc: 0.578125]\n",
            "303: [D loss: 0.179534, acc: 0.933594]  [G loss: 1.969472, acc: 0.585938]\n",
            "304: [D loss: 0.255390, acc: 0.906250]  [G loss: 2.169768, acc: 0.539062]\n",
            "305: [D loss: 0.183846, acc: 0.917969]  [G loss: 2.309045, acc: 0.507812]\n",
            "306: [D loss: 0.253522, acc: 0.917969]  [G loss: 2.910745, acc: 0.414062]\n",
            "307: [D loss: 0.344055, acc: 0.898438]  [G loss: 1.954266, acc: 0.601562]\n",
            "308: [D loss: 0.214455, acc: 0.937500]  [G loss: 2.119789, acc: 0.523438]\n",
            "309: [D loss: 0.398082, acc: 0.867188]  [G loss: 3.493485, acc: 0.289062]\n",
            "310: [D loss: 0.525026, acc: 0.839844]  [G loss: 2.301993, acc: 0.578125]\n",
            "311: [D loss: 0.600590, acc: 0.777344]  [G loss: 5.737796, acc: 0.070312]\n",
            "312: [D loss: 0.895407, acc: 0.792969]  [G loss: 2.925971, acc: 0.296875]\n",
            "313: [D loss: 0.373425, acc: 0.855469]  [G loss: 1.916040, acc: 0.476562]\n",
            "314: [D loss: 0.554976, acc: 0.792969]  [G loss: 4.654255, acc: 0.140625]\n",
            "315: [D loss: 0.281032, acc: 0.859375]  [G loss: 6.114843, acc: 0.031250]\n",
            "316: [D loss: 0.526175, acc: 0.835938]  [G loss: 2.961442, acc: 0.257812]\n",
            "317: [D loss: 0.247097, acc: 0.894531]  [G loss: 2.600938, acc: 0.390625]\n",
            "318: [D loss: 0.352759, acc: 0.855469]  [G loss: 4.439527, acc: 0.117188]\n",
            "319: [D loss: 0.198009, acc: 0.906250]  [G loss: 6.907005, acc: 0.015625]\n",
            "320: [D loss: 0.263354, acc: 0.890625]  [G loss: 7.304686, acc: 0.023438]\n",
            "321: [D loss: 0.268222, acc: 0.878906]  [G loss: 5.966193, acc: 0.132812]\n",
            "322: [D loss: 0.327038, acc: 0.871094]  [G loss: 5.304745, acc: 0.148438]\n",
            "323: [D loss: 0.343489, acc: 0.843750]  [G loss: 6.465313, acc: 0.070312]\n",
            "324: [D loss: 0.272167, acc: 0.894531]  [G loss: 6.700211, acc: 0.101562]\n",
            "325: [D loss: 0.357095, acc: 0.839844]  [G loss: 6.870945, acc: 0.109375]\n",
            "326: [D loss: 0.321744, acc: 0.859375]  [G loss: 5.353839, acc: 0.210938]\n",
            "327: [D loss: 0.286592, acc: 0.898438]  [G loss: 3.662440, acc: 0.242188]\n",
            "328: [D loss: 0.472304, acc: 0.804688]  [G loss: 3.844208, acc: 0.289062]\n",
            "329: [D loss: 0.307390, acc: 0.890625]  [G loss: 3.284735, acc: 0.250000]\n",
            "330: [D loss: 0.210226, acc: 0.906250]  [G loss: 2.942074, acc: 0.281250]\n",
            "331: [D loss: 0.191192, acc: 0.921875]  [G loss: 2.429005, acc: 0.460938]\n",
            "332: [D loss: 0.215266, acc: 0.917969]  [G loss: 1.893415, acc: 0.554688]\n",
            "333: [D loss: 0.188372, acc: 0.925781]  [G loss: 1.962117, acc: 0.468750]\n",
            "334: [D loss: 0.205234, acc: 0.898438]  [G loss: 2.269801, acc: 0.343750]\n",
            "335: [D loss: 0.183749, acc: 0.933594]  [G loss: 1.964795, acc: 0.367188]\n",
            "336: [D loss: 0.216429, acc: 0.925781]  [G loss: 1.595654, acc: 0.390625]\n",
            "337: [D loss: 0.158720, acc: 0.937500]  [G loss: 1.488529, acc: 0.429688]\n",
            "338: [D loss: 0.164891, acc: 0.937500]  [G loss: 1.471617, acc: 0.398438]\n",
            "339: [D loss: 0.131713, acc: 0.957031]  [G loss: 1.750468, acc: 0.421875]\n",
            "340: [D loss: 0.151263, acc: 0.941406]  [G loss: 1.270366, acc: 0.460938]\n",
            "341: [D loss: 0.104912, acc: 0.972656]  [G loss: 1.334486, acc: 0.476562]\n",
            "342: [D loss: 0.138692, acc: 0.949219]  [G loss: 1.102087, acc: 0.562500]\n",
            "343: [D loss: 0.088055, acc: 0.960938]  [G loss: 1.466860, acc: 0.445312]\n",
            "344: [D loss: 0.084557, acc: 0.968750]  [G loss: 1.970347, acc: 0.421875]\n",
            "345: [D loss: 0.124805, acc: 0.949219]  [G loss: 1.911507, acc: 0.343750]\n",
            "346: [D loss: 0.172771, acc: 0.933594]  [G loss: 1.179137, acc: 0.562500]\n",
            "347: [D loss: 0.106886, acc: 0.960938]  [G loss: 0.692159, acc: 0.703125]\n",
            "348: [D loss: 0.135201, acc: 0.960938]  [G loss: 0.832079, acc: 0.695312]\n",
            "349: [D loss: 0.170759, acc: 0.921875]  [G loss: 2.273297, acc: 0.250000]\n",
            "350: [D loss: 0.175051, acc: 0.929688]  [G loss: 3.107307, acc: 0.187500]\n",
            "351: [D loss: 0.158766, acc: 0.937500]  [G loss: 2.150471, acc: 0.320312]\n",
            "352: [D loss: 0.125923, acc: 0.953125]  [G loss: 1.393638, acc: 0.500000]\n",
            "353: [D loss: 0.128964, acc: 0.964844]  [G loss: 1.062945, acc: 0.617188]\n",
            "354: [D loss: 0.199251, acc: 0.917969]  [G loss: 1.796970, acc: 0.492188]\n",
            "355: [D loss: 0.125751, acc: 0.949219]  [G loss: 2.716216, acc: 0.296875]\n",
            "356: [D loss: 0.133158, acc: 0.949219]  [G loss: 2.755002, acc: 0.367188]\n",
            "357: [D loss: 0.135974, acc: 0.949219]  [G loss: 2.639060, acc: 0.453125]\n",
            "358: [D loss: 0.071339, acc: 0.980469]  [G loss: 1.599058, acc: 0.562500]\n",
            "359: [D loss: 0.111118, acc: 0.957031]  [G loss: 1.613203, acc: 0.578125]\n",
            "360: [D loss: 0.146271, acc: 0.945312]  [G loss: 2.022740, acc: 0.523438]\n",
            "361: [D loss: 0.195324, acc: 0.925781]  [G loss: 1.494104, acc: 0.476562]\n",
            "362: [D loss: 0.193809, acc: 0.910156]  [G loss: 1.386368, acc: 0.539062]\n",
            "363: [D loss: 0.136877, acc: 0.964844]  [G loss: 1.015521, acc: 0.625000]\n",
            "364: [D loss: 0.141395, acc: 0.941406]  [G loss: 0.971668, acc: 0.609375]\n",
            "365: [D loss: 0.080856, acc: 0.976562]  [G loss: 1.070082, acc: 0.617188]\n",
            "366: [D loss: 0.164624, acc: 0.933594]  [G loss: 0.972843, acc: 0.601562]\n",
            "367: [D loss: 0.107588, acc: 0.960938]  [G loss: 1.106559, acc: 0.656250]\n",
            "368: [D loss: 0.160659, acc: 0.929688]  [G loss: 1.285825, acc: 0.609375]\n",
            "369: [D loss: 0.160136, acc: 0.937500]  [G loss: 1.081358, acc: 0.632812]\n",
            "370: [D loss: 0.121711, acc: 0.953125]  [G loss: 1.790942, acc: 0.554688]\n",
            "371: [D loss: 0.171233, acc: 0.933594]  [G loss: 1.724734, acc: 0.523438]\n",
            "372: [D loss: 0.108481, acc: 0.968750]  [G loss: 2.100747, acc: 0.500000]\n",
            "373: [D loss: 0.100704, acc: 0.960938]  [G loss: 1.980407, acc: 0.453125]\n",
            "374: [D loss: 0.130428, acc: 0.941406]  [G loss: 2.682935, acc: 0.421875]\n",
            "375: [D loss: 0.163325, acc: 0.937500]  [G loss: 2.640252, acc: 0.492188]\n",
            "376: [D loss: 0.196439, acc: 0.921875]  [G loss: 1.960686, acc: 0.593750]\n",
            "377: [D loss: 0.185216, acc: 0.898438]  [G loss: 2.210797, acc: 0.546875]\n",
            "378: [D loss: 0.190200, acc: 0.921875]  [G loss: 3.411513, acc: 0.445312]\n",
            "379: [D loss: 0.121893, acc: 0.960938]  [G loss: 3.976946, acc: 0.445312]\n",
            "380: [D loss: 0.189368, acc: 0.933594]  [G loss: 3.534052, acc: 0.429688]\n",
            "381: [D loss: 0.199810, acc: 0.937500]  [G loss: 2.056352, acc: 0.539062]\n",
            "382: [D loss: 0.346734, acc: 0.867188]  [G loss: 3.765825, acc: 0.460938]\n",
            "383: [D loss: 0.218014, acc: 0.925781]  [G loss: 3.939974, acc: 0.398438]\n",
            "384: [D loss: 0.287285, acc: 0.890625]  [G loss: 2.071410, acc: 0.593750]\n",
            "385: [D loss: 0.305215, acc: 0.871094]  [G loss: 1.579861, acc: 0.703125]\n",
            "386: [D loss: 0.403017, acc: 0.847656]  [G loss: 2.016807, acc: 0.601562]\n",
            "387: [D loss: 0.373234, acc: 0.894531]  [G loss: 2.343988, acc: 0.554688]\n",
            "388: [D loss: 0.262696, acc: 0.925781]  [G loss: 1.691786, acc: 0.585938]\n",
            "389: [D loss: 0.150361, acc: 0.937500]  [G loss: 1.322966, acc: 0.656250]\n",
            "390: [D loss: 0.193575, acc: 0.917969]  [G loss: 1.012828, acc: 0.757812]\n",
            "391: [D loss: 0.168856, acc: 0.917969]  [G loss: 1.195127, acc: 0.710938]\n",
            "392: [D loss: 0.115956, acc: 0.964844]  [G loss: 1.271275, acc: 0.679688]\n",
            "393: [D loss: 0.081752, acc: 0.968750]  [G loss: 1.338244, acc: 0.664062]\n",
            "394: [D loss: 0.093744, acc: 0.964844]  [G loss: 1.023110, acc: 0.781250]\n",
            "395: [D loss: 0.054301, acc: 0.980469]  [G loss: 0.910034, acc: 0.742188]\n",
            "396: [D loss: 0.078757, acc: 0.976562]  [G loss: 0.868510, acc: 0.757812]\n",
            "397: [D loss: 0.065609, acc: 0.984375]  [G loss: 0.507032, acc: 0.789062]\n",
            "398: [D loss: 0.084959, acc: 0.972656]  [G loss: 0.628106, acc: 0.773438]\n",
            "399: [D loss: 0.068504, acc: 0.972656]  [G loss: 0.851470, acc: 0.710938]\n",
            "400: [D loss: 0.109038, acc: 0.972656]  [G loss: 0.938012, acc: 0.703125]\n",
            "401: [D loss: 0.056253, acc: 0.984375]  [G loss: 0.659185, acc: 0.750000]\n",
            "402: [D loss: 0.106638, acc: 0.957031]  [G loss: 0.589267, acc: 0.742188]\n",
            "403: [D loss: 0.071499, acc: 0.972656]  [G loss: 0.421696, acc: 0.843750]\n",
            "404: [D loss: 0.071078, acc: 0.968750]  [G loss: 0.377449, acc: 0.835938]\n",
            "405: [D loss: 0.078665, acc: 0.964844]  [G loss: 0.570678, acc: 0.773438]\n",
            "406: [D loss: 0.102539, acc: 0.964844]  [G loss: 0.716998, acc: 0.734375]\n",
            "407: [D loss: 0.126572, acc: 0.957031]  [G loss: 0.766233, acc: 0.796875]\n",
            "408: [D loss: 0.178399, acc: 0.937500]  [G loss: 0.482727, acc: 0.835938]\n",
            "409: [D loss: 0.222270, acc: 0.914062]  [G loss: 0.313265, acc: 0.890625]\n",
            "410: [D loss: 0.243604, acc: 0.902344]  [G loss: 0.456769, acc: 0.851562]\n",
            "411: [D loss: 0.158428, acc: 0.949219]  [G loss: 0.623378, acc: 0.718750]\n",
            "412: [D loss: 0.324732, acc: 0.902344]  [G loss: 0.279567, acc: 0.890625]\n",
            "413: [D loss: 0.159651, acc: 0.941406]  [G loss: 0.355931, acc: 0.859375]\n",
            "414: [D loss: 0.205201, acc: 0.898438]  [G loss: 0.532458, acc: 0.789062]\n",
            "415: [D loss: 0.202662, acc: 0.914062]  [G loss: 0.491155, acc: 0.750000]\n",
            "416: [D loss: 0.158584, acc: 0.945312]  [G loss: 0.603356, acc: 0.757812]\n",
            "417: [D loss: 0.188635, acc: 0.929688]  [G loss: 0.367542, acc: 0.828125]\n",
            "418: [D loss: 0.195732, acc: 0.921875]  [G loss: 0.352643, acc: 0.859375]\n",
            "419: [D loss: 0.182536, acc: 0.929688]  [G loss: 0.388217, acc: 0.843750]\n",
            "420: [D loss: 0.198589, acc: 0.914062]  [G loss: 0.573504, acc: 0.796875]\n",
            "421: [D loss: 0.181883, acc: 0.957031]  [G loss: 0.877494, acc: 0.671875]\n",
            "422: [D loss: 0.140959, acc: 0.949219]  [G loss: 0.754245, acc: 0.648438]\n",
            "423: [D loss: 0.149994, acc: 0.945312]  [G loss: 0.730802, acc: 0.750000]\n",
            "424: [D loss: 0.124903, acc: 0.957031]  [G loss: 0.598614, acc: 0.734375]\n",
            "425: [D loss: 0.184985, acc: 0.921875]  [G loss: 0.604655, acc: 0.757812]\n",
            "426: [D loss: 0.186316, acc: 0.921875]  [G loss: 0.936192, acc: 0.640625]\n",
            "427: [D loss: 0.151962, acc: 0.937500]  [G loss: 1.338968, acc: 0.554688]\n",
            "428: [D loss: 0.130649, acc: 0.949219]  [G loss: 1.175416, acc: 0.617188]\n",
            "429: [D loss: 0.085884, acc: 0.972656]  [G loss: 1.146730, acc: 0.601562]\n",
            "430: [D loss: 0.071965, acc: 0.976562]  [G loss: 0.925449, acc: 0.671875]\n",
            "431: [D loss: 0.091058, acc: 0.976562]  [G loss: 0.836342, acc: 0.703125]\n",
            "432: [D loss: 0.170448, acc: 0.933594]  [G loss: 0.945920, acc: 0.695312]\n",
            "433: [D loss: 0.179464, acc: 0.933594]  [G loss: 1.426142, acc: 0.617188]\n",
            "434: [D loss: 0.131373, acc: 0.953125]  [G loss: 1.555335, acc: 0.585938]\n",
            "435: [D loss: 0.142063, acc: 0.949219]  [G loss: 1.701860, acc: 0.585938]\n",
            "436: [D loss: 0.127447, acc: 0.960938]  [G loss: 1.313165, acc: 0.609375]\n",
            "437: [D loss: 0.105939, acc: 0.976562]  [G loss: 0.905877, acc: 0.695312]\n",
            "438: [D loss: 0.190707, acc: 0.929688]  [G loss: 0.864140, acc: 0.648438]\n",
            "439: [D loss: 0.118712, acc: 0.976562]  [G loss: 0.998692, acc: 0.695312]\n",
            "440: [D loss: 0.114283, acc: 0.960938]  [G loss: 1.291040, acc: 0.531250]\n",
            "441: [D loss: 0.130726, acc: 0.964844]  [G loss: 1.369533, acc: 0.515625]\n",
            "442: [D loss: 0.160155, acc: 0.929688]  [G loss: 1.391987, acc: 0.445312]\n",
            "443: [D loss: 0.158859, acc: 0.937500]  [G loss: 1.384496, acc: 0.515625]\n",
            "444: [D loss: 0.172531, acc: 0.917969]  [G loss: 1.192061, acc: 0.593750]\n",
            "445: [D loss: 0.177364, acc: 0.949219]  [G loss: 1.269420, acc: 0.562500]\n",
            "446: [D loss: 0.186828, acc: 0.929688]  [G loss: 1.172046, acc: 0.570312]\n",
            "447: [D loss: 0.207694, acc: 0.917969]  [G loss: 1.315580, acc: 0.570312]\n",
            "448: [D loss: 0.131176, acc: 0.945312]  [G loss: 1.289349, acc: 0.617188]\n",
            "449: [D loss: 0.136959, acc: 0.957031]  [G loss: 1.266836, acc: 0.609375]\n",
            "450: [D loss: 0.134424, acc: 0.949219]  [G loss: 1.377893, acc: 0.507812]\n",
            "451: [D loss: 0.134547, acc: 0.953125]  [G loss: 1.841911, acc: 0.406250]\n",
            "452: [D loss: 0.134049, acc: 0.949219]  [G loss: 1.836303, acc: 0.523438]\n",
            "453: [D loss: 0.167002, acc: 0.937500]  [G loss: 1.426282, acc: 0.484375]\n",
            "454: [D loss: 0.148382, acc: 0.925781]  [G loss: 1.566506, acc: 0.476562]\n",
            "455: [D loss: 0.144397, acc: 0.937500]  [G loss: 1.893303, acc: 0.312500]\n",
            "456: [D loss: 0.115443, acc: 0.957031]  [G loss: 2.266610, acc: 0.281250]\n",
            "457: [D loss: 0.187007, acc: 0.945312]  [G loss: 2.094738, acc: 0.375000]\n",
            "458: [D loss: 0.110337, acc: 0.960938]  [G loss: 1.804454, acc: 0.437500]\n",
            "459: [D loss: 0.114516, acc: 0.957031]  [G loss: 1.873227, acc: 0.398438]\n",
            "460: [D loss: 0.121218, acc: 0.945312]  [G loss: 1.794389, acc: 0.414062]\n",
            "461: [D loss: 0.173491, acc: 0.937500]  [G loss: 2.152886, acc: 0.296875]\n",
            "462: [D loss: 0.157762, acc: 0.937500]  [G loss: 2.264218, acc: 0.218750]\n",
            "463: [D loss: 0.111386, acc: 0.949219]  [G loss: 2.527826, acc: 0.187500]\n",
            "464: [D loss: 0.123150, acc: 0.968750]  [G loss: 2.441176, acc: 0.203125]\n",
            "465: [D loss: 0.116333, acc: 0.960938]  [G loss: 2.317429, acc: 0.203125]\n",
            "466: [D loss: 0.109691, acc: 0.964844]  [G loss: 2.432036, acc: 0.195312]\n",
            "467: [D loss: 0.122139, acc: 0.968750]  [G loss: 1.961957, acc: 0.250000]\n",
            "468: [D loss: 0.120268, acc: 0.949219]  [G loss: 2.382487, acc: 0.234375]\n",
            "469: [D loss: 0.096823, acc: 0.964844]  [G loss: 2.581737, acc: 0.179688]\n",
            "470: [D loss: 0.105258, acc: 0.949219]  [G loss: 2.874992, acc: 0.164062]\n",
            "471: [D loss: 0.097072, acc: 0.960938]  [G loss: 3.146471, acc: 0.156250]\n",
            "472: [D loss: 0.158375, acc: 0.933594]  [G loss: 2.947598, acc: 0.218750]\n",
            "473: [D loss: 0.192707, acc: 0.925781]  [G loss: 2.809246, acc: 0.304688]\n",
            "474: [D loss: 0.248483, acc: 0.906250]  [G loss: 3.020927, acc: 0.335938]\n",
            "475: [D loss: 0.179300, acc: 0.921875]  [G loss: 3.230872, acc: 0.273438]\n",
            "476: [D loss: 0.218337, acc: 0.921875]  [G loss: 3.171659, acc: 0.250000]\n",
            "477: [D loss: 0.439164, acc: 0.851562]  [G loss: 2.148645, acc: 0.476562]\n",
            "478: [D loss: 0.386141, acc: 0.855469]  [G loss: 2.050842, acc: 0.453125]\n",
            "479: [D loss: 0.355245, acc: 0.855469]  [G loss: 2.536994, acc: 0.359375]\n",
            "480: [D loss: 0.385328, acc: 0.839844]  [G loss: 3.372794, acc: 0.218750]\n",
            "481: [D loss: 0.304529, acc: 0.890625]  [G loss: 2.564868, acc: 0.296875]\n",
            "482: [D loss: 0.282127, acc: 0.906250]  [G loss: 2.623944, acc: 0.421875]\n",
            "483: [D loss: 0.220706, acc: 0.890625]  [G loss: 2.300494, acc: 0.437500]\n",
            "484: [D loss: 0.219706, acc: 0.917969]  [G loss: 2.520102, acc: 0.382812]\n",
            "485: [D loss: 0.246463, acc: 0.921875]  [G loss: 2.622498, acc: 0.343750]\n",
            "486: [D loss: 0.218860, acc: 0.906250]  [G loss: 2.370510, acc: 0.375000]\n",
            "487: [D loss: 0.139443, acc: 0.945312]  [G loss: 2.930916, acc: 0.273438]\n",
            "488: [D loss: 0.178963, acc: 0.921875]  [G loss: 2.630940, acc: 0.273438]\n",
            "489: [D loss: 0.208806, acc: 0.929688]  [G loss: 2.672158, acc: 0.289062]\n",
            "490: [D loss: 0.174669, acc: 0.921875]  [G loss: 2.327855, acc: 0.343750]\n",
            "491: [D loss: 0.139861, acc: 0.957031]  [G loss: 2.483832, acc: 0.343750]\n",
            "492: [D loss: 0.139543, acc: 0.921875]  [G loss: 2.270308, acc: 0.382812]\n",
            "493: [D loss: 0.229264, acc: 0.906250]  [G loss: 1.895355, acc: 0.460938]\n",
            "494: [D loss: 0.199172, acc: 0.929688]  [G loss: 2.129550, acc: 0.382812]\n",
            "495: [D loss: 0.147632, acc: 0.949219]  [G loss: 2.007278, acc: 0.421875]\n",
            "496: [D loss: 0.129428, acc: 0.941406]  [G loss: 1.683386, acc: 0.382812]\n",
            "497: [D loss: 0.113104, acc: 0.957031]  [G loss: 1.829406, acc: 0.312500]\n",
            "498: [D loss: 0.155466, acc: 0.933594]  [G loss: 1.723770, acc: 0.390625]\n",
            "499: [D loss: 0.249140, acc: 0.929688]  [G loss: 1.222120, acc: 0.539062]\n",
            "500: [D loss: 0.278885, acc: 0.933594]  [G loss: 1.036447, acc: 0.640625]\n",
            "501: [D loss: 0.204382, acc: 0.937500]  [G loss: 0.828065, acc: 0.695312]\n",
            "502: [D loss: 0.140747, acc: 0.937500]  [G loss: 0.842351, acc: 0.664062]\n",
            "503: [D loss: 0.176702, acc: 0.933594]  [G loss: 1.386132, acc: 0.484375]\n",
            "504: [D loss: 0.146107, acc: 0.941406]  [G loss: 1.365866, acc: 0.468750]\n",
            "505: [D loss: 0.196848, acc: 0.921875]  [G loss: 1.697379, acc: 0.421875]\n",
            "506: [D loss: 0.243589, acc: 0.921875]  [G loss: 1.425005, acc: 0.453125]\n",
            "507: [D loss: 0.220986, acc: 0.949219]  [G loss: 0.826568, acc: 0.664062]\n",
            "508: [D loss: 0.176723, acc: 0.917969]  [G loss: 0.783246, acc: 0.648438]\n",
            "509: [D loss: 0.201630, acc: 0.933594]  [G loss: 1.146078, acc: 0.515625]\n",
            "510: [D loss: 0.145457, acc: 0.941406]  [G loss: 1.714718, acc: 0.390625]\n",
            "511: [D loss: 0.091386, acc: 0.968750]  [G loss: 2.563584, acc: 0.226562]\n",
            "512: [D loss: 0.136687, acc: 0.957031]  [G loss: 3.470597, acc: 0.187500]\n",
            "513: [D loss: 0.097396, acc: 0.976562]  [G loss: 2.985563, acc: 0.226562]\n",
            "514: [D loss: 0.044545, acc: 0.984375]  [G loss: 2.644956, acc: 0.351562]\n",
            "515: [D loss: 0.049740, acc: 0.992188]  [G loss: 2.495269, acc: 0.421875]\n",
            "516: [D loss: 0.052107, acc: 0.980469]  [G loss: 2.203409, acc: 0.484375]\n",
            "517: [D loss: 0.061947, acc: 0.968750]  [G loss: 1.875874, acc: 0.539062]\n",
            "518: [D loss: 0.032780, acc: 0.992188]  [G loss: 1.886681, acc: 0.515625]\n",
            "519: [D loss: 0.069173, acc: 0.964844]  [G loss: 1.789654, acc: 0.578125]\n",
            "520: [D loss: 0.054533, acc: 0.988281]  [G loss: 1.918196, acc: 0.539062]\n",
            "521: [D loss: 0.088594, acc: 0.980469]  [G loss: 1.550085, acc: 0.585938]\n",
            "522: [D loss: 0.041139, acc: 0.992188]  [G loss: 1.690688, acc: 0.539062]\n",
            "523: [D loss: 0.030935, acc: 0.980469]  [G loss: 1.579622, acc: 0.546875]\n",
            "524: [D loss: 0.054402, acc: 0.988281]  [G loss: 1.487053, acc: 0.617188]\n",
            "525: [D loss: 0.035183, acc: 0.980469]  [G loss: 0.983605, acc: 0.765625]\n",
            "526: [D loss: 0.034947, acc: 0.980469]  [G loss: 0.593918, acc: 0.804688]\n",
            "527: [D loss: 0.051529, acc: 0.980469]  [G loss: 0.475040, acc: 0.843750]\n",
            "528: [D loss: 0.060042, acc: 0.968750]  [G loss: 0.373565, acc: 0.851562]\n",
            "529: [D loss: 0.078624, acc: 0.964844]  [G loss: 0.206420, acc: 0.906250]\n",
            "530: [D loss: 0.055840, acc: 0.980469]  [G loss: 0.355725, acc: 0.867188]\n",
            "531: [D loss: 0.068368, acc: 0.976562]  [G loss: 0.447640, acc: 0.859375]\n",
            "532: [D loss: 0.093896, acc: 0.960938]  [G loss: 0.248581, acc: 0.882812]\n",
            "533: [D loss: 0.030290, acc: 0.992188]  [G loss: 0.180552, acc: 0.937500]\n",
            "534: [D loss: 0.045871, acc: 0.980469]  [G loss: 0.145061, acc: 0.953125]\n",
            "535: [D loss: 0.071025, acc: 0.964844]  [G loss: 0.146790, acc: 0.945312]\n",
            "536: [D loss: 0.098678, acc: 0.957031]  [G loss: 0.262314, acc: 0.898438]\n",
            "537: [D loss: 0.082514, acc: 0.968750]  [G loss: 0.424774, acc: 0.867188]\n",
            "538: [D loss: 0.063625, acc: 0.968750]  [G loss: 0.371779, acc: 0.843750]\n",
            "539: [D loss: 0.090297, acc: 0.976562]  [G loss: 0.475188, acc: 0.843750]\n",
            "540: [D loss: 0.053898, acc: 0.976562]  [G loss: 0.297943, acc: 0.859375]\n",
            "541: [D loss: 0.097981, acc: 0.960938]  [G loss: 0.405109, acc: 0.851562]\n",
            "542: [D loss: 0.117234, acc: 0.964844]  [G loss: 0.431998, acc: 0.859375]\n",
            "543: [D loss: 0.071351, acc: 0.968750]  [G loss: 0.591053, acc: 0.843750]\n",
            "544: [D loss: 0.097089, acc: 0.960938]  [G loss: 0.725241, acc: 0.812500]\n",
            "545: [D loss: 0.149842, acc: 0.960938]  [G loss: 0.504942, acc: 0.835938]\n",
            "546: [D loss: 0.053022, acc: 0.980469]  [G loss: 0.424476, acc: 0.882812]\n",
            "547: [D loss: 0.050188, acc: 0.980469]  [G loss: 0.327890, acc: 0.890625]\n",
            "548: [D loss: 0.036505, acc: 0.984375]  [G loss: 0.259132, acc: 0.921875]\n",
            "549: [D loss: 0.067644, acc: 0.980469]  [G loss: 0.474983, acc: 0.882812]\n",
            "550: [D loss: 0.050877, acc: 0.980469]  [G loss: 0.470959, acc: 0.835938]\n",
            "551: [D loss: 0.067627, acc: 0.984375]  [G loss: 0.508282, acc: 0.851562]\n",
            "552: [D loss: 0.103679, acc: 0.972656]  [G loss: 0.537382, acc: 0.867188]\n",
            "553: [D loss: 0.056104, acc: 0.980469]  [G loss: 0.318202, acc: 0.890625]\n",
            "554: [D loss: 0.080285, acc: 0.972656]  [G loss: 0.366989, acc: 0.875000]\n",
            "555: [D loss: 0.063620, acc: 0.972656]  [G loss: 0.330616, acc: 0.882812]\n",
            "556: [D loss: 0.074072, acc: 0.988281]  [G loss: 0.355439, acc: 0.867188]\n",
            "557: [D loss: 0.057865, acc: 0.992188]  [G loss: 0.472584, acc: 0.804688]\n",
            "558: [D loss: 0.026577, acc: 0.992188]  [G loss: 0.514479, acc: 0.796875]\n",
            "559: [D loss: 0.052495, acc: 0.972656]  [G loss: 0.340611, acc: 0.851562]\n",
            "560: [D loss: 0.042517, acc: 0.984375]  [G loss: 0.381630, acc: 0.851562]\n",
            "561: [D loss: 0.048186, acc: 0.988281]  [G loss: 0.384574, acc: 0.796875]\n",
            "562: [D loss: 0.026761, acc: 0.988281]  [G loss: 0.214872, acc: 0.921875]\n",
            "563: [D loss: 0.045714, acc: 0.984375]  [G loss: 0.269445, acc: 0.867188]\n",
            "564: [D loss: 0.044463, acc: 0.988281]  [G loss: 0.131946, acc: 0.960938]\n",
            "565: [D loss: 0.045446, acc: 0.984375]  [G loss: 0.122513, acc: 0.968750]\n",
            "566: [D loss: 0.034342, acc: 0.988281]  [G loss: 0.235415, acc: 0.906250]\n",
            "567: [D loss: 0.030595, acc: 0.992188]  [G loss: 0.158235, acc: 0.960938]\n",
            "568: [D loss: 0.015391, acc: 1.000000]  [G loss: 0.267050, acc: 0.906250]\n",
            "569: [D loss: 0.016023, acc: 0.996094]  [G loss: 0.328693, acc: 0.859375]\n",
            "570: [D loss: 0.031034, acc: 0.996094]  [G loss: 0.337155, acc: 0.859375]\n",
            "571: [D loss: 0.049298, acc: 0.992188]  [G loss: 0.377863, acc: 0.828125]\n",
            "572: [D loss: 0.024206, acc: 0.992188]  [G loss: 0.369456, acc: 0.820312]\n",
            "573: [D loss: 0.030654, acc: 0.988281]  [G loss: 0.329943, acc: 0.843750]\n",
            "574: [D loss: 0.019349, acc: 0.996094]  [G loss: 0.225278, acc: 0.906250]\n",
            "575: [D loss: 0.019776, acc: 0.996094]  [G loss: 0.264831, acc: 0.875000]\n",
            "576: [D loss: 0.023033, acc: 0.992188]  [G loss: 0.227364, acc: 0.898438]\n",
            "577: [D loss: 0.040112, acc: 0.992188]  [G loss: 0.208364, acc: 0.929688]\n",
            "578: [D loss: 0.034284, acc: 0.988281]  [G loss: 0.203899, acc: 0.945312]\n",
            "579: [D loss: 0.025508, acc: 0.996094]  [G loss: 0.162877, acc: 0.960938]\n",
            "580: [D loss: 0.026253, acc: 0.988281]  [G loss: 0.184992, acc: 0.953125]\n",
            "581: [D loss: 0.019967, acc: 0.992188]  [G loss: 0.183903, acc: 0.953125]\n",
            "582: [D loss: 0.026756, acc: 0.996094]  [G loss: 0.196695, acc: 0.953125]\n",
            "583: [D loss: 0.017238, acc: 0.996094]  [G loss: 0.193204, acc: 0.953125]\n",
            "584: [D loss: 0.033926, acc: 0.988281]  [G loss: 0.286175, acc: 0.898438]\n",
            "585: [D loss: 0.014134, acc: 1.000000]  [G loss: 0.294576, acc: 0.906250]\n",
            "586: [D loss: 0.022114, acc: 0.988281]  [G loss: 0.413141, acc: 0.781250]\n",
            "587: [D loss: 0.009233, acc: 1.000000]  [G loss: 0.487233, acc: 0.718750]\n",
            "588: [D loss: 0.010923, acc: 0.996094]  [G loss: 0.685328, acc: 0.687500]\n",
            "589: [D loss: 0.018480, acc: 0.996094]  [G loss: 0.755722, acc: 0.539062]\n",
            "590: [D loss: 0.035307, acc: 0.988281]  [G loss: 0.931718, acc: 0.492188]\n",
            "591: [D loss: 0.009267, acc: 1.000000]  [G loss: 0.875907, acc: 0.515625]\n",
            "592: [D loss: 0.020814, acc: 0.992188]  [G loss: 0.961853, acc: 0.523438]\n",
            "593: [D loss: 0.076499, acc: 0.992188]  [G loss: 0.896507, acc: 0.546875]\n",
            "594: [D loss: 0.022132, acc: 0.992188]  [G loss: 0.993351, acc: 0.578125]\n",
            "595: [D loss: 0.015871, acc: 0.996094]  [G loss: 1.202579, acc: 0.531250]\n",
            "596: [D loss: 0.050601, acc: 0.984375]  [G loss: 1.226946, acc: 0.523438]\n",
            "597: [D loss: 0.103687, acc: 0.972656]  [G loss: 1.207386, acc: 0.578125]\n",
            "598: [D loss: 0.024641, acc: 0.996094]  [G loss: 1.414371, acc: 0.546875]\n",
            "599: [D loss: 0.009764, acc: 1.000000]  [G loss: 1.599730, acc: 0.515625]\n",
            "600: [D loss: 0.024454, acc: 0.992188]  [G loss: 1.528706, acc: 0.531250]\n",
            "601: [D loss: 0.057562, acc: 0.988281]  [G loss: 1.815354, acc: 0.468750]\n",
            "602: [D loss: 0.025436, acc: 0.992188]  [G loss: 1.671165, acc: 0.609375]\n",
            "603: [D loss: 0.026097, acc: 0.988281]  [G loss: 1.847713, acc: 0.617188]\n",
            "604: [D loss: 0.048986, acc: 0.968750]  [G loss: 2.199137, acc: 0.523438]\n",
            "605: [D loss: 0.148261, acc: 0.949219]  [G loss: 2.241387, acc: 0.546875]\n",
            "606: [D loss: 0.022681, acc: 0.996094]  [G loss: 2.163866, acc: 0.546875]\n",
            "607: [D loss: 0.044255, acc: 0.980469]  [G loss: 2.224046, acc: 0.523438]\n",
            "608: [D loss: 0.090615, acc: 0.960938]  [G loss: 2.475433, acc: 0.531250]\n",
            "609: [D loss: 0.056400, acc: 0.972656]  [G loss: 2.817021, acc: 0.429688]\n",
            "610: [D loss: 0.072602, acc: 0.976562]  [G loss: 2.613047, acc: 0.531250]\n",
            "611: [D loss: 0.107395, acc: 0.972656]  [G loss: 2.679571, acc: 0.429688]\n",
            "612: [D loss: 0.110724, acc: 0.953125]  [G loss: 3.140255, acc: 0.398438]\n",
            "613: [D loss: 0.181688, acc: 0.949219]  [G loss: 2.787718, acc: 0.484375]\n",
            "614: [D loss: 0.197760, acc: 0.937500]  [G loss: 2.548480, acc: 0.507812]\n",
            "615: [D loss: 0.237574, acc: 0.921875]  [G loss: 2.233045, acc: 0.570312]\n",
            "616: [D loss: 0.236823, acc: 0.914062]  [G loss: 3.539546, acc: 0.328125]\n",
            "617: [D loss: 0.304235, acc: 0.910156]  [G loss: 3.174548, acc: 0.328125]\n",
            "618: [D loss: 0.196587, acc: 0.914062]  [G loss: 2.933568, acc: 0.328125]\n",
            "619: [D loss: 0.201164, acc: 0.906250]  [G loss: 2.514088, acc: 0.460938]\n",
            "620: [D loss: 0.205620, acc: 0.910156]  [G loss: 3.676482, acc: 0.250000]\n",
            "621: [D loss: 0.330564, acc: 0.875000]  [G loss: 3.907763, acc: 0.210938]\n",
            "622: [D loss: 0.286207, acc: 0.898438]  [G loss: 3.453472, acc: 0.195312]\n",
            "623: [D loss: 0.261393, acc: 0.902344]  [G loss: 2.774740, acc: 0.328125]\n",
            "624: [D loss: 0.263605, acc: 0.871094]  [G loss: 1.971954, acc: 0.414062]\n",
            "625: [D loss: 0.144776, acc: 0.929688]  [G loss: 1.858705, acc: 0.468750]\n",
            "626: [D loss: 0.136709, acc: 0.953125]  [G loss: 1.579385, acc: 0.507812]\n",
            "627: [D loss: 0.179591, acc: 0.937500]  [G loss: 1.215167, acc: 0.562500]\n",
            "628: [D loss: 0.141419, acc: 0.937500]  [G loss: 1.095999, acc: 0.664062]\n",
            "629: [D loss: 0.098288, acc: 0.960938]  [G loss: 1.425551, acc: 0.648438]\n",
            "630: [D loss: 0.136849, acc: 0.937500]  [G loss: 1.295720, acc: 0.625000]\n",
            "631: [D loss: 0.171541, acc: 0.933594]  [G loss: 1.215106, acc: 0.601562]\n",
            "632: [D loss: 0.133064, acc: 0.957031]  [G loss: 0.945879, acc: 0.648438]\n",
            "633: [D loss: 0.074180, acc: 0.976562]  [G loss: 0.798868, acc: 0.757812]\n",
            "634: [D loss: 0.039530, acc: 0.988281]  [G loss: 0.583283, acc: 0.781250]\n",
            "635: [D loss: 0.031681, acc: 0.992188]  [G loss: 0.281340, acc: 0.890625]\n",
            "636: [D loss: 0.133672, acc: 0.976562]  [G loss: 0.133925, acc: 0.953125]\n",
            "637: [D loss: 0.130311, acc: 0.953125]  [G loss: 0.217446, acc: 0.945312]\n",
            "638: [D loss: 0.077615, acc: 0.972656]  [G loss: 0.077023, acc: 0.968750]\n",
            "639: [D loss: 0.055879, acc: 0.980469]  [G loss: 0.133096, acc: 0.937500]\n",
            "640: [D loss: 0.077958, acc: 0.972656]  [G loss: 0.219869, acc: 0.906250]\n",
            "641: [D loss: 0.085453, acc: 0.984375]  [G loss: 0.215165, acc: 0.890625]\n",
            "642: [D loss: 0.077741, acc: 0.980469]  [G loss: 0.261642, acc: 0.898438]\n",
            "643: [D loss: 0.169378, acc: 0.949219]  [G loss: 0.101114, acc: 0.960938]\n",
            "644: [D loss: 0.071630, acc: 0.968750]  [G loss: 0.030622, acc: 0.984375]\n",
            "645: [D loss: 0.088438, acc: 0.964844]  [G loss: 0.051397, acc: 0.968750]\n",
            "646: [D loss: 0.111704, acc: 0.949219]  [G loss: 0.030009, acc: 0.984375]\n",
            "647: [D loss: 0.116260, acc: 0.953125]  [G loss: 0.089722, acc: 0.960938]\n",
            "648: [D loss: 0.044300, acc: 0.988281]  [G loss: 0.148488, acc: 0.945312]\n",
            "649: [D loss: 0.014424, acc: 0.996094]  [G loss: 0.333346, acc: 0.875000]\n",
            "650: [D loss: 0.083443, acc: 0.984375]  [G loss: 0.215398, acc: 0.898438]\n",
            "651: [D loss: 0.033030, acc: 0.988281]  [G loss: 0.189724, acc: 0.906250]\n",
            "652: [D loss: 0.065497, acc: 0.988281]  [G loss: 0.264889, acc: 0.921875]\n",
            "653: [D loss: 0.047046, acc: 0.992188]  [G loss: 0.185104, acc: 0.921875]\n",
            "654: [D loss: 0.082730, acc: 0.980469]  [G loss: 0.058754, acc: 0.976562]\n",
            "655: [D loss: 0.126771, acc: 0.949219]  [G loss: 0.140129, acc: 0.953125]\n",
            "656: [D loss: 0.094107, acc: 0.968750]  [G loss: 0.224273, acc: 0.890625]\n",
            "657: [D loss: 0.139756, acc: 0.957031]  [G loss: 0.417412, acc: 0.812500]\n",
            "658: [D loss: 0.174450, acc: 0.968750]  [G loss: 0.569035, acc: 0.773438]\n",
            "659: [D loss: 0.176599, acc: 0.941406]  [G loss: 0.432755, acc: 0.820312]\n",
            "660: [D loss: 0.099591, acc: 0.960938]  [G loss: 0.311899, acc: 0.843750]\n",
            "661: [D loss: 0.175277, acc: 0.941406]  [G loss: 0.397328, acc: 0.859375]\n",
            "662: [D loss: 0.252266, acc: 0.890625]  [G loss: 0.721226, acc: 0.710938]\n",
            "663: [D loss: 0.161524, acc: 0.941406]  [G loss: 0.918003, acc: 0.609375]\n",
            "664: [D loss: 0.294354, acc: 0.917969]  [G loss: 1.017790, acc: 0.648438]\n",
            "665: [D loss: 0.275056, acc: 0.910156]  [G loss: 0.745323, acc: 0.718750]\n",
            "666: [D loss: 0.208904, acc: 0.921875]  [G loss: 0.757711, acc: 0.726562]\n",
            "667: [D loss: 0.260107, acc: 0.898438]  [G loss: 0.965215, acc: 0.609375]\n",
            "668: [D loss: 0.349989, acc: 0.863281]  [G loss: 1.897343, acc: 0.398438]\n",
            "669: [D loss: 0.225978, acc: 0.941406]  [G loss: 2.296824, acc: 0.351562]\n",
            "670: [D loss: 0.339333, acc: 0.875000]  [G loss: 1.707989, acc: 0.398438]\n",
            "671: [D loss: 0.357841, acc: 0.851562]  [G loss: 1.117363, acc: 0.601562]\n",
            "672: [D loss: 0.273457, acc: 0.863281]  [G loss: 0.949065, acc: 0.609375]\n",
            "673: [D loss: 0.193611, acc: 0.921875]  [G loss: 1.484991, acc: 0.406250]\n",
            "674: [D loss: 0.255444, acc: 0.906250]  [G loss: 1.789843, acc: 0.296875]\n",
            "675: [D loss: 0.207197, acc: 0.894531]  [G loss: 1.983026, acc: 0.289062]\n",
            "676: [D loss: 0.165288, acc: 0.945312]  [G loss: 1.988376, acc: 0.234375]\n",
            "677: [D loss: 0.100951, acc: 0.972656]  [G loss: 1.593566, acc: 0.421875]\n",
            "678: [D loss: 0.112226, acc: 0.964844]  [G loss: 1.275709, acc: 0.437500]\n",
            "679: [D loss: 0.140715, acc: 0.953125]  [G loss: 1.316528, acc: 0.484375]\n",
            "680: [D loss: 0.106199, acc: 0.972656]  [G loss: 1.291778, acc: 0.437500]\n",
            "681: [D loss: 0.196325, acc: 0.906250]  [G loss: 1.380054, acc: 0.484375]\n",
            "682: [D loss: 0.142789, acc: 0.933594]  [G loss: 1.832233, acc: 0.367188]\n",
            "683: [D loss: 0.119082, acc: 0.964844]  [G loss: 1.684857, acc: 0.421875]\n",
            "684: [D loss: 0.125863, acc: 0.968750]  [G loss: 1.416030, acc: 0.460938]\n",
            "685: [D loss: 0.207197, acc: 0.910156]  [G loss: 1.192204, acc: 0.585938]\n",
            "686: [D loss: 0.245831, acc: 0.890625]  [G loss: 0.971146, acc: 0.617188]\n",
            "687: [D loss: 0.313097, acc: 0.882812]  [G loss: 0.873853, acc: 0.703125]\n",
            "688: [D loss: 0.262024, acc: 0.921875]  [G loss: 0.934319, acc: 0.664062]\n",
            "689: [D loss: 0.243149, acc: 0.894531]  [G loss: 1.282345, acc: 0.585938]\n",
            "690: [D loss: 0.324302, acc: 0.886719]  [G loss: 1.630191, acc: 0.531250]\n",
            "691: [D loss: 0.241378, acc: 0.894531]  [G loss: 1.388618, acc: 0.617188]\n",
            "692: [D loss: 0.253070, acc: 0.906250]  [G loss: 0.922749, acc: 0.718750]\n",
            "693: [D loss: 0.190423, acc: 0.929688]  [G loss: 0.858591, acc: 0.718750]\n",
            "694: [D loss: 0.233519, acc: 0.890625]  [G loss: 1.123291, acc: 0.640625]\n",
            "695: [D loss: 0.177363, acc: 0.921875]  [G loss: 1.836400, acc: 0.500000]\n",
            "696: [D loss: 0.167077, acc: 0.941406]  [G loss: 1.678405, acc: 0.546875]\n",
            "697: [D loss: 0.195905, acc: 0.917969]  [G loss: 1.387231, acc: 0.585938]\n",
            "698: [D loss: 0.179654, acc: 0.933594]  [G loss: 1.151507, acc: 0.671875]\n",
            "699: [D loss: 0.110406, acc: 0.972656]  [G loss: 0.937144, acc: 0.703125]\n",
            "700: [D loss: 0.133943, acc: 0.941406]  [G loss: 0.693164, acc: 0.750000]\n",
            "701: [D loss: 0.200353, acc: 0.910156]  [G loss: 0.702898, acc: 0.726562]\n",
            "702: [D loss: 0.153059, acc: 0.949219]  [G loss: 1.277839, acc: 0.539062]\n",
            "703: [D loss: 0.145386, acc: 0.937500]  [G loss: 1.197545, acc: 0.562500]\n",
            "704: [D loss: 0.154163, acc: 0.949219]  [G loss: 1.506091, acc: 0.539062]\n",
            "705: [D loss: 0.132206, acc: 0.945312]  [G loss: 1.243884, acc: 0.562500]\n",
            "706: [D loss: 0.142469, acc: 0.937500]  [G loss: 1.035464, acc: 0.601562]\n",
            "707: [D loss: 0.155116, acc: 0.933594]  [G loss: 0.782660, acc: 0.679688]\n",
            "708: [D loss: 0.160662, acc: 0.949219]  [G loss: 0.604797, acc: 0.773438]\n",
            "709: [D loss: 0.159371, acc: 0.929688]  [G loss: 0.963142, acc: 0.632812]\n",
            "710: [D loss: 0.135780, acc: 0.933594]  [G loss: 0.977845, acc: 0.679688]\n",
            "711: [D loss: 0.106436, acc: 0.968750]  [G loss: 1.552002, acc: 0.460938]\n",
            "712: [D loss: 0.129316, acc: 0.953125]  [G loss: 1.965951, acc: 0.421875]\n",
            "713: [D loss: 0.164146, acc: 0.933594]  [G loss: 1.761860, acc: 0.531250]\n",
            "714: [D loss: 0.153691, acc: 0.945312]  [G loss: 1.710978, acc: 0.476562]\n",
            "715: [D loss: 0.191041, acc: 0.929688]  [G loss: 1.437519, acc: 0.585938]\n",
            "716: [D loss: 0.148816, acc: 0.937500]  [G loss: 1.317614, acc: 0.593750]\n",
            "717: [D loss: 0.172689, acc: 0.941406]  [G loss: 1.566704, acc: 0.507812]\n",
            "718: [D loss: 0.128867, acc: 0.945312]  [G loss: 2.539676, acc: 0.359375]\n",
            "719: [D loss: 0.199958, acc: 0.925781]  [G loss: 2.477621, acc: 0.375000]\n",
            "720: [D loss: 0.155426, acc: 0.925781]  [G loss: 3.555099, acc: 0.273438]\n",
            "721: [D loss: 0.246378, acc: 0.925781]  [G loss: 3.294706, acc: 0.328125]\n",
            "722: [D loss: 0.203684, acc: 0.921875]  [G loss: 3.239292, acc: 0.289062]\n",
            "723: [D loss: 0.203847, acc: 0.929688]  [G loss: 3.013805, acc: 0.312500]\n",
            "724: [D loss: 0.283731, acc: 0.906250]  [G loss: 2.791801, acc: 0.390625]\n",
            "725: [D loss: 0.200239, acc: 0.921875]  [G loss: 2.642981, acc: 0.375000]\n",
            "726: [D loss: 0.241856, acc: 0.937500]  [G loss: 3.468430, acc: 0.281250]\n",
            "727: [D loss: 0.172890, acc: 0.933594]  [G loss: 3.404156, acc: 0.328125]\n",
            "728: [D loss: 0.213965, acc: 0.929688]  [G loss: 2.830985, acc: 0.375000]\n",
            "729: [D loss: 0.129445, acc: 0.957031]  [G loss: 2.915403, acc: 0.375000]\n",
            "730: [D loss: 0.176606, acc: 0.929688]  [G loss: 2.180602, acc: 0.476562]\n",
            "731: [D loss: 0.179855, acc: 0.933594]  [G loss: 1.654905, acc: 0.539062]\n",
            "732: [D loss: 0.167966, acc: 0.945312]  [G loss: 1.350604, acc: 0.570312]\n",
            "733: [D loss: 0.112622, acc: 0.953125]  [G loss: 1.240739, acc: 0.617188]\n",
            "734: [D loss: 0.070003, acc: 0.964844]  [G loss: 1.576328, acc: 0.492188]\n",
            "735: [D loss: 0.038014, acc: 0.992188]  [G loss: 1.449080, acc: 0.539062]\n",
            "736: [D loss: 0.051837, acc: 0.988281]  [G loss: 1.597066, acc: 0.546875]\n",
            "737: [D loss: 0.054303, acc: 0.988281]  [G loss: 1.363680, acc: 0.523438]\n",
            "738: [D loss: 0.042816, acc: 0.988281]  [G loss: 1.350729, acc: 0.593750]\n",
            "739: [D loss: 0.023116, acc: 0.992188]  [G loss: 1.089458, acc: 0.632812]\n",
            "740: [D loss: 0.047378, acc: 0.984375]  [G loss: 0.809453, acc: 0.718750]\n",
            "741: [D loss: 0.051320, acc: 0.984375]  [G loss: 0.608458, acc: 0.765625]\n",
            "742: [D loss: 0.121736, acc: 0.968750]  [G loss: 0.693018, acc: 0.765625]\n",
            "743: [D loss: 0.109344, acc: 0.976562]  [G loss: 0.496418, acc: 0.757812]\n",
            "744: [D loss: 0.088785, acc: 0.968750]  [G loss: 0.802520, acc: 0.632812]\n",
            "745: [D loss: 0.191620, acc: 0.937500]  [G loss: 0.607718, acc: 0.750000]\n",
            "746: [D loss: 0.174566, acc: 0.933594]  [G loss: 0.427471, acc: 0.804688]\n",
            "747: [D loss: 0.217069, acc: 0.914062]  [G loss: 0.418433, acc: 0.812500]\n",
            "748: [D loss: 0.210387, acc: 0.914062]  [G loss: 0.766598, acc: 0.679688]\n",
            "749: [D loss: 0.204239, acc: 0.921875]  [G loss: 0.712805, acc: 0.671875]\n",
            "750: [D loss: 0.214458, acc: 0.921875]  [G loss: 0.399837, acc: 0.820312]\n",
            "751: [D loss: 0.266130, acc: 0.859375]  [G loss: 0.614556, acc: 0.750000]\n",
            "752: [D loss: 0.190533, acc: 0.933594]  [G loss: 1.165098, acc: 0.632812]\n",
            "753: [D loss: 0.229595, acc: 0.933594]  [G loss: 1.080048, acc: 0.664062]\n",
            "754: [D loss: 0.267185, acc: 0.917969]  [G loss: 1.017503, acc: 0.640625]\n",
            "755: [D loss: 0.265831, acc: 0.917969]  [G loss: 0.561917, acc: 0.742188]\n",
            "756: [D loss: 0.314733, acc: 0.875000]  [G loss: 1.050814, acc: 0.593750]\n",
            "757: [D loss: 0.209763, acc: 0.929688]  [G loss: 1.480498, acc: 0.554688]\n",
            "758: [D loss: 0.273500, acc: 0.875000]  [G loss: 0.598937, acc: 0.718750]\n",
            "759: [D loss: 0.182265, acc: 0.910156]  [G loss: 0.338532, acc: 0.851562]\n",
            "760: [D loss: 0.273749, acc: 0.871094]  [G loss: 0.540460, acc: 0.789062]\n",
            "761: [D loss: 0.146885, acc: 0.945312]  [G loss: 1.158366, acc: 0.632812]\n",
            "762: [D loss: 0.144199, acc: 0.949219]  [G loss: 1.415335, acc: 0.648438]\n",
            "763: [D loss: 0.239084, acc: 0.917969]  [G loss: 1.124553, acc: 0.671875]\n",
            "764: [D loss: 0.121835, acc: 0.960938]  [G loss: 0.858322, acc: 0.726562]\n",
            "765: [D loss: 0.180202, acc: 0.921875]  [G loss: 0.663358, acc: 0.742188]\n",
            "766: [D loss: 0.255341, acc: 0.894531]  [G loss: 0.608590, acc: 0.750000]\n",
            "767: [D loss: 0.163287, acc: 0.937500]  [G loss: 0.915092, acc: 0.664062]\n",
            "768: [D loss: 0.110689, acc: 0.957031]  [G loss: 1.545786, acc: 0.437500]\n",
            "769: [D loss: 0.102675, acc: 0.960938]  [G loss: 2.171885, acc: 0.359375]\n",
            "770: [D loss: 0.147951, acc: 0.945312]  [G loss: 2.729673, acc: 0.195312]\n",
            "771: [D loss: 0.121161, acc: 0.953125]  [G loss: 2.978686, acc: 0.218750]\n",
            "772: [D loss: 0.200474, acc: 0.949219]  [G loss: 1.999521, acc: 0.351562]\n",
            "773: [D loss: 0.098371, acc: 0.957031]  [G loss: 1.881299, acc: 0.453125]\n",
            "774: [D loss: 0.093074, acc: 0.972656]  [G loss: 1.392117, acc: 0.546875]\n",
            "775: [D loss: 0.088415, acc: 0.972656]  [G loss: 1.926540, acc: 0.429688]\n",
            "776: [D loss: 0.164663, acc: 0.953125]  [G loss: 2.052395, acc: 0.453125]\n",
            "777: [D loss: 0.094633, acc: 0.980469]  [G loss: 2.543169, acc: 0.328125]\n",
            "778: [D loss: 0.111828, acc: 0.953125]  [G loss: 3.087817, acc: 0.335938]\n",
            "779: [D loss: 0.082740, acc: 0.960938]  [G loss: 2.943618, acc: 0.335938]\n",
            "780: [D loss: 0.095597, acc: 0.964844]  [G loss: 2.968011, acc: 0.257812]\n",
            "781: [D loss: 0.107034, acc: 0.972656]  [G loss: 2.880454, acc: 0.265625]\n",
            "782: [D loss: 0.088168, acc: 0.964844]  [G loss: 2.425558, acc: 0.351562]\n",
            "783: [D loss: 0.123386, acc: 0.941406]  [G loss: 2.150478, acc: 0.375000]\n",
            "784: [D loss: 0.078358, acc: 0.964844]  [G loss: 2.314424, acc: 0.390625]\n",
            "785: [D loss: 0.057217, acc: 0.980469]  [G loss: 2.738534, acc: 0.320312]\n",
            "786: [D loss: 0.069566, acc: 0.968750]  [G loss: 2.767875, acc: 0.359375]\n",
            "787: [D loss: 0.096798, acc: 0.968750]  [G loss: 2.674731, acc: 0.367188]\n",
            "788: [D loss: 0.084985, acc: 0.972656]  [G loss: 2.258333, acc: 0.484375]\n",
            "789: [D loss: 0.068017, acc: 0.980469]  [G loss: 2.536534, acc: 0.406250]\n",
            "790: [D loss: 0.058427, acc: 0.972656]  [G loss: 2.710862, acc: 0.398438]\n",
            "791: [D loss: 0.083410, acc: 0.968750]  [G loss: 2.014498, acc: 0.437500]\n",
            "792: [D loss: 0.086152, acc: 0.980469]  [G loss: 2.400525, acc: 0.421875]\n",
            "793: [D loss: 0.068014, acc: 0.968750]  [G loss: 3.133693, acc: 0.312500]\n",
            "794: [D loss: 0.093832, acc: 0.980469]  [G loss: 2.934128, acc: 0.359375]\n",
            "795: [D loss: 0.032795, acc: 0.988281]  [G loss: 3.545666, acc: 0.289062]\n",
            "796: [D loss: 0.061917, acc: 0.980469]  [G loss: 3.489894, acc: 0.281250]\n",
            "797: [D loss: 0.072175, acc: 0.992188]  [G loss: 3.138457, acc: 0.421875]\n",
            "798: [D loss: 0.103038, acc: 0.972656]  [G loss: 2.290015, acc: 0.562500]\n",
            "799: [D loss: 0.095855, acc: 0.972656]  [G loss: 1.634310, acc: 0.679688]\n",
            "800: [D loss: 0.150111, acc: 0.953125]  [G loss: 1.305672, acc: 0.687500]\n",
            "801: [D loss: 0.122265, acc: 0.960938]  [G loss: 2.059204, acc: 0.437500]\n",
            "802: [D loss: 0.092345, acc: 0.968750]  [G loss: 2.752769, acc: 0.343750]\n",
            "803: [D loss: 0.116710, acc: 0.976562]  [G loss: 2.792708, acc: 0.242188]\n",
            "804: [D loss: 0.121516, acc: 0.968750]  [G loss: 1.051833, acc: 0.625000]\n",
            "805: [D loss: 0.115220, acc: 0.949219]  [G loss: 0.864205, acc: 0.750000]\n",
            "806: [D loss: 0.113327, acc: 0.953125]  [G loss: 0.778483, acc: 0.734375]\n",
            "807: [D loss: 0.145946, acc: 0.933594]  [G loss: 1.125406, acc: 0.640625]\n",
            "808: [D loss: 0.178452, acc: 0.953125]  [G loss: 0.738235, acc: 0.710938]\n",
            "809: [D loss: 0.126383, acc: 0.960938]  [G loss: 0.819008, acc: 0.734375]\n",
            "810: [D loss: 0.121260, acc: 0.953125]  [G loss: 1.055402, acc: 0.671875]\n",
            "811: [D loss: 0.299234, acc: 0.921875]  [G loss: 0.519431, acc: 0.835938]\n",
            "812: [D loss: 0.169250, acc: 0.953125]  [G loss: 0.446631, acc: 0.804688]\n",
            "813: [D loss: 0.309776, acc: 0.898438]  [G loss: 1.074629, acc: 0.718750]\n",
            "814: [D loss: 0.272622, acc: 0.902344]  [G loss: 1.495457, acc: 0.562500]\n",
            "815: [D loss: 0.177716, acc: 0.941406]  [G loss: 1.326251, acc: 0.562500]\n",
            "816: [D loss: 0.202673, acc: 0.933594]  [G loss: 0.826633, acc: 0.679688]\n",
            "817: [D loss: 0.245079, acc: 0.945312]  [G loss: 0.548077, acc: 0.796875]\n",
            "818: [D loss: 0.300405, acc: 0.863281]  [G loss: 1.115022, acc: 0.625000]\n",
            "819: [D loss: 0.241177, acc: 0.910156]  [G loss: 1.655066, acc: 0.429688]\n",
            "820: [D loss: 0.305725, acc: 0.906250]  [G loss: 1.190401, acc: 0.570312]\n",
            "821: [D loss: 0.241935, acc: 0.910156]  [G loss: 1.013431, acc: 0.554688]\n",
            "822: [D loss: 0.279706, acc: 0.910156]  [G loss: 0.877186, acc: 0.617188]\n",
            "823: [D loss: 0.285927, acc: 0.871094]  [G loss: 0.808088, acc: 0.687500]\n",
            "824: [D loss: 0.345326, acc: 0.851562]  [G loss: 1.407626, acc: 0.398438]\n",
            "825: [D loss: 0.461498, acc: 0.808594]  [G loss: 0.996859, acc: 0.593750]\n",
            "826: [D loss: 0.501759, acc: 0.773438]  [G loss: 0.788986, acc: 0.664062]\n",
            "827: [D loss: 0.322362, acc: 0.871094]  [G loss: 1.217601, acc: 0.476562]\n",
            "828: [D loss: 0.395028, acc: 0.832031]  [G loss: 1.713147, acc: 0.335938]\n",
            "829: [D loss: 0.322598, acc: 0.863281]  [G loss: 2.038505, acc: 0.304688]\n",
            "830: [D loss: 0.216836, acc: 0.925781]  [G loss: 2.305374, acc: 0.289062]\n",
            "831: [D loss: 0.203639, acc: 0.910156]  [G loss: 1.915781, acc: 0.382812]\n",
            "832: [D loss: 0.175227, acc: 0.937500]  [G loss: 2.009833, acc: 0.375000]\n",
            "833: [D loss: 0.139860, acc: 0.953125]  [G loss: 1.826401, acc: 0.414062]\n",
            "834: [D loss: 0.104317, acc: 0.972656]  [G loss: 1.797706, acc: 0.453125]\n",
            "835: [D loss: 0.092354, acc: 0.980469]  [G loss: 1.668571, acc: 0.468750]\n",
            "836: [D loss: 0.070780, acc: 0.984375]  [G loss: 2.127869, acc: 0.382812]\n",
            "837: [D loss: 0.063048, acc: 0.992188]  [G loss: 1.735491, acc: 0.421875]\n",
            "838: [D loss: 0.054939, acc: 0.984375]  [G loss: 2.056234, acc: 0.359375]\n",
            "839: [D loss: 0.056091, acc: 0.992188]  [G loss: 1.943858, acc: 0.367188]\n",
            "840: [D loss: 0.039085, acc: 0.992188]  [G loss: 1.544726, acc: 0.453125]\n",
            "841: [D loss: 0.042155, acc: 0.984375]  [G loss: 1.359689, acc: 0.484375]\n",
            "842: [D loss: 0.035308, acc: 0.980469]  [G loss: 0.964001, acc: 0.578125]\n",
            "843: [D loss: 0.050255, acc: 0.984375]  [G loss: 0.903200, acc: 0.585938]\n",
            "844: [D loss: 0.089309, acc: 0.964844]  [G loss: 0.643225, acc: 0.710938]\n",
            "845: [D loss: 0.056222, acc: 0.980469]  [G loss: 0.570968, acc: 0.679688]\n",
            "846: [D loss: 0.088871, acc: 0.972656]  [G loss: 0.685827, acc: 0.664062]\n",
            "847: [D loss: 0.052224, acc: 0.980469]  [G loss: 0.709254, acc: 0.703125]\n",
            "848: [D loss: 0.050046, acc: 0.980469]  [G loss: 1.386312, acc: 0.570312]\n",
            "849: [D loss: 0.032030, acc: 0.988281]  [G loss: 1.434830, acc: 0.601562]\n",
            "850: [D loss: 0.073589, acc: 0.972656]  [G loss: 1.604147, acc: 0.625000]\n",
            "851: [D loss: 0.014692, acc: 0.992188]  [G loss: 1.512389, acc: 0.640625]\n",
            "852: [D loss: 0.091452, acc: 0.976562]  [G loss: 1.070312, acc: 0.703125]\n",
            "853: [D loss: 0.082198, acc: 0.972656]  [G loss: 0.442533, acc: 0.882812]\n",
            "854: [D loss: 0.127560, acc: 0.937500]  [G loss: 0.192776, acc: 0.929688]\n",
            "855: [D loss: 0.150603, acc: 0.937500]  [G loss: 0.651189, acc: 0.820312]\n",
            "856: [D loss: 0.100561, acc: 0.972656]  [G loss: 0.738656, acc: 0.781250]\n",
            "857: [D loss: 0.216650, acc: 0.968750]  [G loss: 0.855010, acc: 0.773438]\n",
            "858: [D loss: 0.182632, acc: 0.945312]  [G loss: 0.488628, acc: 0.859375]\n",
            "859: [D loss: 0.227194, acc: 0.906250]  [G loss: 0.981796, acc: 0.757812]\n",
            "860: [D loss: 0.132200, acc: 0.949219]  [G loss: 1.365136, acc: 0.742188]\n",
            "861: [D loss: 0.221129, acc: 0.949219]  [G loss: 1.328096, acc: 0.718750]\n",
            "862: [D loss: 0.214455, acc: 0.933594]  [G loss: 1.589668, acc: 0.703125]\n",
            "863: [D loss: 0.279289, acc: 0.910156]  [G loss: 1.497493, acc: 0.703125]\n",
            "864: [D loss: 0.149888, acc: 0.949219]  [G loss: 0.945455, acc: 0.757812]\n",
            "865: [D loss: 0.273673, acc: 0.906250]  [G loss: 0.996188, acc: 0.757812]\n",
            "866: [D loss: 0.272702, acc: 0.933594]  [G loss: 1.020538, acc: 0.742188]\n",
            "867: [D loss: 0.208341, acc: 0.925781]  [G loss: 2.242639, acc: 0.453125]\n",
            "868: [D loss: 0.323194, acc: 0.902344]  [G loss: 1.219899, acc: 0.687500]\n",
            "869: [D loss: 0.348187, acc: 0.902344]  [G loss: 1.026549, acc: 0.796875]\n",
            "870: [D loss: 0.215778, acc: 0.906250]  [G loss: 1.310981, acc: 0.757812]\n",
            "871: [D loss: 0.237723, acc: 0.910156]  [G loss: 1.059049, acc: 0.820312]\n",
            "872: [D loss: 0.152812, acc: 0.933594]  [G loss: 1.342423, acc: 0.796875]\n",
            "873: [D loss: 0.176730, acc: 0.929688]  [G loss: 1.280198, acc: 0.789062]\n",
            "874: [D loss: 0.136799, acc: 0.949219]  [G loss: 1.066447, acc: 0.812500]\n",
            "875: [D loss: 0.171639, acc: 0.910156]  [G loss: 0.863124, acc: 0.789062]\n",
            "876: [D loss: 0.148273, acc: 0.937500]  [G loss: 0.835565, acc: 0.796875]\n",
            "877: [D loss: 0.250268, acc: 0.882812]  [G loss: 0.769859, acc: 0.804688]\n",
            "878: [D loss: 0.205134, acc: 0.917969]  [G loss: 0.653149, acc: 0.781250]\n",
            "879: [D loss: 0.194320, acc: 0.914062]  [G loss: 0.715728, acc: 0.687500]\n",
            "880: [D loss: 0.143970, acc: 0.957031]  [G loss: 0.693605, acc: 0.679688]\n",
            "881: [D loss: 0.158358, acc: 0.937500]  [G loss: 0.766294, acc: 0.640625]\n",
            "882: [D loss: 0.167032, acc: 0.929688]  [G loss: 1.169163, acc: 0.492188]\n",
            "883: [D loss: 0.138940, acc: 0.933594]  [G loss: 0.948221, acc: 0.515625]\n",
            "884: [D loss: 0.156815, acc: 0.949219]  [G loss: 1.229559, acc: 0.429688]\n",
            "885: [D loss: 0.237631, acc: 0.906250]  [G loss: 1.116189, acc: 0.460938]\n",
            "886: [D loss: 0.264773, acc: 0.886719]  [G loss: 0.993465, acc: 0.562500]\n",
            "887: [D loss: 0.201411, acc: 0.917969]  [G loss: 0.864902, acc: 0.601562]\n",
            "888: [D loss: 0.187581, acc: 0.933594]  [G loss: 1.360265, acc: 0.406250]\n",
            "889: [D loss: 0.175411, acc: 0.937500]  [G loss: 1.933615, acc: 0.296875]\n",
            "890: [D loss: 0.144401, acc: 0.949219]  [G loss: 1.922276, acc: 0.234375]\n",
            "891: [D loss: 0.132608, acc: 0.960938]  [G loss: 2.373454, acc: 0.195312]\n",
            "892: [D loss: 0.172713, acc: 0.933594]  [G loss: 1.793866, acc: 0.359375]\n",
            "893: [D loss: 0.164092, acc: 0.945312]  [G loss: 1.360030, acc: 0.523438]\n",
            "894: [D loss: 0.177002, acc: 0.929688]  [G loss: 1.317064, acc: 0.500000]\n",
            "895: [D loss: 0.153236, acc: 0.953125]  [G loss: 1.351743, acc: 0.484375]\n",
            "896: [D loss: 0.163896, acc: 0.937500]  [G loss: 1.652176, acc: 0.359375]\n",
            "897: [D loss: 0.155887, acc: 0.945312]  [G loss: 2.399828, acc: 0.203125]\n",
            "898: [D loss: 0.151067, acc: 0.945312]  [G loss: 2.799145, acc: 0.109375]\n",
            "899: [D loss: 0.164404, acc: 0.937500]  [G loss: 2.296476, acc: 0.210938]\n",
            "900: [D loss: 0.214061, acc: 0.917969]  [G loss: 1.834578, acc: 0.351562]\n",
            "901: [D loss: 0.104329, acc: 0.972656]  [G loss: 1.533247, acc: 0.453125]\n",
            "902: [D loss: 0.118111, acc: 0.972656]  [G loss: 1.655126, acc: 0.398438]\n",
            "903: [D loss: 0.135455, acc: 0.937500]  [G loss: 2.281493, acc: 0.320312]\n",
            "904: [D loss: 0.128615, acc: 0.945312]  [G loss: 2.843090, acc: 0.273438]\n",
            "905: [D loss: 0.125556, acc: 0.953125]  [G loss: 2.926368, acc: 0.171875]\n",
            "906: [D loss: 0.142040, acc: 0.941406]  [G loss: 2.857027, acc: 0.226562]\n",
            "907: [D loss: 0.067117, acc: 0.976562]  [G loss: 2.585924, acc: 0.218750]\n",
            "908: [D loss: 0.215364, acc: 0.929688]  [G loss: 1.933420, acc: 0.343750]\n",
            "909: [D loss: 0.200943, acc: 0.929688]  [G loss: 1.925627, acc: 0.398438]\n",
            "910: [D loss: 0.072170, acc: 0.984375]  [G loss: 2.061678, acc: 0.312500]\n",
            "911: [D loss: 0.069675, acc: 0.984375]  [G loss: 2.508533, acc: 0.250000]\n",
            "912: [D loss: 0.102325, acc: 0.968750]  [G loss: 2.600297, acc: 0.210938]\n",
            "913: [D loss: 0.086047, acc: 0.957031]  [G loss: 2.924311, acc: 0.187500]\n",
            "914: [D loss: 0.161832, acc: 0.949219]  [G loss: 3.059994, acc: 0.156250]\n",
            "915: [D loss: 0.076262, acc: 0.976562]  [G loss: 2.613609, acc: 0.304688]\n",
            "916: [D loss: 0.137795, acc: 0.949219]  [G loss: 2.965559, acc: 0.234375]\n",
            "917: [D loss: 0.182885, acc: 0.933594]  [G loss: 3.955564, acc: 0.210938]\n",
            "918: [D loss: 0.246846, acc: 0.925781]  [G loss: 2.781522, acc: 0.289062]\n",
            "919: [D loss: 0.135864, acc: 0.949219]  [G loss: 2.556166, acc: 0.375000]\n",
            "920: [D loss: 0.197247, acc: 0.933594]  [G loss: 2.307096, acc: 0.460938]\n",
            "921: [D loss: 0.197485, acc: 0.925781]  [G loss: 2.889646, acc: 0.226562]\n",
            "922: [D loss: 0.132569, acc: 0.957031]  [G loss: 3.418844, acc: 0.210938]\n",
            "923: [D loss: 0.187059, acc: 0.929688]  [G loss: 3.046994, acc: 0.296875]\n",
            "924: [D loss: 0.104579, acc: 0.953125]  [G loss: 1.892672, acc: 0.445312]\n",
            "925: [D loss: 0.059231, acc: 0.980469]  [G loss: 1.445180, acc: 0.507812]\n",
            "926: [D loss: 0.094334, acc: 0.953125]  [G loss: 2.218527, acc: 0.250000]\n",
            "927: [D loss: 0.019661, acc: 1.000000]  [G loss: 3.278782, acc: 0.125000]\n",
            "928: [D loss: 0.070525, acc: 0.972656]  [G loss: 2.584391, acc: 0.218750]\n",
            "929: [D loss: 0.055313, acc: 0.976562]  [G loss: 1.845931, acc: 0.421875]\n",
            "930: [D loss: 0.059202, acc: 0.980469]  [G loss: 0.918675, acc: 0.609375]\n",
            "931: [D loss: 0.023730, acc: 0.992188]  [G loss: 0.437538, acc: 0.804688]\n",
            "932: [D loss: 0.033481, acc: 0.992188]  [G loss: 0.238066, acc: 0.914062]\n",
            "933: [D loss: 0.089098, acc: 0.964844]  [G loss: 0.603756, acc: 0.718750]\n",
            "934: [D loss: 0.017194, acc: 0.996094]  [G loss: 0.615470, acc: 0.703125]\n",
            "935: [D loss: 0.060078, acc: 0.988281]  [G loss: 0.830223, acc: 0.625000]\n",
            "936: [D loss: 0.038102, acc: 0.992188]  [G loss: 1.003006, acc: 0.632812]\n",
            "937: [D loss: 0.056030, acc: 0.972656]  [G loss: 0.604298, acc: 0.773438]\n",
            "938: [D loss: 0.076104, acc: 0.976562]  [G loss: 0.292762, acc: 0.875000]\n",
            "939: [D loss: 0.045183, acc: 0.988281]  [G loss: 0.239426, acc: 0.906250]\n",
            "940: [D loss: 0.121433, acc: 0.949219]  [G loss: 0.445337, acc: 0.812500]\n",
            "941: [D loss: 0.109093, acc: 0.964844]  [G loss: 0.924346, acc: 0.718750]\n",
            "942: [D loss: 0.217017, acc: 0.929688]  [G loss: 0.318286, acc: 0.835938]\n",
            "943: [D loss: 0.131875, acc: 0.964844]  [G loss: 0.146014, acc: 0.937500]\n",
            "944: [D loss: 0.217957, acc: 0.929688]  [G loss: 0.392820, acc: 0.859375]\n",
            "945: [D loss: 0.091274, acc: 0.964844]  [G loss: 0.645253, acc: 0.679688]\n",
            "946: [D loss: 0.201611, acc: 0.941406]  [G loss: 0.758148, acc: 0.703125]\n",
            "947: [D loss: 0.174469, acc: 0.960938]  [G loss: 0.445070, acc: 0.812500]\n",
            "948: [D loss: 0.131712, acc: 0.945312]  [G loss: 0.445402, acc: 0.851562]\n",
            "949: [D loss: 0.185460, acc: 0.949219]  [G loss: 0.573494, acc: 0.789062]\n",
            "950: [D loss: 0.249528, acc: 0.921875]  [G loss: 0.559712, acc: 0.804688]\n",
            "951: [D loss: 0.246701, acc: 0.906250]  [G loss: 0.700049, acc: 0.765625]\n",
            "952: [D loss: 0.193237, acc: 0.910156]  [G loss: 0.836093, acc: 0.765625]\n",
            "953: [D loss: 0.384590, acc: 0.894531]  [G loss: 0.704643, acc: 0.750000]\n",
            "954: [D loss: 0.251769, acc: 0.894531]  [G loss: 0.506604, acc: 0.859375]\n",
            "955: [D loss: 0.277372, acc: 0.910156]  [G loss: 0.619620, acc: 0.757812]\n",
            "956: [D loss: 0.138142, acc: 0.949219]  [G loss: 0.710170, acc: 0.742188]\n",
            "957: [D loss: 0.238018, acc: 0.902344]  [G loss: 0.746047, acc: 0.679688]\n",
            "958: [D loss: 0.184912, acc: 0.921875]  [G loss: 1.144014, acc: 0.593750]\n",
            "959: [D loss: 0.205528, acc: 0.929688]  [G loss: 1.204837, acc: 0.453125]\n",
            "960: [D loss: 0.181763, acc: 0.925781]  [G loss: 1.026048, acc: 0.609375]\n",
            "961: [D loss: 0.152929, acc: 0.957031]  [G loss: 1.080771, acc: 0.585938]\n",
            "962: [D loss: 0.136176, acc: 0.949219]  [G loss: 1.135937, acc: 0.554688]\n",
            "963: [D loss: 0.134594, acc: 0.941406]  [G loss: 1.282864, acc: 0.539062]\n",
            "964: [D loss: 0.127547, acc: 0.960938]  [G loss: 1.309744, acc: 0.554688]\n",
            "965: [D loss: 0.095631, acc: 0.976562]  [G loss: 1.101530, acc: 0.593750]\n",
            "966: [D loss: 0.155666, acc: 0.953125]  [G loss: 1.233275, acc: 0.617188]\n",
            "967: [D loss: 0.082622, acc: 0.980469]  [G loss: 1.068825, acc: 0.617188]\n",
            "968: [D loss: 0.120548, acc: 0.957031]  [G loss: 1.027088, acc: 0.687500]\n",
            "969: [D loss: 0.136682, acc: 0.949219]  [G loss: 0.822031, acc: 0.679688]\n",
            "970: [D loss: 0.114313, acc: 0.949219]  [G loss: 0.840187, acc: 0.656250]\n",
            "971: [D loss: 0.109300, acc: 0.972656]  [G loss: 0.874091, acc: 0.625000]\n",
            "972: [D loss: 0.165576, acc: 0.960938]  [G loss: 0.967216, acc: 0.640625]\n",
            "973: [D loss: 0.110820, acc: 0.953125]  [G loss: 1.142822, acc: 0.570312]\n",
            "974: [D loss: 0.150647, acc: 0.941406]  [G loss: 0.995146, acc: 0.578125]\n",
            "975: [D loss: 0.100226, acc: 0.972656]  [G loss: 0.994009, acc: 0.617188]\n",
            "976: [D loss: 0.085096, acc: 0.984375]  [G loss: 1.301099, acc: 0.578125]\n",
            "977: [D loss: 0.085435, acc: 0.984375]  [G loss: 1.478893, acc: 0.523438]\n",
            "978: [D loss: 0.138450, acc: 0.960938]  [G loss: 1.263906, acc: 0.609375]\n",
            "979: [D loss: 0.114895, acc: 0.972656]  [G loss: 0.991590, acc: 0.640625]\n",
            "980: [D loss: 0.091640, acc: 0.972656]  [G loss: 0.932334, acc: 0.640625]\n",
            "981: [D loss: 0.073694, acc: 0.984375]  [G loss: 0.901494, acc: 0.640625]\n",
            "982: [D loss: 0.092255, acc: 0.968750]  [G loss: 0.822606, acc: 0.679688]\n",
            "983: [D loss: 0.068744, acc: 0.988281]  [G loss: 0.791501, acc: 0.656250]\n",
            "984: [D loss: 0.068311, acc: 0.992188]  [G loss: 0.828529, acc: 0.656250]\n",
            "985: [D loss: 0.097739, acc: 0.972656]  [G loss: 1.120628, acc: 0.507812]\n",
            "986: [D loss: 0.050539, acc: 0.984375]  [G loss: 1.055237, acc: 0.546875]\n",
            "987: [D loss: 0.048270, acc: 0.988281]  [G loss: 1.002939, acc: 0.625000]\n",
            "988: [D loss: 0.073351, acc: 0.976562]  [G loss: 0.899217, acc: 0.609375]\n",
            "989: [D loss: 0.049350, acc: 0.980469]  [G loss: 1.384397, acc: 0.500000]\n",
            "990: [D loss: 0.027146, acc: 0.992188]  [G loss: 1.381648, acc: 0.562500]\n",
            "991: [D loss: 0.028430, acc: 0.996094]  [G loss: 1.640222, acc: 0.476562]\n",
            "992: [D loss: 0.027521, acc: 0.992188]  [G loss: 1.977735, acc: 0.382812]\n",
            "993: [D loss: 0.037877, acc: 0.988281]  [G loss: 2.282963, acc: 0.398438]\n",
            "994: [D loss: 0.038047, acc: 0.992188]  [G loss: 2.061085, acc: 0.375000]\n",
            "995: [D loss: 0.034738, acc: 0.992188]  [G loss: 2.243431, acc: 0.343750]\n",
            "996: [D loss: 0.024363, acc: 0.992188]  [G loss: 2.192426, acc: 0.359375]\n",
            "997: [D loss: 0.080443, acc: 0.976562]  [G loss: 2.154115, acc: 0.445312]\n",
            "998: [D loss: 0.060855, acc: 0.980469]  [G loss: 2.206204, acc: 0.437500]\n",
            "999: [D loss: 0.084485, acc: 0.980469]  [G loss: 2.351981, acc: 0.367188]\n",
            "1000: [D loss: 0.087210, acc: 0.972656]  [G loss: 2.340446, acc: 0.390625]\n",
            "1001: [D loss: 0.067334, acc: 0.980469]  [G loss: 2.368222, acc: 0.375000]\n",
            "1002: [D loss: 0.035524, acc: 0.992188]  [G loss: 2.466848, acc: 0.312500]\n",
            "1003: [D loss: 0.061371, acc: 0.988281]  [G loss: 3.223203, acc: 0.195312]\n",
            "1004: [D loss: 0.094290, acc: 0.968750]  [G loss: 3.156250, acc: 0.210938]\n",
            "1005: [D loss: 0.070796, acc: 0.972656]  [G loss: 3.584110, acc: 0.203125]\n",
            "1006: [D loss: 0.097145, acc: 0.964844]  [G loss: 3.619634, acc: 0.171875]\n",
            "1007: [D loss: 0.097220, acc: 0.964844]  [G loss: 3.652085, acc: 0.101562]\n",
            "1008: [D loss: 0.084394, acc: 0.968750]  [G loss: 4.105317, acc: 0.085938]\n",
            "1009: [D loss: 0.049623, acc: 0.984375]  [G loss: 4.683196, acc: 0.132812]\n",
            "1010: [D loss: 0.054916, acc: 0.980469]  [G loss: 3.899207, acc: 0.187500]\n",
            "1011: [D loss: 0.059988, acc: 0.976562]  [G loss: 3.362789, acc: 0.234375]\n",
            "1012: [D loss: 0.068604, acc: 0.972656]  [G loss: 3.751737, acc: 0.257812]\n",
            "1013: [D loss: 0.090069, acc: 0.937500]  [G loss: 3.459827, acc: 0.312500]\n",
            "1014: [D loss: 0.070077, acc: 0.980469]  [G loss: 3.729825, acc: 0.218750]\n",
            "1015: [D loss: 0.038730, acc: 0.988281]  [G loss: 3.743982, acc: 0.289062]\n",
            "1016: [D loss: 0.080218, acc: 0.984375]  [G loss: 3.493659, acc: 0.304688]\n",
            "1017: [D loss: 0.065164, acc: 0.984375]  [G loss: 2.266471, acc: 0.476562]\n",
            "1018: [D loss: 0.090213, acc: 0.980469]  [G loss: 1.691690, acc: 0.664062]\n",
            "1019: [D loss: 0.143644, acc: 0.945312]  [G loss: 1.591324, acc: 0.648438]\n",
            "1020: [D loss: 0.137072, acc: 0.953125]  [G loss: 1.498706, acc: 0.656250]\n",
            "1021: [D loss: 0.103843, acc: 0.964844]  [G loss: 1.366144, acc: 0.656250]\n",
            "1022: [D loss: 0.103657, acc: 0.960938]  [G loss: 1.244218, acc: 0.703125]\n",
            "1023: [D loss: 0.165128, acc: 0.949219]  [G loss: 0.960033, acc: 0.742188]\n",
            "1024: [D loss: 0.161595, acc: 0.957031]  [G loss: 0.758383, acc: 0.773438]\n",
            "1025: [D loss: 0.471821, acc: 0.890625]  [G loss: 1.149697, acc: 0.703125]\n",
            "1026: [D loss: 0.257278, acc: 0.925781]  [G loss: 1.682574, acc: 0.671875]\n",
            "1027: [D loss: 0.229806, acc: 0.921875]  [G loss: 1.208532, acc: 0.687500]\n",
            "1028: [D loss: 0.525761, acc: 0.878906]  [G loss: 0.753339, acc: 0.757812]\n",
            "1029: [D loss: 0.395834, acc: 0.871094]  [G loss: 0.838345, acc: 0.765625]\n",
            "1030: [D loss: 0.567729, acc: 0.835938]  [G loss: 2.617534, acc: 0.343750]\n",
            "1031: [D loss: 0.389955, acc: 0.917969]  [G loss: 3.669830, acc: 0.242188]\n",
            "1032: [D loss: 0.301995, acc: 0.929688]  [G loss: 3.228818, acc: 0.289062]\n",
            "1033: [D loss: 0.126907, acc: 0.957031]  [G loss: 2.612987, acc: 0.460938]\n",
            "1034: [D loss: 0.110439, acc: 0.949219]  [G loss: 2.014846, acc: 0.546875]\n",
            "1035: [D loss: 0.219720, acc: 0.914062]  [G loss: 1.601729, acc: 0.593750]\n",
            "1036: [D loss: 0.138906, acc: 0.949219]  [G loss: 2.389457, acc: 0.460938]\n",
            "1037: [D loss: 0.211156, acc: 0.925781]  [G loss: 2.830821, acc: 0.398438]\n",
            "1038: [D loss: 0.228062, acc: 0.921875]  [G loss: 2.612738, acc: 0.445312]\n",
            "1039: [D loss: 0.134747, acc: 0.964844]  [G loss: 2.437495, acc: 0.429688]\n",
            "1040: [D loss: 0.139035, acc: 0.929688]  [G loss: 1.906723, acc: 0.539062]\n",
            "1041: [D loss: 0.094506, acc: 0.968750]  [G loss: 1.286839, acc: 0.617188]\n",
            "1042: [D loss: 0.078453, acc: 0.972656]  [G loss: 1.258354, acc: 0.601562]\n",
            "1043: [D loss: 0.112828, acc: 0.964844]  [G loss: 1.294021, acc: 0.609375]\n",
            "1044: [D loss: 0.126453, acc: 0.968750]  [G loss: 2.123542, acc: 0.492188]\n",
            "1045: [D loss: 0.087412, acc: 0.972656]  [G loss: 2.427136, acc: 0.398438]\n",
            "1046: [D loss: 0.229621, acc: 0.941406]  [G loss: 0.972497, acc: 0.632812]\n",
            "1047: [D loss: 0.133098, acc: 0.953125]  [G loss: 0.378269, acc: 0.867188]\n",
            "1048: [D loss: 0.110254, acc: 0.972656]  [G loss: 0.110452, acc: 0.960938]\n",
            "1049: [D loss: 0.184362, acc: 0.914062]  [G loss: 0.239354, acc: 0.875000]\n",
            "1050: [D loss: 0.210040, acc: 0.929688]  [G loss: 0.661500, acc: 0.742188]\n",
            "1051: [D loss: 0.148256, acc: 0.941406]  [G loss: 0.822146, acc: 0.710938]\n",
            "1052: [D loss: 0.160234, acc: 0.949219]  [G loss: 0.530762, acc: 0.820312]\n",
            "1053: [D loss: 0.123558, acc: 0.960938]  [G loss: 0.289666, acc: 0.867188]\n",
            "1054: [D loss: 0.090498, acc: 0.968750]  [G loss: 0.202437, acc: 0.937500]\n",
            "1055: [D loss: 0.124585, acc: 0.957031]  [G loss: 0.230917, acc: 0.882812]\n",
            "1056: [D loss: 0.110175, acc: 0.953125]  [G loss: 0.309176, acc: 0.867188]\n",
            "1057: [D loss: 0.114304, acc: 0.960938]  [G loss: 0.399897, acc: 0.804688]\n",
            "1058: [D loss: 0.107758, acc: 0.964844]  [G loss: 0.433521, acc: 0.835938]\n",
            "1059: [D loss: 0.076696, acc: 0.976562]  [G loss: 0.459391, acc: 0.789062]\n",
            "1060: [D loss: 0.110449, acc: 0.949219]  [G loss: 0.344699, acc: 0.875000]\n",
            "1061: [D loss: 0.071063, acc: 0.984375]  [G loss: 0.486264, acc: 0.796875]\n",
            "1062: [D loss: 0.076423, acc: 0.976562]  [G loss: 0.321982, acc: 0.875000]\n",
            "1063: [D loss: 0.067310, acc: 0.980469]  [G loss: 0.382410, acc: 0.796875]\n",
            "1064: [D loss: 0.103396, acc: 0.964844]  [G loss: 0.667854, acc: 0.742188]\n",
            "1065: [D loss: 0.122131, acc: 0.945312]  [G loss: 0.404549, acc: 0.843750]\n",
            "1066: [D loss: 0.085677, acc: 0.988281]  [G loss: 0.487111, acc: 0.773438]\n",
            "1067: [D loss: 0.074677, acc: 0.968750]  [G loss: 0.474965, acc: 0.796875]\n",
            "1068: [D loss: 0.058459, acc: 0.984375]  [G loss: 0.426340, acc: 0.789062]\n",
            "1069: [D loss: 0.082115, acc: 0.972656]  [G loss: 0.481125, acc: 0.843750]\n",
            "1070: [D loss: 0.070774, acc: 0.972656]  [G loss: 0.630976, acc: 0.773438]\n",
            "1071: [D loss: 0.079114, acc: 0.957031]  [G loss: 0.555747, acc: 0.773438]\n",
            "1072: [D loss: 0.062542, acc: 0.980469]  [G loss: 0.597214, acc: 0.773438]\n",
            "1073: [D loss: 0.070004, acc: 0.980469]  [G loss: 0.736150, acc: 0.664062]\n",
            "1074: [D loss: 0.068894, acc: 0.980469]  [G loss: 0.590219, acc: 0.789062]\n",
            "1075: [D loss: 0.107863, acc: 0.957031]  [G loss: 0.462726, acc: 0.828125]\n",
            "1076: [D loss: 0.118061, acc: 0.949219]  [G loss: 0.327915, acc: 0.875000]\n",
            "1077: [D loss: 0.105965, acc: 0.953125]  [G loss: 0.314630, acc: 0.875000]\n",
            "1078: [D loss: 0.087184, acc: 0.976562]  [G loss: 0.543311, acc: 0.796875]\n",
            "1079: [D loss: 0.133042, acc: 0.957031]  [G loss: 0.819285, acc: 0.710938]\n",
            "1080: [D loss: 0.063907, acc: 0.976562]  [G loss: 1.205692, acc: 0.546875]\n",
            "1081: [D loss: 0.167803, acc: 0.949219]  [G loss: 0.906461, acc: 0.625000]\n",
            "1082: [D loss: 0.176560, acc: 0.941406]  [G loss: 0.670779, acc: 0.703125]\n",
            "1083: [D loss: 0.092318, acc: 0.964844]  [G loss: 0.467224, acc: 0.796875]\n",
            "1084: [D loss: 0.217590, acc: 0.898438]  [G loss: 0.404642, acc: 0.835938]\n",
            "1085: [D loss: 0.156324, acc: 0.937500]  [G loss: 0.909256, acc: 0.625000]\n",
            "1086: [D loss: 0.211282, acc: 0.925781]  [G loss: 1.701758, acc: 0.421875]\n",
            "1087: [D loss: 0.162197, acc: 0.937500]  [G loss: 1.432149, acc: 0.476562]\n",
            "1088: [D loss: 0.274504, acc: 0.917969]  [G loss: 0.725326, acc: 0.710938]\n",
            "1089: [D loss: 0.249720, acc: 0.902344]  [G loss: 0.520184, acc: 0.812500]\n",
            "1090: [D loss: 0.225238, acc: 0.914062]  [G loss: 0.926710, acc: 0.656250]\n",
            "1091: [D loss: 0.165713, acc: 0.921875]  [G loss: 1.493689, acc: 0.562500]\n",
            "1092: [D loss: 0.145495, acc: 0.949219]  [G loss: 2.142817, acc: 0.382812]\n",
            "1093: [D loss: 0.107622, acc: 0.949219]  [G loss: 3.094824, acc: 0.234375]\n",
            "1094: [D loss: 0.117364, acc: 0.976562]  [G loss: 3.114391, acc: 0.257812]\n",
            "1095: [D loss: 0.182663, acc: 0.933594]  [G loss: 1.837282, acc: 0.382812]\n",
            "1096: [D loss: 0.095926, acc: 0.957031]  [G loss: 1.805634, acc: 0.460938]\n",
            "1097: [D loss: 0.122213, acc: 0.960938]  [G loss: 2.914793, acc: 0.210938]\n",
            "1098: [D loss: 0.142409, acc: 0.957031]  [G loss: 3.042692, acc: 0.242188]\n",
            "1099: [D loss: 0.063219, acc: 0.980469]  [G loss: 3.879933, acc: 0.117188]\n",
            "1100: [D loss: 0.085187, acc: 0.964844]  [G loss: 4.389852, acc: 0.203125]\n",
            "1101: [D loss: 0.057847, acc: 0.976562]  [G loss: 4.644832, acc: 0.140625]\n",
            "1102: [D loss: 0.073427, acc: 0.964844]  [G loss: 5.537265, acc: 0.125000]\n",
            "1103: [D loss: 0.085214, acc: 0.964844]  [G loss: 5.659745, acc: 0.101562]\n",
            "1104: [D loss: 0.072816, acc: 0.976562]  [G loss: 5.451926, acc: 0.125000]\n",
            "1105: [D loss: 0.113811, acc: 0.953125]  [G loss: 5.099727, acc: 0.179688]\n",
            "1106: [D loss: 0.079450, acc: 0.976562]  [G loss: 4.283741, acc: 0.171875]\n",
            "1107: [D loss: 0.110874, acc: 0.968750]  [G loss: 4.085884, acc: 0.179688]\n",
            "1108: [D loss: 0.081214, acc: 0.972656]  [G loss: 4.757813, acc: 0.125000]\n",
            "1109: [D loss: 0.056004, acc: 0.976562]  [G loss: 5.881483, acc: 0.070312]\n",
            "1110: [D loss: 0.122929, acc: 0.976562]  [G loss: 5.836192, acc: 0.046875]\n",
            "1111: [D loss: 0.111254, acc: 0.972656]  [G loss: 4.309383, acc: 0.156250]\n",
            "1112: [D loss: 0.132366, acc: 0.964844]  [G loss: 2.131675, acc: 0.406250]\n",
            "1113: [D loss: 0.077919, acc: 0.988281]  [G loss: 0.561932, acc: 0.773438]\n",
            "1114: [D loss: 0.124338, acc: 0.949219]  [G loss: 1.060380, acc: 0.648438]\n",
            "1115: [D loss: 0.020427, acc: 0.992188]  [G loss: 1.791782, acc: 0.531250]\n",
            "1116: [D loss: 0.045615, acc: 0.996094]  [G loss: 1.487907, acc: 0.632812]\n",
            "1117: [D loss: 0.048747, acc: 0.988281]  [G loss: 1.917949, acc: 0.554688]\n",
            "1118: [D loss: 0.020819, acc: 0.992188]  [G loss: 1.452044, acc: 0.640625]\n",
            "1119: [D loss: 0.008963, acc: 0.996094]  [G loss: 1.209523, acc: 0.679688]\n",
            "1120: [D loss: 0.044733, acc: 0.984375]  [G loss: 0.494465, acc: 0.820312]\n",
            "1121: [D loss: 0.058022, acc: 0.984375]  [G loss: 0.227289, acc: 0.929688]\n",
            "1122: [D loss: 0.056800, acc: 0.980469]  [G loss: 0.116947, acc: 0.976562]\n",
            "1123: [D loss: 0.085168, acc: 0.984375]  [G loss: 0.103013, acc: 0.976562]\n",
            "1124: [D loss: 0.032180, acc: 0.988281]  [G loss: 0.535957, acc: 0.789062]\n",
            "1125: [D loss: 0.062043, acc: 0.992188]  [G loss: 0.797869, acc: 0.703125]\n",
            "1126: [D loss: 0.005788, acc: 1.000000]  [G loss: 0.825088, acc: 0.671875]\n",
            "1127: [D loss: 0.080874, acc: 0.984375]  [G loss: 0.989165, acc: 0.585938]\n",
            "1128: [D loss: 0.070950, acc: 0.988281]  [G loss: 0.644403, acc: 0.734375]\n",
            "1129: [D loss: 0.030186, acc: 0.988281]  [G loss: 0.306468, acc: 0.875000]\n",
            "1130: [D loss: 0.048281, acc: 0.992188]  [G loss: 0.105710, acc: 0.945312]\n",
            "1131: [D loss: 0.090133, acc: 0.968750]  [G loss: 0.068174, acc: 0.984375]\n",
            "1132: [D loss: 0.044630, acc: 0.976562]  [G loss: 0.074732, acc: 0.984375]\n",
            "1133: [D loss: 0.044833, acc: 0.992188]  [G loss: 0.122657, acc: 0.945312]\n",
            "1134: [D loss: 0.033782, acc: 0.992188]  [G loss: 0.119601, acc: 0.953125]\n",
            "1135: [D loss: 0.025647, acc: 0.992188]  [G loss: 0.111857, acc: 0.960938]\n",
            "1136: [D loss: 0.051651, acc: 0.980469]  [G loss: 0.112388, acc: 0.960938]\n",
            "1137: [D loss: 0.014846, acc: 1.000000]  [G loss: 0.037897, acc: 0.992188]\n",
            "1138: [D loss: 0.013327, acc: 0.996094]  [G loss: 0.054263, acc: 0.976562]\n",
            "1139: [D loss: 0.048517, acc: 0.988281]  [G loss: 0.061349, acc: 0.968750]\n",
            "1140: [D loss: 0.007156, acc: 1.000000]  [G loss: 0.055811, acc: 0.992188]\n",
            "1141: [D loss: 0.041480, acc: 0.984375]  [G loss: 0.056365, acc: 0.976562]\n",
            "1142: [D loss: 0.021767, acc: 0.996094]  [G loss: 0.026128, acc: 0.992188]\n",
            "1143: [D loss: 0.030942, acc: 0.992188]  [G loss: 0.041124, acc: 0.976562]\n",
            "1144: [D loss: 0.031523, acc: 0.984375]  [G loss: 0.045047, acc: 0.984375]\n",
            "1145: [D loss: 0.027078, acc: 0.992188]  [G loss: 0.066958, acc: 0.976562]\n",
            "1146: [D loss: 0.024454, acc: 0.988281]  [G loss: 0.027401, acc: 0.992188]\n",
            "1147: [D loss: 0.016166, acc: 0.996094]  [G loss: 0.063864, acc: 0.984375]\n",
            "1148: [D loss: 0.029938, acc: 0.992188]  [G loss: 0.066866, acc: 0.992188]\n",
            "1149: [D loss: 0.019258, acc: 0.996094]  [G loss: 0.035628, acc: 1.000000]\n",
            "1150: [D loss: 0.030826, acc: 0.992188]  [G loss: 0.046177, acc: 0.984375]\n",
            "1151: [D loss: 0.020793, acc: 0.988281]  [G loss: 0.065597, acc: 0.976562]\n",
            "1152: [D loss: 0.031482, acc: 0.988281]  [G loss: 0.043537, acc: 0.992188]\n",
            "1153: [D loss: 0.029577, acc: 0.988281]  [G loss: 0.059224, acc: 0.984375]\n",
            "1154: [D loss: 0.020151, acc: 0.996094]  [G loss: 0.103087, acc: 0.960938]\n",
            "1155: [D loss: 0.040414, acc: 0.984375]  [G loss: 0.104990, acc: 0.968750]\n",
            "1156: [D loss: 0.024382, acc: 0.992188]  [G loss: 0.250230, acc: 0.898438]\n",
            "1157: [D loss: 0.054037, acc: 0.988281]  [G loss: 0.176666, acc: 0.953125]\n",
            "1158: [D loss: 0.037337, acc: 0.996094]  [G loss: 0.147041, acc: 0.945312]\n",
            "1159: [D loss: 0.081031, acc: 0.988281]  [G loss: 0.216779, acc: 0.898438]\n",
            "1160: [D loss: 0.049646, acc: 0.984375]  [G loss: 0.316613, acc: 0.929688]\n",
            "1161: [D loss: 0.052269, acc: 0.988281]  [G loss: 0.243441, acc: 0.929688]\n",
            "1162: [D loss: 0.048195, acc: 0.976562]  [G loss: 0.461967, acc: 0.843750]\n",
            "1163: [D loss: 0.041554, acc: 0.980469]  [G loss: 0.648156, acc: 0.773438]\n",
            "1164: [D loss: 0.033094, acc: 0.992188]  [G loss: 0.876700, acc: 0.664062]\n",
            "1165: [D loss: 0.130082, acc: 0.964844]  [G loss: 0.926372, acc: 0.687500]\n",
            "1166: [D loss: 0.089012, acc: 0.984375]  [G loss: 0.625711, acc: 0.718750]\n",
            "1167: [D loss: 0.069887, acc: 0.968750]  [G loss: 0.754183, acc: 0.664062]\n",
            "1168: [D loss: 0.070505, acc: 0.988281]  [G loss: 0.957682, acc: 0.578125]\n",
            "1169: [D loss: 0.145181, acc: 0.964844]  [G loss: 0.973021, acc: 0.554688]\n",
            "1170: [D loss: 0.099246, acc: 0.968750]  [G loss: 0.995880, acc: 0.578125]\n",
            "1171: [D loss: 0.117877, acc: 0.960938]  [G loss: 1.007298, acc: 0.484375]\n",
            "1172: [D loss: 0.093585, acc: 0.972656]  [G loss: 1.305028, acc: 0.484375]\n",
            "1173: [D loss: 0.144361, acc: 0.964844]  [G loss: 1.280578, acc: 0.507812]\n",
            "1174: [D loss: 0.189444, acc: 0.929688]  [G loss: 0.866862, acc: 0.609375]\n",
            "1175: [D loss: 0.127483, acc: 0.945312]  [G loss: 1.005995, acc: 0.601562]\n",
            "1176: [D loss: 0.156549, acc: 0.933594]  [G loss: 1.152510, acc: 0.539062]\n",
            "1177: [D loss: 0.113313, acc: 0.968750]  [G loss: 1.174973, acc: 0.531250]\n",
            "1178: [D loss: 0.152463, acc: 0.957031]  [G loss: 1.524954, acc: 0.437500]\n",
            "1179: [D loss: 0.106227, acc: 0.972656]  [G loss: 1.268153, acc: 0.492188]\n",
            "1180: [D loss: 0.097552, acc: 0.976562]  [G loss: 1.253233, acc: 0.406250]\n",
            "1181: [D loss: 0.069870, acc: 0.988281]  [G loss: 1.436397, acc: 0.328125]\n",
            "1182: [D loss: 0.066602, acc: 0.984375]  [G loss: 1.549620, acc: 0.320312]\n",
            "1183: [D loss: 0.044610, acc: 0.992188]  [G loss: 1.957334, acc: 0.234375]\n",
            "1184: [D loss: 0.073707, acc: 0.972656]  [G loss: 2.144919, acc: 0.226562]\n",
            "1185: [D loss: 0.072645, acc: 0.972656]  [G loss: 2.006182, acc: 0.312500]\n",
            "1186: [D loss: 0.064618, acc: 0.980469]  [G loss: 2.095603, acc: 0.265625]\n",
            "1187: [D loss: 0.062371, acc: 0.964844]  [G loss: 1.897343, acc: 0.390625]\n",
            "1188: [D loss: 0.058548, acc: 0.972656]  [G loss: 1.678699, acc: 0.460938]\n",
            "1189: [D loss: 0.039504, acc: 0.984375]  [G loss: 2.002546, acc: 0.414062]\n",
            "1190: [D loss: 0.036909, acc: 0.988281]  [G loss: 2.326156, acc: 0.390625]\n",
            "1191: [D loss: 0.042727, acc: 0.988281]  [G loss: 2.052966, acc: 0.406250]\n",
            "1192: [D loss: 0.079722, acc: 0.976562]  [G loss: 1.958407, acc: 0.437500]\n",
            "1193: [D loss: 0.033756, acc: 0.992188]  [G loss: 1.556934, acc: 0.539062]\n",
            "1194: [D loss: 0.035705, acc: 0.988281]  [G loss: 1.523815, acc: 0.593750]\n",
            "1195: [D loss: 0.020263, acc: 1.000000]  [G loss: 1.682321, acc: 0.460938]\n",
            "1196: [D loss: 0.065867, acc: 0.980469]  [G loss: 1.591161, acc: 0.523438]\n",
            "1197: [D loss: 0.122504, acc: 0.984375]  [G loss: 1.477715, acc: 0.546875]\n",
            "1198: [D loss: 0.141499, acc: 0.972656]  [G loss: 1.352638, acc: 0.570312]\n",
            "1199: [D loss: 0.184067, acc: 0.964844]  [G loss: 1.018580, acc: 0.718750]\n",
            "1200: [D loss: 0.174083, acc: 0.933594]  [G loss: 1.909737, acc: 0.515625]\n",
            "1201: [D loss: 0.236503, acc: 0.933594]  [G loss: 2.380458, acc: 0.445312]\n",
            "1202: [D loss: 0.144039, acc: 0.953125]  [G loss: 2.399201, acc: 0.359375]\n",
            "1203: [D loss: 0.134633, acc: 0.964844]  [G loss: 2.143272, acc: 0.453125]\n",
            "1204: [D loss: 0.416299, acc: 0.898438]  [G loss: 1.503384, acc: 0.515625]\n",
            "1205: [D loss: 0.252251, acc: 0.898438]  [G loss: 3.179379, acc: 0.320312]\n",
            "1206: [D loss: 0.171799, acc: 0.953125]  [G loss: 4.491423, acc: 0.242188]\n",
            "1207: [D loss: 0.212762, acc: 0.925781]  [G loss: 3.548120, acc: 0.343750]\n",
            "1208: [D loss: 0.156313, acc: 0.933594]  [G loss: 2.602950, acc: 0.460938]\n",
            "1209: [D loss: 0.368442, acc: 0.886719]  [G loss: 2.540681, acc: 0.468750]\n",
            "1210: [D loss: 0.286513, acc: 0.914062]  [G loss: 3.295476, acc: 0.359375]\n",
            "1211: [D loss: 0.138597, acc: 0.964844]  [G loss: 4.045793, acc: 0.171875]\n",
            "1212: [D loss: 0.123565, acc: 0.945312]  [G loss: 2.766822, acc: 0.312500]\n",
            "1213: [D loss: 0.311690, acc: 0.875000]  [G loss: 2.130755, acc: 0.359375]\n",
            "1214: [D loss: 0.244714, acc: 0.914062]  [G loss: 2.620140, acc: 0.296875]\n",
            "1215: [D loss: 0.267326, acc: 0.890625]  [G loss: 3.871194, acc: 0.179688]\n",
            "1216: [D loss: 0.349278, acc: 0.882812]  [G loss: 3.045514, acc: 0.343750]\n",
            "1217: [D loss: 0.191307, acc: 0.933594]  [G loss: 1.346148, acc: 0.648438]\n",
            "1218: [D loss: 0.411236, acc: 0.871094]  [G loss: 2.158259, acc: 0.468750]\n",
            "1219: [D loss: 0.309083, acc: 0.902344]  [G loss: 2.809188, acc: 0.453125]\n",
            "1220: [D loss: 0.274158, acc: 0.902344]  [G loss: 1.817212, acc: 0.562500]\n",
            "1221: [D loss: 0.179455, acc: 0.933594]  [G loss: 1.590386, acc: 0.640625]\n",
            "1222: [D loss: 0.243351, acc: 0.921875]  [G loss: 1.563114, acc: 0.625000]\n",
            "1223: [D loss: 0.174947, acc: 0.937500]  [G loss: 1.518646, acc: 0.625000]\n",
            "1224: [D loss: 0.093755, acc: 0.953125]  [G loss: 1.591215, acc: 0.593750]\n",
            "1225: [D loss: 0.121656, acc: 0.941406]  [G loss: 1.516600, acc: 0.578125]\n",
            "1226: [D loss: 0.094499, acc: 0.960938]  [G loss: 1.467748, acc: 0.640625]\n",
            "1227: [D loss: 0.087649, acc: 0.964844]  [G loss: 1.335071, acc: 0.640625]\n",
            "1228: [D loss: 0.116834, acc: 0.949219]  [G loss: 1.233377, acc: 0.625000]\n",
            "1229: [D loss: 0.124278, acc: 0.949219]  [G loss: 1.334012, acc: 0.648438]\n",
            "1230: [D loss: 0.101675, acc: 0.976562]  [G loss: 1.306846, acc: 0.640625]\n",
            "1231: [D loss: 0.146051, acc: 0.949219]  [G loss: 1.274038, acc: 0.609375]\n",
            "1232: [D loss: 0.138582, acc: 0.957031]  [G loss: 0.798618, acc: 0.750000]\n",
            "1233: [D loss: 0.113913, acc: 0.949219]  [G loss: 0.953126, acc: 0.710938]\n",
            "1234: [D loss: 0.116131, acc: 0.960938]  [G loss: 1.588308, acc: 0.593750]\n",
            "1235: [D loss: 0.062364, acc: 0.972656]  [G loss: 2.137773, acc: 0.562500]\n",
            "1236: [D loss: 0.161158, acc: 0.953125]  [G loss: 2.024415, acc: 0.570312]\n",
            "1237: [D loss: 0.069266, acc: 0.968750]  [G loss: 1.678617, acc: 0.609375]\n",
            "1238: [D loss: 0.110140, acc: 0.976562]  [G loss: 1.289482, acc: 0.671875]\n",
            "1239: [D loss: 0.071447, acc: 0.972656]  [G loss: 1.419206, acc: 0.648438]\n",
            "1240: [D loss: 0.175096, acc: 0.945312]  [G loss: 1.527000, acc: 0.593750]\n",
            "1241: [D loss: 0.108262, acc: 0.957031]  [G loss: 2.283405, acc: 0.500000]\n",
            "1242: [D loss: 0.147465, acc: 0.953125]  [G loss: 2.862004, acc: 0.351562]\n",
            "1243: [D loss: 0.112543, acc: 0.960938]  [G loss: 2.492760, acc: 0.375000]\n",
            "1244: [D loss: 0.094712, acc: 0.968750]  [G loss: 2.021219, acc: 0.546875]\n",
            "1245: [D loss: 0.101880, acc: 0.976562]  [G loss: 1.167693, acc: 0.695312]\n",
            "1246: [D loss: 0.128763, acc: 0.957031]  [G loss: 0.938089, acc: 0.789062]\n",
            "1247: [D loss: 0.175594, acc: 0.914062]  [G loss: 1.479742, acc: 0.578125]\n",
            "1248: [D loss: 0.085084, acc: 0.972656]  [G loss: 2.335776, acc: 0.398438]\n",
            "1249: [D loss: 0.097823, acc: 0.960938]  [G loss: 2.753593, acc: 0.359375]\n",
            "1250: [D loss: 0.084415, acc: 0.980469]  [G loss: 2.760154, acc: 0.328125]\n",
            "1251: [D loss: 0.146115, acc: 0.957031]  [G loss: 1.768546, acc: 0.515625]\n",
            "1252: [D loss: 0.172709, acc: 0.957031]  [G loss: 1.171192, acc: 0.687500]\n",
            "1253: [D loss: 0.146484, acc: 0.933594]  [G loss: 0.901559, acc: 0.742188]\n",
            "1254: [D loss: 0.165403, acc: 0.929688]  [G loss: 1.798261, acc: 0.515625]\n",
            "1255: [D loss: 0.071575, acc: 0.964844]  [G loss: 2.996469, acc: 0.304688]\n",
            "1256: [D loss: 0.105758, acc: 0.964844]  [G loss: 3.299133, acc: 0.257812]\n",
            "1257: [D loss: 0.105491, acc: 0.957031]  [G loss: 3.272596, acc: 0.289062]\n",
            "1258: [D loss: 0.114100, acc: 0.964844]  [G loss: 2.187787, acc: 0.468750]\n",
            "1259: [D loss: 0.069340, acc: 0.980469]  [G loss: 1.670283, acc: 0.570312]\n",
            "1260: [D loss: 0.144570, acc: 0.953125]  [G loss: 1.453543, acc: 0.546875]\n",
            "1261: [D loss: 0.104126, acc: 0.972656]  [G loss: 1.760291, acc: 0.523438]\n",
            "1262: [D loss: 0.090569, acc: 0.960938]  [G loss: 1.736151, acc: 0.562500]\n",
            "1263: [D loss: 0.095167, acc: 0.960938]  [G loss: 1.960506, acc: 0.484375]\n",
            "1264: [D loss: 0.135650, acc: 0.953125]  [G loss: 1.945781, acc: 0.507812]\n",
            "1265: [D loss: 0.100888, acc: 0.968750]  [G loss: 1.745399, acc: 0.476562]\n",
            "1266: [D loss: 0.064467, acc: 0.984375]  [G loss: 1.750140, acc: 0.570312]\n",
            "1267: [D loss: 0.102143, acc: 0.972656]  [G loss: 1.298353, acc: 0.609375]\n",
            "1268: [D loss: 0.099397, acc: 0.964844]  [G loss: 1.099053, acc: 0.640625]\n",
            "1269: [D loss: 0.065551, acc: 0.980469]  [G loss: 1.093567, acc: 0.625000]\n",
            "1270: [D loss: 0.084506, acc: 0.968750]  [G loss: 0.710416, acc: 0.656250]\n",
            "1271: [D loss: 0.128315, acc: 0.953125]  [G loss: 0.882783, acc: 0.640625]\n",
            "1272: [D loss: 0.089917, acc: 0.984375]  [G loss: 0.728261, acc: 0.718750]\n",
            "1273: [D loss: 0.093328, acc: 0.976562]  [G loss: 0.783546, acc: 0.679688]\n",
            "1274: [D loss: 0.159395, acc: 0.945312]  [G loss: 0.775630, acc: 0.718750]\n",
            "1275: [D loss: 0.078822, acc: 0.988281]  [G loss: 0.822875, acc: 0.710938]\n",
            "1276: [D loss: 0.078350, acc: 0.988281]  [G loss: 0.877330, acc: 0.703125]\n",
            "1277: [D loss: 0.051783, acc: 0.988281]  [G loss: 0.823061, acc: 0.710938]\n",
            "1278: [D loss: 0.081508, acc: 0.972656]  [G loss: 0.806748, acc: 0.757812]\n",
            "1279: [D loss: 0.086858, acc: 0.968750]  [G loss: 0.609330, acc: 0.789062]\n",
            "1280: [D loss: 0.096421, acc: 0.964844]  [G loss: 0.564712, acc: 0.765625]\n",
            "1281: [D loss: 0.136668, acc: 0.945312]  [G loss: 0.628420, acc: 0.765625]\n",
            "1282: [D loss: 0.066482, acc: 0.988281]  [G loss: 0.617616, acc: 0.765625]\n",
            "1283: [D loss: 0.096523, acc: 0.964844]  [G loss: 0.894665, acc: 0.625000]\n",
            "1284: [D loss: 0.104459, acc: 0.957031]  [G loss: 0.625543, acc: 0.757812]\n",
            "1285: [D loss: 0.067596, acc: 0.972656]  [G loss: 0.398083, acc: 0.851562]\n",
            "1286: [D loss: 0.100215, acc: 0.957031]  [G loss: 0.569008, acc: 0.820312]\n",
            "1287: [D loss: 0.084595, acc: 0.964844]  [G loss: 0.771498, acc: 0.726562]\n",
            "1288: [D loss: 0.079365, acc: 0.976562]  [G loss: 1.005591, acc: 0.671875]\n",
            "1289: [D loss: 0.098719, acc: 0.968750]  [G loss: 0.690124, acc: 0.679688]\n",
            "1290: [D loss: 0.085078, acc: 0.972656]  [G loss: 0.545917, acc: 0.781250]\n",
            "1291: [D loss: 0.108796, acc: 0.960938]  [G loss: 0.942183, acc: 0.640625]\n",
            "1292: [D loss: 0.083343, acc: 0.968750]  [G loss: 1.028342, acc: 0.578125]\n",
            "1293: [D loss: 0.060661, acc: 0.976562]  [G loss: 1.209299, acc: 0.601562]\n",
            "1294: [D loss: 0.093049, acc: 0.968750]  [G loss: 0.988601, acc: 0.609375]\n",
            "1295: [D loss: 0.129235, acc: 0.968750]  [G loss: 0.775388, acc: 0.687500]\n",
            "1296: [D loss: 0.072069, acc: 0.972656]  [G loss: 0.552562, acc: 0.781250]\n",
            "1297: [D loss: 0.083796, acc: 0.968750]  [G loss: 0.709311, acc: 0.671875]\n",
            "1298: [D loss: 0.064924, acc: 0.988281]  [G loss: 1.069711, acc: 0.546875]\n",
            "1299: [D loss: 0.027146, acc: 0.992188]  [G loss: 1.004361, acc: 0.585938]\n",
            "1300: [D loss: 0.060290, acc: 0.988281]  [G loss: 1.308985, acc: 0.492188]\n",
            "1301: [D loss: 0.064606, acc: 0.980469]  [G loss: 1.095767, acc: 0.507812]\n",
            "1302: [D loss: 0.049486, acc: 0.980469]  [G loss: 1.139081, acc: 0.609375]\n",
            "1303: [D loss: 0.063546, acc: 0.972656]  [G loss: 0.828240, acc: 0.718750]\n",
            "1304: [D loss: 0.053476, acc: 0.992188]  [G loss: 0.592295, acc: 0.789062]\n",
            "1305: [D loss: 0.109876, acc: 0.968750]  [G loss: 0.865435, acc: 0.687500]\n",
            "1306: [D loss: 0.113528, acc: 0.976562]  [G loss: 1.227149, acc: 0.546875]\n",
            "1307: [D loss: 0.093235, acc: 0.960938]  [G loss: 1.780053, acc: 0.421875]\n",
            "1308: [D loss: 0.042542, acc: 0.988281]  [G loss: 2.276102, acc: 0.367188]\n",
            "1309: [D loss: 0.115426, acc: 0.960938]  [G loss: 2.383368, acc: 0.289062]\n",
            "1310: [D loss: 0.069795, acc: 0.972656]  [G loss: 1.975592, acc: 0.406250]\n",
            "1311: [D loss: 0.064969, acc: 0.984375]  [G loss: 1.858727, acc: 0.460938]\n",
            "1312: [D loss: 0.078640, acc: 0.972656]  [G loss: 1.642509, acc: 0.476562]\n",
            "1313: [D loss: 0.110299, acc: 0.964844]  [G loss: 1.969555, acc: 0.468750]\n",
            "1314: [D loss: 0.057866, acc: 0.984375]  [G loss: 2.666546, acc: 0.320312]\n",
            "1315: [D loss: 0.068167, acc: 0.976562]  [G loss: 2.581027, acc: 0.304688]\n",
            "1316: [D loss: 0.074852, acc: 0.968750]  [G loss: 2.599081, acc: 0.257812]\n",
            "1317: [D loss: 0.046387, acc: 0.980469]  [G loss: 2.269279, acc: 0.289062]\n",
            "1318: [D loss: 0.097782, acc: 0.960938]  [G loss: 2.251379, acc: 0.390625]\n",
            "1319: [D loss: 0.046463, acc: 0.988281]  [G loss: 2.171463, acc: 0.398438]\n",
            "1320: [D loss: 0.094665, acc: 0.953125]  [G loss: 2.281183, acc: 0.328125]\n",
            "1321: [D loss: 0.071048, acc: 0.972656]  [G loss: 2.538500, acc: 0.375000]\n",
            "1322: [D loss: 0.090668, acc: 0.976562]  [G loss: 2.569676, acc: 0.312500]\n",
            "1323: [D loss: 0.040797, acc: 0.992188]  [G loss: 3.136687, acc: 0.343750]\n",
            "1324: [D loss: 0.044879, acc: 0.992188]  [G loss: 2.978073, acc: 0.375000]\n",
            "1325: [D loss: 0.096037, acc: 0.964844]  [G loss: 3.311759, acc: 0.351562]\n",
            "1326: [D loss: 0.025219, acc: 0.992188]  [G loss: 3.742036, acc: 0.359375]\n",
            "1327: [D loss: 0.041771, acc: 0.984375]  [G loss: 3.670330, acc: 0.335938]\n",
            "1328: [D loss: 0.047132, acc: 0.988281]  [G loss: 3.860753, acc: 0.304688]\n",
            "1329: [D loss: 0.050792, acc: 0.984375]  [G loss: 3.843158, acc: 0.351562]\n",
            "1330: [D loss: 0.041125, acc: 0.984375]  [G loss: 4.002561, acc: 0.312500]\n",
            "1331: [D loss: 0.063977, acc: 0.984375]  [G loss: 4.459683, acc: 0.273438]\n",
            "1332: [D loss: 0.023629, acc: 0.996094]  [G loss: 4.721433, acc: 0.179688]\n",
            "1333: [D loss: 0.046015, acc: 0.984375]  [G loss: 4.728507, acc: 0.250000]\n",
            "1334: [D loss: 0.050199, acc: 0.984375]  [G loss: 4.762417, acc: 0.226562]\n",
            "1335: [D loss: 0.029325, acc: 0.988281]  [G loss: 4.613626, acc: 0.312500]\n",
            "1336: [D loss: 0.068559, acc: 0.972656]  [G loss: 4.667698, acc: 0.351562]\n",
            "1337: [D loss: 0.023447, acc: 0.996094]  [G loss: 4.971951, acc: 0.328125]\n",
            "1338: [D loss: 0.015738, acc: 0.996094]  [G loss: 4.626438, acc: 0.304688]\n",
            "1339: [D loss: 0.034407, acc: 0.984375]  [G loss: 4.556389, acc: 0.289062]\n",
            "1340: [D loss: 0.052799, acc: 0.980469]  [G loss: 4.877983, acc: 0.296875]\n",
            "1341: [D loss: 0.055162, acc: 0.980469]  [G loss: 5.199536, acc: 0.265625]\n",
            "1342: [D loss: 0.027452, acc: 0.996094]  [G loss: 4.296239, acc: 0.359375]\n",
            "1343: [D loss: 0.100517, acc: 0.957031]  [G loss: 4.116515, acc: 0.351562]\n",
            "1344: [D loss: 0.123441, acc: 0.953125]  [G loss: 4.188649, acc: 0.328125]\n",
            "1345: [D loss: 0.079291, acc: 0.972656]  [G loss: 4.060259, acc: 0.351562]\n",
            "1346: [D loss: 0.076128, acc: 0.972656]  [G loss: 3.551018, acc: 0.351562]\n",
            "1347: [D loss: 0.135575, acc: 0.945312]  [G loss: 3.344903, acc: 0.343750]\n",
            "1348: [D loss: 0.118378, acc: 0.960938]  [G loss: 3.271242, acc: 0.335938]\n",
            "1349: [D loss: 0.145053, acc: 0.949219]  [G loss: 3.458580, acc: 0.328125]\n",
            "1350: [D loss: 0.237131, acc: 0.937500]  [G loss: 2.680464, acc: 0.375000]\n",
            "1351: [D loss: 0.174050, acc: 0.925781]  [G loss: 2.278484, acc: 0.468750]\n",
            "1352: [D loss: 0.146532, acc: 0.945312]  [G loss: 1.745779, acc: 0.562500]\n",
            "1353: [D loss: 0.147185, acc: 0.933594]  [G loss: 1.931682, acc: 0.500000]\n",
            "1354: [D loss: 0.181278, acc: 0.933594]  [G loss: 1.424319, acc: 0.578125]\n",
            "1355: [D loss: 0.180234, acc: 0.929688]  [G loss: 1.491768, acc: 0.570312]\n",
            "1356: [D loss: 0.146906, acc: 0.941406]  [G loss: 1.249403, acc: 0.578125]\n",
            "1357: [D loss: 0.153342, acc: 0.949219]  [G loss: 1.137052, acc: 0.625000]\n",
            "1358: [D loss: 0.179350, acc: 0.949219]  [G loss: 1.233009, acc: 0.609375]\n",
            "1359: [D loss: 0.149654, acc: 0.957031]  [G loss: 0.701429, acc: 0.773438]\n",
            "1360: [D loss: 0.120632, acc: 0.960938]  [G loss: 0.736262, acc: 0.796875]\n",
            "1361: [D loss: 0.149708, acc: 0.957031]  [G loss: 0.770595, acc: 0.804688]\n",
            "1362: [D loss: 0.135091, acc: 0.949219]  [G loss: 0.932445, acc: 0.828125]\n",
            "1363: [D loss: 0.044003, acc: 0.992188]  [G loss: 0.700486, acc: 0.781250]\n",
            "1364: [D loss: 0.150447, acc: 0.964844]  [G loss: 0.655885, acc: 0.843750]\n",
            "1365: [D loss: 0.135777, acc: 0.968750]  [G loss: 0.319697, acc: 0.875000]\n",
            "1366: [D loss: 0.106668, acc: 0.949219]  [G loss: 0.236599, acc: 0.914062]\n",
            "1367: [D loss: 0.065854, acc: 0.968750]  [G loss: 0.149827, acc: 0.937500]\n",
            "1368: [D loss: 0.041581, acc: 0.996094]  [G loss: 0.409523, acc: 0.890625]\n",
            "1369: [D loss: 0.051348, acc: 0.976562]  [G loss: 0.348524, acc: 0.875000]\n",
            "1370: [D loss: 0.042809, acc: 0.992188]  [G loss: 0.596110, acc: 0.851562]\n",
            "1371: [D loss: 0.053653, acc: 0.980469]  [G loss: 0.499881, acc: 0.812500]\n",
            "1372: [D loss: 0.103758, acc: 0.953125]  [G loss: 0.495353, acc: 0.859375]\n",
            "1373: [D loss: 0.032016, acc: 0.988281]  [G loss: 0.506554, acc: 0.828125]\n",
            "1374: [D loss: 0.056543, acc: 0.976562]  [G loss: 0.307337, acc: 0.906250]\n",
            "1375: [D loss: 0.047774, acc: 0.992188]  [G loss: 0.443814, acc: 0.804688]\n",
            "1376: [D loss: 0.053476, acc: 0.980469]  [G loss: 0.467742, acc: 0.820312]\n",
            "1377: [D loss: 0.048314, acc: 0.984375]  [G loss: 0.601344, acc: 0.812500]\n",
            "1378: [D loss: 0.027616, acc: 0.988281]  [G loss: 0.537261, acc: 0.804688]\n",
            "1379: [D loss: 0.045628, acc: 0.980469]  [G loss: 0.457565, acc: 0.820312]\n",
            "1380: [D loss: 0.022795, acc: 0.992188]  [G loss: 0.765284, acc: 0.734375]\n",
            "1381: [D loss: 0.024177, acc: 0.996094]  [G loss: 0.799132, acc: 0.726562]\n",
            "1382: [D loss: 0.019887, acc: 0.988281]  [G loss: 0.885880, acc: 0.671875]\n",
            "1383: [D loss: 0.015020, acc: 0.996094]  [G loss: 1.134764, acc: 0.570312]\n",
            "1384: [D loss: 0.030215, acc: 0.984375]  [G loss: 1.514719, acc: 0.539062]\n",
            "1385: [D loss: 0.036197, acc: 0.980469]  [G loss: 1.452401, acc: 0.476562]\n",
            "1386: [D loss: 0.021165, acc: 0.988281]  [G loss: 1.323073, acc: 0.507812]\n",
            "1387: [D loss: 0.071083, acc: 0.972656]  [G loss: 1.479306, acc: 0.468750]\n",
            "1388: [D loss: 0.032930, acc: 0.988281]  [G loss: 1.326228, acc: 0.515625]\n",
            "1389: [D loss: 0.039464, acc: 0.980469]  [G loss: 1.314849, acc: 0.523438]\n",
            "1390: [D loss: 0.034461, acc: 0.988281]  [G loss: 1.698509, acc: 0.453125]\n",
            "1391: [D loss: 0.046665, acc: 0.980469]  [G loss: 1.545407, acc: 0.382812]\n",
            "1392: [D loss: 0.071129, acc: 0.972656]  [G loss: 1.917678, acc: 0.273438]\n",
            "1393: [D loss: 0.034337, acc: 0.992188]  [G loss: 1.804640, acc: 0.367188]\n",
            "1394: [D loss: 0.063607, acc: 0.984375]  [G loss: 1.693905, acc: 0.289062]\n",
            "1395: [D loss: 0.033724, acc: 0.992188]  [G loss: 2.234134, acc: 0.195312]\n",
            "1396: [D loss: 0.029810, acc: 0.992188]  [G loss: 2.565702, acc: 0.148438]\n",
            "1397: [D loss: 0.050713, acc: 0.980469]  [G loss: 3.017082, acc: 0.085938]\n",
            "1398: [D loss: 0.018618, acc: 0.996094]  [G loss: 3.310391, acc: 0.085938]\n",
            "1399: [D loss: 0.106390, acc: 0.968750]  [G loss: 2.764721, acc: 0.171875]\n",
            "1400: [D loss: 0.073795, acc: 0.964844]  [G loss: 2.421145, acc: 0.203125]\n",
            "1401: [D loss: 0.033270, acc: 0.996094]  [G loss: 2.554713, acc: 0.164062]\n",
            "1402: [D loss: 0.049907, acc: 0.964844]  [G loss: 2.444165, acc: 0.156250]\n",
            "1403: [D loss: 0.045092, acc: 0.980469]  [G loss: 2.664577, acc: 0.187500]\n",
            "1404: [D loss: 0.025427, acc: 0.988281]  [G loss: 2.951506, acc: 0.187500]\n",
            "1405: [D loss: 0.041628, acc: 0.984375]  [G loss: 2.566565, acc: 0.257812]\n",
            "1406: [D loss: 0.043719, acc: 0.984375]  [G loss: 2.827509, acc: 0.281250]\n",
            "1407: [D loss: 0.059025, acc: 0.976562]  [G loss: 2.118098, acc: 0.359375]\n",
            "1408: [D loss: 0.070598, acc: 0.964844]  [G loss: 1.660271, acc: 0.437500]\n",
            "1409: [D loss: 0.112064, acc: 0.953125]  [G loss: 1.507808, acc: 0.562500]\n",
            "1410: [D loss: 0.043519, acc: 0.972656]  [G loss: 1.377950, acc: 0.585938]\n",
            "1411: [D loss: 0.044998, acc: 0.992188]  [G loss: 1.457510, acc: 0.515625]\n",
            "1412: [D loss: 0.034015, acc: 0.992188]  [G loss: 1.465833, acc: 0.539062]\n",
            "1413: [D loss: 0.110899, acc: 0.976562]  [G loss: 1.161511, acc: 0.632812]\n",
            "1414: [D loss: 0.018751, acc: 0.996094]  [G loss: 0.797465, acc: 0.710938]\n",
            "1415: [D loss: 0.027759, acc: 0.992188]  [G loss: 0.690077, acc: 0.734375]\n",
            "1416: [D loss: 0.031779, acc: 0.992188]  [G loss: 0.734848, acc: 0.718750]\n",
            "1417: [D loss: 0.023567, acc: 0.996094]  [G loss: 0.545287, acc: 0.757812]\n",
            "1418: [D loss: 0.027884, acc: 0.988281]  [G loss: 0.646888, acc: 0.757812]\n",
            "1419: [D loss: 0.042156, acc: 0.984375]  [G loss: 0.472602, acc: 0.796875]\n",
            "1420: [D loss: 0.029958, acc: 0.984375]  [G loss: 0.244406, acc: 0.906250]\n",
            "1421: [D loss: 0.008599, acc: 1.000000]  [G loss: 0.409052, acc: 0.828125]\n",
            "1422: [D loss: 0.013733, acc: 0.996094]  [G loss: 0.379772, acc: 0.835938]\n",
            "1423: [D loss: 0.008068, acc: 1.000000]  [G loss: 0.456852, acc: 0.804688]\n",
            "1424: [D loss: 0.028493, acc: 0.996094]  [G loss: 0.539975, acc: 0.773438]\n",
            "1425: [D loss: 0.061427, acc: 0.984375]  [G loss: 0.408027, acc: 0.820312]\n",
            "1426: [D loss: 0.070673, acc: 0.980469]  [G loss: 0.404983, acc: 0.835938]\n",
            "1427: [D loss: 0.096049, acc: 0.964844]  [G loss: 0.146029, acc: 0.929688]\n",
            "1428: [D loss: 0.101034, acc: 0.964844]  [G loss: 0.160010, acc: 0.921875]\n",
            "1429: [D loss: 0.156144, acc: 0.949219]  [G loss: 0.331550, acc: 0.867188]\n",
            "1430: [D loss: 0.046253, acc: 0.992188]  [G loss: 0.519983, acc: 0.828125]\n",
            "1431: [D loss: 0.119390, acc: 0.968750]  [G loss: 0.777901, acc: 0.765625]\n",
            "1432: [D loss: 0.018531, acc: 0.996094]  [G loss: 0.961219, acc: 0.687500]\n",
            "1433: [D loss: 0.061859, acc: 0.968750]  [G loss: 0.586203, acc: 0.820312]\n",
            "1434: [D loss: 0.062679, acc: 0.980469]  [G loss: 0.280376, acc: 0.906250]\n",
            "1435: [D loss: 0.087272, acc: 0.976562]  [G loss: 0.298200, acc: 0.875000]\n",
            "1436: [D loss: 0.077143, acc: 0.964844]  [G loss: 0.230846, acc: 0.914062]\n",
            "1437: [D loss: 0.141615, acc: 0.953125]  [G loss: 0.446014, acc: 0.851562]\n",
            "1438: [D loss: 0.074758, acc: 0.968750]  [G loss: 0.880753, acc: 0.781250]\n",
            "1439: [D loss: 0.097715, acc: 0.968750]  [G loss: 0.879231, acc: 0.796875]\n",
            "1440: [D loss: 0.075268, acc: 0.984375]  [G loss: 0.886507, acc: 0.773438]\n",
            "1441: [D loss: 0.122253, acc: 0.960938]  [G loss: 0.618618, acc: 0.828125]\n",
            "1442: [D loss: 0.102917, acc: 0.960938]  [G loss: 0.767277, acc: 0.789062]\n",
            "1443: [D loss: 0.124127, acc: 0.953125]  [G loss: 0.849484, acc: 0.781250]\n",
            "1444: [D loss: 0.130460, acc: 0.953125]  [G loss: 0.598838, acc: 0.781250]\n",
            "1445: [D loss: 0.049848, acc: 0.984375]  [G loss: 0.780672, acc: 0.718750]\n",
            "1446: [D loss: 0.147776, acc: 0.953125]  [G loss: 1.145903, acc: 0.710938]\n",
            "1447: [D loss: 0.060842, acc: 0.984375]  [G loss: 1.776613, acc: 0.593750]\n",
            "1448: [D loss: 0.130923, acc: 0.964844]  [G loss: 1.316701, acc: 0.679688]\n",
            "1449: [D loss: 0.126972, acc: 0.957031]  [G loss: 0.913744, acc: 0.750000]\n",
            "1450: [D loss: 0.219533, acc: 0.914062]  [G loss: 0.942046, acc: 0.765625]\n",
            "1451: [D loss: 0.188629, acc: 0.917969]  [G loss: 1.338527, acc: 0.710938]\n",
            "1452: [D loss: 0.150334, acc: 0.953125]  [G loss: 1.864765, acc: 0.679688]\n",
            "1453: [D loss: 0.070134, acc: 0.964844]  [G loss: 1.806415, acc: 0.703125]\n",
            "1454: [D loss: 0.195958, acc: 0.957031]  [G loss: 2.081987, acc: 0.671875]\n",
            "1455: [D loss: 0.169314, acc: 0.937500]  [G loss: 1.506381, acc: 0.734375]\n",
            "1456: [D loss: 0.160688, acc: 0.945312]  [G loss: 1.430790, acc: 0.718750]\n",
            "1457: [D loss: 0.076988, acc: 0.964844]  [G loss: 1.071557, acc: 0.750000]\n",
            "1458: [D loss: 0.131323, acc: 0.964844]  [G loss: 1.190933, acc: 0.750000]\n",
            "1459: [D loss: 0.114307, acc: 0.964844]  [G loss: 1.507353, acc: 0.664062]\n",
            "1460: [D loss: 0.187324, acc: 0.941406]  [G loss: 1.648787, acc: 0.632812]\n",
            "1461: [D loss: 0.106061, acc: 0.976562]  [G loss: 1.481378, acc: 0.632812]\n",
            "1462: [D loss: 0.072992, acc: 0.972656]  [G loss: 1.118262, acc: 0.687500]\n",
            "1463: [D loss: 0.039201, acc: 0.988281]  [G loss: 1.250703, acc: 0.703125]\n",
            "1464: [D loss: 0.065001, acc: 0.972656]  [G loss: 1.475372, acc: 0.695312]\n",
            "1465: [D loss: 0.059747, acc: 0.980469]  [G loss: 1.125715, acc: 0.804688]\n",
            "1466: [D loss: 0.108229, acc: 0.957031]  [G loss: 1.094878, acc: 0.812500]\n",
            "1467: [D loss: 0.084396, acc: 0.968750]  [G loss: 1.189158, acc: 0.789062]\n",
            "1468: [D loss: 0.099414, acc: 0.957031]  [G loss: 1.536352, acc: 0.742188]\n",
            "1469: [D loss: 0.108226, acc: 0.968750]  [G loss: 1.541701, acc: 0.695312]\n",
            "1470: [D loss: 0.087108, acc: 0.980469]  [G loss: 0.972106, acc: 0.750000]\n",
            "1471: [D loss: 0.139703, acc: 0.945312]  [G loss: 1.441354, acc: 0.710938]\n",
            "1472: [D loss: 0.153382, acc: 0.953125]  [G loss: 1.858616, acc: 0.648438]\n",
            "1473: [D loss: 0.176557, acc: 0.945312]  [G loss: 2.411822, acc: 0.562500]\n",
            "1474: [D loss: 0.148743, acc: 0.945312]  [G loss: 2.814743, acc: 0.531250]\n",
            "1475: [D loss: 0.128889, acc: 0.957031]  [G loss: 2.261026, acc: 0.625000]\n",
            "1476: [D loss: 0.181440, acc: 0.953125]  [G loss: 1.763240, acc: 0.726562]\n",
            "1477: [D loss: 0.170891, acc: 0.937500]  [G loss: 2.366468, acc: 0.601562]\n",
            "1478: [D loss: 0.167068, acc: 0.945312]  [G loss: 2.336997, acc: 0.523438]\n",
            "1479: [D loss: 0.142620, acc: 0.941406]  [G loss: 1.685446, acc: 0.601562]\n",
            "1480: [D loss: 0.215424, acc: 0.929688]  [G loss: 1.098418, acc: 0.734375]\n",
            "1481: [D loss: 0.238270, acc: 0.898438]  [G loss: 2.101741, acc: 0.429688]\n",
            "1482: [D loss: 0.134710, acc: 0.960938]  [G loss: 2.658764, acc: 0.328125]\n",
            "1483: [D loss: 0.180818, acc: 0.937500]  [G loss: 2.257909, acc: 0.367188]\n",
            "1484: [D loss: 0.126945, acc: 0.953125]  [G loss: 2.022418, acc: 0.437500]\n",
            "1485: [D loss: 0.054187, acc: 0.984375]  [G loss: 1.573182, acc: 0.515625]\n",
            "1486: [D loss: 0.116288, acc: 0.960938]  [G loss: 1.553307, acc: 0.531250]\n",
            "1487: [D loss: 0.069482, acc: 0.968750]  [G loss: 1.701213, acc: 0.500000]\n",
            "1488: [D loss: 0.099759, acc: 0.972656]  [G loss: 1.433004, acc: 0.507812]\n",
            "1489: [D loss: 0.086249, acc: 0.972656]  [G loss: 1.377201, acc: 0.593750]\n",
            "1490: [D loss: 0.089161, acc: 0.960938]  [G loss: 1.264473, acc: 0.687500]\n",
            "1491: [D loss: 0.068308, acc: 0.972656]  [G loss: 1.429438, acc: 0.617188]\n",
            "1492: [D loss: 0.045613, acc: 0.988281]  [G loss: 1.481927, acc: 0.617188]\n",
            "1493: [D loss: 0.057437, acc: 0.984375]  [G loss: 1.742888, acc: 0.625000]\n",
            "1494: [D loss: 0.053580, acc: 0.984375]  [G loss: 1.373992, acc: 0.679688]\n",
            "1495: [D loss: 0.100640, acc: 0.960938]  [G loss: 1.060260, acc: 0.742188]\n",
            "1496: [D loss: 0.018968, acc: 0.996094]  [G loss: 1.005847, acc: 0.742188]\n",
            "1497: [D loss: 0.060666, acc: 0.964844]  [G loss: 0.945714, acc: 0.765625]\n",
            "1498: [D loss: 0.082330, acc: 0.984375]  [G loss: 0.793830, acc: 0.773438]\n",
            "1499: [D loss: 0.082569, acc: 0.972656]  [G loss: 0.903629, acc: 0.750000]\n",
            "1500: [D loss: 0.113876, acc: 0.960938]  [G loss: 1.323612, acc: 0.648438]\n",
            "1501: [D loss: 0.030831, acc: 0.992188]  [G loss: 1.570719, acc: 0.515625]\n",
            "1502: [D loss: 0.054900, acc: 0.980469]  [G loss: 1.644547, acc: 0.578125]\n",
            "1503: [D loss: 0.078280, acc: 0.984375]  [G loss: 1.809251, acc: 0.531250]\n",
            "1504: [D loss: 0.097584, acc: 0.949219]  [G loss: 1.301704, acc: 0.609375]\n",
            "1505: [D loss: 0.098142, acc: 0.976562]  [G loss: 0.939163, acc: 0.718750]\n",
            "1506: [D loss: 0.102670, acc: 0.964844]  [G loss: 0.859417, acc: 0.726562]\n",
            "1507: [D loss: 0.099927, acc: 0.960938]  [G loss: 0.867572, acc: 0.796875]\n",
            "1508: [D loss: 0.095950, acc: 0.968750]  [G loss: 1.055027, acc: 0.742188]\n",
            "1509: [D loss: 0.105795, acc: 0.960938]  [G loss: 1.100024, acc: 0.718750]\n",
            "1510: [D loss: 0.068778, acc: 0.980469]  [G loss: 1.217527, acc: 0.640625]\n",
            "1511: [D loss: 0.047649, acc: 0.992188]  [G loss: 1.200470, acc: 0.656250]\n",
            "1512: [D loss: 0.104547, acc: 0.968750]  [G loss: 1.358532, acc: 0.656250]\n",
            "1513: [D loss: 0.063575, acc: 0.964844]  [G loss: 1.269177, acc: 0.632812]\n",
            "1514: [D loss: 0.069566, acc: 0.972656]  [G loss: 1.319210, acc: 0.625000]\n",
            "1515: [D loss: 0.067191, acc: 0.988281]  [G loss: 1.446692, acc: 0.562500]\n",
            "1516: [D loss: 0.054302, acc: 0.988281]  [G loss: 1.762859, acc: 0.515625]\n",
            "1517: [D loss: 0.053032, acc: 0.988281]  [G loss: 1.510460, acc: 0.546875]\n",
            "1518: [D loss: 0.036602, acc: 0.988281]  [G loss: 1.740677, acc: 0.507812]\n",
            "1519: [D loss: 0.055031, acc: 0.980469]  [G loss: 1.754553, acc: 0.507812]\n",
            "1520: [D loss: 0.072208, acc: 0.968750]  [G loss: 1.586863, acc: 0.507812]\n",
            "1521: [D loss: 0.056890, acc: 0.980469]  [G loss: 2.066393, acc: 0.445312]\n",
            "1522: [D loss: 0.072861, acc: 0.964844]  [G loss: 2.068106, acc: 0.453125]\n",
            "1523: [D loss: 0.055729, acc: 0.976562]  [G loss: 2.165896, acc: 0.429688]\n",
            "1524: [D loss: 0.058020, acc: 0.980469]  [G loss: 2.458054, acc: 0.359375]\n",
            "1525: [D loss: 0.035974, acc: 0.984375]  [G loss: 2.881749, acc: 0.414062]\n",
            "1526: [D loss: 0.048405, acc: 0.984375]  [G loss: 2.746203, acc: 0.398438]\n",
            "1527: [D loss: 0.032463, acc: 0.988281]  [G loss: 2.447809, acc: 0.382812]\n",
            "1528: [D loss: 0.072153, acc: 0.976562]  [G loss: 2.734502, acc: 0.406250]\n",
            "1529: [D loss: 0.043917, acc: 0.984375]  [G loss: 2.076519, acc: 0.476562]\n",
            "1530: [D loss: 0.127161, acc: 0.949219]  [G loss: 1.808672, acc: 0.523438]\n",
            "1531: [D loss: 0.098181, acc: 0.964844]  [G loss: 1.357583, acc: 0.640625]\n",
            "1532: [D loss: 0.092878, acc: 0.964844]  [G loss: 1.164379, acc: 0.671875]\n",
            "1533: [D loss: 0.123004, acc: 0.960938]  [G loss: 1.613046, acc: 0.554688]\n",
            "1534: [D loss: 0.082864, acc: 0.976562]  [G loss: 2.095950, acc: 0.554688]\n",
            "1535: [D loss: 0.044409, acc: 0.984375]  [G loss: 2.116569, acc: 0.468750]\n",
            "1536: [D loss: 0.077335, acc: 0.968750]  [G loss: 2.694381, acc: 0.437500]\n",
            "1537: [D loss: 0.127973, acc: 0.960938]  [G loss: 2.009833, acc: 0.531250]\n",
            "1538: [D loss: 0.033431, acc: 0.992188]  [G loss: 1.620573, acc: 0.617188]\n",
            "1539: [D loss: 0.058056, acc: 0.972656]  [G loss: 1.402886, acc: 0.703125]\n",
            "1540: [D loss: 0.070235, acc: 0.976562]  [G loss: 1.131393, acc: 0.718750]\n",
            "1541: [D loss: 0.073024, acc: 0.964844]  [G loss: 1.106283, acc: 0.726562]\n",
            "1542: [D loss: 0.043979, acc: 0.992188]  [G loss: 1.430193, acc: 0.679688]\n",
            "1543: [D loss: 0.036469, acc: 0.984375]  [G loss: 1.166630, acc: 0.648438]\n",
            "1544: [D loss: 0.044661, acc: 0.984375]  [G loss: 1.407620, acc: 0.632812]\n",
            "1545: [D loss: 0.062334, acc: 0.988281]  [G loss: 1.492269, acc: 0.648438]\n",
            "1546: [D loss: 0.076784, acc: 0.968750]  [G loss: 1.249099, acc: 0.648438]\n",
            "1547: [D loss: 0.026546, acc: 0.988281]  [G loss: 1.052875, acc: 0.687500]\n",
            "1548: [D loss: 0.086511, acc: 0.968750]  [G loss: 1.085969, acc: 0.679688]\n",
            "1549: [D loss: 0.031567, acc: 0.988281]  [G loss: 1.145025, acc: 0.664062]\n",
            "1550: [D loss: 0.063841, acc: 0.972656]  [G loss: 1.118789, acc: 0.710938]\n",
            "1551: [D loss: 0.048317, acc: 0.984375]  [G loss: 1.599225, acc: 0.539062]\n",
            "1552: [D loss: 0.021819, acc: 0.992188]  [G loss: 1.298131, acc: 0.625000]\n",
            "1553: [D loss: 0.086868, acc: 0.980469]  [G loss: 1.154478, acc: 0.695312]\n",
            "1554: [D loss: 0.113714, acc: 0.964844]  [G loss: 1.238672, acc: 0.679688]\n",
            "1555: [D loss: 0.051380, acc: 0.984375]  [G loss: 1.223989, acc: 0.695312]\n",
            "1556: [D loss: 0.105726, acc: 0.964844]  [G loss: 0.955169, acc: 0.726562]\n",
            "1557: [D loss: 0.084669, acc: 0.972656]  [G loss: 1.216558, acc: 0.664062]\n",
            "1558: [D loss: 0.161473, acc: 0.941406]  [G loss: 1.185961, acc: 0.664062]\n",
            "1559: [D loss: 0.077264, acc: 0.980469]  [G loss: 1.108150, acc: 0.703125]\n",
            "1560: [D loss: 0.090453, acc: 0.976562]  [G loss: 0.867207, acc: 0.765625]\n",
            "1561: [D loss: 0.109472, acc: 0.953125]  [G loss: 1.384918, acc: 0.593750]\n",
            "1562: [D loss: 0.238757, acc: 0.917969]  [G loss: 1.068675, acc: 0.679688]\n",
            "1563: [D loss: 0.181914, acc: 0.933594]  [G loss: 1.196881, acc: 0.671875]\n",
            "1564: [D loss: 0.101508, acc: 0.960938]  [G loss: 1.061280, acc: 0.648438]\n",
            "1565: [D loss: 0.192720, acc: 0.929688]  [G loss: 1.392058, acc: 0.593750]\n",
            "1566: [D loss: 0.164036, acc: 0.953125]  [G loss: 2.058196, acc: 0.484375]\n",
            "1567: [D loss: 0.174688, acc: 0.937500]  [G loss: 2.292846, acc: 0.437500]\n",
            "1568: [D loss: 0.176065, acc: 0.949219]  [G loss: 1.979494, acc: 0.484375]\n",
            "1569: [D loss: 0.161111, acc: 0.925781]  [G loss: 2.544716, acc: 0.382812]\n",
            "1570: [D loss: 0.126026, acc: 0.964844]  [G loss: 2.777807, acc: 0.296875]\n",
            "1571: [D loss: 0.117397, acc: 0.945312]  [G loss: 3.010996, acc: 0.218750]\n",
            "1572: [D loss: 0.080482, acc: 0.964844]  [G loss: 3.033082, acc: 0.281250]\n",
            "1573: [D loss: 0.162509, acc: 0.957031]  [G loss: 2.844784, acc: 0.281250]\n",
            "1574: [D loss: 0.162280, acc: 0.937500]  [G loss: 3.520960, acc: 0.203125]\n",
            "1575: [D loss: 0.098748, acc: 0.976562]  [G loss: 3.165096, acc: 0.281250]\n",
            "1576: [D loss: 0.118732, acc: 0.953125]  [G loss: 3.235426, acc: 0.234375]\n",
            "1577: [D loss: 0.126515, acc: 0.957031]  [G loss: 2.995595, acc: 0.320312]\n",
            "1578: [D loss: 0.089614, acc: 0.964844]  [G loss: 2.805258, acc: 0.328125]\n",
            "1579: [D loss: 0.044709, acc: 0.988281]  [G loss: 3.343599, acc: 0.304688]\n",
            "1580: [D loss: 0.096730, acc: 0.968750]  [G loss: 3.238698, acc: 0.304688]\n",
            "1581: [D loss: 0.100842, acc: 0.968750]  [G loss: 3.401999, acc: 0.242188]\n",
            "1582: [D loss: 0.076581, acc: 0.984375]  [G loss: 3.057449, acc: 0.289062]\n",
            "1583: [D loss: 0.176121, acc: 0.960938]  [G loss: 2.584244, acc: 0.359375]\n",
            "1584: [D loss: 0.094072, acc: 0.968750]  [G loss: 2.301699, acc: 0.335938]\n",
            "1585: [D loss: 0.178646, acc: 0.921875]  [G loss: 2.653609, acc: 0.203125]\n",
            "1586: [D loss: 0.132155, acc: 0.960938]  [G loss: 3.346173, acc: 0.171875]\n",
            "1587: [D loss: 0.183959, acc: 0.933594]  [G loss: 2.789136, acc: 0.179688]\n",
            "1588: [D loss: 0.149331, acc: 0.957031]  [G loss: 2.554768, acc: 0.218750]\n",
            "1589: [D loss: 0.117734, acc: 0.941406]  [G loss: 2.225549, acc: 0.296875]\n",
            "1590: [D loss: 0.160671, acc: 0.945312]  [G loss: 2.306099, acc: 0.234375]\n",
            "1591: [D loss: 0.130282, acc: 0.968750]  [G loss: 2.033836, acc: 0.304688]\n",
            "1592: [D loss: 0.087435, acc: 0.972656]  [G loss: 2.145555, acc: 0.304688]\n",
            "1593: [D loss: 0.097008, acc: 0.964844]  [G loss: 2.232429, acc: 0.289062]\n",
            "1594: [D loss: 0.067599, acc: 0.988281]  [G loss: 2.054187, acc: 0.320312]\n",
            "1595: [D loss: 0.061039, acc: 0.976562]  [G loss: 2.091110, acc: 0.234375]\n",
            "1596: [D loss: 0.060508, acc: 0.980469]  [G loss: 2.266736, acc: 0.203125]\n",
            "1597: [D loss: 0.087413, acc: 0.964844]  [G loss: 2.405071, acc: 0.171875]\n",
            "1598: [D loss: 0.057685, acc: 0.980469]  [G loss: 2.068263, acc: 0.265625]\n",
            "1599: [D loss: 0.049096, acc: 0.988281]  [G loss: 1.774008, acc: 0.242188]\n",
            "1600: [D loss: 0.052540, acc: 0.992188]  [G loss: 1.867071, acc: 0.312500]\n",
            "1601: [D loss: 0.095456, acc: 0.960938]  [G loss: 1.940084, acc: 0.312500]\n",
            "1602: [D loss: 0.115216, acc: 0.953125]  [G loss: 2.328229, acc: 0.203125]\n",
            "1603: [D loss: 0.077322, acc: 0.972656]  [G loss: 2.256603, acc: 0.257812]\n",
            "1604: [D loss: 0.082444, acc: 0.980469]  [G loss: 2.384652, acc: 0.242188]\n",
            "1605: [D loss: 0.110337, acc: 0.957031]  [G loss: 1.886199, acc: 0.375000]\n",
            "1606: [D loss: 0.081757, acc: 0.957031]  [G loss: 1.602180, acc: 0.390625]\n",
            "1607: [D loss: 0.129254, acc: 0.949219]  [G loss: 1.205140, acc: 0.546875]\n",
            "1608: [D loss: 0.100791, acc: 0.957031]  [G loss: 1.751127, acc: 0.468750]\n",
            "1609: [D loss: 0.127408, acc: 0.964844]  [G loss: 2.307210, acc: 0.398438]\n",
            "1610: [D loss: 0.049775, acc: 0.980469]  [G loss: 3.426035, acc: 0.234375]\n",
            "1611: [D loss: 0.075716, acc: 0.980469]  [G loss: 3.457926, acc: 0.234375]\n",
            "1612: [D loss: 0.118458, acc: 0.949219]  [G loss: 2.786783, acc: 0.375000]\n",
            "1613: [D loss: 0.037086, acc: 0.988281]  [G loss: 1.761750, acc: 0.554688]\n",
            "1614: [D loss: 0.055669, acc: 0.976562]  [G loss: 1.367443, acc: 0.601562]\n",
            "1615: [D loss: 0.106968, acc: 0.945312]  [G loss: 1.595383, acc: 0.546875]\n",
            "1616: [D loss: 0.054308, acc: 0.980469]  [G loss: 2.134394, acc: 0.414062]\n",
            "1617: [D loss: 0.039856, acc: 0.988281]  [G loss: 2.464793, acc: 0.398438]\n",
            "1618: [D loss: 0.045538, acc: 0.984375]  [G loss: 2.550936, acc: 0.359375]\n",
            "1619: [D loss: 0.145866, acc: 0.976562]  [G loss: 2.022840, acc: 0.445312]\n",
            "1620: [D loss: 0.029747, acc: 0.984375]  [G loss: 1.476661, acc: 0.546875]\n",
            "1621: [D loss: 0.040118, acc: 0.984375]  [G loss: 0.857391, acc: 0.687500]\n",
            "1622: [D loss: 0.071393, acc: 0.976562]  [G loss: 1.044359, acc: 0.679688]\n",
            "1623: [D loss: 0.053030, acc: 0.976562]  [G loss: 1.393116, acc: 0.625000]\n",
            "1624: [D loss: 0.075395, acc: 0.988281]  [G loss: 1.159098, acc: 0.664062]\n",
            "1625: [D loss: 0.028384, acc: 0.988281]  [G loss: 0.930062, acc: 0.750000]\n",
            "1626: [D loss: 0.025462, acc: 0.984375]  [G loss: 0.923685, acc: 0.734375]\n",
            "1627: [D loss: 0.055525, acc: 0.976562]  [G loss: 0.651482, acc: 0.796875]\n",
            "1628: [D loss: 0.018976, acc: 0.992188]  [G loss: 0.806329, acc: 0.765625]\n",
            "1629: [D loss: 0.052033, acc: 0.984375]  [G loss: 0.713373, acc: 0.796875]\n",
            "1630: [D loss: 0.046261, acc: 0.980469]  [G loss: 0.609538, acc: 0.796875]\n",
            "1631: [D loss: 0.043397, acc: 0.992188]  [G loss: 0.645911, acc: 0.851562]\n",
            "1632: [D loss: 0.046344, acc: 0.984375]  [G loss: 0.755729, acc: 0.789062]\n",
            "1633: [D loss: 0.071852, acc: 0.972656]  [G loss: 0.686342, acc: 0.812500]\n",
            "1634: [D loss: 0.053411, acc: 0.976562]  [G loss: 0.615788, acc: 0.820312]\n",
            "1635: [D loss: 0.027261, acc: 0.988281]  [G loss: 0.362827, acc: 0.851562]\n",
            "1636: [D loss: 0.032533, acc: 0.988281]  [G loss: 0.440729, acc: 0.867188]\n",
            "1637: [D loss: 0.048611, acc: 0.976562]  [G loss: 0.406444, acc: 0.875000]\n",
            "1638: [D loss: 0.069792, acc: 0.984375]  [G loss: 0.704542, acc: 0.765625]\n",
            "1639: [D loss: 0.055588, acc: 0.980469]  [G loss: 0.783332, acc: 0.710938]\n",
            "1640: [D loss: 0.041837, acc: 0.980469]  [G loss: 0.857316, acc: 0.773438]\n",
            "1641: [D loss: 0.046905, acc: 0.984375]  [G loss: 0.744477, acc: 0.781250]\n",
            "1642: [D loss: 0.048833, acc: 0.984375]  [G loss: 0.361701, acc: 0.851562]\n",
            "1643: [D loss: 0.062055, acc: 0.976562]  [G loss: 0.549218, acc: 0.835938]\n",
            "1644: [D loss: 0.116144, acc: 0.968750]  [G loss: 0.478518, acc: 0.851562]\n",
            "1645: [D loss: 0.063097, acc: 0.972656]  [G loss: 0.644964, acc: 0.828125]\n",
            "1646: [D loss: 0.060431, acc: 0.972656]  [G loss: 0.524039, acc: 0.835938]\n",
            "1647: [D loss: 0.049862, acc: 0.972656]  [G loss: 0.706036, acc: 0.796875]\n",
            "1648: [D loss: 0.051764, acc: 0.972656]  [G loss: 0.590760, acc: 0.820312]\n",
            "1649: [D loss: 0.062148, acc: 0.976562]  [G loss: 0.564569, acc: 0.835938]\n",
            "1650: [D loss: 0.034381, acc: 0.988281]  [G loss: 0.698599, acc: 0.828125]\n",
            "1651: [D loss: 0.060402, acc: 0.976562]  [G loss: 0.707090, acc: 0.750000]\n",
            "1652: [D loss: 0.065174, acc: 0.976562]  [G loss: 0.821212, acc: 0.710938]\n",
            "1653: [D loss: 0.086342, acc: 0.957031]  [G loss: 0.688939, acc: 0.718750]\n",
            "1654: [D loss: 0.100173, acc: 0.972656]  [G loss: 0.590398, acc: 0.781250]\n",
            "1655: [D loss: 0.096452, acc: 0.972656]  [G loss: 0.488947, acc: 0.851562]\n",
            "1656: [D loss: 0.089676, acc: 0.976562]  [G loss: 0.511991, acc: 0.820312]\n",
            "1657: [D loss: 0.072698, acc: 0.976562]  [G loss: 0.459483, acc: 0.828125]\n",
            "1658: [D loss: 0.079614, acc: 0.976562]  [G loss: 0.656677, acc: 0.726562]\n",
            "1659: [D loss: 0.097541, acc: 0.972656]  [G loss: 1.036847, acc: 0.617188]\n",
            "1660: [D loss: 0.062572, acc: 0.976562]  [G loss: 1.188463, acc: 0.585938]\n",
            "1661: [D loss: 0.047050, acc: 0.988281]  [G loss: 0.877791, acc: 0.632812]\n",
            "1662: [D loss: 0.076774, acc: 0.984375]  [G loss: 0.635292, acc: 0.734375]\n",
            "1663: [D loss: 0.108636, acc: 0.968750]  [G loss: 0.535808, acc: 0.812500]\n",
            "1664: [D loss: 0.089744, acc: 0.972656]  [G loss: 0.645263, acc: 0.734375]\n",
            "1665: [D loss: 0.100887, acc: 0.960938]  [G loss: 1.040538, acc: 0.609375]\n",
            "1666: [D loss: 0.089127, acc: 0.968750]  [G loss: 1.476738, acc: 0.500000]\n",
            "1667: [D loss: 0.172418, acc: 0.925781]  [G loss: 1.482755, acc: 0.554688]\n",
            "1668: [D loss: 0.191316, acc: 0.933594]  [G loss: 0.899813, acc: 0.703125]\n",
            "1669: [D loss: 0.137263, acc: 0.953125]  [G loss: 0.527497, acc: 0.773438]\n",
            "1670: [D loss: 0.340588, acc: 0.847656]  [G loss: 1.846370, acc: 0.429688]\n",
            "1671: [D loss: 0.156363, acc: 0.929688]  [G loss: 2.835882, acc: 0.242188]\n",
            "1672: [D loss: 0.276414, acc: 0.910156]  [G loss: 1.961682, acc: 0.351562]\n",
            "1673: [D loss: 0.318318, acc: 0.914062]  [G loss: 1.067369, acc: 0.609375]\n",
            "1674: [D loss: 0.254797, acc: 0.890625]  [G loss: 0.913296, acc: 0.593750]\n",
            "1675: [D loss: 0.244921, acc: 0.894531]  [G loss: 1.942542, acc: 0.304688]\n",
            "1676: [D loss: 0.201207, acc: 0.921875]  [G loss: 2.858350, acc: 0.109375]\n",
            "1677: [D loss: 0.213819, acc: 0.933594]  [G loss: 2.700813, acc: 0.125000]\n",
            "1678: [D loss: 0.194234, acc: 0.929688]  [G loss: 2.141125, acc: 0.179688]\n",
            "1679: [D loss: 0.203817, acc: 0.910156]  [G loss: 1.586546, acc: 0.226562]\n",
            "1680: [D loss: 0.100851, acc: 0.968750]  [G loss: 1.256779, acc: 0.421875]\n",
            "1681: [D loss: 0.084752, acc: 0.976562]  [G loss: 1.461886, acc: 0.296875]\n",
            "1682: [D loss: 0.096146, acc: 0.964844]  [G loss: 1.739228, acc: 0.187500]\n",
            "1683: [D loss: 0.048297, acc: 0.992188]  [G loss: 2.121094, acc: 0.187500]\n",
            "1684: [D loss: 0.058194, acc: 0.980469]  [G loss: 2.862444, acc: 0.085938]\n",
            "1685: [D loss: 0.087150, acc: 0.980469]  [G loss: 3.072352, acc: 0.101562]\n",
            "1686: [D loss: 0.047486, acc: 0.984375]  [G loss: 3.319758, acc: 0.187500]\n",
            "1687: [D loss: 0.041791, acc: 0.988281]  [G loss: 3.143859, acc: 0.195312]\n",
            "1688: [D loss: 0.089389, acc: 0.972656]  [G loss: 2.856227, acc: 0.250000]\n",
            "1689: [D loss: 0.077849, acc: 0.976562]  [G loss: 2.608401, acc: 0.312500]\n",
            "1690: [D loss: 0.077576, acc: 0.972656]  [G loss: 2.622059, acc: 0.328125]\n",
            "1691: [D loss: 0.101125, acc: 0.960938]  [G loss: 2.393200, acc: 0.359375]\n",
            "1692: [D loss: 0.079342, acc: 0.972656]  [G loss: 2.691852, acc: 0.343750]\n",
            "1693: [D loss: 0.178486, acc: 0.933594]  [G loss: 2.721158, acc: 0.406250]\n",
            "1694: [D loss: 0.144450, acc: 0.953125]  [G loss: 2.818708, acc: 0.414062]\n",
            "1695: [D loss: 0.154641, acc: 0.949219]  [G loss: 2.345274, acc: 0.429688]\n",
            "1696: [D loss: 0.175840, acc: 0.929688]  [G loss: 2.208153, acc: 0.453125]\n",
            "1697: [D loss: 0.247774, acc: 0.921875]  [G loss: 2.391266, acc: 0.429688]\n",
            "1698: [D loss: 0.247784, acc: 0.925781]  [G loss: 3.001486, acc: 0.359375]\n",
            "1699: [D loss: 0.468705, acc: 0.871094]  [G loss: 2.680587, acc: 0.359375]\n",
            "1700: [D loss: 0.270034, acc: 0.917969]  [G loss: 1.787260, acc: 0.429688]\n",
            "1701: [D loss: 0.463514, acc: 0.867188]  [G loss: 1.204597, acc: 0.593750]\n",
            "1702: [D loss: 0.466814, acc: 0.820312]  [G loss: 1.582198, acc: 0.453125]\n",
            "1703: [D loss: 0.478570, acc: 0.839844]  [G loss: 2.477195, acc: 0.289062]\n",
            "1704: [D loss: 0.228055, acc: 0.933594]  [G loss: 2.964711, acc: 0.218750]\n",
            "1705: [D loss: 0.404413, acc: 0.878906]  [G loss: 1.945145, acc: 0.429688]\n",
            "1706: [D loss: 0.242785, acc: 0.894531]  [G loss: 1.012331, acc: 0.656250]\n",
            "1707: [D loss: 0.298774, acc: 0.894531]  [G loss: 0.748768, acc: 0.695312]\n",
            "1708: [D loss: 0.356926, acc: 0.871094]  [G loss: 1.032586, acc: 0.656250]\n",
            "1709: [D loss: 0.145374, acc: 0.941406]  [G loss: 0.999141, acc: 0.617188]\n",
            "1710: [D loss: 0.180526, acc: 0.941406]  [G loss: 1.302227, acc: 0.531250]\n",
            "1711: [D loss: 0.151230, acc: 0.933594]  [G loss: 1.227967, acc: 0.562500]\n",
            "1712: [D loss: 0.143459, acc: 0.949219]  [G loss: 1.759012, acc: 0.437500]\n",
            "1713: [D loss: 0.146007, acc: 0.953125]  [G loss: 1.529390, acc: 0.492188]\n",
            "1714: [D loss: 0.131747, acc: 0.941406]  [G loss: 1.304091, acc: 0.531250]\n",
            "1715: [D loss: 0.265216, acc: 0.902344]  [G loss: 1.318542, acc: 0.523438]\n",
            "1716: [D loss: 0.121146, acc: 0.964844]  [G loss: 1.316951, acc: 0.562500]\n",
            "1717: [D loss: 0.161846, acc: 0.933594]  [G loss: 1.138699, acc: 0.492188]\n",
            "1718: [D loss: 0.146076, acc: 0.945312]  [G loss: 1.520891, acc: 0.460938]\n",
            "1719: [D loss: 0.159628, acc: 0.949219]  [G loss: 1.628832, acc: 0.406250]\n",
            "1720: [D loss: 0.132204, acc: 0.953125]  [G loss: 1.910285, acc: 0.343750]\n",
            "1721: [D loss: 0.139056, acc: 0.949219]  [G loss: 1.970054, acc: 0.343750]\n",
            "1722: [D loss: 0.162047, acc: 0.937500]  [G loss: 2.205363, acc: 0.335938]\n",
            "1723: [D loss: 0.147338, acc: 0.953125]  [G loss: 1.911602, acc: 0.359375]\n",
            "1724: [D loss: 0.109429, acc: 0.957031]  [G loss: 2.340717, acc: 0.335938]\n",
            "1725: [D loss: 0.207227, acc: 0.925781]  [G loss: 2.329885, acc: 0.312500]\n",
            "1726: [D loss: 0.185675, acc: 0.921875]  [G loss: 2.344902, acc: 0.320312]\n",
            "1727: [D loss: 0.112570, acc: 0.957031]  [G loss: 2.636667, acc: 0.257812]\n",
            "1728: [D loss: 0.225774, acc: 0.933594]  [G loss: 1.939958, acc: 0.375000]\n",
            "1729: [D loss: 0.237124, acc: 0.917969]  [G loss: 1.632040, acc: 0.429688]\n",
            "1730: [D loss: 0.161858, acc: 0.933594]  [G loss: 1.799648, acc: 0.382812]\n",
            "1731: [D loss: 0.204673, acc: 0.925781]  [G loss: 2.259830, acc: 0.273438]\n",
            "1732: [D loss: 0.108558, acc: 0.960938]  [G loss: 2.236421, acc: 0.328125]\n",
            "1733: [D loss: 0.129132, acc: 0.949219]  [G loss: 2.254417, acc: 0.281250]\n",
            "1734: [D loss: 0.151597, acc: 0.941406]  [G loss: 2.169826, acc: 0.335938]\n",
            "1735: [D loss: 0.239745, acc: 0.902344]  [G loss: 2.042468, acc: 0.304688]\n",
            "1736: [D loss: 0.213342, acc: 0.933594]  [G loss: 2.483585, acc: 0.234375]\n",
            "1737: [D loss: 0.256175, acc: 0.914062]  [G loss: 2.050092, acc: 0.296875]\n",
            "1738: [D loss: 0.222186, acc: 0.902344]  [G loss: 1.615703, acc: 0.320312]\n",
            "1739: [D loss: 0.275096, acc: 0.882812]  [G loss: 1.918795, acc: 0.265625]\n",
            "1740: [D loss: 0.181957, acc: 0.937500]  [G loss: 1.838130, acc: 0.289062]\n",
            "1741: [D loss: 0.148462, acc: 0.953125]  [G loss: 1.725736, acc: 0.328125]\n",
            "1742: [D loss: 0.182800, acc: 0.929688]  [G loss: 1.785711, acc: 0.312500]\n",
            "1743: [D loss: 0.162116, acc: 0.945312]  [G loss: 1.855910, acc: 0.359375]\n",
            "1744: [D loss: 0.149584, acc: 0.949219]  [G loss: 1.603418, acc: 0.445312]\n",
            "1745: [D loss: 0.108345, acc: 0.945312]  [G loss: 1.592955, acc: 0.398438]\n",
            "1746: [D loss: 0.175462, acc: 0.925781]  [G loss: 1.682927, acc: 0.398438]\n",
            "1747: [D loss: 0.147065, acc: 0.937500]  [G loss: 1.803687, acc: 0.375000]\n",
            "1748: [D loss: 0.109655, acc: 0.960938]  [G loss: 1.748982, acc: 0.343750]\n",
            "1749: [D loss: 0.090750, acc: 0.972656]  [G loss: 1.654359, acc: 0.382812]\n",
            "1750: [D loss: 0.131096, acc: 0.957031]  [G loss: 1.879103, acc: 0.335938]\n",
            "1751: [D loss: 0.159801, acc: 0.933594]  [G loss: 1.378775, acc: 0.414062]\n",
            "1752: [D loss: 0.093201, acc: 0.964844]  [G loss: 1.626210, acc: 0.375000]\n",
            "1753: [D loss: 0.113833, acc: 0.953125]  [G loss: 1.293950, acc: 0.445312]\n",
            "1754: [D loss: 0.122534, acc: 0.949219]  [G loss: 1.170693, acc: 0.531250]\n",
            "1755: [D loss: 0.139788, acc: 0.957031]  [G loss: 1.202318, acc: 0.414062]\n",
            "1756: [D loss: 0.073141, acc: 0.984375]  [G loss: 2.035566, acc: 0.265625]\n",
            "1757: [D loss: 0.179258, acc: 0.945312]  [G loss: 2.010274, acc: 0.296875]\n",
            "1758: [D loss: 0.083278, acc: 0.964844]  [G loss: 1.927743, acc: 0.257812]\n",
            "1759: [D loss: 0.059454, acc: 0.980469]  [G loss: 2.120038, acc: 0.296875]\n",
            "1760: [D loss: 0.092441, acc: 0.960938]  [G loss: 1.779803, acc: 0.359375]\n",
            "1761: [D loss: 0.123464, acc: 0.960938]  [G loss: 2.317150, acc: 0.265625]\n",
            "1762: [D loss: 0.080724, acc: 0.980469]  [G loss: 2.413505, acc: 0.265625]\n",
            "1763: [D loss: 0.084305, acc: 0.968750]  [G loss: 2.649575, acc: 0.250000]\n",
            "1764: [D loss: 0.071398, acc: 0.972656]  [G loss: 2.381035, acc: 0.320312]\n",
            "1765: [D loss: 0.080669, acc: 0.968750]  [G loss: 1.898652, acc: 0.421875]\n",
            "1766: [D loss: 0.123038, acc: 0.960938]  [G loss: 1.682969, acc: 0.437500]\n",
            "1767: [D loss: 0.075163, acc: 0.964844]  [G loss: 1.784523, acc: 0.453125]\n",
            "1768: [D loss: 0.096223, acc: 0.957031]  [G loss: 2.192805, acc: 0.390625]\n",
            "1769: [D loss: 0.054761, acc: 0.984375]  [G loss: 2.225985, acc: 0.343750]\n",
            "1770: [D loss: 0.053796, acc: 0.984375]  [G loss: 2.552804, acc: 0.312500]\n",
            "1771: [D loss: 0.093177, acc: 0.968750]  [G loss: 1.808560, acc: 0.460938]\n",
            "1772: [D loss: 0.048015, acc: 0.980469]  [G loss: 1.275657, acc: 0.539062]\n",
            "1773: [D loss: 0.129797, acc: 0.960938]  [G loss: 1.382660, acc: 0.500000]\n",
            "1774: [D loss: 0.069561, acc: 0.976562]  [G loss: 1.993128, acc: 0.375000]\n",
            "1775: [D loss: 0.046796, acc: 0.988281]  [G loss: 2.523862, acc: 0.265625]\n",
            "1776: [D loss: 0.089608, acc: 0.968750]  [G loss: 2.511567, acc: 0.234375]\n",
            "1777: [D loss: 0.055277, acc: 0.992188]  [G loss: 1.760858, acc: 0.390625]\n",
            "1778: [D loss: 0.061735, acc: 0.976562]  [G loss: 1.540747, acc: 0.523438]\n",
            "1779: [D loss: 0.070763, acc: 0.980469]  [G loss: 0.903700, acc: 0.625000]\n",
            "1780: [D loss: 0.090999, acc: 0.972656]  [G loss: 0.848677, acc: 0.617188]\n",
            "1781: [D loss: 0.028398, acc: 0.992188]  [G loss: 1.083282, acc: 0.500000]\n",
            "1782: [D loss: 0.028380, acc: 0.992188]  [G loss: 1.085640, acc: 0.562500]\n",
            "1783: [D loss: 0.083725, acc: 0.968750]  [G loss: 1.149122, acc: 0.609375]\n",
            "1784: [D loss: 0.022298, acc: 0.992188]  [G loss: 1.285722, acc: 0.570312]\n",
            "1785: [D loss: 0.056111, acc: 0.980469]  [G loss: 1.265871, acc: 0.523438]\n",
            "1786: [D loss: 0.026054, acc: 0.984375]  [G loss: 1.149961, acc: 0.570312]\n",
            "1787: [D loss: 0.011293, acc: 1.000000]  [G loss: 1.355241, acc: 0.539062]\n",
            "1788: [D loss: 0.032219, acc: 0.988281]  [G loss: 0.825515, acc: 0.656250]\n",
            "1789: [D loss: 0.038566, acc: 0.984375]  [G loss: 0.447599, acc: 0.789062]\n",
            "1790: [D loss: 0.073744, acc: 0.972656]  [G loss: 0.320155, acc: 0.890625]\n",
            "1791: [D loss: 0.075408, acc: 0.976562]  [G loss: 0.374905, acc: 0.867188]\n",
            "1792: [D loss: 0.033289, acc: 0.984375]  [G loss: 0.516838, acc: 0.812500]\n",
            "1793: [D loss: 0.056340, acc: 0.980469]  [G loss: 0.497923, acc: 0.812500]\n",
            "1794: [D loss: 0.031770, acc: 0.988281]  [G loss: 0.525970, acc: 0.781250]\n",
            "1795: [D loss: 0.073029, acc: 0.976562]  [G loss: 0.188181, acc: 0.929688]\n",
            "1796: [D loss: 0.051061, acc: 0.976562]  [G loss: 0.301028, acc: 0.875000]\n",
            "1797: [D loss: 0.070733, acc: 0.984375]  [G loss: 0.198640, acc: 0.914062]\n",
            "1798: [D loss: 0.091452, acc: 0.968750]  [G loss: 0.304408, acc: 0.859375]\n",
            "1799: [D loss: 0.095302, acc: 0.980469]  [G loss: 0.284307, acc: 0.835938]\n",
            "1800: [D loss: 0.028972, acc: 0.988281]  [G loss: 0.405013, acc: 0.835938]\n",
            "1801: [D loss: 0.026396, acc: 0.992188]  [G loss: 0.492703, acc: 0.820312]\n",
            "1802: [D loss: 0.035994, acc: 0.992188]  [G loss: 0.428322, acc: 0.835938]\n",
            "1803: [D loss: 0.070040, acc: 0.984375]  [G loss: 0.432814, acc: 0.835938]\n",
            "1804: [D loss: 0.016966, acc: 0.992188]  [G loss: 0.543506, acc: 0.835938]\n",
            "1805: [D loss: 0.033714, acc: 0.984375]  [G loss: 0.441115, acc: 0.820312]\n",
            "1806: [D loss: 0.053748, acc: 0.976562]  [G loss: 0.396514, acc: 0.867188]\n",
            "1807: [D loss: 0.014315, acc: 0.992188]  [G loss: 0.616522, acc: 0.789062]\n",
            "1808: [D loss: 0.020344, acc: 0.992188]  [G loss: 0.515488, acc: 0.796875]\n",
            "1809: [D loss: 0.088602, acc: 0.976562]  [G loss: 0.416599, acc: 0.851562]\n",
            "1810: [D loss: 0.054678, acc: 0.988281]  [G loss: 0.272905, acc: 0.882812]\n",
            "1811: [D loss: 0.076714, acc: 0.980469]  [G loss: 0.116059, acc: 0.929688]\n",
            "1812: [D loss: 0.041439, acc: 0.988281]  [G loss: 0.071433, acc: 0.984375]\n",
            "1813: [D loss: 0.063550, acc: 0.980469]  [G loss: 0.135808, acc: 0.960938]\n",
            "1814: [D loss: 0.035445, acc: 0.988281]  [G loss: 0.140344, acc: 0.960938]\n",
            "1815: [D loss: 0.048738, acc: 0.980469]  [G loss: 0.329756, acc: 0.820312]\n",
            "1816: [D loss: 0.035535, acc: 0.984375]  [G loss: 0.345954, acc: 0.859375]\n",
            "1817: [D loss: 0.022384, acc: 0.996094]  [G loss: 0.398637, acc: 0.820312]\n",
            "1818: [D loss: 0.017451, acc: 0.992188]  [G loss: 0.414601, acc: 0.796875]\n",
            "1819: [D loss: 0.041258, acc: 0.984375]  [G loss: 0.288153, acc: 0.859375]\n",
            "1820: [D loss: 0.061997, acc: 0.992188]  [G loss: 0.319402, acc: 0.835938]\n",
            "1821: [D loss: 0.068967, acc: 0.984375]  [G loss: 0.290743, acc: 0.851562]\n",
            "1822: [D loss: 0.033567, acc: 0.992188]  [G loss: 0.287919, acc: 0.835938]\n",
            "1823: [D loss: 0.049966, acc: 0.980469]  [G loss: 0.374696, acc: 0.804688]\n",
            "1824: [D loss: 0.033275, acc: 0.992188]  [G loss: 0.526486, acc: 0.726562]\n",
            "1825: [D loss: 0.035153, acc: 0.992188]  [G loss: 0.676859, acc: 0.710938]\n",
            "1826: [D loss: 0.015954, acc: 0.996094]  [G loss: 0.947139, acc: 0.593750]\n",
            "1827: [D loss: 0.058818, acc: 0.988281]  [G loss: 0.978791, acc: 0.625000]\n",
            "1828: [D loss: 0.028585, acc: 0.988281]  [G loss: 0.744804, acc: 0.656250]\n",
            "1829: [D loss: 0.019778, acc: 0.992188]  [G loss: 0.600247, acc: 0.718750]\n",
            "1830: [D loss: 0.012251, acc: 0.992188]  [G loss: 0.563967, acc: 0.726562]\n",
            "1831: [D loss: 0.016948, acc: 1.000000]  [G loss: 0.593485, acc: 0.718750]\n",
            "1832: [D loss: 0.015297, acc: 0.992188]  [G loss: 0.547852, acc: 0.757812]\n",
            "1833: [D loss: 0.007379, acc: 1.000000]  [G loss: 0.516977, acc: 0.742188]\n",
            "1834: [D loss: 0.007585, acc: 1.000000]  [G loss: 0.497832, acc: 0.718750]\n",
            "1835: [D loss: 0.010346, acc: 1.000000]  [G loss: 0.653778, acc: 0.632812]\n",
            "1836: [D loss: 0.008234, acc: 1.000000]  [G loss: 0.825840, acc: 0.601562]\n",
            "1837: [D loss: 0.010750, acc: 0.992188]  [G loss: 0.671670, acc: 0.671875]\n",
            "1838: [D loss: 0.025347, acc: 0.996094]  [G loss: 0.710465, acc: 0.632812]\n",
            "1839: [D loss: 0.063041, acc: 0.980469]  [G loss: 0.482850, acc: 0.742188]\n",
            "1840: [D loss: 0.044802, acc: 0.992188]  [G loss: 0.394529, acc: 0.781250]\n",
            "1841: [D loss: 0.021691, acc: 0.996094]  [G loss: 0.297975, acc: 0.812500]\n",
            "1842: [D loss: 0.070965, acc: 0.988281]  [G loss: 0.415327, acc: 0.765625]\n",
            "1843: [D loss: 0.038524, acc: 0.988281]  [G loss: 0.555786, acc: 0.710938]\n",
            "1844: [D loss: 0.017708, acc: 0.996094]  [G loss: 0.815067, acc: 0.640625]\n",
            "1845: [D loss: 0.020791, acc: 0.996094]  [G loss: 1.259817, acc: 0.539062]\n",
            "1846: [D loss: 0.012002, acc: 0.992188]  [G loss: 1.438980, acc: 0.460938]\n",
            "1847: [D loss: 0.023616, acc: 0.988281]  [G loss: 1.795145, acc: 0.406250]\n",
            "1848: [D loss: 0.037222, acc: 0.984375]  [G loss: 1.279694, acc: 0.546875]\n",
            "1849: [D loss: 0.013917, acc: 0.992188]  [G loss: 0.755858, acc: 0.671875]\n",
            "1850: [D loss: 0.052363, acc: 0.992188]  [G loss: 0.541282, acc: 0.718750]\n",
            "1851: [D loss: 0.048261, acc: 0.984375]  [G loss: 0.797181, acc: 0.632812]\n",
            "1852: [D loss: 0.044250, acc: 0.984375]  [G loss: 0.801009, acc: 0.617188]\n",
            "1853: [D loss: 0.023865, acc: 0.992188]  [G loss: 1.121344, acc: 0.570312]\n",
            "1854: [D loss: 0.061806, acc: 0.988281]  [G loss: 1.218548, acc: 0.500000]\n",
            "1855: [D loss: 0.032464, acc: 0.992188]  [G loss: 1.589510, acc: 0.468750]\n",
            "1856: [D loss: 0.077503, acc: 0.984375]  [G loss: 1.448027, acc: 0.515625]\n",
            "1857: [D loss: 0.058771, acc: 0.988281]  [G loss: 1.636130, acc: 0.445312]\n",
            "1858: [D loss: 0.039450, acc: 0.984375]  [G loss: 1.710565, acc: 0.476562]\n",
            "1859: [D loss: 0.054222, acc: 0.980469]  [G loss: 1.754164, acc: 0.460938]\n",
            "1860: [D loss: 0.076781, acc: 0.964844]  [G loss: 1.651605, acc: 0.476562]\n",
            "1861: [D loss: 0.095894, acc: 0.964844]  [G loss: 1.522086, acc: 0.500000]\n",
            "1862: [D loss: 0.104151, acc: 0.972656]  [G loss: 1.635621, acc: 0.468750]\n",
            "1863: [D loss: 0.098021, acc: 0.972656]  [G loss: 1.715444, acc: 0.445312]\n",
            "1864: [D loss: 0.073996, acc: 0.972656]  [G loss: 1.527483, acc: 0.507812]\n",
            "1865: [D loss: 0.073194, acc: 0.972656]  [G loss: 1.671828, acc: 0.445312]\n",
            "1866: [D loss: 0.071668, acc: 0.972656]  [G loss: 2.141126, acc: 0.281250]\n",
            "1867: [D loss: 0.044872, acc: 0.988281]  [G loss: 2.660458, acc: 0.304688]\n",
            "1868: [D loss: 0.063158, acc: 0.972656]  [G loss: 2.873925, acc: 0.281250]\n",
            "1869: [D loss: 0.028176, acc: 0.988281]  [G loss: 2.998759, acc: 0.210938]\n",
            "1870: [D loss: 0.095792, acc: 0.968750]  [G loss: 2.545628, acc: 0.304688]\n",
            "1871: [D loss: 0.050697, acc: 0.984375]  [G loss: 2.798271, acc: 0.265625]\n",
            "1872: [D loss: 0.074525, acc: 0.968750]  [G loss: 3.140095, acc: 0.242188]\n",
            "1873: [D loss: 0.066578, acc: 0.976562]  [G loss: 4.178235, acc: 0.203125]\n",
            "1874: [D loss: 0.060028, acc: 0.984375]  [G loss: 4.216959, acc: 0.210938]\n",
            "1875: [D loss: 0.170437, acc: 0.945312]  [G loss: 3.345781, acc: 0.265625]\n",
            "1876: [D loss: 0.105481, acc: 0.957031]  [G loss: 3.388487, acc: 0.265625]\n",
            "1877: [D loss: 0.060160, acc: 0.980469]  [G loss: 3.674678, acc: 0.273438]\n",
            "1878: [D loss: 0.053543, acc: 0.976562]  [G loss: 4.333664, acc: 0.203125]\n",
            "1879: [D loss: 0.084225, acc: 0.968750]  [G loss: 5.863540, acc: 0.171875]\n",
            "1880: [D loss: 0.082472, acc: 0.964844]  [G loss: 6.589401, acc: 0.132812]\n",
            "1881: [D loss: 0.037801, acc: 0.988281]  [G loss: 6.650376, acc: 0.125000]\n",
            "1882: [D loss: 0.067725, acc: 0.976562]  [G loss: 6.366719, acc: 0.164062]\n",
            "1883: [D loss: 0.062527, acc: 0.976562]  [G loss: 6.025326, acc: 0.210938]\n",
            "1884: [D loss: 0.045840, acc: 0.972656]  [G loss: 6.348500, acc: 0.164062]\n",
            "1885: [D loss: 0.135642, acc: 0.964844]  [G loss: 5.970580, acc: 0.171875]\n",
            "1886: [D loss: 0.033389, acc: 0.984375]  [G loss: 6.627716, acc: 0.179688]\n",
            "1887: [D loss: 0.101382, acc: 0.976562]  [G loss: 6.230295, acc: 0.125000]\n",
            "1888: [D loss: 0.080435, acc: 0.980469]  [G loss: 6.607895, acc: 0.156250]\n",
            "1889: [D loss: 0.063140, acc: 0.984375]  [G loss: 6.659719, acc: 0.125000]\n",
            "1890: [D loss: 0.020041, acc: 0.996094]  [G loss: 5.789556, acc: 0.164062]\n",
            "1891: [D loss: 0.010922, acc: 0.996094]  [G loss: 5.752149, acc: 0.250000]\n",
            "1892: [D loss: 0.043777, acc: 0.980469]  [G loss: 5.103139, acc: 0.335938]\n",
            "1893: [D loss: 0.056854, acc: 0.984375]  [G loss: 5.408129, acc: 0.335938]\n",
            "1894: [D loss: 0.183703, acc: 0.957031]  [G loss: 5.277851, acc: 0.343750]\n",
            "1895: [D loss: 0.100881, acc: 0.968750]  [G loss: 4.845015, acc: 0.320312]\n",
            "1896: [D loss: 0.125331, acc: 0.949219]  [G loss: 4.335835, acc: 0.398438]\n",
            "1897: [D loss: 0.101188, acc: 0.972656]  [G loss: 3.731943, acc: 0.406250]\n",
            "1898: [D loss: 0.118993, acc: 0.949219]  [G loss: 3.403009, acc: 0.492188]\n",
            "1899: [D loss: 0.157281, acc: 0.933594]  [G loss: 3.431498, acc: 0.445312]\n",
            "1900: [D loss: 0.155906, acc: 0.953125]  [G loss: 3.356264, acc: 0.437500]\n",
            "1901: [D loss: 0.109190, acc: 0.964844]  [G loss: 3.557256, acc: 0.359375]\n",
            "1902: [D loss: 0.186952, acc: 0.941406]  [G loss: 2.842077, acc: 0.406250]\n",
            "1903: [D loss: 0.155287, acc: 0.960938]  [G loss: 2.980649, acc: 0.445312]\n",
            "1904: [D loss: 0.136961, acc: 0.960938]  [G loss: 2.502086, acc: 0.468750]\n",
            "1905: [D loss: 0.211938, acc: 0.917969]  [G loss: 2.131494, acc: 0.515625]\n",
            "1906: [D loss: 0.199411, acc: 0.941406]  [G loss: 2.582303, acc: 0.484375]\n",
            "1907: [D loss: 0.270167, acc: 0.894531]  [G loss: 2.228693, acc: 0.484375]\n",
            "1908: [D loss: 0.194488, acc: 0.949219]  [G loss: 2.000795, acc: 0.539062]\n",
            "1909: [D loss: 0.169352, acc: 0.953125]  [G loss: 1.464586, acc: 0.617188]\n",
            "1910: [D loss: 0.108695, acc: 0.949219]  [G loss: 1.112705, acc: 0.695312]\n",
            "1911: [D loss: 0.139016, acc: 0.949219]  [G loss: 1.253377, acc: 0.671875]\n",
            "1912: [D loss: 0.121352, acc: 0.949219]  [G loss: 0.792992, acc: 0.710938]\n",
            "1913: [D loss: 0.105484, acc: 0.957031]  [G loss: 0.697512, acc: 0.734375]\n",
            "1914: [D loss: 0.145526, acc: 0.957031]  [G loss: 0.902837, acc: 0.679688]\n",
            "1915: [D loss: 0.089881, acc: 0.957031]  [G loss: 0.870633, acc: 0.671875]\n",
            "1916: [D loss: 0.196340, acc: 0.933594]  [G loss: 1.023320, acc: 0.679688]\n",
            "1917: [D loss: 0.238382, acc: 0.929688]  [G loss: 1.541632, acc: 0.585938]\n",
            "1918: [D loss: 0.107620, acc: 0.964844]  [G loss: 1.079461, acc: 0.648438]\n",
            "1919: [D loss: 0.142940, acc: 0.941406]  [G loss: 1.691876, acc: 0.554688]\n",
            "1920: [D loss: 0.309109, acc: 0.882812]  [G loss: 1.606748, acc: 0.507812]\n",
            "1921: [D loss: 0.201691, acc: 0.921875]  [G loss: 2.610641, acc: 0.421875]\n",
            "1922: [D loss: 0.223623, acc: 0.902344]  [G loss: 3.208705, acc: 0.335938]\n",
            "1923: [D loss: 0.239758, acc: 0.906250]  [G loss: 3.275719, acc: 0.390625]\n",
            "1924: [D loss: 0.238472, acc: 0.894531]  [G loss: 3.477278, acc: 0.281250]\n",
            "1925: [D loss: 0.187325, acc: 0.937500]  [G loss: 4.460839, acc: 0.242188]\n",
            "1926: [D loss: 0.169128, acc: 0.937500]  [G loss: 4.980235, acc: 0.171875]\n",
            "1927: [D loss: 0.168181, acc: 0.949219]  [G loss: 3.555873, acc: 0.257812]\n",
            "1928: [D loss: 0.075616, acc: 0.976562]  [G loss: 2.839697, acc: 0.343750]\n",
            "1929: [D loss: 0.141284, acc: 0.945312]  [G loss: 3.047519, acc: 0.320312]\n",
            "1930: [D loss: 0.090942, acc: 0.976562]  [G loss: 3.342124, acc: 0.218750]\n",
            "1931: [D loss: 0.119126, acc: 0.953125]  [G loss: 3.259925, acc: 0.312500]\n",
            "1932: [D loss: 0.081225, acc: 0.968750]  [G loss: 3.086769, acc: 0.304688]\n",
            "1933: [D loss: 0.080396, acc: 0.964844]  [G loss: 4.097675, acc: 0.226562]\n",
            "1934: [D loss: 0.083394, acc: 0.960938]  [G loss: 4.164658, acc: 0.257812]\n",
            "1935: [D loss: 0.109494, acc: 0.957031]  [G loss: 3.154015, acc: 0.289062]\n",
            "1936: [D loss: 0.129513, acc: 0.949219]  [G loss: 2.098604, acc: 0.453125]\n",
            "1937: [D loss: 0.149652, acc: 0.933594]  [G loss: 3.110628, acc: 0.351562]\n",
            "1938: [D loss: 0.111446, acc: 0.964844]  [G loss: 3.785010, acc: 0.281250]\n",
            "1939: [D loss: 0.081338, acc: 0.976562]  [G loss: 3.307138, acc: 0.359375]\n",
            "1940: [D loss: 0.085269, acc: 0.972656]  [G loss: 3.329770, acc: 0.335938]\n",
            "1941: [D loss: 0.099153, acc: 0.953125]  [G loss: 2.737984, acc: 0.335938]\n",
            "1942: [D loss: 0.138375, acc: 0.937500]  [G loss: 1.982853, acc: 0.468750]\n",
            "1943: [D loss: 0.098788, acc: 0.949219]  [G loss: 2.189743, acc: 0.351562]\n",
            "1944: [D loss: 0.108748, acc: 0.968750]  [G loss: 2.275685, acc: 0.359375]\n",
            "1945: [D loss: 0.105487, acc: 0.972656]  [G loss: 2.117873, acc: 0.429688]\n",
            "1946: [D loss: 0.143137, acc: 0.953125]  [G loss: 1.352027, acc: 0.531250]\n",
            "1947: [D loss: 0.166113, acc: 0.933594]  [G loss: 3.024063, acc: 0.289062]\n",
            "1948: [D loss: 0.138207, acc: 0.941406]  [G loss: 2.866460, acc: 0.296875]\n",
            "1949: [D loss: 0.246929, acc: 0.953125]  [G loss: 1.510088, acc: 0.460938]\n",
            "1950: [D loss: 0.172716, acc: 0.941406]  [G loss: 2.073608, acc: 0.453125]\n",
            "1951: [D loss: 0.081965, acc: 0.972656]  [G loss: 3.641139, acc: 0.242188]\n",
            "1952: [D loss: 0.165104, acc: 0.937500]  [G loss: 3.172768, acc: 0.250000]\n",
            "1953: [D loss: 0.130827, acc: 0.964844]  [G loss: 2.134475, acc: 0.453125]\n",
            "1954: [D loss: 0.170926, acc: 0.937500]  [G loss: 3.014471, acc: 0.265625]\n",
            "1955: [D loss: 0.101501, acc: 0.968750]  [G loss: 4.626764, acc: 0.140625]\n",
            "1956: [D loss: 0.141927, acc: 0.945312]  [G loss: 3.074882, acc: 0.273438]\n",
            "1957: [D loss: 0.109173, acc: 0.957031]  [G loss: 2.563727, acc: 0.359375]\n",
            "1958: [D loss: 0.128575, acc: 0.941406]  [G loss: 3.568780, acc: 0.210938]\n",
            "1959: [D loss: 0.120161, acc: 0.957031]  [G loss: 3.959481, acc: 0.156250]\n",
            "1960: [D loss: 0.224878, acc: 0.941406]  [G loss: 2.283218, acc: 0.382812]\n",
            "1961: [D loss: 0.138013, acc: 0.929688]  [G loss: 2.123470, acc: 0.406250]\n",
            "1962: [D loss: 0.175766, acc: 0.945312]  [G loss: 4.109447, acc: 0.164062]\n",
            "1963: [D loss: 0.091478, acc: 0.960938]  [G loss: 4.862679, acc: 0.093750]\n",
            "1964: [D loss: 0.343773, acc: 0.886719]  [G loss: 2.612389, acc: 0.281250]\n",
            "1965: [D loss: 0.103400, acc: 0.964844]  [G loss: 1.364486, acc: 0.507812]\n",
            "1966: [D loss: 0.257528, acc: 0.878906]  [G loss: 2.435358, acc: 0.281250]\n",
            "1967: [D loss: 0.122967, acc: 0.960938]  [G loss: 4.009993, acc: 0.140625]\n",
            "1968: [D loss: 0.106847, acc: 0.953125]  [G loss: 4.256015, acc: 0.117188]\n",
            "1969: [D loss: 0.218368, acc: 0.933594]  [G loss: 3.319637, acc: 0.187500]\n",
            "1970: [D loss: 0.152049, acc: 0.949219]  [G loss: 1.678607, acc: 0.421875]\n",
            "1971: [D loss: 0.086141, acc: 0.976562]  [G loss: 1.051151, acc: 0.554688]\n",
            "1972: [D loss: 0.157320, acc: 0.945312]  [G loss: 1.579321, acc: 0.398438]\n",
            "1973: [D loss: 0.084356, acc: 0.972656]  [G loss: 2.331086, acc: 0.304688]\n",
            "1974: [D loss: 0.044454, acc: 0.988281]  [G loss: 2.886324, acc: 0.210938]\n",
            "1975: [D loss: 0.048782, acc: 0.988281]  [G loss: 3.521885, acc: 0.156250]\n",
            "1976: [D loss: 0.114448, acc: 0.968750]  [G loss: 3.010801, acc: 0.234375]\n",
            "1977: [D loss: 0.106633, acc: 0.960938]  [G loss: 2.635563, acc: 0.273438]\n",
            "1978: [D loss: 0.049723, acc: 0.980469]  [G loss: 1.982674, acc: 0.375000]\n",
            "1979: [D loss: 0.085978, acc: 0.976562]  [G loss: 1.748036, acc: 0.406250]\n",
            "1980: [D loss: 0.098951, acc: 0.980469]  [G loss: 1.471595, acc: 0.492188]\n",
            "1981: [D loss: 0.109741, acc: 0.964844]  [G loss: 1.486093, acc: 0.476562]\n",
            "1982: [D loss: 0.082951, acc: 0.972656]  [G loss: 1.628979, acc: 0.445312]\n",
            "1983: [D loss: 0.050508, acc: 0.988281]  [G loss: 1.740346, acc: 0.351562]\n",
            "1984: [D loss: 0.039462, acc: 0.992188]  [G loss: 2.091707, acc: 0.320312]\n",
            "1985: [D loss: 0.068747, acc: 0.980469]  [G loss: 1.975609, acc: 0.351562]\n",
            "1986: [D loss: 0.082281, acc: 0.972656]  [G loss: 1.711323, acc: 0.375000]\n",
            "1987: [D loss: 0.115343, acc: 0.949219]  [G loss: 1.373278, acc: 0.437500]\n",
            "1988: [D loss: 0.123374, acc: 0.949219]  [G loss: 1.230875, acc: 0.453125]\n",
            "1989: [D loss: 0.110485, acc: 0.957031]  [G loss: 1.235383, acc: 0.562500]\n",
            "1990: [D loss: 0.186052, acc: 0.917969]  [G loss: 1.056067, acc: 0.625000]\n",
            "1991: [D loss: 0.114924, acc: 0.960938]  [G loss: 1.322331, acc: 0.531250]\n",
            "1992: [D loss: 0.140898, acc: 0.945312]  [G loss: 1.104853, acc: 0.578125]\n",
            "1993: [D loss: 0.155388, acc: 0.929688]  [G loss: 1.141928, acc: 0.531250]\n",
            "1994: [D loss: 0.129302, acc: 0.949219]  [G loss: 1.348167, acc: 0.476562]\n",
            "1995: [D loss: 0.109065, acc: 0.972656]  [G loss: 1.193806, acc: 0.523438]\n",
            "1996: [D loss: 0.127049, acc: 0.933594]  [G loss: 0.877292, acc: 0.656250]\n",
            "1997: [D loss: 0.110761, acc: 0.960938]  [G loss: 0.958340, acc: 0.601562]\n",
            "1998: [D loss: 0.118943, acc: 0.964844]  [G loss: 0.721178, acc: 0.679688]\n",
            "1999: [D loss: 0.115280, acc: 0.964844]  [G loss: 0.923097, acc: 0.593750]\n",
            "2000: [D loss: 0.064160, acc: 0.972656]  [G loss: 1.218561, acc: 0.460938]\n",
            "2001: [D loss: 0.056989, acc: 0.984375]  [G loss: 1.245454, acc: 0.414062]\n",
            "2002: [D loss: 0.080469, acc: 0.972656]  [G loss: 1.600808, acc: 0.320312]\n",
            "2003: [D loss: 0.105594, acc: 0.960938]  [G loss: 1.472413, acc: 0.320312]\n",
            "2004: [D loss: 0.118357, acc: 0.957031]  [G loss: 1.602421, acc: 0.312500]\n",
            "2005: [D loss: 0.102239, acc: 0.964844]  [G loss: 1.874630, acc: 0.273438]\n",
            "2006: [D loss: 0.112153, acc: 0.960938]  [G loss: 1.617644, acc: 0.406250]\n",
            "2007: [D loss: 0.118601, acc: 0.957031]  [G loss: 1.493181, acc: 0.328125]\n",
            "2008: [D loss: 0.088102, acc: 0.972656]  [G loss: 1.631394, acc: 0.343750]\n",
            "2009: [D loss: 0.062679, acc: 0.984375]  [G loss: 1.889367, acc: 0.304688]\n",
            "2010: [D loss: 0.081696, acc: 0.964844]  [G loss: 2.156231, acc: 0.242188]\n",
            "2011: [D loss: 0.058267, acc: 0.980469]  [G loss: 2.214036, acc: 0.320312]\n",
            "2012: [D loss: 0.101661, acc: 0.964844]  [G loss: 2.489337, acc: 0.296875]\n",
            "2013: [D loss: 0.125132, acc: 0.968750]  [G loss: 2.677529, acc: 0.257812]\n",
            "2014: [D loss: 0.030565, acc: 0.992188]  [G loss: 2.479208, acc: 0.265625]\n",
            "2015: [D loss: 0.098997, acc: 0.972656]  [G loss: 2.470064, acc: 0.281250]\n",
            "2016: [D loss: 0.079891, acc: 0.968750]  [G loss: 2.263918, acc: 0.390625]\n",
            "2017: [D loss: 0.106437, acc: 0.972656]  [G loss: 2.499912, acc: 0.351562]\n",
            "2018: [D loss: 0.079766, acc: 0.968750]  [G loss: 2.356539, acc: 0.375000]\n",
            "2019: [D loss: 0.065375, acc: 0.976562]  [G loss: 2.894974, acc: 0.312500]\n",
            "2020: [D loss: 0.054161, acc: 0.984375]  [G loss: 3.384721, acc: 0.335938]\n",
            "2021: [D loss: 0.103998, acc: 0.968750]  [G loss: 3.124669, acc: 0.351562]\n",
            "2022: [D loss: 0.056749, acc: 0.976562]  [G loss: 3.094248, acc: 0.406250]\n",
            "2023: [D loss: 0.092624, acc: 0.972656]  [G loss: 3.111992, acc: 0.375000]\n",
            "2024: [D loss: 0.125016, acc: 0.960938]  [G loss: 3.175467, acc: 0.351562]\n",
            "2025: [D loss: 0.188192, acc: 0.941406]  [G loss: 3.001916, acc: 0.328125]\n",
            "2026: [D loss: 0.219484, acc: 0.929688]  [G loss: 3.274278, acc: 0.312500]\n",
            "2027: [D loss: 0.201978, acc: 0.925781]  [G loss: 2.604544, acc: 0.351562]\n",
            "2028: [D loss: 0.157557, acc: 0.933594]  [G loss: 3.440066, acc: 0.335938]\n",
            "2029: [D loss: 0.136293, acc: 0.953125]  [G loss: 3.281434, acc: 0.335938]\n",
            "2030: [D loss: 0.076420, acc: 0.968750]  [G loss: 3.786681, acc: 0.304688]\n",
            "2031: [D loss: 0.175488, acc: 0.949219]  [G loss: 3.792581, acc: 0.304688]\n",
            "2032: [D loss: 0.232840, acc: 0.914062]  [G loss: 4.344866, acc: 0.195312]\n",
            "2033: [D loss: 0.239064, acc: 0.921875]  [G loss: 3.756349, acc: 0.304688]\n",
            "2034: [D loss: 0.134578, acc: 0.945312]  [G loss: 3.282933, acc: 0.359375]\n",
            "2035: [D loss: 0.159811, acc: 0.949219]  [G loss: 2.592561, acc: 0.468750]\n",
            "2036: [D loss: 0.245954, acc: 0.925781]  [G loss: 2.850355, acc: 0.351562]\n",
            "2037: [D loss: 0.171639, acc: 0.949219]  [G loss: 3.714486, acc: 0.250000]\n",
            "2038: [D loss: 0.088515, acc: 0.968750]  [G loss: 4.811350, acc: 0.132812]\n",
            "2039: [D loss: 0.084633, acc: 0.972656]  [G loss: 4.780010, acc: 0.117188]\n",
            "2040: [D loss: 0.210115, acc: 0.941406]  [G loss: 3.473094, acc: 0.242188]\n",
            "2041: [D loss: 0.193945, acc: 0.945312]  [G loss: 3.864134, acc: 0.242188]\n",
            "2042: [D loss: 0.168026, acc: 0.945312]  [G loss: 4.591515, acc: 0.242188]\n",
            "2043: [D loss: 0.116698, acc: 0.964844]  [G loss: 4.324837, acc: 0.289062]\n",
            "2044: [D loss: 0.147543, acc: 0.953125]  [G loss: 4.140657, acc: 0.312500]\n",
            "2045: [D loss: 0.086374, acc: 0.968750]  [G loss: 3.277283, acc: 0.406250]\n",
            "2046: [D loss: 0.085994, acc: 0.968750]  [G loss: 2.259627, acc: 0.515625]\n",
            "2047: [D loss: 0.097331, acc: 0.957031]  [G loss: 2.096588, acc: 0.531250]\n",
            "2048: [D loss: 0.115124, acc: 0.953125]  [G loss: 1.948963, acc: 0.531250]\n",
            "2049: [D loss: 0.073227, acc: 0.964844]  [G loss: 2.457127, acc: 0.531250]\n",
            "2050: [D loss: 0.112711, acc: 0.960938]  [G loss: 2.590748, acc: 0.484375]\n",
            "2051: [D loss: 0.079979, acc: 0.968750]  [G loss: 1.648748, acc: 0.671875]\n",
            "2052: [D loss: 0.074013, acc: 0.980469]  [G loss: 1.132502, acc: 0.671875]\n",
            "2053: [D loss: 0.140772, acc: 0.957031]  [G loss: 1.466479, acc: 0.679688]\n",
            "2054: [D loss: 0.045135, acc: 0.984375]  [G loss: 1.843046, acc: 0.625000]\n",
            "2055: [D loss: 0.183619, acc: 0.937500]  [G loss: 1.391786, acc: 0.734375]\n",
            "2056: [D loss: 0.119077, acc: 0.957031]  [G loss: 1.248512, acc: 0.742188]\n",
            "2057: [D loss: 0.113619, acc: 0.953125]  [G loss: 0.791903, acc: 0.835938]\n",
            "2058: [D loss: 0.076486, acc: 0.972656]  [G loss: 1.008928, acc: 0.828125]\n",
            "2059: [D loss: 0.054747, acc: 0.968750]  [G loss: 1.076972, acc: 0.796875]\n",
            "2060: [D loss: 0.134531, acc: 0.953125]  [G loss: 0.702215, acc: 0.867188]\n",
            "2061: [D loss: 0.145771, acc: 0.953125]  [G loss: 0.775509, acc: 0.851562]\n",
            "2062: [D loss: 0.124738, acc: 0.953125]  [G loss: 0.773130, acc: 0.875000]\n",
            "2063: [D loss: 0.121316, acc: 0.941406]  [G loss: 0.815620, acc: 0.859375]\n",
            "2064: [D loss: 0.136178, acc: 0.953125]  [G loss: 0.961686, acc: 0.867188]\n",
            "2065: [D loss: 0.075263, acc: 0.976562]  [G loss: 0.727158, acc: 0.867188]\n",
            "2066: [D loss: 0.139362, acc: 0.949219]  [G loss: 1.433149, acc: 0.781250]\n",
            "2067: [D loss: 0.132216, acc: 0.964844]  [G loss: 2.005749, acc: 0.609375]\n",
            "2068: [D loss: 0.131170, acc: 0.960938]  [G loss: 1.874039, acc: 0.617188]\n",
            "2069: [D loss: 0.124788, acc: 0.949219]  [G loss: 0.992465, acc: 0.718750]\n",
            "2070: [D loss: 0.090112, acc: 0.976562]  [G loss: 0.432590, acc: 0.828125]\n",
            "2071: [D loss: 0.234315, acc: 0.925781]  [G loss: 1.179788, acc: 0.726562]\n",
            "2072: [D loss: 0.155299, acc: 0.949219]  [G loss: 2.118573, acc: 0.523438]\n",
            "2073: [D loss: 0.105342, acc: 0.960938]  [G loss: 2.652761, acc: 0.453125]\n",
            "2074: [D loss: 0.109184, acc: 0.949219]  [G loss: 2.094579, acc: 0.554688]\n",
            "2075: [D loss: 0.216765, acc: 0.937500]  [G loss: 0.655843, acc: 0.757812]\n",
            "2076: [D loss: 0.104582, acc: 0.941406]  [G loss: 0.655641, acc: 0.773438]\n",
            "2077: [D loss: 0.154367, acc: 0.937500]  [G loss: 1.305854, acc: 0.609375]\n",
            "2078: [D loss: 0.132646, acc: 0.953125]  [G loss: 3.384101, acc: 0.296875]\n",
            "2079: [D loss: 0.142327, acc: 0.960938]  [G loss: 5.582732, acc: 0.195312]\n",
            "2080: [D loss: 0.180135, acc: 0.953125]  [G loss: 5.033859, acc: 0.218750]\n",
            "2081: [D loss: 0.202943, acc: 0.949219]  [G loss: 3.126998, acc: 0.335938]\n",
            "2082: [D loss: 0.154401, acc: 0.949219]  [G loss: 1.575302, acc: 0.632812]\n",
            "2083: [D loss: 0.138064, acc: 0.953125]  [G loss: 1.607964, acc: 0.554688]\n",
            "2084: [D loss: 0.141885, acc: 0.937500]  [G loss: 2.089026, acc: 0.382812]\n",
            "2085: [D loss: 0.096002, acc: 0.957031]  [G loss: 2.825933, acc: 0.304688]\n",
            "2086: [D loss: 0.152540, acc: 0.957031]  [G loss: 2.981237, acc: 0.250000]\n",
            "2087: [D loss: 0.174350, acc: 0.953125]  [G loss: 2.238729, acc: 0.375000]\n",
            "2088: [D loss: 0.067987, acc: 0.972656]  [G loss: 1.847556, acc: 0.398438]\n",
            "2089: [D loss: 0.112092, acc: 0.949219]  [G loss: 1.642109, acc: 0.500000]\n",
            "2090: [D loss: 0.041691, acc: 0.988281]  [G loss: 1.899135, acc: 0.437500]\n",
            "2091: [D loss: 0.074158, acc: 0.984375]  [G loss: 2.432176, acc: 0.414062]\n",
            "2092: [D loss: 0.087257, acc: 0.976562]  [G loss: 2.739306, acc: 0.382812]\n",
            "2093: [D loss: 0.073192, acc: 0.972656]  [G loss: 3.320386, acc: 0.335938]\n",
            "2094: [D loss: 0.079233, acc: 0.976562]  [G loss: 3.015557, acc: 0.367188]\n",
            "2095: [D loss: 0.061658, acc: 0.972656]  [G loss: 3.239595, acc: 0.367188]\n",
            "2096: [D loss: 0.100730, acc: 0.968750]  [G loss: 3.367639, acc: 0.289062]\n",
            "2097: [D loss: 0.125106, acc: 0.960938]  [G loss: 3.142847, acc: 0.265625]\n",
            "2098: [D loss: 0.092695, acc: 0.960938]  [G loss: 2.803241, acc: 0.328125]\n",
            "2099: [D loss: 0.080021, acc: 0.968750]  [G loss: 2.506680, acc: 0.343750]\n",
            "2100: [D loss: 0.087779, acc: 0.984375]  [G loss: 2.377542, acc: 0.320312]\n",
            "2101: [D loss: 0.130346, acc: 0.960938]  [G loss: 3.367947, acc: 0.203125]\n",
            "2102: [D loss: 0.074073, acc: 0.968750]  [G loss: 3.534993, acc: 0.187500]\n",
            "2103: [D loss: 0.047290, acc: 0.984375]  [G loss: 4.256464, acc: 0.195312]\n",
            "2104: [D loss: 0.068421, acc: 0.972656]  [G loss: 4.140339, acc: 0.156250]\n",
            "2105: [D loss: 0.110172, acc: 0.949219]  [G loss: 3.276155, acc: 0.250000]\n",
            "2106: [D loss: 0.144688, acc: 0.957031]  [G loss: 2.896571, acc: 0.234375]\n",
            "2107: [D loss: 0.072175, acc: 0.972656]  [G loss: 1.992474, acc: 0.406250]\n",
            "2108: [D loss: 0.067303, acc: 0.960938]  [G loss: 2.210414, acc: 0.382812]\n",
            "2109: [D loss: 0.065447, acc: 0.972656]  [G loss: 3.107405, acc: 0.250000]\n",
            "2110: [D loss: 0.086090, acc: 0.980469]  [G loss: 3.690222, acc: 0.226562]\n",
            "2111: [D loss: 0.070782, acc: 0.972656]  [G loss: 4.540176, acc: 0.148438]\n",
            "2112: [D loss: 0.049730, acc: 0.980469]  [G loss: 4.230726, acc: 0.187500]\n",
            "2113: [D loss: 0.086341, acc: 0.964844]  [G loss: 3.928567, acc: 0.148438]\n",
            "2114: [D loss: 0.079069, acc: 0.968750]  [G loss: 3.172670, acc: 0.304688]\n",
            "2115: [D loss: 0.091762, acc: 0.976562]  [G loss: 3.157026, acc: 0.273438]\n",
            "2116: [D loss: 0.191949, acc: 0.953125]  [G loss: 3.089076, acc: 0.281250]\n",
            "2117: [D loss: 0.074524, acc: 0.972656]  [G loss: 4.205234, acc: 0.226562]\n",
            "2118: [D loss: 0.104966, acc: 0.964844]  [G loss: 4.724033, acc: 0.210938]\n",
            "2119: [D loss: 0.060299, acc: 0.988281]  [G loss: 4.745645, acc: 0.179688]\n",
            "2120: [D loss: 0.108471, acc: 0.968750]  [G loss: 3.699484, acc: 0.273438]\n",
            "2121: [D loss: 0.114318, acc: 0.972656]  [G loss: 3.362235, acc: 0.273438]\n",
            "2122: [D loss: 0.098363, acc: 0.968750]  [G loss: 3.377140, acc: 0.281250]\n",
            "2123: [D loss: 0.123384, acc: 0.949219]  [G loss: 4.807818, acc: 0.164062]\n",
            "2124: [D loss: 0.179648, acc: 0.968750]  [G loss: 5.319172, acc: 0.109375]\n",
            "2125: [D loss: 0.286895, acc: 0.937500]  [G loss: 4.020846, acc: 0.187500]\n",
            "2126: [D loss: 0.156715, acc: 0.964844]  [G loss: 3.631603, acc: 0.273438]\n",
            "2127: [D loss: 0.104986, acc: 0.960938]  [G loss: 3.838346, acc: 0.289062]\n",
            "2128: [D loss: 0.227801, acc: 0.945312]  [G loss: 3.539076, acc: 0.257812]\n",
            "2129: [D loss: 0.141104, acc: 0.957031]  [G loss: 3.780981, acc: 0.210938]\n",
            "2130: [D loss: 0.171900, acc: 0.953125]  [G loss: 3.737256, acc: 0.265625]\n",
            "2131: [D loss: 0.120229, acc: 0.964844]  [G loss: 3.835280, acc: 0.234375]\n",
            "2132: [D loss: 0.164431, acc: 0.941406]  [G loss: 2.876742, acc: 0.257812]\n",
            "2133: [D loss: 0.141680, acc: 0.945312]  [G loss: 2.106808, acc: 0.421875]\n",
            "2134: [D loss: 0.168843, acc: 0.917969]  [G loss: 2.855075, acc: 0.367188]\n",
            "2135: [D loss: 0.158641, acc: 0.960938]  [G loss: 2.856912, acc: 0.335938]\n",
            "2136: [D loss: 0.095851, acc: 0.968750]  [G loss: 2.627363, acc: 0.343750]\n",
            "2137: [D loss: 0.137505, acc: 0.949219]  [G loss: 1.548418, acc: 0.531250]\n",
            "2138: [D loss: 0.181474, acc: 0.945312]  [G loss: 1.258869, acc: 0.578125]\n",
            "2139: [D loss: 0.212570, acc: 0.929688]  [G loss: 1.349631, acc: 0.531250]\n",
            "2140: [D loss: 0.151387, acc: 0.945312]  [G loss: 1.714951, acc: 0.445312]\n",
            "2141: [D loss: 0.199529, acc: 0.925781]  [G loss: 1.583736, acc: 0.437500]\n",
            "2142: [D loss: 0.140158, acc: 0.937500]  [G loss: 1.639615, acc: 0.437500]\n",
            "2143: [D loss: 0.193931, acc: 0.945312]  [G loss: 1.316310, acc: 0.531250]\n",
            "2144: [D loss: 0.140166, acc: 0.953125]  [G loss: 1.019883, acc: 0.609375]\n",
            "2145: [D loss: 0.082032, acc: 0.980469]  [G loss: 1.163678, acc: 0.578125]\n",
            "2146: [D loss: 0.117564, acc: 0.968750]  [G loss: 1.211269, acc: 0.578125]\n",
            "2147: [D loss: 0.147138, acc: 0.953125]  [G loss: 1.382198, acc: 0.601562]\n",
            "2148: [D loss: 0.206050, acc: 0.933594]  [G loss: 1.240221, acc: 0.617188]\n",
            "2149: [D loss: 0.188148, acc: 0.949219]  [G loss: 1.747891, acc: 0.460938]\n",
            "2150: [D loss: 0.162217, acc: 0.945312]  [G loss: 1.215860, acc: 0.640625]\n",
            "2151: [D loss: 0.189494, acc: 0.933594]  [G loss: 1.186542, acc: 0.640625]\n",
            "2152: [D loss: 0.175935, acc: 0.921875]  [G loss: 1.295520, acc: 0.585938]\n",
            "2153: [D loss: 0.126211, acc: 0.941406]  [G loss: 1.581426, acc: 0.500000]\n",
            "2154: [D loss: 0.147632, acc: 0.949219]  [G loss: 1.966304, acc: 0.398438]\n",
            "2155: [D loss: 0.145228, acc: 0.941406]  [G loss: 1.817841, acc: 0.421875]\n",
            "2156: [D loss: 0.126579, acc: 0.949219]  [G loss: 2.003264, acc: 0.351562]\n",
            "2157: [D loss: 0.112727, acc: 0.964844]  [G loss: 2.036168, acc: 0.367188]\n",
            "2158: [D loss: 0.089362, acc: 0.968750]  [G loss: 2.251816, acc: 0.250000]\n",
            "2159: [D loss: 0.066804, acc: 0.980469]  [G loss: 2.622490, acc: 0.148438]\n",
            "2160: [D loss: 0.123047, acc: 0.960938]  [G loss: 2.939134, acc: 0.046875]\n",
            "2161: [D loss: 0.116648, acc: 0.964844]  [G loss: 3.244666, acc: 0.031250]\n",
            "2162: [D loss: 0.070478, acc: 0.976562]  [G loss: 3.739098, acc: 0.015625]\n",
            "2163: [D loss: 0.054950, acc: 0.980469]  [G loss: 4.154484, acc: 0.007812]\n",
            "2164: [D loss: 0.025338, acc: 1.000000]  [G loss: 4.621494, acc: 0.007812]\n",
            "2165: [D loss: 0.032260, acc: 0.992188]  [G loss: 5.182117, acc: 0.023438]\n",
            "2166: [D loss: 0.032246, acc: 0.992188]  [G loss: 5.189732, acc: 0.007812]\n",
            "2167: [D loss: 0.023245, acc: 1.000000]  [G loss: 5.568719, acc: 0.015625]\n",
            "2168: [D loss: 0.015112, acc: 0.996094]  [G loss: 6.072341, acc: 0.007812]\n",
            "2169: [D loss: 0.030217, acc: 0.988281]  [G loss: 6.250503, acc: 0.000000]\n",
            "2170: [D loss: 0.028604, acc: 0.992188]  [G loss: 6.945671, acc: 0.000000]\n",
            "2171: [D loss: 0.025868, acc: 0.992188]  [G loss: 7.726068, acc: 0.000000]\n",
            "2172: [D loss: 0.032070, acc: 0.988281]  [G loss: 8.360363, acc: 0.007812]\n",
            "2173: [D loss: 0.025913, acc: 0.992188]  [G loss: 8.969051, acc: 0.000000]\n",
            "2174: [D loss: 0.035838, acc: 0.980469]  [G loss: 9.229401, acc: 0.000000]\n",
            "2175: [D loss: 0.064245, acc: 0.972656]  [G loss: 8.888240, acc: 0.007812]\n",
            "2176: [D loss: 0.057426, acc: 0.972656]  [G loss: 8.990244, acc: 0.015625]\n",
            "2177: [D loss: 0.075312, acc: 0.976562]  [G loss: 9.560868, acc: 0.000000]\n",
            "2178: [D loss: 0.052434, acc: 0.976562]  [G loss: 9.240421, acc: 0.015625]\n",
            "2179: [D loss: 0.082625, acc: 0.964844]  [G loss: 9.824574, acc: 0.015625]\n",
            "2180: [D loss: 0.152855, acc: 0.937500]  [G loss: 11.135969, acc: 0.000000]\n",
            "2181: [D loss: 0.047782, acc: 0.980469]  [G loss: 11.677999, acc: 0.007812]\n",
            "2182: [D loss: 0.259010, acc: 0.925781]  [G loss: 10.703645, acc: 0.000000]\n",
            "2183: [D loss: 0.306635, acc: 0.906250]  [G loss: 8.967564, acc: 0.000000]\n",
            "2184: [D loss: 0.527724, acc: 0.875000]  [G loss: 10.202940, acc: 0.007812]\n",
            "2185: [D loss: 0.332435, acc: 0.910156]  [G loss: 9.555166, acc: 0.000000]\n",
            "2186: [D loss: 0.224791, acc: 0.933594]  [G loss: 9.181335, acc: 0.007812]\n",
            "2187: [D loss: 0.208512, acc: 0.953125]  [G loss: 7.709078, acc: 0.015625]\n",
            "2188: [D loss: 0.048971, acc: 0.976562]  [G loss: 6.061396, acc: 0.046875]\n",
            "2189: [D loss: 0.091013, acc: 0.957031]  [G loss: 4.618778, acc: 0.062500]\n",
            "2190: [D loss: 0.071271, acc: 0.968750]  [G loss: 4.222812, acc: 0.109375]\n",
            "2191: [D loss: 0.072780, acc: 0.972656]  [G loss: 5.400734, acc: 0.046875]\n",
            "2192: [D loss: 0.071055, acc: 0.960938]  [G loss: 5.837772, acc: 0.078125]\n",
            "2193: [D loss: 0.157092, acc: 0.945312]  [G loss: 5.683372, acc: 0.140625]\n",
            "2194: [D loss: 0.156497, acc: 0.941406]  [G loss: 5.824539, acc: 0.125000]\n",
            "2195: [D loss: 0.105638, acc: 0.964844]  [G loss: 4.529068, acc: 0.203125]\n",
            "2196: [D loss: 0.065019, acc: 0.984375]  [G loss: 4.154236, acc: 0.281250]\n",
            "2197: [D loss: 0.077154, acc: 0.976562]  [G loss: 2.671884, acc: 0.421875]\n",
            "2198: [D loss: 0.162594, acc: 0.929688]  [G loss: 2.811427, acc: 0.460938]\n",
            "2199: [D loss: 0.112411, acc: 0.957031]  [G loss: 3.176020, acc: 0.398438]\n",
            "2200: [D loss: 0.049735, acc: 0.976562]  [G loss: 3.812445, acc: 0.359375]\n",
            "2201: [D loss: 0.126290, acc: 0.957031]  [G loss: 3.824430, acc: 0.335938]\n",
            "2202: [D loss: 0.107888, acc: 0.968750]  [G loss: 3.114263, acc: 0.375000]\n",
            "2203: [D loss: 0.054279, acc: 0.976562]  [G loss: 2.497696, acc: 0.453125]\n",
            "2204: [D loss: 0.112944, acc: 0.976562]  [G loss: 2.066674, acc: 0.453125]\n",
            "2205: [D loss: 0.077498, acc: 0.964844]  [G loss: 1.839248, acc: 0.523438]\n",
            "2206: [D loss: 0.032718, acc: 0.984375]  [G loss: 2.312451, acc: 0.437500]\n",
            "2207: [D loss: 0.048233, acc: 0.972656]  [G loss: 2.204712, acc: 0.515625]\n",
            "2208: [D loss: 0.026103, acc: 0.996094]  [G loss: 2.387359, acc: 0.484375]\n",
            "2209: [D loss: 0.046583, acc: 0.988281]  [G loss: 2.462504, acc: 0.453125]\n",
            "2210: [D loss: 0.034025, acc: 0.980469]  [G loss: 2.205619, acc: 0.460938]\n",
            "2211: [D loss: 0.019972, acc: 0.996094]  [G loss: 2.024623, acc: 0.593750]\n",
            "2212: [D loss: 0.054328, acc: 0.980469]  [G loss: 1.615750, acc: 0.578125]\n",
            "2213: [D loss: 0.039351, acc: 0.988281]  [G loss: 1.200386, acc: 0.726562]\n",
            "2214: [D loss: 0.045579, acc: 0.980469]  [G loss: 1.039251, acc: 0.695312]\n",
            "2215: [D loss: 0.036282, acc: 0.980469]  [G loss: 1.001197, acc: 0.671875]\n",
            "2216: [D loss: 0.040675, acc: 0.984375]  [G loss: 1.151038, acc: 0.687500]\n",
            "2217: [D loss: 0.042209, acc: 0.988281]  [G loss: 1.067708, acc: 0.695312]\n",
            "2218: [D loss: 0.019522, acc: 0.992188]  [G loss: 1.211641, acc: 0.671875]\n",
            "2219: [D loss: 0.025142, acc: 0.984375]  [G loss: 1.199763, acc: 0.648438]\n",
            "2220: [D loss: 0.088540, acc: 0.968750]  [G loss: 1.002630, acc: 0.710938]\n",
            "2221: [D loss: 0.010704, acc: 1.000000]  [G loss: 0.678433, acc: 0.757812]\n",
            "2222: [D loss: 0.071014, acc: 0.976562]  [G loss: 0.358837, acc: 0.859375]\n",
            "2223: [D loss: 0.053185, acc: 0.984375]  [G loss: 0.390688, acc: 0.843750]\n",
            "2224: [D loss: 0.073910, acc: 0.968750]  [G loss: 0.401826, acc: 0.851562]\n",
            "2225: [D loss: 0.070636, acc: 0.972656]  [G loss: 0.359136, acc: 0.851562]\n",
            "2226: [D loss: 0.070510, acc: 0.984375]  [G loss: 0.412080, acc: 0.828125]\n",
            "2227: [D loss: 0.107657, acc: 0.964844]  [G loss: 0.489662, acc: 0.781250]\n",
            "2228: [D loss: 0.055281, acc: 0.984375]  [G loss: 0.576755, acc: 0.796875]\n",
            "2229: [D loss: 0.032289, acc: 0.988281]  [G loss: 0.708574, acc: 0.726562]\n",
            "2230: [D loss: 0.101337, acc: 0.984375]  [G loss: 0.740116, acc: 0.757812]\n",
            "2231: [D loss: 0.051479, acc: 0.992188]  [G loss: 0.707778, acc: 0.742188]\n",
            "2232: [D loss: 0.061767, acc: 0.976562]  [G loss: 0.824072, acc: 0.695312]\n",
            "2233: [D loss: 0.056063, acc: 0.992188]  [G loss: 0.749632, acc: 0.679688]\n",
            "2234: [D loss: 0.050846, acc: 0.988281]  [G loss: 0.741739, acc: 0.687500]\n",
            "2235: [D loss: 0.104556, acc: 0.953125]  [G loss: 0.840807, acc: 0.640625]\n",
            "2236: [D loss: 0.051319, acc: 0.976562]  [G loss: 1.096175, acc: 0.593750]\n",
            "2237: [D loss: 0.073655, acc: 0.976562]  [G loss: 0.937965, acc: 0.601562]\n",
            "2238: [D loss: 0.052281, acc: 0.988281]  [G loss: 1.027391, acc: 0.648438]\n",
            "2239: [D loss: 0.082095, acc: 0.976562]  [G loss: 1.305703, acc: 0.507812]\n",
            "2240: [D loss: 0.046464, acc: 0.992188]  [G loss: 1.452747, acc: 0.437500]\n",
            "2241: [D loss: 0.073304, acc: 0.980469]  [G loss: 1.316895, acc: 0.562500]\n",
            "2242: [D loss: 0.092151, acc: 0.953125]  [G loss: 1.724199, acc: 0.414062]\n",
            "2243: [D loss: 0.077749, acc: 0.960938]  [G loss: 1.315393, acc: 0.515625]\n",
            "2244: [D loss: 0.089835, acc: 0.960938]  [G loss: 1.663877, acc: 0.460938]\n",
            "2245: [D loss: 0.102052, acc: 0.953125]  [G loss: 1.361380, acc: 0.523438]\n",
            "2246: [D loss: 0.064701, acc: 0.980469]  [G loss: 1.691801, acc: 0.484375]\n",
            "2247: [D loss: 0.082212, acc: 0.972656]  [G loss: 1.640289, acc: 0.406250]\n",
            "2248: [D loss: 0.107155, acc: 0.972656]  [G loss: 1.504853, acc: 0.429688]\n",
            "2249: [D loss: 0.140313, acc: 0.957031]  [G loss: 1.227047, acc: 0.539062]\n",
            "2250: [D loss: 0.103251, acc: 0.964844]  [G loss: 1.245553, acc: 0.531250]\n",
            "2251: [D loss: 0.069545, acc: 0.980469]  [G loss: 1.706377, acc: 0.382812]\n",
            "2252: [D loss: 0.075757, acc: 0.972656]  [G loss: 1.985785, acc: 0.304688]\n",
            "2253: [D loss: 0.087929, acc: 0.968750]  [G loss: 1.708633, acc: 0.398438]\n",
            "2254: [D loss: 0.064092, acc: 0.984375]  [G loss: 1.670744, acc: 0.460938]\n",
            "2255: [D loss: 0.108065, acc: 0.980469]  [G loss: 1.660336, acc: 0.429688]\n",
            "2256: [D loss: 0.084561, acc: 0.972656]  [G loss: 1.517822, acc: 0.484375]\n",
            "2257: [D loss: 0.104883, acc: 0.953125]  [G loss: 1.540333, acc: 0.437500]\n",
            "2258: [D loss: 0.144988, acc: 0.960938]  [G loss: 1.591311, acc: 0.390625]\n",
            "2259: [D loss: 0.052770, acc: 0.980469]  [G loss: 2.216845, acc: 0.187500]\n",
            "2260: [D loss: 0.111190, acc: 0.953125]  [G loss: 1.804456, acc: 0.320312]\n",
            "2261: [D loss: 0.059050, acc: 0.984375]  [G loss: 2.145719, acc: 0.257812]\n",
            "2262: [D loss: 0.085160, acc: 0.976562]  [G loss: 1.499705, acc: 0.367188]\n",
            "2263: [D loss: 0.045327, acc: 0.988281]  [G loss: 1.572561, acc: 0.367188]\n",
            "2264: [D loss: 0.087976, acc: 0.980469]  [G loss: 1.850961, acc: 0.289062]\n",
            "2265: [D loss: 0.080543, acc: 0.972656]  [G loss: 2.397453, acc: 0.187500]\n",
            "2266: [D loss: 0.137367, acc: 0.957031]  [G loss: 2.436272, acc: 0.210938]\n",
            "2267: [D loss: 0.179223, acc: 0.953125]  [G loss: 2.023813, acc: 0.273438]\n",
            "2268: [D loss: 0.079418, acc: 0.980469]  [G loss: 1.629425, acc: 0.281250]\n",
            "2269: [D loss: 0.099919, acc: 0.964844]  [G loss: 2.077243, acc: 0.250000]\n",
            "2270: [D loss: 0.046910, acc: 0.988281]  [G loss: 2.499914, acc: 0.171875]\n",
            "2271: [D loss: 0.066681, acc: 0.980469]  [G loss: 2.776371, acc: 0.171875]\n",
            "2272: [D loss: 0.119432, acc: 0.976562]  [G loss: 2.762434, acc: 0.234375]\n",
            "2273: [D loss: 0.096388, acc: 0.957031]  [G loss: 2.688443, acc: 0.195312]\n",
            "2274: [D loss: 0.041714, acc: 0.984375]  [G loss: 2.616806, acc: 0.195312]\n",
            "2275: [D loss: 0.083367, acc: 0.968750]  [G loss: 2.521320, acc: 0.242188]\n",
            "2276: [D loss: 0.076442, acc: 0.972656]  [G loss: 3.156553, acc: 0.148438]\n",
            "2277: [D loss: 0.054125, acc: 0.980469]  [G loss: 3.046364, acc: 0.148438]\n",
            "2278: [D loss: 0.039037, acc: 0.992188]  [G loss: 3.476313, acc: 0.179688]\n",
            "2279: [D loss: 0.066550, acc: 0.968750]  [G loss: 3.359414, acc: 0.132812]\n",
            "2280: [D loss: 0.057674, acc: 0.984375]  [G loss: 3.094352, acc: 0.171875]\n",
            "2281: [D loss: 0.088718, acc: 0.968750]  [G loss: 3.708570, acc: 0.156250]\n",
            "2282: [D loss: 0.068085, acc: 0.964844]  [G loss: 4.014286, acc: 0.179688]\n",
            "2283: [D loss: 0.072575, acc: 0.976562]  [G loss: 4.497269, acc: 0.101562]\n",
            "2284: [D loss: 0.112708, acc: 0.960938]  [G loss: 4.260477, acc: 0.109375]\n",
            "2285: [D loss: 0.082083, acc: 0.964844]  [G loss: 4.067585, acc: 0.164062]\n",
            "2286: [D loss: 0.065882, acc: 0.980469]  [G loss: 3.597134, acc: 0.218750]\n",
            "2287: [D loss: 0.070114, acc: 0.980469]  [G loss: 3.485265, acc: 0.179688]\n",
            "2288: [D loss: 0.060978, acc: 0.972656]  [G loss: 3.865104, acc: 0.148438]\n",
            "2289: [D loss: 0.079753, acc: 0.972656]  [G loss: 3.710408, acc: 0.203125]\n",
            "2290: [D loss: 0.104544, acc: 0.953125]  [G loss: 4.063904, acc: 0.156250]\n",
            "2291: [D loss: 0.091160, acc: 0.980469]  [G loss: 4.257862, acc: 0.132812]\n",
            "2292: [D loss: 0.122245, acc: 0.964844]  [G loss: 4.224274, acc: 0.156250]\n",
            "2293: [D loss: 0.090411, acc: 0.972656]  [G loss: 4.296171, acc: 0.140625]\n",
            "2294: [D loss: 0.094500, acc: 0.964844]  [G loss: 4.510846, acc: 0.164062]\n",
            "2295: [D loss: 0.050616, acc: 0.976562]  [G loss: 4.447567, acc: 0.140625]\n",
            "2296: [D loss: 0.107238, acc: 0.968750]  [G loss: 4.201580, acc: 0.164062]\n",
            "2297: [D loss: 0.061968, acc: 0.972656]  [G loss: 4.451635, acc: 0.171875]\n",
            "2298: [D loss: 0.119967, acc: 0.960938]  [G loss: 4.354000, acc: 0.148438]\n",
            "2299: [D loss: 0.069835, acc: 0.968750]  [G loss: 3.871845, acc: 0.164062]\n",
            "2300: [D loss: 0.108079, acc: 0.949219]  [G loss: 3.577801, acc: 0.242188]\n",
            "2301: [D loss: 0.075754, acc: 0.968750]  [G loss: 3.558457, acc: 0.218750]\n",
            "2302: [D loss: 0.070481, acc: 0.972656]  [G loss: 4.298229, acc: 0.203125]\n",
            "2303: [D loss: 0.058249, acc: 0.988281]  [G loss: 4.316287, acc: 0.179688]\n",
            "2304: [D loss: 0.036739, acc: 0.992188]  [G loss: 4.378822, acc: 0.171875]\n",
            "2305: [D loss: 0.039144, acc: 0.992188]  [G loss: 4.775105, acc: 0.203125]\n",
            "2306: [D loss: 0.057511, acc: 0.984375]  [G loss: 4.575675, acc: 0.195312]\n",
            "2307: [D loss: 0.018525, acc: 0.992188]  [G loss: 4.196754, acc: 0.203125]\n",
            "2308: [D loss: 0.032176, acc: 0.992188]  [G loss: 4.230803, acc: 0.203125]\n",
            "2309: [D loss: 0.036954, acc: 0.988281]  [G loss: 4.124112, acc: 0.273438]\n",
            "2310: [D loss: 0.074043, acc: 0.976562]  [G loss: 4.325545, acc: 0.242188]\n",
            "2311: [D loss: 0.049472, acc: 0.980469]  [G loss: 4.574775, acc: 0.226562]\n",
            "2312: [D loss: 0.094460, acc: 0.960938]  [G loss: 5.541413, acc: 0.132812]\n",
            "2313: [D loss: 0.127131, acc: 0.953125]  [G loss: 5.427407, acc: 0.148438]\n",
            "2314: [D loss: 0.069756, acc: 0.972656]  [G loss: 5.710077, acc: 0.101562]\n",
            "2315: [D loss: 0.101847, acc: 0.968750]  [G loss: 4.881924, acc: 0.203125]\n",
            "2316: [D loss: 0.057536, acc: 0.972656]  [G loss: 3.761317, acc: 0.187500]\n",
            "2317: [D loss: 0.069350, acc: 0.976562]  [G loss: 3.752759, acc: 0.234375]\n",
            "2318: [D loss: 0.066853, acc: 0.976562]  [G loss: 4.658669, acc: 0.125000]\n",
            "2319: [D loss: 0.099426, acc: 0.957031]  [G loss: 5.354372, acc: 0.117188]\n",
            "2320: [D loss: 0.125839, acc: 0.960938]  [G loss: 5.574912, acc: 0.125000]\n",
            "2321: [D loss: 0.091530, acc: 0.964844]  [G loss: 5.089988, acc: 0.125000]\n",
            "2322: [D loss: 0.134809, acc: 0.941406]  [G loss: 4.113921, acc: 0.195312]\n",
            "2323: [D loss: 0.067448, acc: 0.976562]  [G loss: 4.327165, acc: 0.234375]\n",
            "2324: [D loss: 0.123465, acc: 0.949219]  [G loss: 4.552560, acc: 0.265625]\n",
            "2325: [D loss: 0.070397, acc: 0.968750]  [G loss: 5.532139, acc: 0.265625]\n",
            "2326: [D loss: 0.106843, acc: 0.949219]  [G loss: 3.597797, acc: 0.375000]\n",
            "2327: [D loss: 0.083270, acc: 0.960938]  [G loss: 3.913266, acc: 0.382812]\n",
            "2328: [D loss: 0.139363, acc: 0.945312]  [G loss: 4.662843, acc: 0.320312]\n",
            "2329: [D loss: 0.144819, acc: 0.957031]  [G loss: 4.230687, acc: 0.328125]\n",
            "2330: [D loss: 0.144187, acc: 0.941406]  [G loss: 2.883806, acc: 0.460938]\n",
            "2331: [D loss: 0.121723, acc: 0.941406]  [G loss: 2.939964, acc: 0.421875]\n",
            "2332: [D loss: 0.096215, acc: 0.953125]  [G loss: 3.553223, acc: 0.335938]\n",
            "2333: [D loss: 0.083432, acc: 0.960938]  [G loss: 3.330399, acc: 0.351562]\n",
            "2334: [D loss: 0.139059, acc: 0.949219]  [G loss: 3.388644, acc: 0.328125]\n",
            "2335: [D loss: 0.114632, acc: 0.957031]  [G loss: 3.407443, acc: 0.289062]\n",
            "2336: [D loss: 0.181003, acc: 0.933594]  [G loss: 2.483335, acc: 0.382812]\n",
            "2337: [D loss: 0.244755, acc: 0.898438]  [G loss: 2.868209, acc: 0.328125]\n",
            "2338: [D loss: 0.246355, acc: 0.925781]  [G loss: 3.216930, acc: 0.273438]\n",
            "2339: [D loss: 0.216274, acc: 0.933594]  [G loss: 2.686490, acc: 0.289062]\n",
            "2340: [D loss: 0.165227, acc: 0.941406]  [G loss: 3.240070, acc: 0.289062]\n",
            "2341: [D loss: 0.099284, acc: 0.953125]  [G loss: 3.024331, acc: 0.328125]\n",
            "2342: [D loss: 0.072991, acc: 0.968750]  [G loss: 2.890823, acc: 0.296875]\n",
            "2343: [D loss: 0.063244, acc: 0.968750]  [G loss: 2.911078, acc: 0.296875]\n",
            "2344: [D loss: 0.123892, acc: 0.957031]  [G loss: 2.383597, acc: 0.359375]\n",
            "2345: [D loss: 0.048698, acc: 0.980469]  [G loss: 2.481565, acc: 0.328125]\n",
            "2346: [D loss: 0.109324, acc: 0.960938]  [G loss: 2.735868, acc: 0.234375]\n",
            "2347: [D loss: 0.110877, acc: 0.964844]  [G loss: 3.051993, acc: 0.226562]\n",
            "2348: [D loss: 0.065838, acc: 0.976562]  [G loss: 3.017781, acc: 0.273438]\n",
            "2349: [D loss: 0.090041, acc: 0.980469]  [G loss: 2.433580, acc: 0.343750]\n",
            "2350: [D loss: 0.073048, acc: 0.968750]  [G loss: 2.062687, acc: 0.375000]\n",
            "2351: [D loss: 0.174712, acc: 0.941406]  [G loss: 1.842806, acc: 0.429688]\n",
            "2352: [D loss: 0.045769, acc: 0.992188]  [G loss: 2.290753, acc: 0.351562]\n",
            "2353: [D loss: 0.062149, acc: 0.976562]  [G loss: 2.703410, acc: 0.335938]\n",
            "2354: [D loss: 0.091998, acc: 0.964844]  [G loss: 2.857780, acc: 0.359375]\n",
            "2355: [D loss: 0.052497, acc: 0.976562]  [G loss: 2.579086, acc: 0.296875]\n",
            "2356: [D loss: 0.072843, acc: 0.980469]  [G loss: 2.715902, acc: 0.375000]\n",
            "2357: [D loss: 0.061118, acc: 0.976562]  [G loss: 2.208011, acc: 0.460938]\n",
            "2358: [D loss: 0.099009, acc: 0.968750]  [G loss: 1.680298, acc: 0.539062]\n",
            "2359: [D loss: 0.178563, acc: 0.910156]  [G loss: 1.628248, acc: 0.515625]\n",
            "2360: [D loss: 0.061145, acc: 0.984375]  [G loss: 2.383551, acc: 0.437500]\n",
            "2361: [D loss: 0.095414, acc: 0.976562]  [G loss: 1.955675, acc: 0.484375]\n",
            "2362: [D loss: 0.156964, acc: 0.945312]  [G loss: 1.424635, acc: 0.570312]\n",
            "2363: [D loss: 0.079856, acc: 0.980469]  [G loss: 1.158522, acc: 0.625000]\n",
            "2364: [D loss: 0.081329, acc: 0.968750]  [G loss: 1.090260, acc: 0.609375]\n",
            "2365: [D loss: 0.067908, acc: 0.976562]  [G loss: 1.519682, acc: 0.515625]\n",
            "2366: [D loss: 0.034447, acc: 0.984375]  [G loss: 1.202622, acc: 0.539062]\n",
            "2367: [D loss: 0.061637, acc: 0.976562]  [G loss: 1.198954, acc: 0.609375]\n",
            "2368: [D loss: 0.111358, acc: 0.960938]  [G loss: 0.759680, acc: 0.703125]\n",
            "2369: [D loss: 0.073844, acc: 0.980469]  [G loss: 0.575629, acc: 0.742188]\n",
            "2370: [D loss: 0.126984, acc: 0.960938]  [G loss: 0.778135, acc: 0.710938]\n",
            "2371: [D loss: 0.067598, acc: 0.968750]  [G loss: 1.175945, acc: 0.601562]\n",
            "2372: [D loss: 0.115872, acc: 0.957031]  [G loss: 1.212255, acc: 0.546875]\n",
            "2373: [D loss: 0.096678, acc: 0.957031]  [G loss: 1.013436, acc: 0.703125]\n",
            "2374: [D loss: 0.092095, acc: 0.976562]  [G loss: 0.904273, acc: 0.695312]\n",
            "2375: [D loss: 0.120424, acc: 0.964844]  [G loss: 0.998959, acc: 0.718750]\n",
            "2376: [D loss: 0.060155, acc: 0.976562]  [G loss: 1.394216, acc: 0.585938]\n",
            "2377: [D loss: 0.053841, acc: 0.984375]  [G loss: 1.257115, acc: 0.656250]\n",
            "2378: [D loss: 0.091716, acc: 0.968750]  [G loss: 1.406385, acc: 0.640625]\n",
            "2379: [D loss: 0.070569, acc: 0.972656]  [G loss: 0.897758, acc: 0.703125]\n",
            "2380: [D loss: 0.111406, acc: 0.960938]  [G loss: 0.806035, acc: 0.734375]\n",
            "2381: [D loss: 0.062344, acc: 0.968750]  [G loss: 1.035905, acc: 0.695312]\n",
            "2382: [D loss: 0.096519, acc: 0.949219]  [G loss: 1.249826, acc: 0.632812]\n",
            "2383: [D loss: 0.050819, acc: 0.992188]  [G loss: 1.067450, acc: 0.648438]\n",
            "2384: [D loss: 0.035175, acc: 0.984375]  [G loss: 1.145120, acc: 0.632812]\n",
            "2385: [D loss: 0.061992, acc: 0.976562]  [G loss: 0.939304, acc: 0.718750]\n",
            "2386: [D loss: 0.044878, acc: 0.980469]  [G loss: 0.914167, acc: 0.726562]\n",
            "2387: [D loss: 0.047174, acc: 0.984375]  [G loss: 0.772471, acc: 0.750000]\n",
            "2388: [D loss: 0.096398, acc: 0.964844]  [G loss: 0.849472, acc: 0.765625]\n",
            "2389: [D loss: 0.057764, acc: 0.976562]  [G loss: 1.611101, acc: 0.570312]\n",
            "2390: [D loss: 0.081513, acc: 0.953125]  [G loss: 1.710266, acc: 0.585938]\n",
            "2391: [D loss: 0.105848, acc: 0.968750]  [G loss: 1.272814, acc: 0.671875]\n",
            "2392: [D loss: 0.054665, acc: 0.984375]  [G loss: 1.124574, acc: 0.687500]\n",
            "2393: [D loss: 0.023958, acc: 0.992188]  [G loss: 0.757835, acc: 0.750000]\n",
            "2394: [D loss: 0.059639, acc: 0.976562]  [G loss: 0.926012, acc: 0.703125]\n",
            "2395: [D loss: 0.078092, acc: 0.964844]  [G loss: 0.857567, acc: 0.734375]\n",
            "2396: [D loss: 0.114014, acc: 0.972656]  [G loss: 0.859716, acc: 0.703125]\n",
            "2397: [D loss: 0.067413, acc: 0.976562]  [G loss: 1.075396, acc: 0.679688]\n",
            "2398: [D loss: 0.055855, acc: 0.976562]  [G loss: 1.591573, acc: 0.523438]\n",
            "2399: [D loss: 0.129517, acc: 0.960938]  [G loss: 1.233764, acc: 0.632812]\n",
            "2400: [D loss: 0.120322, acc: 0.957031]  [G loss: 0.811006, acc: 0.734375]\n",
            "2401: [D loss: 0.064148, acc: 0.976562]  [G loss: 0.494658, acc: 0.781250]\n",
            "2402: [D loss: 0.067996, acc: 0.976562]  [G loss: 0.385859, acc: 0.859375]\n",
            "2403: [D loss: 0.095542, acc: 0.949219]  [G loss: 1.143842, acc: 0.632812]\n",
            "2404: [D loss: 0.052607, acc: 0.984375]  [G loss: 1.625186, acc: 0.500000]\n",
            "2405: [D loss: 0.068129, acc: 0.976562]  [G loss: 2.216263, acc: 0.445312]\n",
            "2406: [D loss: 0.063522, acc: 0.980469]  [G loss: 2.079960, acc: 0.445312]\n",
            "2407: [D loss: 0.116281, acc: 0.972656]  [G loss: 2.125460, acc: 0.492188]\n",
            "2408: [D loss: 0.066214, acc: 0.972656]  [G loss: 1.695441, acc: 0.515625]\n",
            "2409: [D loss: 0.069316, acc: 0.984375]  [G loss: 1.007337, acc: 0.625000]\n",
            "2410: [D loss: 0.132355, acc: 0.953125]  [G loss: 0.914741, acc: 0.656250]\n",
            "2411: [D loss: 0.058966, acc: 0.984375]  [G loss: 1.312034, acc: 0.578125]\n",
            "2412: [D loss: 0.088459, acc: 0.964844]  [G loss: 1.721798, acc: 0.507812]\n",
            "2413: [D loss: 0.136648, acc: 0.960938]  [G loss: 1.790305, acc: 0.476562]\n",
            "2414: [D loss: 0.090389, acc: 0.964844]  [G loss: 1.667168, acc: 0.554688]\n",
            "2415: [D loss: 0.054778, acc: 0.984375]  [G loss: 1.223270, acc: 0.640625]\n",
            "2416: [D loss: 0.090567, acc: 0.957031]  [G loss: 1.276768, acc: 0.601562]\n",
            "2417: [D loss: 0.089040, acc: 0.957031]  [G loss: 1.618267, acc: 0.562500]\n",
            "2418: [D loss: 0.088702, acc: 0.968750]  [G loss: 2.250896, acc: 0.492188]\n",
            "2419: [D loss: 0.072715, acc: 0.976562]  [G loss: 2.305472, acc: 0.390625]\n",
            "2420: [D loss: 0.089392, acc: 0.972656]  [G loss: 2.055967, acc: 0.507812]\n",
            "2421: [D loss: 0.050168, acc: 0.984375]  [G loss: 1.744981, acc: 0.531250]\n",
            "2422: [D loss: 0.089327, acc: 0.964844]  [G loss: 1.486198, acc: 0.562500]\n",
            "2423: [D loss: 0.039135, acc: 0.992188]  [G loss: 1.207167, acc: 0.578125]\n",
            "2424: [D loss: 0.094710, acc: 0.960938]  [G loss: 1.292301, acc: 0.593750]\n",
            "2425: [D loss: 0.123690, acc: 0.941406]  [G loss: 2.047056, acc: 0.500000]\n",
            "2426: [D loss: 0.049227, acc: 0.988281]  [G loss: 2.890857, acc: 0.351562]\n",
            "2427: [D loss: 0.083590, acc: 0.972656]  [G loss: 3.237096, acc: 0.304688]\n",
            "2428: [D loss: 0.033361, acc: 0.988281]  [G loss: 3.175272, acc: 0.382812]\n",
            "2429: [D loss: 0.069334, acc: 0.976562]  [G loss: 2.539162, acc: 0.398438]\n",
            "2430: [D loss: 0.077807, acc: 0.968750]  [G loss: 2.668178, acc: 0.414062]\n",
            "2431: [D loss: 0.068231, acc: 0.984375]  [G loss: 2.377105, acc: 0.390625]\n",
            "2432: [D loss: 0.179010, acc: 0.921875]  [G loss: 2.943208, acc: 0.390625]\n",
            "2433: [D loss: 0.157050, acc: 0.949219]  [G loss: 3.767656, acc: 0.304688]\n",
            "2434: [D loss: 0.081718, acc: 0.960938]  [G loss: 4.381202, acc: 0.226562]\n",
            "2435: [D loss: 0.136437, acc: 0.957031]  [G loss: 4.370455, acc: 0.250000]\n",
            "2436: [D loss: 0.070007, acc: 0.964844]  [G loss: 3.880667, acc: 0.335938]\n",
            "2437: [D loss: 0.079317, acc: 0.968750]  [G loss: 3.284402, acc: 0.375000]\n",
            "2438: [D loss: 0.057166, acc: 0.984375]  [G loss: 2.266908, acc: 0.445312]\n",
            "2439: [D loss: 0.088583, acc: 0.968750]  [G loss: 2.341129, acc: 0.468750]\n",
            "2440: [D loss: 0.096782, acc: 0.957031]  [G loss: 3.557720, acc: 0.281250]\n",
            "2441: [D loss: 0.100487, acc: 0.960938]  [G loss: 4.886786, acc: 0.164062]\n",
            "2442: [D loss: 0.085215, acc: 0.976562]  [G loss: 5.744446, acc: 0.156250]\n",
            "2443: [D loss: 0.134129, acc: 0.960938]  [G loss: 5.028917, acc: 0.203125]\n",
            "2444: [D loss: 0.043901, acc: 0.976562]  [G loss: 4.107860, acc: 0.312500]\n",
            "2445: [D loss: 0.112339, acc: 0.957031]  [G loss: 3.863551, acc: 0.328125]\n",
            "2446: [D loss: 0.088484, acc: 0.968750]  [G loss: 3.965636, acc: 0.359375]\n",
            "2447: [D loss: 0.054588, acc: 0.980469]  [G loss: 4.546750, acc: 0.226562]\n",
            "2448: [D loss: 0.098517, acc: 0.960938]  [G loss: 4.326283, acc: 0.320312]\n",
            "2449: [D loss: 0.130900, acc: 0.949219]  [G loss: 4.014169, acc: 0.398438]\n",
            "2450: [D loss: 0.101619, acc: 0.957031]  [G loss: 3.559975, acc: 0.367188]\n",
            "2451: [D loss: 0.116433, acc: 0.953125]  [G loss: 3.059193, acc: 0.460938]\n",
            "2452: [D loss: 0.089702, acc: 0.953125]  [G loss: 3.565382, acc: 0.359375]\n",
            "2453: [D loss: 0.092774, acc: 0.968750]  [G loss: 3.363208, acc: 0.351562]\n",
            "2454: [D loss: 0.130562, acc: 0.945312]  [G loss: 3.995406, acc: 0.210938]\n",
            "2455: [D loss: 0.075216, acc: 0.972656]  [G loss: 4.403713, acc: 0.242188]\n",
            "2456: [D loss: 0.109317, acc: 0.964844]  [G loss: 4.286720, acc: 0.250000]\n",
            "2457: [D loss: 0.104595, acc: 0.972656]  [G loss: 3.222034, acc: 0.320312]\n",
            "2458: [D loss: 0.206595, acc: 0.914062]  [G loss: 3.671113, acc: 0.281250]\n",
            "2459: [D loss: 0.132407, acc: 0.937500]  [G loss: 4.942540, acc: 0.171875]\n",
            "2460: [D loss: 0.189313, acc: 0.941406]  [G loss: 4.818847, acc: 0.218750]\n",
            "2461: [D loss: 0.220436, acc: 0.941406]  [G loss: 3.257695, acc: 0.343750]\n",
            "2462: [D loss: 0.156915, acc: 0.945312]  [G loss: 2.147027, acc: 0.414062]\n",
            "2463: [D loss: 0.190685, acc: 0.914062]  [G loss: 3.781049, acc: 0.234375]\n",
            "2464: [D loss: 0.174658, acc: 0.945312]  [G loss: 4.561821, acc: 0.117188]\n",
            "2465: [D loss: 0.195272, acc: 0.941406]  [G loss: 3.642793, acc: 0.218750]\n",
            "2466: [D loss: 0.190693, acc: 0.925781]  [G loss: 2.311068, acc: 0.429688]\n",
            "2467: [D loss: 0.266178, acc: 0.890625]  [G loss: 2.676548, acc: 0.320312]\n",
            "2468: [D loss: 0.186177, acc: 0.925781]  [G loss: 3.836392, acc: 0.171875]\n",
            "2469: [D loss: 0.122826, acc: 0.960938]  [G loss: 4.303028, acc: 0.125000]\n",
            "2470: [D loss: 0.160671, acc: 0.933594]  [G loss: 3.429997, acc: 0.179688]\n",
            "2471: [D loss: 0.181966, acc: 0.953125]  [G loss: 2.061860, acc: 0.296875]\n",
            "2472: [D loss: 0.344149, acc: 0.910156]  [G loss: 1.965140, acc: 0.343750]\n",
            "2473: [D loss: 0.155837, acc: 0.921875]  [G loss: 2.561134, acc: 0.328125]\n",
            "2474: [D loss: 0.162227, acc: 0.929688]  [G loss: 3.043373, acc: 0.289062]\n",
            "2475: [D loss: 0.177343, acc: 0.937500]  [G loss: 2.742203, acc: 0.390625]\n",
            "2476: [D loss: 0.158841, acc: 0.949219]  [G loss: 2.367845, acc: 0.398438]\n",
            "2477: [D loss: 0.133826, acc: 0.957031]  [G loss: 2.413000, acc: 0.414062]\n",
            "2478: [D loss: 0.092390, acc: 0.972656]  [G loss: 2.396435, acc: 0.359375]\n",
            "2479: [D loss: 0.120841, acc: 0.960938]  [G loss: 2.248089, acc: 0.375000]\n",
            "2480: [D loss: 0.087710, acc: 0.972656]  [G loss: 2.445831, acc: 0.328125]\n",
            "2481: [D loss: 0.153890, acc: 0.964844]  [G loss: 1.835781, acc: 0.375000]\n",
            "2482: [D loss: 0.161058, acc: 0.953125]  [G loss: 2.060533, acc: 0.335938]\n",
            "2483: [D loss: 0.112111, acc: 0.964844]  [G loss: 2.765032, acc: 0.234375]\n",
            "2484: [D loss: 0.150838, acc: 0.945312]  [G loss: 3.361003, acc: 0.171875]\n",
            "2485: [D loss: 0.180169, acc: 0.968750]  [G loss: 3.478232, acc: 0.226562]\n",
            "2486: [D loss: 0.067284, acc: 0.984375]  [G loss: 2.645404, acc: 0.296875]\n",
            "2487: [D loss: 0.098739, acc: 0.953125]  [G loss: 2.777805, acc: 0.281250]\n",
            "2488: [D loss: 0.140448, acc: 0.941406]  [G loss: 1.805604, acc: 0.445312]\n",
            "2489: [D loss: 0.208700, acc: 0.925781]  [G loss: 1.843076, acc: 0.523438]\n",
            "2490: [D loss: 0.188788, acc: 0.933594]  [G loss: 2.492404, acc: 0.343750]\n",
            "2491: [D loss: 0.237406, acc: 0.937500]  [G loss: 2.521547, acc: 0.265625]\n",
            "2492: [D loss: 0.233828, acc: 0.953125]  [G loss: 1.948856, acc: 0.351562]\n",
            "2493: [D loss: 0.158153, acc: 0.957031]  [G loss: 1.597618, acc: 0.492188]\n",
            "2494: [D loss: 0.114306, acc: 0.945312]  [G loss: 1.597321, acc: 0.421875]\n",
            "2495: [D loss: 0.142466, acc: 0.941406]  [G loss: 2.076108, acc: 0.343750]\n",
            "2496: [D loss: 0.092794, acc: 0.964844]  [G loss: 2.361090, acc: 0.351562]\n",
            "2497: [D loss: 0.086817, acc: 0.976562]  [G loss: 2.449504, acc: 0.367188]\n",
            "2498: [D loss: 0.117803, acc: 0.953125]  [G loss: 2.431327, acc: 0.398438]\n",
            "2499: [D loss: 0.087490, acc: 0.980469]  [G loss: 1.861428, acc: 0.437500]\n",
            "2500: [D loss: 0.038059, acc: 0.992188]  [G loss: 1.485364, acc: 0.507812]\n",
            "2501: [D loss: 0.063863, acc: 0.988281]  [G loss: 1.242424, acc: 0.617188]\n",
            "2502: [D loss: 0.104411, acc: 0.964844]  [G loss: 1.508102, acc: 0.523438]\n",
            "2503: [D loss: 0.085301, acc: 0.964844]  [G loss: 1.786458, acc: 0.398438]\n",
            "2504: [D loss: 0.102779, acc: 0.957031]  [G loss: 1.907938, acc: 0.468750]\n",
            "2505: [D loss: 0.123375, acc: 0.964844]  [G loss: 2.085086, acc: 0.382812]\n",
            "2506: [D loss: 0.120994, acc: 0.957031]  [G loss: 1.846806, acc: 0.484375]\n",
            "2507: [D loss: 0.123706, acc: 0.957031]  [G loss: 1.421648, acc: 0.562500]\n",
            "2508: [D loss: 0.106774, acc: 0.960938]  [G loss: 1.678427, acc: 0.468750]\n",
            "2509: [D loss: 0.120243, acc: 0.945312]  [G loss: 1.599033, acc: 0.531250]\n",
            "2510: [D loss: 0.134820, acc: 0.953125]  [G loss: 2.182031, acc: 0.468750]\n",
            "2511: [D loss: 0.127890, acc: 0.953125]  [G loss: 2.318254, acc: 0.460938]\n",
            "2512: [D loss: 0.175730, acc: 0.949219]  [G loss: 2.381624, acc: 0.484375]\n",
            "2513: [D loss: 0.178502, acc: 0.941406]  [G loss: 1.917603, acc: 0.562500]\n",
            "2514: [D loss: 0.082832, acc: 0.964844]  [G loss: 1.654342, acc: 0.562500]\n",
            "2515: [D loss: 0.233167, acc: 0.894531]  [G loss: 1.202923, acc: 0.648438]\n",
            "2516: [D loss: 0.169304, acc: 0.957031]  [G loss: 2.030993, acc: 0.437500]\n",
            "2517: [D loss: 0.170048, acc: 0.929688]  [G loss: 3.236221, acc: 0.312500]\n",
            "2518: [D loss: 0.297063, acc: 0.917969]  [G loss: 3.759592, acc: 0.265625]\n",
            "2519: [D loss: 0.342939, acc: 0.902344]  [G loss: 2.381757, acc: 0.390625]\n",
            "2520: [D loss: 0.205779, acc: 0.933594]  [G loss: 1.665186, acc: 0.460938]\n",
            "2521: [D loss: 0.207927, acc: 0.921875]  [G loss: 1.409281, acc: 0.500000]\n",
            "2522: [D loss: 0.186974, acc: 0.917969]  [G loss: 2.249820, acc: 0.406250]\n",
            "2523: [D loss: 0.233989, acc: 0.933594]  [G loss: 2.659550, acc: 0.312500]\n",
            "2524: [D loss: 0.289385, acc: 0.910156]  [G loss: 3.050272, acc: 0.234375]\n",
            "2525: [D loss: 0.122831, acc: 0.937500]  [G loss: 2.774162, acc: 0.312500]\n",
            "2526: [D loss: 0.150248, acc: 0.953125]  [G loss: 2.299957, acc: 0.398438]\n",
            "2527: [D loss: 0.198214, acc: 0.910156]  [G loss: 1.759778, acc: 0.460938]\n",
            "2528: [D loss: 0.130364, acc: 0.945312]  [G loss: 1.845466, acc: 0.390625]\n",
            "2529: [D loss: 0.138403, acc: 0.949219]  [G loss: 2.353493, acc: 0.320312]\n",
            "2530: [D loss: 0.085501, acc: 0.968750]  [G loss: 2.430002, acc: 0.273438]\n",
            "2531: [D loss: 0.144818, acc: 0.945312]  [G loss: 2.579183, acc: 0.328125]\n",
            "2532: [D loss: 0.093489, acc: 0.964844]  [G loss: 2.707479, acc: 0.289062]\n",
            "2533: [D loss: 0.119190, acc: 0.945312]  [G loss: 2.092325, acc: 0.375000]\n",
            "2534: [D loss: 0.096039, acc: 0.972656]  [G loss: 2.195599, acc: 0.367188]\n",
            "2535: [D loss: 0.046515, acc: 0.992188]  [G loss: 2.433334, acc: 0.390625]\n",
            "2536: [D loss: 0.080977, acc: 0.964844]  [G loss: 2.512214, acc: 0.375000]\n",
            "2537: [D loss: 0.063187, acc: 0.976562]  [G loss: 1.942413, acc: 0.468750]\n",
            "2538: [D loss: 0.075097, acc: 0.968750]  [G loss: 1.970578, acc: 0.398438]\n",
            "2539: [D loss: 0.154697, acc: 0.941406]  [G loss: 2.254018, acc: 0.406250]\n",
            "2540: [D loss: 0.072781, acc: 0.976562]  [G loss: 3.098227, acc: 0.335938]\n",
            "2541: [D loss: 0.082775, acc: 0.972656]  [G loss: 2.945624, acc: 0.320312]\n",
            "2542: [D loss: 0.154518, acc: 0.957031]  [G loss: 2.505971, acc: 0.351562]\n",
            "2543: [D loss: 0.082529, acc: 0.976562]  [G loss: 2.235170, acc: 0.414062]\n",
            "2544: [D loss: 0.072377, acc: 0.980469]  [G loss: 2.155208, acc: 0.445312]\n",
            "2545: [D loss: 0.111322, acc: 0.949219]  [G loss: 2.899413, acc: 0.351562]\n",
            "2546: [D loss: 0.106064, acc: 0.964844]  [G loss: 3.224256, acc: 0.335938]\n",
            "2547: [D loss: 0.112607, acc: 0.972656]  [G loss: 3.288892, acc: 0.312500]\n",
            "2548: [D loss: 0.072287, acc: 0.976562]  [G loss: 2.828086, acc: 0.328125]\n",
            "2549: [D loss: 0.082751, acc: 0.972656]  [G loss: 2.820544, acc: 0.367188]\n",
            "2550: [D loss: 0.110522, acc: 0.968750]  [G loss: 2.908318, acc: 0.351562]\n",
            "2551: [D loss: 0.116884, acc: 0.980469]  [G loss: 2.685550, acc: 0.328125]\n",
            "2552: [D loss: 0.103766, acc: 0.972656]  [G loss: 2.462987, acc: 0.414062]\n",
            "2553: [D loss: 0.107263, acc: 0.953125]  [G loss: 2.307751, acc: 0.476562]\n",
            "2554: [D loss: 0.104649, acc: 0.960938]  [G loss: 2.127699, acc: 0.398438]\n",
            "2555: [D loss: 0.127414, acc: 0.949219]  [G loss: 1.987723, acc: 0.484375]\n",
            "2556: [D loss: 0.098575, acc: 0.949219]  [G loss: 1.963498, acc: 0.429688]\n",
            "2557: [D loss: 0.082718, acc: 0.972656]  [G loss: 2.352264, acc: 0.296875]\n",
            "2558: [D loss: 0.080047, acc: 0.972656]  [G loss: 2.470218, acc: 0.328125]\n",
            "2559: [D loss: 0.095098, acc: 0.957031]  [G loss: 2.864159, acc: 0.265625]\n",
            "2560: [D loss: 0.086308, acc: 0.972656]  [G loss: 1.938378, acc: 0.359375]\n",
            "2561: [D loss: 0.105437, acc: 0.953125]  [G loss: 2.022231, acc: 0.351562]\n",
            "2562: [D loss: 0.095723, acc: 0.972656]  [G loss: 1.976952, acc: 0.390625]\n",
            "2563: [D loss: 0.096104, acc: 0.960938]  [G loss: 1.884157, acc: 0.414062]\n",
            "2564: [D loss: 0.083030, acc: 0.972656]  [G loss: 2.040239, acc: 0.406250]\n",
            "2565: [D loss: 0.093619, acc: 0.964844]  [G loss: 2.168574, acc: 0.312500]\n",
            "2566: [D loss: 0.120632, acc: 0.957031]  [G loss: 2.308757, acc: 0.343750]\n",
            "2567: [D loss: 0.070526, acc: 0.972656]  [G loss: 1.878532, acc: 0.429688]\n",
            "2568: [D loss: 0.135012, acc: 0.960938]  [G loss: 1.436040, acc: 0.476562]\n",
            "2569: [D loss: 0.165664, acc: 0.933594]  [G loss: 1.779578, acc: 0.484375]\n",
            "2570: [D loss: 0.108425, acc: 0.960938]  [G loss: 2.261815, acc: 0.382812]\n",
            "2571: [D loss: 0.077691, acc: 0.972656]  [G loss: 3.218159, acc: 0.265625]\n",
            "2572: [D loss: 0.100857, acc: 0.953125]  [G loss: 3.418735, acc: 0.289062]\n",
            "2573: [D loss: 0.150792, acc: 0.937500]  [G loss: 2.466681, acc: 0.375000]\n",
            "2574: [D loss: 0.124643, acc: 0.949219]  [G loss: 2.285005, acc: 0.390625]\n",
            "2575: [D loss: 0.105801, acc: 0.953125]  [G loss: 2.178235, acc: 0.429688]\n",
            "2576: [D loss: 0.100608, acc: 0.953125]  [G loss: 2.794728, acc: 0.390625]\n",
            "2577: [D loss: 0.121157, acc: 0.964844]  [G loss: 3.413496, acc: 0.289062]\n",
            "2578: [D loss: 0.063308, acc: 0.976562]  [G loss: 3.312620, acc: 0.343750]\n",
            "2579: [D loss: 0.129907, acc: 0.968750]  [G loss: 2.851515, acc: 0.406250]\n",
            "2580: [D loss: 0.098578, acc: 0.964844]  [G loss: 2.682341, acc: 0.421875]\n",
            "2581: [D loss: 0.069994, acc: 0.976562]  [G loss: 2.003912, acc: 0.453125]\n",
            "2582: [D loss: 0.114486, acc: 0.968750]  [G loss: 2.969484, acc: 0.445312]\n",
            "2583: [D loss: 0.119037, acc: 0.968750]  [G loss: 2.808845, acc: 0.437500]\n",
            "2584: [D loss: 0.075512, acc: 0.964844]  [G loss: 3.111259, acc: 0.398438]\n",
            "2585: [D loss: 0.103846, acc: 0.972656]  [G loss: 2.309197, acc: 0.460938]\n",
            "2586: [D loss: 0.117819, acc: 0.972656]  [G loss: 2.629082, acc: 0.468750]\n",
            "2587: [D loss: 0.130040, acc: 0.953125]  [G loss: 1.924591, acc: 0.546875]\n",
            "2588: [D loss: 0.063904, acc: 0.980469]  [G loss: 1.302224, acc: 0.671875]\n",
            "2589: [D loss: 0.097606, acc: 0.953125]  [G loss: 1.821809, acc: 0.601562]\n",
            "2590: [D loss: 0.125314, acc: 0.949219]  [G loss: 1.347736, acc: 0.648438]\n",
            "2591: [D loss: 0.100553, acc: 0.957031]  [G loss: 2.054025, acc: 0.562500]\n",
            "2592: [D loss: 0.185690, acc: 0.945312]  [G loss: 1.822299, acc: 0.617188]\n",
            "2593: [D loss: 0.227779, acc: 0.957031]  [G loss: 1.620049, acc: 0.726562]\n",
            "2594: [D loss: 0.127036, acc: 0.957031]  [G loss: 0.899803, acc: 0.734375]\n",
            "2595: [D loss: 0.297741, acc: 0.921875]  [G loss: 0.881905, acc: 0.773438]\n",
            "2596: [D loss: 0.186303, acc: 0.960938]  [G loss: 0.902520, acc: 0.765625]\n",
            "2597: [D loss: 0.169227, acc: 0.960938]  [G loss: 1.042391, acc: 0.765625]\n",
            "2598: [D loss: 0.279296, acc: 0.929688]  [G loss: 0.613480, acc: 0.867188]\n",
            "2599: [D loss: 0.133979, acc: 0.953125]  [G loss: 0.234638, acc: 0.906250]\n",
            "2600: [D loss: 0.138796, acc: 0.949219]  [G loss: 0.467782, acc: 0.851562]\n",
            "2601: [D loss: 0.194528, acc: 0.937500]  [G loss: 0.558827, acc: 0.835938]\n",
            "2602: [D loss: 0.078038, acc: 0.976562]  [G loss: 0.556550, acc: 0.804688]\n",
            "2603: [D loss: 0.204827, acc: 0.933594]  [G loss: 0.467312, acc: 0.851562]\n",
            "2604: [D loss: 0.298888, acc: 0.933594]  [G loss: 0.236349, acc: 0.906250]\n",
            "2605: [D loss: 0.110412, acc: 0.968750]  [G loss: 0.288975, acc: 0.890625]\n",
            "2606: [D loss: 0.071987, acc: 0.984375]  [G loss: 0.200821, acc: 0.937500]\n",
            "2607: [D loss: 0.150233, acc: 0.941406]  [G loss: 0.183796, acc: 0.953125]\n",
            "2608: [D loss: 0.087485, acc: 0.964844]  [G loss: 0.314138, acc: 0.882812]\n",
            "2609: [D loss: 0.040509, acc: 0.992188]  [G loss: 0.522509, acc: 0.812500]\n",
            "2610: [D loss: 0.107701, acc: 0.957031]  [G loss: 0.817106, acc: 0.656250]\n",
            "2611: [D loss: 0.110497, acc: 0.960938]  [G loss: 0.386536, acc: 0.835938]\n",
            "2612: [D loss: 0.068399, acc: 0.968750]  [G loss: 0.152340, acc: 0.945312]\n",
            "2613: [D loss: 0.105948, acc: 0.957031]  [G loss: 0.101958, acc: 0.953125]\n",
            "2614: [D loss: 0.155563, acc: 0.937500]  [G loss: 0.290867, acc: 0.875000]\n",
            "2615: [D loss: 0.086186, acc: 0.972656]  [G loss: 0.864777, acc: 0.648438]\n",
            "2616: [D loss: 0.095252, acc: 0.960938]  [G loss: 1.200103, acc: 0.554688]\n",
            "2617: [D loss: 0.135545, acc: 0.957031]  [G loss: 0.907299, acc: 0.656250]\n",
            "2618: [D loss: 0.189553, acc: 0.949219]  [G loss: 0.252789, acc: 0.882812]\n",
            "2619: [D loss: 0.077439, acc: 0.968750]  [G loss: 0.172826, acc: 0.929688]\n",
            "2620: [D loss: 0.139348, acc: 0.945312]  [G loss: 0.580703, acc: 0.726562]\n",
            "2621: [D loss: 0.070459, acc: 0.972656]  [G loss: 1.492526, acc: 0.554688]\n",
            "2622: [D loss: 0.132505, acc: 0.964844]  [G loss: 2.068123, acc: 0.367188]\n",
            "2623: [D loss: 0.099604, acc: 0.960938]  [G loss: 2.103563, acc: 0.390625]\n",
            "2624: [D loss: 0.058016, acc: 0.980469]  [G loss: 1.442993, acc: 0.507812]\n",
            "2625: [D loss: 0.054527, acc: 0.980469]  [G loss: 1.251452, acc: 0.601562]\n",
            "2626: [D loss: 0.112819, acc: 0.957031]  [G loss: 1.543206, acc: 0.562500]\n",
            "2627: [D loss: 0.140885, acc: 0.945312]  [G loss: 2.000014, acc: 0.414062]\n",
            "2628: [D loss: 0.084752, acc: 0.976562]  [G loss: 2.786905, acc: 0.296875]\n",
            "2629: [D loss: 0.110528, acc: 0.960938]  [G loss: 2.391854, acc: 0.335938]\n",
            "2630: [D loss: 0.113970, acc: 0.964844]  [G loss: 1.821120, acc: 0.390625]\n",
            "2631: [D loss: 0.102091, acc: 0.953125]  [G loss: 1.796090, acc: 0.492188]\n",
            "2632: [D loss: 0.095100, acc: 0.964844]  [G loss: 1.629639, acc: 0.523438]\n",
            "2633: [D loss: 0.071334, acc: 0.984375]  [G loss: 2.665053, acc: 0.312500]\n",
            "2634: [D loss: 0.064005, acc: 0.976562]  [G loss: 3.749045, acc: 0.210938]\n",
            "2635: [D loss: 0.070626, acc: 0.968750]  [G loss: 4.003938, acc: 0.101562]\n",
            "2636: [D loss: 0.121685, acc: 0.968750]  [G loss: 4.225109, acc: 0.164062]\n",
            "2637: [D loss: 0.207663, acc: 0.921875]  [G loss: 3.366068, acc: 0.250000]\n",
            "2638: [D loss: 0.101955, acc: 0.960938]  [G loss: 2.523615, acc: 0.484375]\n",
            "2639: [D loss: 0.135890, acc: 0.933594]  [G loss: 2.649274, acc: 0.406250]\n",
            "2640: [D loss: 0.135212, acc: 0.933594]  [G loss: 3.402115, acc: 0.304688]\n",
            "2641: [D loss: 0.100942, acc: 0.972656]  [G loss: 3.688888, acc: 0.250000]\n",
            "2642: [D loss: 0.176044, acc: 0.925781]  [G loss: 3.117703, acc: 0.343750]\n",
            "2643: [D loss: 0.155363, acc: 0.941406]  [G loss: 2.524479, acc: 0.304688]\n",
            "2644: [D loss: 0.200783, acc: 0.917969]  [G loss: 2.088334, acc: 0.453125]\n",
            "2645: [D loss: 0.207253, acc: 0.902344]  [G loss: 2.117252, acc: 0.437500]\n",
            "2646: [D loss: 0.165793, acc: 0.941406]  [G loss: 3.452054, acc: 0.218750]\n",
            "2647: [D loss: 0.170802, acc: 0.933594]  [G loss: 3.311603, acc: 0.242188]\n",
            "2648: [D loss: 0.119454, acc: 0.945312]  [G loss: 4.085680, acc: 0.093750]\n",
            "2649: [D loss: 0.140534, acc: 0.957031]  [G loss: 3.177132, acc: 0.218750]\n",
            "2650: [D loss: 0.109658, acc: 0.953125]  [G loss: 3.321817, acc: 0.210938]\n",
            "2651: [D loss: 0.100627, acc: 0.949219]  [G loss: 3.902767, acc: 0.156250]\n",
            "2652: [D loss: 0.097029, acc: 0.964844]  [G loss: 3.466267, acc: 0.156250]\n",
            "2653: [D loss: 0.145022, acc: 0.949219]  [G loss: 2.940551, acc: 0.234375]\n",
            "2654: [D loss: 0.104192, acc: 0.957031]  [G loss: 2.346568, acc: 0.265625]\n",
            "2655: [D loss: 0.116033, acc: 0.945312]  [G loss: 2.465506, acc: 0.281250]\n",
            "2656: [D loss: 0.098710, acc: 0.976562]  [G loss: 2.684753, acc: 0.250000]\n",
            "2657: [D loss: 0.075036, acc: 0.972656]  [G loss: 3.295967, acc: 0.210938]\n",
            "2658: [D loss: 0.069470, acc: 0.976562]  [G loss: 3.527121, acc: 0.132812]\n",
            "2659: [D loss: 0.118961, acc: 0.949219]  [G loss: 3.292645, acc: 0.179688]\n",
            "2660: [D loss: 0.124512, acc: 0.945312]  [G loss: 2.602652, acc: 0.250000]\n",
            "2661: [D loss: 0.054869, acc: 0.976562]  [G loss: 2.379209, acc: 0.234375]\n",
            "2662: [D loss: 0.046864, acc: 0.992188]  [G loss: 2.955611, acc: 0.171875]\n",
            "2663: [D loss: 0.054819, acc: 0.980469]  [G loss: 4.681271, acc: 0.039062]\n",
            "2664: [D loss: 0.079989, acc: 0.972656]  [G loss: 5.512791, acc: 0.015625]\n",
            "2665: [D loss: 0.063066, acc: 0.976562]  [G loss: 5.310831, acc: 0.046875]\n",
            "2666: [D loss: 0.104453, acc: 0.960938]  [G loss: 4.527739, acc: 0.085938]\n",
            "2667: [D loss: 0.067343, acc: 0.976562]  [G loss: 3.707745, acc: 0.156250]\n",
            "2668: [D loss: 0.124402, acc: 0.953125]  [G loss: 3.982821, acc: 0.195312]\n",
            "2669: [D loss: 0.074150, acc: 0.972656]  [G loss: 4.699120, acc: 0.125000]\n",
            "2670: [D loss: 0.037358, acc: 0.992188]  [G loss: 5.299263, acc: 0.101562]\n",
            "2671: [D loss: 0.014173, acc: 0.996094]  [G loss: 5.891531, acc: 0.078125]\n",
            "2672: [D loss: 0.076760, acc: 0.980469]  [G loss: 5.477809, acc: 0.140625]\n",
            "2673: [D loss: 0.081318, acc: 0.972656]  [G loss: 4.665748, acc: 0.148438]\n",
            "2674: [D loss: 0.024089, acc: 0.988281]  [G loss: 4.268821, acc: 0.218750]\n",
            "2675: [D loss: 0.092980, acc: 0.968750]  [G loss: 3.630973, acc: 0.273438]\n",
            "2676: [D loss: 0.089410, acc: 0.972656]  [G loss: 4.279475, acc: 0.250000]\n",
            "2677: [D loss: 0.138554, acc: 0.968750]  [G loss: 4.547747, acc: 0.242188]\n",
            "2678: [D loss: 0.073177, acc: 0.980469]  [G loss: 3.738971, acc: 0.281250]\n",
            "2679: [D loss: 0.062759, acc: 0.972656]  [G loss: 3.090972, acc: 0.351562]\n",
            "2680: [D loss: 0.026556, acc: 0.992188]  [G loss: 2.984825, acc: 0.359375]\n",
            "2681: [D loss: 0.098543, acc: 0.980469]  [G loss: 2.710575, acc: 0.351562]\n",
            "2682: [D loss: 0.058785, acc: 0.980469]  [G loss: 2.223811, acc: 0.429688]\n",
            "2683: [D loss: 0.059514, acc: 0.972656]  [G loss: 2.201612, acc: 0.468750]\n",
            "2684: [D loss: 0.073363, acc: 0.972656]  [G loss: 2.945314, acc: 0.390625]\n",
            "2685: [D loss: 0.085665, acc: 0.972656]  [G loss: 3.087034, acc: 0.335938]\n",
            "2686: [D loss: 0.044250, acc: 0.976562]  [G loss: 2.980961, acc: 0.367188]\n",
            "2687: [D loss: 0.083122, acc: 0.984375]  [G loss: 2.439441, acc: 0.398438]\n",
            "2688: [D loss: 0.055636, acc: 0.980469]  [G loss: 1.699478, acc: 0.500000]\n",
            "2689: [D loss: 0.119784, acc: 0.960938]  [G loss: 1.717617, acc: 0.476562]\n",
            "2690: [D loss: 0.113606, acc: 0.960938]  [G loss: 2.612929, acc: 0.390625]\n",
            "2691: [D loss: 0.043535, acc: 0.984375]  [G loss: 3.211779, acc: 0.328125]\n",
            "2692: [D loss: 0.138693, acc: 0.945312]  [G loss: 1.931408, acc: 0.539062]\n",
            "2693: [D loss: 0.103872, acc: 0.957031]  [G loss: 1.146723, acc: 0.710938]\n",
            "2694: [D loss: 0.099798, acc: 0.953125]  [G loss: 1.101321, acc: 0.695312]\n",
            "2695: [D loss: 0.081163, acc: 0.960938]  [G loss: 1.535799, acc: 0.562500]\n",
            "2696: [D loss: 0.047461, acc: 0.980469]  [G loss: 2.110791, acc: 0.484375]\n",
            "2697: [D loss: 0.137961, acc: 0.968750]  [G loss: 1.500280, acc: 0.578125]\n",
            "2698: [D loss: 0.093203, acc: 0.968750]  [G loss: 0.893765, acc: 0.703125]\n",
            "2699: [D loss: 0.134329, acc: 0.941406]  [G loss: 0.682288, acc: 0.765625]\n",
            "2700: [D loss: 0.201684, acc: 0.929688]  [G loss: 1.108128, acc: 0.640625]\n",
            "2701: [D loss: 0.176301, acc: 0.949219]  [G loss: 1.593392, acc: 0.515625]\n",
            "2702: [D loss: 0.122761, acc: 0.953125]  [G loss: 1.236634, acc: 0.625000]\n",
            "2703: [D loss: 0.190381, acc: 0.933594]  [G loss: 0.694779, acc: 0.734375]\n",
            "2704: [D loss: 0.283425, acc: 0.917969]  [G loss: 1.269981, acc: 0.492188]\n",
            "2705: [D loss: 0.143017, acc: 0.957031]  [G loss: 1.623242, acc: 0.421875]\n",
            "2706: [D loss: 0.195361, acc: 0.949219]  [G loss: 1.422646, acc: 0.484375]\n",
            "2707: [D loss: 0.329193, acc: 0.890625]  [G loss: 1.455665, acc: 0.484375]\n",
            "2708: [D loss: 0.199518, acc: 0.917969]  [G loss: 1.362189, acc: 0.476562]\n",
            "2709: [D loss: 0.265825, acc: 0.925781]  [G loss: 0.873870, acc: 0.585938]\n",
            "2710: [D loss: 0.180973, acc: 0.933594]  [G loss: 0.702855, acc: 0.695312]\n",
            "2711: [D loss: 0.138085, acc: 0.960938]  [G loss: 1.483928, acc: 0.421875]\n",
            "2712: [D loss: 0.164755, acc: 0.953125]  [G loss: 1.883053, acc: 0.328125]\n",
            "2713: [D loss: 0.144992, acc: 0.953125]  [G loss: 1.673260, acc: 0.367188]\n",
            "2714: [D loss: 0.086601, acc: 0.980469]  [G loss: 1.447110, acc: 0.453125]\n",
            "2715: [D loss: 0.095716, acc: 0.964844]  [G loss: 1.330829, acc: 0.445312]\n",
            "2716: [D loss: 0.083963, acc: 0.976562]  [G loss: 1.796870, acc: 0.414062]\n",
            "2717: [D loss: 0.091640, acc: 0.972656]  [G loss: 2.086026, acc: 0.335938]\n",
            "2718: [D loss: 0.106530, acc: 0.972656]  [G loss: 2.444127, acc: 0.281250]\n",
            "2719: [D loss: 0.123206, acc: 0.964844]  [G loss: 2.347438, acc: 0.312500]\n",
            "2720: [D loss: 0.108482, acc: 0.960938]  [G loss: 1.653433, acc: 0.414062]\n",
            "2721: [D loss: 0.094413, acc: 0.957031]  [G loss: 1.628124, acc: 0.445312]\n",
            "2722: [D loss: 0.050058, acc: 0.984375]  [G loss: 1.447357, acc: 0.445312]\n",
            "2723: [D loss: 0.072379, acc: 0.976562]  [G loss: 1.870605, acc: 0.398438]\n",
            "2724: [D loss: 0.067500, acc: 0.980469]  [G loss: 2.159234, acc: 0.304688]\n",
            "2725: [D loss: 0.055546, acc: 0.976562]  [G loss: 2.405283, acc: 0.242188]\n",
            "2726: [D loss: 0.045702, acc: 0.980469]  [G loss: 2.072821, acc: 0.312500]\n",
            "2727: [D loss: 0.032559, acc: 0.992188]  [G loss: 2.312336, acc: 0.296875]\n",
            "2728: [D loss: 0.030084, acc: 0.996094]  [G loss: 2.576840, acc: 0.281250]\n",
            "2729: [D loss: 0.041506, acc: 0.992188]  [G loss: 2.479158, acc: 0.304688]\n",
            "2730: [D loss: 0.032211, acc: 0.992188]  [G loss: 2.860735, acc: 0.273438]\n",
            "2731: [D loss: 0.067176, acc: 0.976562]  [G loss: 2.261065, acc: 0.328125]\n",
            "2732: [D loss: 0.028633, acc: 0.992188]  [G loss: 2.344182, acc: 0.382812]\n",
            "2733: [D loss: 0.031408, acc: 0.996094]  [G loss: 2.183904, acc: 0.382812]\n",
            "2734: [D loss: 0.048977, acc: 0.980469]  [G loss: 2.182723, acc: 0.398438]\n",
            "2735: [D loss: 0.043325, acc: 0.984375]  [G loss: 2.461714, acc: 0.375000]\n",
            "2736: [D loss: 0.027192, acc: 0.996094]  [G loss: 2.620331, acc: 0.398438]\n",
            "2737: [D loss: 0.054180, acc: 0.972656]  [G loss: 2.715455, acc: 0.390625]\n",
            "2738: [D loss: 0.074124, acc: 0.976562]  [G loss: 2.849671, acc: 0.343750]\n",
            "2739: [D loss: 0.058328, acc: 0.976562]  [G loss: 2.407272, acc: 0.382812]\n",
            "2740: [D loss: 0.087567, acc: 0.960938]  [G loss: 2.634186, acc: 0.390625]\n",
            "2741: [D loss: 0.100853, acc: 0.949219]  [G loss: 2.788279, acc: 0.320312]\n",
            "2742: [D loss: 0.093636, acc: 0.953125]  [G loss: 2.858193, acc: 0.265625]\n",
            "2743: [D loss: 0.127904, acc: 0.957031]  [G loss: 3.022773, acc: 0.187500]\n",
            "2744: [D loss: 0.125652, acc: 0.945312]  [G loss: 2.895836, acc: 0.312500]\n",
            "2745: [D loss: 0.081581, acc: 0.972656]  [G loss: 2.424491, acc: 0.320312]\n",
            "2746: [D loss: 0.109065, acc: 0.953125]  [G loss: 1.735729, acc: 0.390625]\n",
            "2747: [D loss: 0.234588, acc: 0.902344]  [G loss: 3.213858, acc: 0.203125]\n",
            "2748: [D loss: 0.125606, acc: 0.957031]  [G loss: 3.624571, acc: 0.132812]\n",
            "2749: [D loss: 0.104452, acc: 0.953125]  [G loss: 3.648690, acc: 0.203125]\n",
            "2750: [D loss: 0.173197, acc: 0.953125]  [G loss: 2.206419, acc: 0.382812]\n",
            "2751: [D loss: 0.123670, acc: 0.941406]  [G loss: 1.834132, acc: 0.546875]\n",
            "2752: [D loss: 0.200476, acc: 0.917969]  [G loss: 2.441135, acc: 0.367188]\n",
            "2753: [D loss: 0.124042, acc: 0.957031]  [G loss: 2.758409, acc: 0.351562]\n",
            "2754: [D loss: 0.080639, acc: 0.964844]  [G loss: 2.988665, acc: 0.335938]\n",
            "2755: [D loss: 0.102778, acc: 0.960938]  [G loss: 2.711086, acc: 0.343750]\n",
            "2756: [D loss: 0.165892, acc: 0.945312]  [G loss: 2.318077, acc: 0.406250]\n",
            "2757: [D loss: 0.127069, acc: 0.929688]  [G loss: 2.025593, acc: 0.476562]\n",
            "2758: [D loss: 0.156425, acc: 0.937500]  [G loss: 2.603698, acc: 0.312500]\n",
            "2759: [D loss: 0.129951, acc: 0.953125]  [G loss: 3.052539, acc: 0.304688]\n",
            "2760: [D loss: 0.169623, acc: 0.933594]  [G loss: 3.292592, acc: 0.242188]\n",
            "2761: [D loss: 0.176062, acc: 0.949219]  [G loss: 2.127691, acc: 0.367188]\n",
            "2762: [D loss: 0.105843, acc: 0.968750]  [G loss: 1.559991, acc: 0.484375]\n",
            "2763: [D loss: 0.196270, acc: 0.917969]  [G loss: 1.775954, acc: 0.453125]\n",
            "2764: [D loss: 0.124236, acc: 0.945312]  [G loss: 2.190718, acc: 0.335938]\n",
            "2765: [D loss: 0.141461, acc: 0.953125]  [G loss: 2.708505, acc: 0.359375]\n",
            "2766: [D loss: 0.112891, acc: 0.960938]  [G loss: 2.938838, acc: 0.359375]\n",
            "2767: [D loss: 0.125001, acc: 0.937500]  [G loss: 2.756191, acc: 0.351562]\n",
            "2768: [D loss: 0.141202, acc: 0.941406]  [G loss: 2.623374, acc: 0.406250]\n",
            "2769: [D loss: 0.163051, acc: 0.933594]  [G loss: 2.869035, acc: 0.359375]\n",
            "2770: [D loss: 0.137492, acc: 0.933594]  [G loss: 3.732517, acc: 0.242188]\n",
            "2771: [D loss: 0.152775, acc: 0.933594]  [G loss: 4.070559, acc: 0.148438]\n",
            "2772: [D loss: 0.163170, acc: 0.933594]  [G loss: 3.409158, acc: 0.234375]\n",
            "2773: [D loss: 0.049266, acc: 0.972656]  [G loss: 2.370566, acc: 0.375000]\n",
            "2774: [D loss: 0.263496, acc: 0.902344]  [G loss: 4.178121, acc: 0.148438]\n",
            "2775: [D loss: 0.057732, acc: 0.976562]  [G loss: 5.913949, acc: 0.031250]\n",
            "2776: [D loss: 0.102965, acc: 0.964844]  [G loss: 6.127472, acc: 0.039062]\n",
            "2777: [D loss: 0.149391, acc: 0.953125]  [G loss: 4.494016, acc: 0.117188]\n",
            "2778: [D loss: 0.105012, acc: 0.964844]  [G loss: 2.868476, acc: 0.273438]\n",
            "2779: [D loss: 0.130729, acc: 0.968750]  [G loss: 3.301119, acc: 0.203125]\n",
            "2780: [D loss: 0.102316, acc: 0.953125]  [G loss: 4.122703, acc: 0.164062]\n",
            "2781: [D loss: 0.090965, acc: 0.960938]  [G loss: 5.696363, acc: 0.054688]\n",
            "2782: [D loss: 0.082759, acc: 0.972656]  [G loss: 6.664965, acc: 0.054688]\n",
            "2783: [D loss: 0.096802, acc: 0.960938]  [G loss: 6.640101, acc: 0.039062]\n",
            "2784: [D loss: 0.060409, acc: 0.980469]  [G loss: 5.629700, acc: 0.109375]\n",
            "2785: [D loss: 0.069148, acc: 0.984375]  [G loss: 4.795725, acc: 0.148438]\n",
            "2786: [D loss: 0.104486, acc: 0.972656]  [G loss: 4.189120, acc: 0.257812]\n",
            "2787: [D loss: 0.082223, acc: 0.968750]  [G loss: 3.829710, acc: 0.289062]\n",
            "2788: [D loss: 0.103523, acc: 0.964844]  [G loss: 4.529149, acc: 0.273438]\n",
            "2789: [D loss: 0.088848, acc: 0.953125]  [G loss: 5.266592, acc: 0.148438]\n",
            "2790: [D loss: 0.108053, acc: 0.941406]  [G loss: 5.586853, acc: 0.187500]\n",
            "2791: [D loss: 0.079583, acc: 0.960938]  [G loss: 4.608397, acc: 0.218750]\n",
            "2792: [D loss: 0.097103, acc: 0.953125]  [G loss: 3.589743, acc: 0.328125]\n",
            "2793: [D loss: 0.059111, acc: 0.988281]  [G loss: 4.314221, acc: 0.281250]\n",
            "2794: [D loss: 0.070570, acc: 0.976562]  [G loss: 4.839028, acc: 0.203125]\n",
            "2795: [D loss: 0.027062, acc: 0.992188]  [G loss: 6.199120, acc: 0.093750]\n",
            "2796: [D loss: 0.050699, acc: 0.984375]  [G loss: 6.449866, acc: 0.132812]\n",
            "2797: [D loss: 0.132087, acc: 0.945312]  [G loss: 5.553430, acc: 0.187500]\n",
            "2798: [D loss: 0.050722, acc: 0.976562]  [G loss: 4.697278, acc: 0.250000]\n",
            "2799: [D loss: 0.086072, acc: 0.957031]  [G loss: 4.080439, acc: 0.281250]\n",
            "2800: [D loss: 0.078180, acc: 0.964844]  [G loss: 3.742389, acc: 0.367188]\n",
            "2801: [D loss: 0.063404, acc: 0.980469]  [G loss: 4.147795, acc: 0.328125]\n",
            "2802: [D loss: 0.080131, acc: 0.980469]  [G loss: 4.814645, acc: 0.234375]\n",
            "2803: [D loss: 0.065924, acc: 0.976562]  [G loss: 5.091934, acc: 0.187500]\n",
            "2804: [D loss: 0.052004, acc: 0.988281]  [G loss: 4.964354, acc: 0.226562]\n",
            "2805: [D loss: 0.117719, acc: 0.964844]  [G loss: 5.007898, acc: 0.164062]\n",
            "2806: [D loss: 0.052504, acc: 0.984375]  [G loss: 4.124934, acc: 0.234375]\n",
            "2807: [D loss: 0.088482, acc: 0.972656]  [G loss: 3.293320, acc: 0.343750]\n",
            "2808: [D loss: 0.289358, acc: 0.894531]  [G loss: 5.940382, acc: 0.171875]\n",
            "2809: [D loss: 0.166484, acc: 0.945312]  [G loss: 5.106997, acc: 0.195312]\n",
            "2810: [D loss: 0.137318, acc: 0.968750]  [G loss: 4.023071, acc: 0.343750]\n",
            "2811: [D loss: 0.063019, acc: 0.972656]  [G loss: 2.409625, acc: 0.453125]\n",
            "2812: [D loss: 0.108849, acc: 0.957031]  [G loss: 2.134544, acc: 0.531250]\n",
            "2813: [D loss: 0.170611, acc: 0.929688]  [G loss: 1.991527, acc: 0.523438]\n",
            "2814: [D loss: 0.095733, acc: 0.968750]  [G loss: 2.771496, acc: 0.359375]\n",
            "2815: [D loss: 0.112662, acc: 0.960938]  [G loss: 3.305149, acc: 0.304688]\n",
            "2816: [D loss: 0.105036, acc: 0.964844]  [G loss: 2.738046, acc: 0.421875]\n",
            "2817: [D loss: 0.123087, acc: 0.968750]  [G loss: 2.037834, acc: 0.507812]\n",
            "2818: [D loss: 0.064077, acc: 0.976562]  [G loss: 2.154349, acc: 0.578125]\n",
            "2819: [D loss: 0.083915, acc: 0.968750]  [G loss: 1.866918, acc: 0.562500]\n",
            "2820: [D loss: 0.059814, acc: 0.960938]  [G loss: 1.779501, acc: 0.585938]\n",
            "2821: [D loss: 0.077476, acc: 0.976562]  [G loss: 2.333527, acc: 0.539062]\n",
            "2822: [D loss: 0.029467, acc: 0.992188]  [G loss: 2.058450, acc: 0.617188]\n",
            "2823: [D loss: 0.065498, acc: 0.980469]  [G loss: 2.215147, acc: 0.593750]\n",
            "2824: [D loss: 0.086795, acc: 0.964844]  [G loss: 2.260115, acc: 0.515625]\n",
            "2825: [D loss: 0.046221, acc: 0.984375]  [G loss: 2.791116, acc: 0.476562]\n",
            "2826: [D loss: 0.089081, acc: 0.972656]  [G loss: 2.496273, acc: 0.539062]\n",
            "2827: [D loss: 0.064043, acc: 0.980469]  [G loss: 2.179831, acc: 0.554688]\n",
            "2828: [D loss: 0.138817, acc: 0.937500]  [G loss: 2.305401, acc: 0.476562]\n",
            "2829: [D loss: 0.170728, acc: 0.957031]  [G loss: 2.102108, acc: 0.468750]\n",
            "2830: [D loss: 0.136328, acc: 0.960938]  [G loss: 1.952404, acc: 0.539062]\n",
            "2831: [D loss: 0.151374, acc: 0.945312]  [G loss: 2.044311, acc: 0.476562]\n",
            "2832: [D loss: 0.156760, acc: 0.925781]  [G loss: 2.925567, acc: 0.375000]\n",
            "2833: [D loss: 0.162746, acc: 0.941406]  [G loss: 3.328172, acc: 0.351562]\n",
            "2834: [D loss: 0.046297, acc: 0.984375]  [G loss: 3.861214, acc: 0.281250]\n",
            "2835: [D loss: 0.070572, acc: 0.988281]  [G loss: 4.345455, acc: 0.203125]\n",
            "2836: [D loss: 0.134655, acc: 0.972656]  [G loss: 3.404118, acc: 0.398438]\n",
            "2837: [D loss: 0.134159, acc: 0.960938]  [G loss: 2.694991, acc: 0.468750]\n",
            "2838: [D loss: 0.078308, acc: 0.968750]  [G loss: 3.513554, acc: 0.335938]\n",
            "2839: [D loss: 0.152807, acc: 0.957031]  [G loss: 3.423432, acc: 0.390625]\n",
            "2840: [D loss: 0.149046, acc: 0.964844]  [G loss: 3.627972, acc: 0.351562]\n",
            "2841: [D loss: 0.069451, acc: 0.972656]  [G loss: 3.754312, acc: 0.343750]\n",
            "2842: [D loss: 0.096831, acc: 0.960938]  [G loss: 3.781007, acc: 0.398438]\n",
            "2843: [D loss: 0.130869, acc: 0.968750]  [G loss: 3.116992, acc: 0.390625]\n",
            "2844: [D loss: 0.096901, acc: 0.964844]  [G loss: 2.593149, acc: 0.414062]\n",
            "2845: [D loss: 0.183426, acc: 0.929688]  [G loss: 3.286334, acc: 0.335938]\n",
            "2846: [D loss: 0.133558, acc: 0.941406]  [G loss: 3.286553, acc: 0.312500]\n",
            "2847: [D loss: 0.123403, acc: 0.960938]  [G loss: 4.415556, acc: 0.242188]\n",
            "2848: [D loss: 0.071682, acc: 0.968750]  [G loss: 3.774121, acc: 0.289062]\n",
            "2849: [D loss: 0.056899, acc: 0.968750]  [G loss: 3.262907, acc: 0.281250]\n",
            "2850: [D loss: 0.082413, acc: 0.972656]  [G loss: 3.690561, acc: 0.343750]\n",
            "2851: [D loss: 0.103873, acc: 0.976562]  [G loss: 3.740726, acc: 0.351562]\n",
            "2852: [D loss: 0.120775, acc: 0.960938]  [G loss: 3.445059, acc: 0.273438]\n",
            "2853: [D loss: 0.043493, acc: 0.980469]  [G loss: 4.536302, acc: 0.195312]\n",
            "2854: [D loss: 0.059418, acc: 0.980469]  [G loss: 4.683912, acc: 0.156250]\n",
            "2855: [D loss: 0.086069, acc: 0.972656]  [G loss: 5.185812, acc: 0.203125]\n",
            "2856: [D loss: 0.217365, acc: 0.941406]  [G loss: 3.394898, acc: 0.273438]\n",
            "2857: [D loss: 0.089749, acc: 0.957031]  [G loss: 3.449566, acc: 0.312500]\n",
            "2858: [D loss: 0.202062, acc: 0.933594]  [G loss: 4.146077, acc: 0.281250]\n",
            "2859: [D loss: 0.086970, acc: 0.972656]  [G loss: 5.206566, acc: 0.265625]\n",
            "2860: [D loss: 0.185847, acc: 0.945312]  [G loss: 4.286509, acc: 0.265625]\n",
            "2861: [D loss: 0.128266, acc: 0.960938]  [G loss: 3.448268, acc: 0.335938]\n",
            "2862: [D loss: 0.092196, acc: 0.972656]  [G loss: 2.652971, acc: 0.421875]\n",
            "2863: [D loss: 0.203575, acc: 0.921875]  [G loss: 3.944924, acc: 0.296875]\n",
            "2864: [D loss: 0.082883, acc: 0.960938]  [G loss: 4.657222, acc: 0.210938]\n",
            "2865: [D loss: 0.086503, acc: 0.980469]  [G loss: 5.766780, acc: 0.085938]\n",
            "2866: [D loss: 0.156619, acc: 0.953125]  [G loss: 4.851437, acc: 0.109375]\n",
            "2867: [D loss: 0.111184, acc: 0.953125]  [G loss: 2.682918, acc: 0.304688]\n",
            "2868: [D loss: 0.128769, acc: 0.941406]  [G loss: 2.785583, acc: 0.343750]\n",
            "2869: [D loss: 0.105445, acc: 0.976562]  [G loss: 3.818403, acc: 0.179688]\n",
            "2870: [D loss: 0.123396, acc: 0.953125]  [G loss: 4.765451, acc: 0.085938]\n",
            "2871: [D loss: 0.099192, acc: 0.957031]  [G loss: 4.779458, acc: 0.140625]\n",
            "2872: [D loss: 0.115392, acc: 0.945312]  [G loss: 3.517562, acc: 0.281250]\n",
            "2873: [D loss: 0.120382, acc: 0.976562]  [G loss: 2.774654, acc: 0.406250]\n",
            "2874: [D loss: 0.181202, acc: 0.925781]  [G loss: 3.209263, acc: 0.296875]\n",
            "2875: [D loss: 0.157972, acc: 0.945312]  [G loss: 3.835538, acc: 0.257812]\n",
            "2876: [D loss: 0.099021, acc: 0.968750]  [G loss: 4.193393, acc: 0.171875]\n",
            "2877: [D loss: 0.184758, acc: 0.937500]  [G loss: 2.984452, acc: 0.335938]\n",
            "2878: [D loss: 0.116694, acc: 0.949219]  [G loss: 1.870384, acc: 0.476562]\n",
            "2879: [D loss: 0.187742, acc: 0.933594]  [G loss: 2.157636, acc: 0.398438]\n",
            "2880: [D loss: 0.104370, acc: 0.960938]  [G loss: 3.393073, acc: 0.304688]\n",
            "2881: [D loss: 0.105211, acc: 0.953125]  [G loss: 4.438985, acc: 0.179688]\n",
            "2882: [D loss: 0.055175, acc: 0.984375]  [G loss: 4.886967, acc: 0.109375]\n",
            "2883: [D loss: 0.142544, acc: 0.945312]  [G loss: 4.270442, acc: 0.164062]\n",
            "2884: [D loss: 0.113848, acc: 0.957031]  [G loss: 2.580790, acc: 0.335938]\n",
            "2885: [D loss: 0.050915, acc: 0.980469]  [G loss: 2.197348, acc: 0.468750]\n",
            "2886: [D loss: 0.140975, acc: 0.929688]  [G loss: 2.235022, acc: 0.406250]\n",
            "2887: [D loss: 0.087349, acc: 0.972656]  [G loss: 2.735589, acc: 0.359375]\n",
            "2888: [D loss: 0.093569, acc: 0.964844]  [G loss: 3.372125, acc: 0.343750]\n",
            "2889: [D loss: 0.050341, acc: 0.976562]  [G loss: 3.094686, acc: 0.390625]\n",
            "2890: [D loss: 0.093330, acc: 0.968750]  [G loss: 3.137135, acc: 0.382812]\n",
            "2891: [D loss: 0.116624, acc: 0.968750]  [G loss: 2.536307, acc: 0.468750]\n",
            "2892: [D loss: 0.131895, acc: 0.937500]  [G loss: 2.345126, acc: 0.476562]\n",
            "2893: [D loss: 0.121653, acc: 0.941406]  [G loss: 1.743287, acc: 0.546875]\n",
            "2894: [D loss: 0.076383, acc: 0.972656]  [G loss: 1.682759, acc: 0.531250]\n",
            "2895: [D loss: 0.045827, acc: 0.980469]  [G loss: 2.000244, acc: 0.398438]\n",
            "2896: [D loss: 0.069990, acc: 0.972656]  [G loss: 2.429554, acc: 0.382812]\n",
            "2897: [D loss: 0.053168, acc: 0.976562]  [G loss: 3.129308, acc: 0.273438]\n",
            "2898: [D loss: 0.052752, acc: 0.976562]  [G loss: 3.353566, acc: 0.289062]\n",
            "2899: [D loss: 0.164812, acc: 0.953125]  [G loss: 2.707500, acc: 0.343750]\n",
            "2900: [D loss: 0.076370, acc: 0.960938]  [G loss: 1.558010, acc: 0.546875]\n",
            "2901: [D loss: 0.078446, acc: 0.972656]  [G loss: 1.190330, acc: 0.609375]\n",
            "2902: [D loss: 0.109508, acc: 0.957031]  [G loss: 1.524739, acc: 0.570312]\n",
            "2903: [D loss: 0.106537, acc: 0.968750]  [G loss: 2.275592, acc: 0.476562]\n",
            "2904: [D loss: 0.103334, acc: 0.964844]  [G loss: 3.429251, acc: 0.343750]\n",
            "2905: [D loss: 0.104135, acc: 0.968750]  [G loss: 3.436359, acc: 0.359375]\n",
            "2906: [D loss: 0.110050, acc: 0.972656]  [G loss: 3.135777, acc: 0.445312]\n",
            "2907: [D loss: 0.046043, acc: 0.980469]  [G loss: 3.029732, acc: 0.445312]\n",
            "2908: [D loss: 0.030853, acc: 0.992188]  [G loss: 2.365172, acc: 0.515625]\n",
            "2909: [D loss: 0.053971, acc: 0.976562]  [G loss: 2.509208, acc: 0.437500]\n",
            "2910: [D loss: 0.079848, acc: 0.964844]  [G loss: 3.098025, acc: 0.406250]\n",
            "2911: [D loss: 0.055905, acc: 0.988281]  [G loss: 4.024076, acc: 0.265625]\n",
            "2912: [D loss: 0.032610, acc: 0.984375]  [G loss: 4.402813, acc: 0.187500]\n",
            "2913: [D loss: 0.189760, acc: 0.945312]  [G loss: 2.923136, acc: 0.382812]\n",
            "2914: [D loss: 0.063593, acc: 0.972656]  [G loss: 2.023034, acc: 0.554688]\n",
            "2915: [D loss: 0.204768, acc: 0.906250]  [G loss: 3.108010, acc: 0.359375]\n",
            "2916: [D loss: 0.100170, acc: 0.972656]  [G loss: 4.772089, acc: 0.203125]\n",
            "2917: [D loss: 0.168684, acc: 0.953125]  [G loss: 4.065247, acc: 0.195312]\n",
            "2918: [D loss: 0.148904, acc: 0.941406]  [G loss: 2.056438, acc: 0.515625]\n",
            "2919: [D loss: 0.162574, acc: 0.937500]  [G loss: 0.934176, acc: 0.695312]\n",
            "2920: [D loss: 0.492473, acc: 0.832031]  [G loss: 3.635887, acc: 0.140625]\n",
            "2921: [D loss: 0.236648, acc: 0.914062]  [G loss: 5.266509, acc: 0.039062]\n",
            "2922: [D loss: 0.284844, acc: 0.917969]  [G loss: 4.198910, acc: 0.140625]\n",
            "2923: [D loss: 0.081791, acc: 0.972656]  [G loss: 2.531507, acc: 0.328125]\n",
            "2924: [D loss: 0.147949, acc: 0.960938]  [G loss: 1.926199, acc: 0.421875]\n",
            "2925: [D loss: 0.136943, acc: 0.933594]  [G loss: 2.392870, acc: 0.375000]\n",
            "2926: [D loss: 0.124025, acc: 0.953125]  [G loss: 2.482201, acc: 0.320312]\n",
            "2927: [D loss: 0.182338, acc: 0.949219]  [G loss: 1.958024, acc: 0.382812]\n",
            "2928: [D loss: 0.049407, acc: 0.976562]  [G loss: 1.726871, acc: 0.539062]\n",
            "2929: [D loss: 0.108723, acc: 0.972656]  [G loss: 0.966591, acc: 0.718750]\n",
            "2930: [D loss: 0.110414, acc: 0.960938]  [G loss: 0.925663, acc: 0.679688]\n",
            "2931: [D loss: 0.106572, acc: 0.957031]  [G loss: 1.099070, acc: 0.648438]\n",
            "2932: [D loss: 0.074689, acc: 0.972656]  [G loss: 1.080403, acc: 0.703125]\n",
            "2933: [D loss: 0.023592, acc: 1.000000]  [G loss: 0.935368, acc: 0.710938]\n",
            "2934: [D loss: 0.089367, acc: 0.964844]  [G loss: 0.902831, acc: 0.687500]\n",
            "2935: [D loss: 0.051222, acc: 0.984375]  [G loss: 0.741241, acc: 0.687500]\n",
            "2936: [D loss: 0.053538, acc: 0.980469]  [G loss: 0.684355, acc: 0.757812]\n",
            "2937: [D loss: 0.064971, acc: 0.972656]  [G loss: 0.686173, acc: 0.726562]\n",
            "2938: [D loss: 0.031686, acc: 0.988281]  [G loss: 0.687506, acc: 0.734375]\n",
            "2939: [D loss: 0.062602, acc: 0.980469]  [G loss: 0.741206, acc: 0.734375]\n",
            "2940: [D loss: 0.091838, acc: 0.972656]  [G loss: 0.541881, acc: 0.773438]\n",
            "2941: [D loss: 0.090550, acc: 0.968750]  [G loss: 0.489101, acc: 0.781250]\n",
            "2942: [D loss: 0.088721, acc: 0.957031]  [G loss: 0.507986, acc: 0.820312]\n",
            "2943: [D loss: 0.127005, acc: 0.949219]  [G loss: 0.636380, acc: 0.765625]\n",
            "2944: [D loss: 0.096794, acc: 0.964844]  [G loss: 1.045570, acc: 0.640625]\n",
            "2945: [D loss: 0.149746, acc: 0.953125]  [G loss: 1.399196, acc: 0.546875]\n",
            "2946: [D loss: 0.116365, acc: 0.957031]  [G loss: 1.106017, acc: 0.648438]\n",
            "2947: [D loss: 0.082727, acc: 0.976562]  [G loss: 0.911561, acc: 0.710938]\n",
            "2948: [D loss: 0.198105, acc: 0.929688]  [G loss: 0.756417, acc: 0.703125]\n",
            "2949: [D loss: 0.220622, acc: 0.914062]  [G loss: 0.749257, acc: 0.726562]\n",
            "2950: [D loss: 0.159972, acc: 0.949219]  [G loss: 1.327935, acc: 0.468750]\n",
            "2951: [D loss: 0.189149, acc: 0.941406]  [G loss: 1.300466, acc: 0.429688]\n",
            "2952: [D loss: 0.191522, acc: 0.933594]  [G loss: 1.488643, acc: 0.367188]\n",
            "2953: [D loss: 0.187460, acc: 0.925781]  [G loss: 1.805045, acc: 0.187500]\n",
            "2954: [D loss: 0.241183, acc: 0.933594]  [G loss: 1.855885, acc: 0.171875]\n",
            "2955: [D loss: 0.185544, acc: 0.957031]  [G loss: 2.195519, acc: 0.031250]\n",
            "2956: [D loss: 0.122521, acc: 0.976562]  [G loss: 2.639130, acc: 0.007812]\n",
            "2957: [D loss: 0.085196, acc: 0.984375]  [G loss: 3.328868, acc: 0.000000]\n",
            "2958: [D loss: 0.064796, acc: 0.984375]  [G loss: 3.760982, acc: 0.000000]\n",
            "2959: [D loss: 0.057059, acc: 0.992188]  [G loss: 4.313372, acc: 0.000000]\n",
            "2960: [D loss: 0.048495, acc: 0.992188]  [G loss: 4.838569, acc: 0.000000]\n",
            "2961: [D loss: 0.023459, acc: 0.988281]  [G loss: 5.237115, acc: 0.000000]\n",
            "2962: [D loss: 0.030974, acc: 0.992188]  [G loss: 5.600487, acc: 0.000000]\n",
            "2963: [D loss: 0.020231, acc: 0.988281]  [G loss: 5.943295, acc: 0.000000]\n",
            "2964: [D loss: 0.008992, acc: 0.996094]  [G loss: 6.376498, acc: 0.000000]\n",
            "2965: [D loss: 0.000550, acc: 1.000000]  [G loss: 6.509456, acc: 0.000000]\n",
            "2966: [D loss: 0.000526, acc: 1.000000]  [G loss: 6.716356, acc: 0.000000]\n",
            "2967: [D loss: 0.000364, acc: 1.000000]  [G loss: 6.853364, acc: 0.000000]\n",
            "2968: [D loss: 0.000329, acc: 1.000000]  [G loss: 6.472806, acc: 0.000000]\n",
            "2969: [D loss: 0.000825, acc: 1.000000]  [G loss: 6.067184, acc: 0.000000]\n",
            "2970: [D loss: 0.000608, acc: 1.000000]  [G loss: 5.568286, acc: 0.000000]\n",
            "2971: [D loss: 0.000601, acc: 1.000000]  [G loss: 4.395714, acc: 0.039062]\n",
            "2972: [D loss: 0.001260, acc: 1.000000]  [G loss: 3.293261, acc: 0.187500]\n",
            "2973: [D loss: 0.004287, acc: 1.000000]  [G loss: 2.321051, acc: 0.367188]\n",
            "2974: [D loss: 0.018587, acc: 1.000000]  [G loss: 1.619141, acc: 0.507812]\n",
            "2975: [D loss: 0.100204, acc: 0.957031]  [G loss: 1.326792, acc: 0.585938]\n",
            "2976: [D loss: 0.141535, acc: 0.949219]  [G loss: 1.755772, acc: 0.492188]\n",
            "2977: [D loss: 0.115164, acc: 0.964844]  [G loss: 3.106097, acc: 0.398438]\n",
            "2978: [D loss: 0.061836, acc: 0.980469]  [G loss: 4.701141, acc: 0.304688]\n",
            "2979: [D loss: 0.018711, acc: 0.996094]  [G loss: 6.548482, acc: 0.179688]\n",
            "2980: [D loss: 0.168353, acc: 0.945312]  [G loss: 5.638641, acc: 0.218750]\n",
            "2981: [D loss: 0.318937, acc: 0.910156]  [G loss: 3.164516, acc: 0.484375]\n",
            "2982: [D loss: 0.114556, acc: 0.964844]  [G loss: 1.624236, acc: 0.648438]\n",
            "2983: [D loss: 0.409351, acc: 0.878906]  [G loss: 2.743175, acc: 0.500000]\n",
            "2984: [D loss: 0.305793, acc: 0.894531]  [G loss: 4.142853, acc: 0.289062]\n",
            "2985: [D loss: 0.178937, acc: 0.953125]  [G loss: 5.204518, acc: 0.171875]\n",
            "2986: [D loss: 0.179639, acc: 0.945312]  [G loss: 5.224380, acc: 0.109375]\n",
            "2987: [D loss: 0.072266, acc: 0.968750]  [G loss: 4.514408, acc: 0.210938]\n",
            "2988: [D loss: 0.088371, acc: 0.980469]  [G loss: 3.516119, acc: 0.304688]\n",
            "2989: [D loss: 0.136867, acc: 0.953125]  [G loss: 2.699674, acc: 0.421875]\n",
            "2990: [D loss: 0.095756, acc: 0.960938]  [G loss: 2.774716, acc: 0.375000]\n",
            "2991: [D loss: 0.121126, acc: 0.957031]  [G loss: 3.347279, acc: 0.367188]\n",
            "2992: [D loss: 0.051945, acc: 0.980469]  [G loss: 3.802740, acc: 0.390625]\n",
            "2993: [D loss: 0.028406, acc: 0.992188]  [G loss: 4.023287, acc: 0.406250]\n",
            "2994: [D loss: 0.088993, acc: 0.976562]  [G loss: 4.183669, acc: 0.414062]\n",
            "2995: [D loss: 0.074332, acc: 0.972656]  [G loss: 4.281826, acc: 0.359375]\n",
            "2996: [D loss: 0.076994, acc: 0.980469]  [G loss: 4.050943, acc: 0.343750]\n",
            "2997: [D loss: 0.061377, acc: 0.984375]  [G loss: 4.529156, acc: 0.312500]\n",
            "2998: [D loss: 0.089615, acc: 0.972656]  [G loss: 4.385072, acc: 0.335938]\n",
            "2999: [D loss: 0.082450, acc: 0.972656]  [G loss: 4.414744, acc: 0.359375]\n",
            "3000: [D loss: 0.056457, acc: 0.968750]  [G loss: 3.982913, acc: 0.398438]\n",
            "3001: [D loss: 0.169602, acc: 0.945312]  [G loss: 3.842524, acc: 0.382812]\n",
            "3002: [D loss: 0.175328, acc: 0.929688]  [G loss: 4.711326, acc: 0.367188]\n",
            "3003: [D loss: 0.122615, acc: 0.960938]  [G loss: 4.636902, acc: 0.382812]\n",
            "3004: [D loss: 0.232344, acc: 0.945312]  [G loss: 5.152453, acc: 0.351562]\n",
            "3005: [D loss: 0.157026, acc: 0.945312]  [G loss: 4.834981, acc: 0.320312]\n",
            "3006: [D loss: 0.106936, acc: 0.976562]  [G loss: 4.088192, acc: 0.414062]\n",
            "3007: [D loss: 0.170604, acc: 0.949219]  [G loss: 3.186193, acc: 0.476562]\n",
            "3008: [D loss: 0.066539, acc: 0.972656]  [G loss: 3.140084, acc: 0.460938]\n",
            "3009: [D loss: 0.119862, acc: 0.972656]  [G loss: 3.113284, acc: 0.468750]\n",
            "3010: [D loss: 0.139487, acc: 0.945312]  [G loss: 3.802264, acc: 0.460938]\n",
            "3011: [D loss: 0.102530, acc: 0.957031]  [G loss: 4.651452, acc: 0.359375]\n",
            "3012: [D loss: 0.181958, acc: 0.949219]  [G loss: 4.479162, acc: 0.312500]\n",
            "3013: [D loss: 0.217301, acc: 0.945312]  [G loss: 3.954731, acc: 0.296875]\n",
            "3014: [D loss: 0.152299, acc: 0.933594]  [G loss: 3.292674, acc: 0.359375]\n",
            "3015: [D loss: 0.104424, acc: 0.964844]  [G loss: 2.756599, acc: 0.406250]\n",
            "3016: [D loss: 0.229223, acc: 0.925781]  [G loss: 3.695390, acc: 0.367188]\n",
            "3017: [D loss: 0.142104, acc: 0.964844]  [G loss: 3.918925, acc: 0.281250]\n",
            "3018: [D loss: 0.180083, acc: 0.945312]  [G loss: 4.090930, acc: 0.164062]\n",
            "3019: [D loss: 0.164464, acc: 0.957031]  [G loss: 3.887087, acc: 0.304688]\n",
            "3020: [D loss: 0.098118, acc: 0.957031]  [G loss: 4.436148, acc: 0.210938]\n",
            "3021: [D loss: 0.137415, acc: 0.953125]  [G loss: 3.772146, acc: 0.250000]\n",
            "3022: [D loss: 0.171447, acc: 0.937500]  [G loss: 4.641776, acc: 0.156250]\n",
            "3023: [D loss: 0.094371, acc: 0.968750]  [G loss: 4.595659, acc: 0.148438]\n",
            "3024: [D loss: 0.173684, acc: 0.960938]  [G loss: 4.320758, acc: 0.171875]\n",
            "3025: [D loss: 0.068222, acc: 0.972656]  [G loss: 4.397416, acc: 0.187500]\n",
            "3026: [D loss: 0.139233, acc: 0.957031]  [G loss: 3.385863, acc: 0.328125]\n",
            "3027: [D loss: 0.098529, acc: 0.964844]  [G loss: 3.559304, acc: 0.304688]\n",
            "3028: [D loss: 0.088059, acc: 0.976562]  [G loss: 3.006753, acc: 0.382812]\n",
            "3029: [D loss: 0.099649, acc: 0.972656]  [G loss: 3.191011, acc: 0.296875]\n",
            "3030: [D loss: 0.049179, acc: 0.984375]  [G loss: 3.387226, acc: 0.328125]\n",
            "3031: [D loss: 0.042519, acc: 0.980469]  [G loss: 2.946524, acc: 0.257812]\n",
            "3032: [D loss: 0.081492, acc: 0.976562]  [G loss: 3.390719, acc: 0.210938]\n",
            "3033: [D loss: 0.042844, acc: 0.984375]  [G loss: 3.084025, acc: 0.304688]\n",
            "3034: [D loss: 0.066689, acc: 0.984375]  [G loss: 2.934869, acc: 0.304688]\n",
            "3035: [D loss: 0.072205, acc: 0.960938]  [G loss: 2.977062, acc: 0.335938]\n",
            "3036: [D loss: 0.051138, acc: 0.988281]  [G loss: 2.900468, acc: 0.390625]\n",
            "3037: [D loss: 0.085431, acc: 0.964844]  [G loss: 2.311939, acc: 0.460938]\n",
            "3038: [D loss: 0.060526, acc: 0.984375]  [G loss: 2.951948, acc: 0.343750]\n",
            "3039: [D loss: 0.045559, acc: 0.984375]  [G loss: 3.741689, acc: 0.250000]\n",
            "3040: [D loss: 0.081894, acc: 0.972656]  [G loss: 3.445929, acc: 0.187500]\n",
            "3041: [D loss: 0.106895, acc: 0.968750]  [G loss: 2.960886, acc: 0.281250]\n",
            "3042: [D loss: 0.061584, acc: 0.988281]  [G loss: 2.014209, acc: 0.476562]\n",
            "3043: [D loss: 0.037949, acc: 0.996094]  [G loss: 1.495278, acc: 0.593750]\n",
            "3044: [D loss: 0.040291, acc: 0.984375]  [G loss: 1.572170, acc: 0.546875]\n",
            "3045: [D loss: 0.075213, acc: 0.972656]  [G loss: 1.828682, acc: 0.554688]\n",
            "3046: [D loss: 0.044733, acc: 0.984375]  [G loss: 2.496994, acc: 0.398438]\n",
            "3047: [D loss: 0.085253, acc: 0.984375]  [G loss: 3.005085, acc: 0.304688]\n",
            "3048: [D loss: 0.088494, acc: 0.980469]  [G loss: 3.144693, acc: 0.257812]\n",
            "3049: [D loss: 0.128417, acc: 0.968750]  [G loss: 2.608018, acc: 0.421875]\n",
            "3050: [D loss: 0.065452, acc: 0.972656]  [G loss: 1.735488, acc: 0.546875]\n",
            "3051: [D loss: 0.060315, acc: 0.980469]  [G loss: 1.702291, acc: 0.601562]\n",
            "3052: [D loss: 0.041947, acc: 0.988281]  [G loss: 1.706765, acc: 0.492188]\n",
            "3053: [D loss: 0.029069, acc: 0.984375]  [G loss: 2.265975, acc: 0.359375]\n",
            "3054: [D loss: 0.072015, acc: 0.968750]  [G loss: 3.455414, acc: 0.164062]\n",
            "3055: [D loss: 0.125609, acc: 0.964844]  [G loss: 3.180440, acc: 0.187500]\n",
            "3056: [D loss: 0.062470, acc: 0.980469]  [G loss: 2.593954, acc: 0.265625]\n",
            "3057: [D loss: 0.064372, acc: 0.976562]  [G loss: 1.975891, acc: 0.484375]\n",
            "3058: [D loss: 0.078394, acc: 0.980469]  [G loss: 1.506341, acc: 0.578125]\n",
            "3059: [D loss: 0.060054, acc: 0.976562]  [G loss: 1.082944, acc: 0.664062]\n",
            "3060: [D loss: 0.045764, acc: 0.984375]  [G loss: 1.526796, acc: 0.625000]\n",
            "3061: [D loss: 0.040833, acc: 0.988281]  [G loss: 1.735990, acc: 0.578125]\n",
            "3062: [D loss: 0.021063, acc: 1.000000]  [G loss: 2.325042, acc: 0.507812]\n",
            "3063: [D loss: 0.050110, acc: 0.984375]  [G loss: 1.628392, acc: 0.632812]\n",
            "3064: [D loss: 0.037568, acc: 0.984375]  [G loss: 1.807526, acc: 0.578125]\n",
            "3065: [D loss: 0.022923, acc: 0.992188]  [G loss: 1.442713, acc: 0.671875]\n",
            "3066: [D loss: 0.041473, acc: 0.992188]  [G loss: 1.456033, acc: 0.617188]\n",
            "3067: [D loss: 0.082064, acc: 0.972656]  [G loss: 1.362742, acc: 0.679688]\n",
            "3068: [D loss: 0.039037, acc: 0.992188]  [G loss: 0.986681, acc: 0.734375]\n",
            "3069: [D loss: 0.035828, acc: 0.988281]  [G loss: 1.527518, acc: 0.632812]\n",
            "3070: [D loss: 0.047650, acc: 0.988281]  [G loss: 1.749717, acc: 0.632812]\n",
            "3071: [D loss: 0.084799, acc: 0.964844]  [G loss: 1.848551, acc: 0.562500]\n",
            "3072: [D loss: 0.072454, acc: 0.976562]  [G loss: 1.966161, acc: 0.585938]\n",
            "3073: [D loss: 0.049284, acc: 0.976562]  [G loss: 1.775206, acc: 0.539062]\n",
            "3074: [D loss: 0.068147, acc: 0.976562]  [G loss: 1.292104, acc: 0.703125]\n",
            "3075: [D loss: 0.105263, acc: 0.964844]  [G loss: 1.189749, acc: 0.734375]\n",
            "3076: [D loss: 0.073726, acc: 0.960938]  [G loss: 1.220917, acc: 0.679688]\n",
            "3077: [D loss: 0.031658, acc: 0.988281]  [G loss: 2.195643, acc: 0.570312]\n",
            "3078: [D loss: 0.038719, acc: 0.984375]  [G loss: 2.588663, acc: 0.445312]\n",
            "3079: [D loss: 0.079567, acc: 0.976562]  [G loss: 2.298117, acc: 0.468750]\n",
            "3080: [D loss: 0.050392, acc: 0.984375]  [G loss: 2.029793, acc: 0.539062]\n",
            "3081: [D loss: 0.122694, acc: 0.968750]  [G loss: 1.473159, acc: 0.632812]\n",
            "3082: [D loss: 0.060719, acc: 0.980469]  [G loss: 1.192186, acc: 0.656250]\n",
            "3083: [D loss: 0.047844, acc: 0.980469]  [G loss: 1.231978, acc: 0.609375]\n",
            "3084: [D loss: 0.047655, acc: 0.984375]  [G loss: 1.904856, acc: 0.523438]\n",
            "3085: [D loss: 0.058478, acc: 0.988281]  [G loss: 2.366635, acc: 0.375000]\n",
            "3086: [D loss: 0.039846, acc: 0.984375]  [G loss: 2.233535, acc: 0.382812]\n",
            "3087: [D loss: 0.094703, acc: 0.972656]  [G loss: 1.973377, acc: 0.500000]\n",
            "3088: [D loss: 0.018379, acc: 1.000000]  [G loss: 1.431724, acc: 0.593750]\n",
            "3089: [D loss: 0.102282, acc: 0.964844]  [G loss: 1.645675, acc: 0.554688]\n",
            "3090: [D loss: 0.153316, acc: 0.937500]  [G loss: 1.734187, acc: 0.562500]\n",
            "3091: [D loss: 0.048264, acc: 0.984375]  [G loss: 2.096760, acc: 0.468750]\n",
            "3092: [D loss: 0.063489, acc: 0.980469]  [G loss: 2.402660, acc: 0.507812]\n",
            "3093: [D loss: 0.082081, acc: 0.964844]  [G loss: 2.190724, acc: 0.554688]\n",
            "3094: [D loss: 0.042432, acc: 0.980469]  [G loss: 2.168625, acc: 0.570312]\n",
            "3095: [D loss: 0.126050, acc: 0.964844]  [G loss: 1.630933, acc: 0.625000]\n",
            "3096: [D loss: 0.115821, acc: 0.968750]  [G loss: 1.719259, acc: 0.601562]\n",
            "3097: [D loss: 0.100794, acc: 0.976562]  [G loss: 1.768935, acc: 0.601562]\n",
            "3098: [D loss: 0.065717, acc: 0.988281]  [G loss: 1.753444, acc: 0.570312]\n",
            "3099: [D loss: 0.048511, acc: 0.980469]  [G loss: 1.825199, acc: 0.531250]\n",
            "3100: [D loss: 0.129219, acc: 0.957031]  [G loss: 1.987137, acc: 0.507812]\n",
            "3101: [D loss: 0.062278, acc: 0.972656]  [G loss: 2.428607, acc: 0.367188]\n",
            "3102: [D loss: 0.052161, acc: 0.980469]  [G loss: 2.540065, acc: 0.382812]\n",
            "3103: [D loss: 0.041886, acc: 0.980469]  [G loss: 2.499663, acc: 0.382812]\n",
            "3104: [D loss: 0.108025, acc: 0.960938]  [G loss: 2.428676, acc: 0.359375]\n",
            "3105: [D loss: 0.022909, acc: 0.992188]  [G loss: 2.214110, acc: 0.351562]\n",
            "3106: [D loss: 0.028560, acc: 0.988281]  [G loss: 2.058462, acc: 0.375000]\n",
            "3107: [D loss: 0.023747, acc: 0.992188]  [G loss: 1.712463, acc: 0.570312]\n",
            "3108: [D loss: 0.076869, acc: 0.968750]  [G loss: 1.536060, acc: 0.601562]\n",
            "3109: [D loss: 0.064658, acc: 0.972656]  [G loss: 1.849120, acc: 0.539062]\n",
            "3110: [D loss: 0.029343, acc: 0.992188]  [G loss: 1.976286, acc: 0.437500]\n",
            "3111: [D loss: 0.030405, acc: 0.984375]  [G loss: 2.748207, acc: 0.359375]\n",
            "3112: [D loss: 0.024966, acc: 0.992188]  [G loss: 2.995959, acc: 0.289062]\n",
            "3113: [D loss: 0.058618, acc: 0.976562]  [G loss: 3.330208, acc: 0.312500]\n",
            "3114: [D loss: 0.036157, acc: 0.992188]  [G loss: 3.194667, acc: 0.289062]\n",
            "3115: [D loss: 0.072013, acc: 0.968750]  [G loss: 2.654663, acc: 0.367188]\n",
            "3116: [D loss: 0.091978, acc: 0.957031]  [G loss: 2.674638, acc: 0.343750]\n",
            "3117: [D loss: 0.043955, acc: 0.984375]  [G loss: 2.405213, acc: 0.351562]\n",
            "3118: [D loss: 0.071868, acc: 0.972656]  [G loss: 2.201882, acc: 0.421875]\n",
            "3119: [D loss: 0.039780, acc: 0.992188]  [G loss: 2.994941, acc: 0.312500]\n",
            "3120: [D loss: 0.034435, acc: 0.988281]  [G loss: 3.020802, acc: 0.335938]\n",
            "3121: [D loss: 0.018070, acc: 0.996094]  [G loss: 3.239801, acc: 0.171875]\n",
            "3122: [D loss: 0.048839, acc: 0.988281]  [G loss: 3.610011, acc: 0.179688]\n",
            "3123: [D loss: 0.028618, acc: 0.992188]  [G loss: 3.427070, acc: 0.164062]\n",
            "3124: [D loss: 0.024083, acc: 0.996094]  [G loss: 3.614232, acc: 0.117188]\n",
            "3125: [D loss: 0.033894, acc: 0.992188]  [G loss: 3.240512, acc: 0.140625]\n",
            "3126: [D loss: 0.028359, acc: 1.000000]  [G loss: 3.235086, acc: 0.148438]\n",
            "3127: [D loss: 0.030182, acc: 0.996094]  [G loss: 3.044289, acc: 0.179688]\n",
            "3128: [D loss: 0.041645, acc: 0.988281]  [G loss: 2.794165, acc: 0.218750]\n",
            "3129: [D loss: 0.020753, acc: 0.996094]  [G loss: 2.740074, acc: 0.250000]\n",
            "3130: [D loss: 0.022262, acc: 0.996094]  [G loss: 2.847084, acc: 0.250000]\n",
            "3131: [D loss: 0.038136, acc: 0.992188]  [G loss: 2.930341, acc: 0.359375]\n",
            "3132: [D loss: 0.022361, acc: 0.992188]  [G loss: 2.886951, acc: 0.312500]\n",
            "3133: [D loss: 0.009552, acc: 1.000000]  [G loss: 3.264945, acc: 0.335938]\n",
            "3134: [D loss: 0.008391, acc: 1.000000]  [G loss: 3.151943, acc: 0.359375]\n",
            "3135: [D loss: 0.025137, acc: 0.996094]  [G loss: 3.110054, acc: 0.343750]\n",
            "3136: [D loss: 0.018668, acc: 0.996094]  [G loss: 3.466025, acc: 0.320312]\n",
            "3137: [D loss: 0.018438, acc: 0.992188]  [G loss: 3.466323, acc: 0.343750]\n",
            "3138: [D loss: 0.016152, acc: 0.996094]  [G loss: 3.427675, acc: 0.312500]\n",
            "3139: [D loss: 0.012772, acc: 0.996094]  [G loss: 3.545126, acc: 0.359375]\n",
            "3140: [D loss: 0.033963, acc: 0.988281]  [G loss: 3.723960, acc: 0.343750]\n",
            "3141: [D loss: 0.010997, acc: 1.000000]  [G loss: 3.715796, acc: 0.398438]\n",
            "3142: [D loss: 0.006133, acc: 1.000000]  [G loss: 3.765480, acc: 0.343750]\n",
            "3143: [D loss: 0.007557, acc: 1.000000]  [G loss: 3.323505, acc: 0.414062]\n",
            "3144: [D loss: 0.026729, acc: 0.992188]  [G loss: 3.541358, acc: 0.390625]\n",
            "3145: [D loss: 0.005550, acc: 1.000000]  [G loss: 3.078571, acc: 0.406250]\n",
            "3146: [D loss: 0.012300, acc: 0.992188]  [G loss: 3.153183, acc: 0.375000]\n",
            "3147: [D loss: 0.005274, acc: 1.000000]  [G loss: 2.384879, acc: 0.492188]\n",
            "3148: [D loss: 0.019952, acc: 0.992188]  [G loss: 2.440980, acc: 0.515625]\n",
            "3149: [D loss: 0.018060, acc: 0.988281]  [G loss: 1.984557, acc: 0.578125]\n",
            "3150: [D loss: 0.030234, acc: 0.988281]  [G loss: 1.897326, acc: 0.578125]\n",
            "3151: [D loss: 0.054175, acc: 0.980469]  [G loss: 2.279822, acc: 0.539062]\n",
            "3152: [D loss: 0.011271, acc: 1.000000]  [G loss: 2.548322, acc: 0.562500]\n",
            "3153: [D loss: 0.015347, acc: 0.996094]  [G loss: 2.833642, acc: 0.500000]\n",
            "3154: [D loss: 0.048013, acc: 0.988281]  [G loss: 3.175721, acc: 0.468750]\n",
            "3155: [D loss: 0.059265, acc: 0.988281]  [G loss: 3.142133, acc: 0.507812]\n",
            "3156: [D loss: 0.086175, acc: 0.972656]  [G loss: 2.503341, acc: 0.531250]\n",
            "3157: [D loss: 0.114343, acc: 0.957031]  [G loss: 2.584051, acc: 0.554688]\n",
            "3158: [D loss: 0.071751, acc: 0.972656]  [G loss: 2.104779, acc: 0.593750]\n",
            "3159: [D loss: 0.048382, acc: 0.984375]  [G loss: 1.705759, acc: 0.625000]\n",
            "3160: [D loss: 0.029423, acc: 0.988281]  [G loss: 1.684960, acc: 0.671875]\n",
            "3161: [D loss: 0.095799, acc: 0.976562]  [G loss: 2.069176, acc: 0.585938]\n",
            "3162: [D loss: 0.017965, acc: 1.000000]  [G loss: 2.896489, acc: 0.492188]\n",
            "3163: [D loss: 0.061816, acc: 0.984375]  [G loss: 3.856716, acc: 0.375000]\n",
            "3164: [D loss: 0.168735, acc: 0.957031]  [G loss: 2.387491, acc: 0.570312]\n",
            "3165: [D loss: 0.123813, acc: 0.941406]  [G loss: 1.530001, acc: 0.648438]\n",
            "3166: [D loss: 0.148747, acc: 0.945312]  [G loss: 1.550539, acc: 0.562500]\n",
            "3167: [D loss: 0.047458, acc: 0.988281]  [G loss: 3.411602, acc: 0.398438]\n",
            "3168: [D loss: 0.098152, acc: 0.968750]  [G loss: 3.394811, acc: 0.382812]\n",
            "3169: [D loss: 0.043085, acc: 0.976562]  [G loss: 2.542581, acc: 0.476562]\n",
            "3170: [D loss: 0.096358, acc: 0.949219]  [G loss: 1.134194, acc: 0.679688]\n",
            "3171: [D loss: 0.145703, acc: 0.953125]  [G loss: 1.746669, acc: 0.554688]\n",
            "3172: [D loss: 0.131157, acc: 0.945312]  [G loss: 2.987059, acc: 0.351562]\n",
            "3173: [D loss: 0.132320, acc: 0.964844]  [G loss: 3.400419, acc: 0.296875]\n",
            "3174: [D loss: 0.089770, acc: 0.968750]  [G loss: 2.713276, acc: 0.367188]\n",
            "3175: [D loss: 0.170007, acc: 0.968750]  [G loss: 2.660219, acc: 0.390625]\n",
            "3176: [D loss: 0.093460, acc: 0.964844]  [G loss: 2.704648, acc: 0.367188]\n",
            "3177: [D loss: 0.040900, acc: 0.980469]  [G loss: 2.752007, acc: 0.390625]\n",
            "3178: [D loss: 0.071730, acc: 0.984375]  [G loss: 2.424053, acc: 0.468750]\n",
            "3179: [D loss: 0.050101, acc: 0.980469]  [G loss: 1.999660, acc: 0.515625]\n",
            "3180: [D loss: 0.075061, acc: 0.968750]  [G loss: 3.192850, acc: 0.382812]\n",
            "3181: [D loss: 0.069023, acc: 0.976562]  [G loss: 2.738631, acc: 0.445312]\n",
            "3182: [D loss: 0.037917, acc: 0.992188]  [G loss: 3.288286, acc: 0.351562]\n",
            "3183: [D loss: 0.155868, acc: 0.964844]  [G loss: 2.429626, acc: 0.406250]\n",
            "3184: [D loss: 0.071089, acc: 0.988281]  [G loss: 1.877874, acc: 0.546875]\n",
            "3185: [D loss: 0.110402, acc: 0.960938]  [G loss: 1.844742, acc: 0.515625]\n",
            "3186: [D loss: 0.096788, acc: 0.968750]  [G loss: 2.809475, acc: 0.453125]\n",
            "3187: [D loss: 0.065713, acc: 0.968750]  [G loss: 3.823677, acc: 0.335938]\n",
            "3188: [D loss: 0.117941, acc: 0.972656]  [G loss: 4.403863, acc: 0.273438]\n",
            "3189: [D loss: 0.115100, acc: 0.972656]  [G loss: 4.284768, acc: 0.203125]\n",
            "3190: [D loss: 0.033973, acc: 0.980469]  [G loss: 3.252027, acc: 0.257812]\n",
            "3191: [D loss: 0.141126, acc: 0.960938]  [G loss: 1.862881, acc: 0.570312]\n",
            "3192: [D loss: 0.199630, acc: 0.937500]  [G loss: 1.124674, acc: 0.640625]\n",
            "3193: [D loss: 0.192868, acc: 0.945312]  [G loss: 1.850327, acc: 0.515625]\n",
            "3194: [D loss: 0.066386, acc: 0.968750]  [G loss: 3.275925, acc: 0.296875]\n",
            "3195: [D loss: 0.118665, acc: 0.976562]  [G loss: 3.261868, acc: 0.304688]\n",
            "3196: [D loss: 0.081977, acc: 0.972656]  [G loss: 3.281193, acc: 0.359375]\n",
            "3197: [D loss: 0.112810, acc: 0.968750]  [G loss: 2.036141, acc: 0.546875]\n",
            "3198: [D loss: 0.116109, acc: 0.957031]  [G loss: 1.651945, acc: 0.609375]\n",
            "3199: [D loss: 0.067312, acc: 0.972656]  [G loss: 2.071441, acc: 0.476562]\n",
            "3200: [D loss: 0.054278, acc: 0.980469]  [G loss: 2.671821, acc: 0.289062]\n",
            "3201: [D loss: 0.072713, acc: 0.972656]  [G loss: 3.717307, acc: 0.179688]\n",
            "3202: [D loss: 0.121803, acc: 0.957031]  [G loss: 3.535347, acc: 0.234375]\n",
            "3203: [D loss: 0.055294, acc: 0.972656]  [G loss: 2.512278, acc: 0.359375]\n",
            "3204: [D loss: 0.048274, acc: 0.976562]  [G loss: 1.945641, acc: 0.468750]\n",
            "3205: [D loss: 0.158903, acc: 0.953125]  [G loss: 3.519555, acc: 0.210938]\n",
            "3206: [D loss: 0.021225, acc: 0.992188]  [G loss: 4.002895, acc: 0.164062]\n",
            "3207: [D loss: 0.141082, acc: 0.960938]  [G loss: 3.263116, acc: 0.289062]\n",
            "3208: [D loss: 0.097042, acc: 0.988281]  [G loss: 1.445733, acc: 0.531250]\n",
            "3209: [D loss: 0.031752, acc: 0.988281]  [G loss: 1.499823, acc: 0.531250]\n",
            "3210: [D loss: 0.145482, acc: 0.964844]  [G loss: 1.984065, acc: 0.429688]\n",
            "3211: [D loss: 0.195033, acc: 0.968750]  [G loss: 2.032703, acc: 0.445312]\n",
            "3212: [D loss: 0.023222, acc: 0.996094]  [G loss: 2.293597, acc: 0.429688]\n",
            "3213: [D loss: 0.034723, acc: 0.980469]  [G loss: 1.926277, acc: 0.359375]\n",
            "3214: [D loss: 0.041929, acc: 0.992188]  [G loss: 1.629958, acc: 0.445312]\n",
            "3215: [D loss: 0.066982, acc: 0.980469]  [G loss: 1.479127, acc: 0.476562]\n",
            "3216: [D loss: 0.045413, acc: 0.980469]  [G loss: 1.163671, acc: 0.554688]\n",
            "3217: [D loss: 0.040528, acc: 0.992188]  [G loss: 0.982895, acc: 0.617188]\n",
            "3218: [D loss: 0.025956, acc: 0.992188]  [G loss: 1.085482, acc: 0.632812]\n",
            "3219: [D loss: 0.020729, acc: 0.996094]  [G loss: 1.451008, acc: 0.531250]\n",
            "3220: [D loss: 0.037850, acc: 0.996094]  [G loss: 1.932524, acc: 0.445312]\n",
            "3221: [D loss: 0.070594, acc: 0.976562]  [G loss: 1.576153, acc: 0.539062]\n",
            "3222: [D loss: 0.019788, acc: 0.992188]  [G loss: 1.622113, acc: 0.546875]\n",
            "3223: [D loss: 0.078473, acc: 0.968750]  [G loss: 0.818403, acc: 0.695312]\n",
            "3224: [D loss: 0.038480, acc: 0.988281]  [G loss: 1.003496, acc: 0.656250]\n",
            "3225: [D loss: 0.021647, acc: 0.996094]  [G loss: 1.863478, acc: 0.515625]\n",
            "3226: [D loss: 0.071634, acc: 0.976562]  [G loss: 1.974742, acc: 0.507812]\n",
            "3227: [D loss: 0.071695, acc: 0.984375]  [G loss: 1.646257, acc: 0.515625]\n",
            "3228: [D loss: 0.017206, acc: 0.996094]  [G loss: 1.249249, acc: 0.625000]\n",
            "3229: [D loss: 0.126543, acc: 0.964844]  [G loss: 0.542818, acc: 0.843750]\n",
            "3230: [D loss: 0.106884, acc: 0.976562]  [G loss: 0.300343, acc: 0.867188]\n",
            "3231: [D loss: 0.087151, acc: 0.964844]  [G loss: 1.356372, acc: 0.578125]\n",
            "3232: [D loss: 0.089956, acc: 0.976562]  [G loss: 1.655362, acc: 0.492188]\n",
            "3233: [D loss: 0.053458, acc: 0.984375]  [G loss: 2.270228, acc: 0.398438]\n",
            "3234: [D loss: 0.051940, acc: 0.988281]  [G loss: 1.857507, acc: 0.453125]\n",
            "3235: [D loss: 0.012574, acc: 0.996094]  [G loss: 1.580065, acc: 0.500000]\n",
            "3236: [D loss: 0.066575, acc: 0.980469]  [G loss: 0.742468, acc: 0.695312]\n",
            "3237: [D loss: 0.079939, acc: 0.964844]  [G loss: 0.305263, acc: 0.867188]\n",
            "3238: [D loss: 0.119433, acc: 0.960938]  [G loss: 0.477033, acc: 0.812500]\n",
            "3239: [D loss: 0.041260, acc: 0.980469]  [G loss: 1.320824, acc: 0.531250]\n",
            "3240: [D loss: 0.029579, acc: 0.992188]  [G loss: 1.883578, acc: 0.359375]\n",
            "3241: [D loss: 0.057245, acc: 0.976562]  [G loss: 2.248510, acc: 0.351562]\n",
            "3242: [D loss: 0.133165, acc: 0.968750]  [G loss: 1.151838, acc: 0.601562]\n",
            "3243: [D loss: 0.021607, acc: 0.992188]  [G loss: 0.671787, acc: 0.734375]\n",
            "3244: [D loss: 0.025532, acc: 0.988281]  [G loss: 0.483377, acc: 0.789062]\n",
            "3245: [D loss: 0.023672, acc: 0.996094]  [G loss: 0.335205, acc: 0.851562]\n",
            "3246: [D loss: 0.077921, acc: 0.972656]  [G loss: 0.422963, acc: 0.851562]\n",
            "3247: [D loss: 0.030388, acc: 0.984375]  [G loss: 0.584129, acc: 0.796875]\n",
            "3248: [D loss: 0.017363, acc: 0.996094]  [G loss: 1.140829, acc: 0.578125]\n",
            "3249: [D loss: 0.042774, acc: 0.988281]  [G loss: 1.062514, acc: 0.609375]\n",
            "3250: [D loss: 0.035314, acc: 0.984375]  [G loss: 0.920700, acc: 0.640625]\n",
            "3251: [D loss: 0.018910, acc: 0.992188]  [G loss: 1.017897, acc: 0.609375]\n",
            "3252: [D loss: 0.046977, acc: 0.992188]  [G loss: 0.742631, acc: 0.695312]\n",
            "3253: [D loss: 0.038099, acc: 0.988281]  [G loss: 0.350456, acc: 0.828125]\n",
            "3254: [D loss: 0.040679, acc: 0.988281]  [G loss: 0.309011, acc: 0.859375]\n",
            "3255: [D loss: 0.056938, acc: 0.984375]  [G loss: 0.215762, acc: 0.914062]\n",
            "3256: [D loss: 0.021027, acc: 0.992188]  [G loss: 0.425233, acc: 0.820312]\n",
            "3257: [D loss: 0.045421, acc: 0.980469]  [G loss: 0.437375, acc: 0.835938]\n",
            "3258: [D loss: 0.011582, acc: 1.000000]  [G loss: 0.603901, acc: 0.804688]\n",
            "3259: [D loss: 0.014833, acc: 1.000000]  [G loss: 0.592998, acc: 0.781250]\n",
            "3260: [D loss: 0.011625, acc: 0.996094]  [G loss: 0.893535, acc: 0.734375]\n",
            "3261: [D loss: 0.011884, acc: 0.996094]  [G loss: 0.903585, acc: 0.671875]\n",
            "3262: [D loss: 0.023831, acc: 0.988281]  [G loss: 1.049451, acc: 0.664062]\n",
            "3263: [D loss: 0.009160, acc: 1.000000]  [G loss: 0.971553, acc: 0.671875]\n",
            "3264: [D loss: 0.034791, acc: 0.992188]  [G loss: 0.814250, acc: 0.718750]\n",
            "3265: [D loss: 0.062719, acc: 0.984375]  [G loss: 0.585380, acc: 0.773438]\n",
            "3266: [D loss: 0.063273, acc: 0.972656]  [G loss: 0.481250, acc: 0.789062]\n",
            "3267: [D loss: 0.060705, acc: 0.972656]  [G loss: 0.762467, acc: 0.726562]\n",
            "3268: [D loss: 0.050545, acc: 0.980469]  [G loss: 0.865609, acc: 0.664062]\n",
            "3269: [D loss: 0.096460, acc: 0.972656]  [G loss: 0.701692, acc: 0.742188]\n",
            "3270: [D loss: 0.030050, acc: 0.996094]  [G loss: 1.144957, acc: 0.632812]\n",
            "3271: [D loss: 0.052020, acc: 0.984375]  [G loss: 1.198530, acc: 0.601562]\n",
            "3272: [D loss: 0.070479, acc: 0.972656]  [G loss: 0.910678, acc: 0.687500]\n",
            "3273: [D loss: 0.070636, acc: 0.976562]  [G loss: 0.825784, acc: 0.687500]\n",
            "3274: [D loss: 0.107309, acc: 0.964844]  [G loss: 0.839669, acc: 0.695312]\n",
            "3275: [D loss: 0.086218, acc: 0.972656]  [G loss: 1.331931, acc: 0.539062]\n",
            "3276: [D loss: 0.052512, acc: 0.992188]  [G loss: 1.939489, acc: 0.390625]\n",
            "3277: [D loss: 0.087677, acc: 0.980469]  [G loss: 1.914145, acc: 0.375000]\n",
            "3278: [D loss: 0.066632, acc: 0.980469]  [G loss: 2.114190, acc: 0.242188]\n",
            "3279: [D loss: 0.054381, acc: 0.980469]  [G loss: 2.222676, acc: 0.210938]\n",
            "3280: [D loss: 0.104654, acc: 0.972656]  [G loss: 2.125691, acc: 0.281250]\n",
            "3281: [D loss: 0.065674, acc: 0.976562]  [G loss: 2.166900, acc: 0.257812]\n",
            "3282: [D loss: 0.060470, acc: 0.988281]  [G loss: 2.305703, acc: 0.273438]\n",
            "3283: [D loss: 0.065948, acc: 0.972656]  [G loss: 3.079871, acc: 0.109375]\n",
            "3284: [D loss: 0.047912, acc: 0.984375]  [G loss: 4.184049, acc: 0.078125]\n",
            "3285: [D loss: 0.057356, acc: 0.976562]  [G loss: 4.432930, acc: 0.031250]\n",
            "3286: [D loss: 0.044844, acc: 0.988281]  [G loss: 4.753414, acc: 0.039062]\n",
            "3287: [D loss: 0.039678, acc: 0.976562]  [G loss: 4.791279, acc: 0.023438]\n",
            "3288: [D loss: 0.029050, acc: 0.984375]  [G loss: 4.771923, acc: 0.039062]\n",
            "3289: [D loss: 0.022748, acc: 0.992188]  [G loss: 4.647002, acc: 0.093750]\n",
            "3290: [D loss: 0.042928, acc: 0.984375]  [G loss: 4.558058, acc: 0.046875]\n",
            "3291: [D loss: 0.015245, acc: 0.996094]  [G loss: 4.605218, acc: 0.078125]\n",
            "3292: [D loss: 0.051035, acc: 0.980469]  [G loss: 4.736322, acc: 0.085938]\n",
            "3293: [D loss: 0.016084, acc: 1.000000]  [G loss: 5.114206, acc: 0.054688]\n",
            "3294: [D loss: 0.023355, acc: 0.992188]  [G loss: 5.279469, acc: 0.093750]\n",
            "3295: [D loss: 0.031690, acc: 0.992188]  [G loss: 5.257075, acc: 0.054688]\n",
            "3296: [D loss: 0.015843, acc: 1.000000]  [G loss: 4.908361, acc: 0.093750]\n",
            "3297: [D loss: 0.028276, acc: 0.992188]  [G loss: 5.260613, acc: 0.085938]\n",
            "3298: [D loss: 0.053662, acc: 0.984375]  [G loss: 4.955285, acc: 0.109375]\n",
            "3299: [D loss: 0.025904, acc: 0.988281]  [G loss: 4.465270, acc: 0.171875]\n",
            "3300: [D loss: 0.062371, acc: 0.988281]  [G loss: 4.476108, acc: 0.179688]\n",
            "3301: [D loss: 0.095281, acc: 0.960938]  [G loss: 4.373865, acc: 0.125000]\n",
            "3302: [D loss: 0.024856, acc: 0.988281]  [G loss: 4.230190, acc: 0.210938]\n",
            "3303: [D loss: 0.023072, acc: 0.992188]  [G loss: 4.099533, acc: 0.218750]\n",
            "3304: [D loss: 0.061429, acc: 0.976562]  [G loss: 3.262493, acc: 0.296875]\n",
            "3305: [D loss: 0.024472, acc: 0.988281]  [G loss: 2.135034, acc: 0.492188]\n",
            "3306: [D loss: 0.030461, acc: 0.992188]  [G loss: 2.037104, acc: 0.570312]\n",
            "3307: [D loss: 0.058887, acc: 0.984375]  [G loss: 1.774655, acc: 0.640625]\n",
            "3308: [D loss: 0.103600, acc: 0.972656]  [G loss: 1.970350, acc: 0.679688]\n",
            "3309: [D loss: 0.031297, acc: 0.984375]  [G loss: 2.746063, acc: 0.578125]\n",
            "3310: [D loss: 0.022126, acc: 0.988281]  [G loss: 2.849165, acc: 0.578125]\n",
            "3311: [D loss: 0.066123, acc: 0.988281]  [G loss: 2.378470, acc: 0.640625]\n",
            "3312: [D loss: 0.061998, acc: 0.976562]  [G loss: 1.740590, acc: 0.671875]\n",
            "3313: [D loss: 0.046581, acc: 0.984375]  [G loss: 1.539544, acc: 0.765625]\n",
            "3314: [D loss: 0.077807, acc: 0.980469]  [G loss: 1.131442, acc: 0.796875]\n",
            "3315: [D loss: 0.062327, acc: 0.976562]  [G loss: 1.819809, acc: 0.687500]\n",
            "3316: [D loss: 0.074378, acc: 0.976562]  [G loss: 2.484074, acc: 0.562500]\n",
            "3317: [D loss: 0.071084, acc: 0.972656]  [G loss: 1.865162, acc: 0.625000]\n",
            "3318: [D loss: 0.196017, acc: 0.953125]  [G loss: 1.175173, acc: 0.757812]\n",
            "3319: [D loss: 0.210311, acc: 0.921875]  [G loss: 1.268697, acc: 0.742188]\n",
            "3320: [D loss: 0.121307, acc: 0.960938]  [G loss: 2.658405, acc: 0.515625]\n",
            "3321: [D loss: 0.179766, acc: 0.945312]  [G loss: 2.533705, acc: 0.492188]\n",
            "3322: [D loss: 0.137369, acc: 0.949219]  [G loss: 1.685240, acc: 0.656250]\n",
            "3323: [D loss: 0.151850, acc: 0.945312]  [G loss: 1.771480, acc: 0.625000]\n",
            "3324: [D loss: 0.239395, acc: 0.921875]  [G loss: 2.606760, acc: 0.382812]\n",
            "3325: [D loss: 0.243605, acc: 0.933594]  [G loss: 2.034642, acc: 0.429688]\n",
            "3326: [D loss: 0.295624, acc: 0.925781]  [G loss: 2.029827, acc: 0.500000]\n",
            "3327: [D loss: 0.215405, acc: 0.902344]  [G loss: 2.853276, acc: 0.375000]\n",
            "3328: [D loss: 0.245732, acc: 0.941406]  [G loss: 2.793783, acc: 0.429688]\n",
            "3329: [D loss: 0.131438, acc: 0.945312]  [G loss: 2.394620, acc: 0.523438]\n",
            "3330: [D loss: 0.195915, acc: 0.933594]  [G loss: 2.583929, acc: 0.507812]\n",
            "3331: [D loss: 0.201637, acc: 0.949219]  [G loss: 3.783299, acc: 0.304688]\n",
            "3332: [D loss: 0.163671, acc: 0.953125]  [G loss: 4.556322, acc: 0.203125]\n",
            "3333: [D loss: 0.211433, acc: 0.953125]  [G loss: 3.128882, acc: 0.367188]\n",
            "3334: [D loss: 0.108718, acc: 0.960938]  [G loss: 2.786702, acc: 0.453125]\n",
            "3335: [D loss: 0.113864, acc: 0.953125]  [G loss: 4.313234, acc: 0.242188]\n",
            "3336: [D loss: 0.066813, acc: 0.980469]  [G loss: 6.336110, acc: 0.109375]\n",
            "3337: [D loss: 0.059773, acc: 0.980469]  [G loss: 6.732413, acc: 0.031250]\n",
            "3338: [D loss: 0.131018, acc: 0.957031]  [G loss: 5.662489, acc: 0.125000]\n",
            "3339: [D loss: 0.116764, acc: 0.957031]  [G loss: 3.244043, acc: 0.273438]\n",
            "3340: [D loss: 0.130903, acc: 0.957031]  [G loss: 2.445004, acc: 0.460938]\n",
            "3341: [D loss: 0.116032, acc: 0.941406]  [G loss: 2.778996, acc: 0.453125]\n",
            "3342: [D loss: 0.223747, acc: 0.953125]  [G loss: 3.929676, acc: 0.343750]\n",
            "3343: [D loss: 0.144239, acc: 0.960938]  [G loss: 3.947961, acc: 0.304688]\n",
            "3344: [D loss: 0.080117, acc: 0.976562]  [G loss: 3.498775, acc: 0.304688]\n",
            "3345: [D loss: 0.064733, acc: 0.972656]  [G loss: 2.798871, acc: 0.343750]\n",
            "3346: [D loss: 0.046896, acc: 0.972656]  [G loss: 2.960139, acc: 0.437500]\n",
            "3347: [D loss: 0.143153, acc: 0.968750]  [G loss: 1.808131, acc: 0.500000]\n",
            "3348: [D loss: 0.141359, acc: 0.953125]  [G loss: 2.326906, acc: 0.484375]\n",
            "3349: [D loss: 0.126628, acc: 0.953125]  [G loss: 2.738555, acc: 0.492188]\n",
            "3350: [D loss: 0.114843, acc: 0.964844]  [G loss: 2.711845, acc: 0.421875]\n",
            "3351: [D loss: 0.152384, acc: 0.953125]  [G loss: 2.664747, acc: 0.414062]\n",
            "3352: [D loss: 0.055415, acc: 0.980469]  [G loss: 2.215877, acc: 0.500000]\n",
            "3353: [D loss: 0.165886, acc: 0.937500]  [G loss: 2.812778, acc: 0.468750]\n",
            "3354: [D loss: 0.067692, acc: 0.980469]  [G loss: 2.995429, acc: 0.437500]\n",
            "3355: [D loss: 0.086268, acc: 0.972656]  [G loss: 2.839882, acc: 0.429688]\n",
            "3356: [D loss: 0.086624, acc: 0.968750]  [G loss: 2.061867, acc: 0.492188]\n",
            "3357: [D loss: 0.110031, acc: 0.968750]  [G loss: 1.712968, acc: 0.507812]\n",
            "3358: [D loss: 0.112816, acc: 0.957031]  [G loss: 2.045814, acc: 0.460938]\n",
            "3359: [D loss: 0.111689, acc: 0.957031]  [G loss: 2.279850, acc: 0.429688]\n",
            "3360: [D loss: 0.051540, acc: 0.980469]  [G loss: 2.551219, acc: 0.351562]\n",
            "3361: [D loss: 0.080214, acc: 0.976562]  [G loss: 2.494584, acc: 0.414062]\n",
            "3362: [D loss: 0.039025, acc: 0.996094]  [G loss: 2.154213, acc: 0.429688]\n",
            "3363: [D loss: 0.049301, acc: 0.980469]  [G loss: 1.651626, acc: 0.562500]\n",
            "3364: [D loss: 0.083036, acc: 0.968750]  [G loss: 1.862134, acc: 0.484375]\n",
            "3365: [D loss: 0.042075, acc: 0.980469]  [G loss: 2.652203, acc: 0.359375]\n",
            "3366: [D loss: 0.082713, acc: 0.980469]  [G loss: 3.589100, acc: 0.281250]\n",
            "3367: [D loss: 0.135354, acc: 0.964844]  [G loss: 2.591138, acc: 0.398438]\n",
            "3368: [D loss: 0.123442, acc: 0.968750]  [G loss: 1.499502, acc: 0.515625]\n",
            "3369: [D loss: 0.107919, acc: 0.953125]  [G loss: 1.471581, acc: 0.515625]\n",
            "3370: [D loss: 0.122154, acc: 0.945312]  [G loss: 2.215321, acc: 0.375000]\n",
            "3371: [D loss: 0.114261, acc: 0.964844]  [G loss: 3.499338, acc: 0.156250]\n",
            "3372: [D loss: 0.110734, acc: 0.964844]  [G loss: 3.794685, acc: 0.093750]\n",
            "3373: [D loss: 0.127649, acc: 0.964844]  [G loss: 2.379029, acc: 0.312500]\n",
            "3374: [D loss: 0.141717, acc: 0.945312]  [G loss: 2.187966, acc: 0.335938]\n",
            "3375: [D loss: 0.133724, acc: 0.949219]  [G loss: 1.865752, acc: 0.437500]\n",
            "3376: [D loss: 0.172573, acc: 0.925781]  [G loss: 2.723082, acc: 0.312500]\n",
            "3377: [D loss: 0.104461, acc: 0.964844]  [G loss: 3.162930, acc: 0.234375]\n",
            "3378: [D loss: 0.139984, acc: 0.945312]  [G loss: 2.489583, acc: 0.398438]\n",
            "3379: [D loss: 0.127219, acc: 0.953125]  [G loss: 2.135228, acc: 0.445312]\n",
            "3380: [D loss: 0.085836, acc: 0.964844]  [G loss: 1.281872, acc: 0.554688]\n",
            "3381: [D loss: 0.068228, acc: 0.968750]  [G loss: 1.357792, acc: 0.578125]\n",
            "3382: [D loss: 0.072750, acc: 0.972656]  [G loss: 1.002088, acc: 0.695312]\n",
            "3383: [D loss: 0.061003, acc: 0.980469]  [G loss: 1.339278, acc: 0.578125]\n",
            "3384: [D loss: 0.051034, acc: 0.980469]  [G loss: 1.225370, acc: 0.609375]\n",
            "3385: [D loss: 0.023098, acc: 0.996094]  [G loss: 1.108721, acc: 0.601562]\n",
            "3386: [D loss: 0.034604, acc: 0.988281]  [G loss: 1.343737, acc: 0.585938]\n",
            "3387: [D loss: 0.052558, acc: 0.984375]  [G loss: 1.189432, acc: 0.625000]\n",
            "3388: [D loss: 0.050056, acc: 0.988281]  [G loss: 1.063610, acc: 0.664062]\n",
            "3389: [D loss: 0.016600, acc: 1.000000]  [G loss: 0.798066, acc: 0.726562]\n",
            "3390: [D loss: 0.020079, acc: 0.996094]  [G loss: 0.874742, acc: 0.679688]\n",
            "3391: [D loss: 0.045661, acc: 0.988281]  [G loss: 0.723906, acc: 0.765625]\n",
            "3392: [D loss: 0.046105, acc: 0.988281]  [G loss: 0.899969, acc: 0.718750]\n",
            "3393: [D loss: 0.018027, acc: 0.996094]  [G loss: 0.840233, acc: 0.703125]\n",
            "3394: [D loss: 0.034375, acc: 0.988281]  [G loss: 0.857699, acc: 0.718750]\n",
            "3395: [D loss: 0.097067, acc: 0.972656]  [G loss: 0.584272, acc: 0.796875]\n",
            "3396: [D loss: 0.038477, acc: 0.984375]  [G loss: 0.561626, acc: 0.789062]\n",
            "3397: [D loss: 0.014842, acc: 1.000000]  [G loss: 0.416626, acc: 0.875000]\n",
            "3398: [D loss: 0.032555, acc: 0.992188]  [G loss: 0.401960, acc: 0.851562]\n",
            "3399: [D loss: 0.037492, acc: 0.992188]  [G loss: 0.302834, acc: 0.875000]\n",
            "3400: [D loss: 0.058611, acc: 0.988281]  [G loss: 0.352567, acc: 0.882812]\n",
            "3401: [D loss: 0.037161, acc: 0.988281]  [G loss: 0.536038, acc: 0.820312]\n",
            "3402: [D loss: 0.048463, acc: 0.980469]  [G loss: 0.639749, acc: 0.781250]\n",
            "3403: [D loss: 0.018670, acc: 0.996094]  [G loss: 0.585071, acc: 0.796875]\n",
            "3404: [D loss: 0.044889, acc: 0.984375]  [G loss: 0.463808, acc: 0.804688]\n",
            "3405: [D loss: 0.018089, acc: 0.996094]  [G loss: 0.259755, acc: 0.898438]\n",
            "3406: [D loss: 0.033908, acc: 0.992188]  [G loss: 0.346854, acc: 0.843750]\n",
            "3407: [D loss: 0.033931, acc: 0.996094]  [G loss: 0.282815, acc: 0.875000]\n",
            "3408: [D loss: 0.072062, acc: 0.976562]  [G loss: 0.231974, acc: 0.898438]\n",
            "3409: [D loss: 0.057592, acc: 0.980469]  [G loss: 0.375881, acc: 0.828125]\n",
            "3410: [D loss: 0.046088, acc: 0.988281]  [G loss: 0.654330, acc: 0.750000]\n",
            "3411: [D loss: 0.014256, acc: 1.000000]  [G loss: 1.016319, acc: 0.632812]\n",
            "3412: [D loss: 0.074634, acc: 0.976562]  [G loss: 1.036325, acc: 0.625000]\n",
            "3413: [D loss: 0.105817, acc: 0.976562]  [G loss: 0.678952, acc: 0.695312]\n",
            "3414: [D loss: 0.037245, acc: 0.988281]  [G loss: 0.563935, acc: 0.750000]\n",
            "3415: [D loss: 0.067105, acc: 0.980469]  [G loss: 0.934954, acc: 0.609375]\n",
            "3416: [D loss: 0.081911, acc: 0.972656]  [G loss: 1.912712, acc: 0.210938]\n",
            "3417: [D loss: 0.091154, acc: 0.976562]  [G loss: 2.359386, acc: 0.148438]\n",
            "3418: [D loss: 0.029380, acc: 0.988281]  [G loss: 2.549949, acc: 0.101562]\n",
            "3419: [D loss: 0.027924, acc: 0.988281]  [G loss: 2.912834, acc: 0.070312]\n",
            "3420: [D loss: 0.067699, acc: 0.980469]  [G loss: 2.770890, acc: 0.054688]\n",
            "3421: [D loss: 0.024400, acc: 0.996094]  [G loss: 3.060905, acc: 0.054688]\n",
            "3422: [D loss: 0.035511, acc: 0.980469]  [G loss: 3.322481, acc: 0.031250]\n",
            "3423: [D loss: 0.026640, acc: 0.996094]  [G loss: 3.678143, acc: 0.039062]\n",
            "3424: [D loss: 0.019883, acc: 0.996094]  [G loss: 3.981490, acc: 0.007812]\n",
            "3425: [D loss: 0.019699, acc: 0.992188]  [G loss: 4.258938, acc: 0.015625]\n",
            "3426: [D loss: 0.012375, acc: 1.000000]  [G loss: 4.645317, acc: 0.000000]\n",
            "3427: [D loss: 0.019206, acc: 0.992188]  [G loss: 4.563684, acc: 0.000000]\n",
            "3428: [D loss: 0.006865, acc: 1.000000]  [G loss: 4.890049, acc: 0.000000]\n",
            "3429: [D loss: 0.020520, acc: 0.996094]  [G loss: 5.209105, acc: 0.000000]\n",
            "3430: [D loss: 0.015679, acc: 0.992188]  [G loss: 5.345468, acc: 0.000000]\n",
            "3431: [D loss: 0.015069, acc: 0.996094]  [G loss: 5.386400, acc: 0.000000]\n",
            "3432: [D loss: 0.013579, acc: 0.996094]  [G loss: 5.419300, acc: 0.000000]\n",
            "3433: [D loss: 0.008041, acc: 1.000000]  [G loss: 5.412567, acc: 0.000000]\n",
            "3434: [D loss: 0.007010, acc: 1.000000]  [G loss: 5.618843, acc: 0.000000]\n",
            "3435: [D loss: 0.009813, acc: 1.000000]  [G loss: 5.534984, acc: 0.007812]\n",
            "3436: [D loss: 0.011810, acc: 0.996094]  [G loss: 5.722760, acc: 0.000000]\n",
            "3437: [D loss: 0.014378, acc: 0.996094]  [G loss: 5.839663, acc: 0.000000]\n",
            "3438: [D loss: 0.013923, acc: 1.000000]  [G loss: 6.227040, acc: 0.000000]\n",
            "3439: [D loss: 0.007873, acc: 1.000000]  [G loss: 6.360695, acc: 0.007812]\n",
            "3440: [D loss: 0.008336, acc: 1.000000]  [G loss: 6.819076, acc: 0.007812]\n",
            "3441: [D loss: 0.010109, acc: 0.996094]  [G loss: 7.013369, acc: 0.000000]\n",
            "3442: [D loss: 0.003925, acc: 1.000000]  [G loss: 7.019986, acc: 0.000000]\n",
            "3443: [D loss: 0.032664, acc: 0.988281]  [G loss: 6.819631, acc: 0.000000]\n",
            "3444: [D loss: 0.006345, acc: 1.000000]  [G loss: 6.520302, acc: 0.015625]\n",
            "3445: [D loss: 0.055008, acc: 0.984375]  [G loss: 6.050514, acc: 0.023438]\n",
            "3446: [D loss: 0.031271, acc: 0.988281]  [G loss: 6.282013, acc: 0.000000]\n",
            "3447: [D loss: 0.074006, acc: 0.972656]  [G loss: 7.089283, acc: 0.007812]\n",
            "3448: [D loss: 0.037559, acc: 0.992188]  [G loss: 7.401402, acc: 0.000000]\n",
            "3449: [D loss: 0.100559, acc: 0.976562]  [G loss: 7.021141, acc: 0.015625]\n",
            "3450: [D loss: 0.053199, acc: 0.976562]  [G loss: 5.875239, acc: 0.015625]\n",
            "3451: [D loss: 0.052246, acc: 0.980469]  [G loss: 4.400857, acc: 0.109375]\n",
            "3452: [D loss: 0.076549, acc: 0.972656]  [G loss: 3.645837, acc: 0.187500]\n",
            "3453: [D loss: 0.151150, acc: 0.945312]  [G loss: 4.640090, acc: 0.101562]\n",
            "3454: [D loss: 0.086017, acc: 0.953125]  [G loss: 6.410357, acc: 0.046875]\n",
            "3455: [D loss: 0.060810, acc: 0.968750]  [G loss: 6.899816, acc: 0.039062]\n",
            "3456: [D loss: 0.209937, acc: 0.945312]  [G loss: 5.937041, acc: 0.078125]\n",
            "3457: [D loss: 0.095573, acc: 0.953125]  [G loss: 4.030561, acc: 0.218750]\n",
            "3458: [D loss: 0.180008, acc: 0.945312]  [G loss: 3.197327, acc: 0.265625]\n",
            "3459: [D loss: 0.158252, acc: 0.937500]  [G loss: 3.391639, acc: 0.218750]\n",
            "3460: [D loss: 0.064463, acc: 0.980469]  [G loss: 4.627333, acc: 0.156250]\n",
            "3461: [D loss: 0.047673, acc: 0.980469]  [G loss: 3.919140, acc: 0.226562]\n",
            "3462: [D loss: 0.074163, acc: 0.968750]  [G loss: 4.010779, acc: 0.171875]\n",
            "3463: [D loss: 0.101494, acc: 0.960938]  [G loss: 3.458325, acc: 0.281250]\n",
            "3464: [D loss: 0.050717, acc: 0.984375]  [G loss: 3.295571, acc: 0.335938]\n",
            "3465: [D loss: 0.064454, acc: 0.976562]  [G loss: 3.284496, acc: 0.273438]\n",
            "3466: [D loss: 0.114546, acc: 0.964844]  [G loss: 2.831320, acc: 0.351562]\n",
            "3467: [D loss: 0.099924, acc: 0.976562]  [G loss: 2.930185, acc: 0.375000]\n",
            "3468: [D loss: 0.056800, acc: 0.980469]  [G loss: 2.700157, acc: 0.398438]\n",
            "3469: [D loss: 0.147035, acc: 0.957031]  [G loss: 2.884679, acc: 0.390625]\n",
            "3470: [D loss: 0.051312, acc: 0.980469]  [G loss: 3.431412, acc: 0.421875]\n",
            "3471: [D loss: 0.168568, acc: 0.957031]  [G loss: 3.407172, acc: 0.289062]\n",
            "3472: [D loss: 0.095520, acc: 0.976562]  [G loss: 2.558698, acc: 0.437500]\n",
            "3473: [D loss: 0.168410, acc: 0.960938]  [G loss: 2.592768, acc: 0.453125]\n",
            "3474: [D loss: 0.095367, acc: 0.964844]  [G loss: 3.110498, acc: 0.359375]\n",
            "3475: [D loss: 0.123951, acc: 0.945312]  [G loss: 3.719712, acc: 0.312500]\n",
            "3476: [D loss: 0.110747, acc: 0.968750]  [G loss: 3.686531, acc: 0.265625]\n",
            "3477: [D loss: 0.116245, acc: 0.957031]  [G loss: 2.883605, acc: 0.359375]\n",
            "3478: [D loss: 0.220691, acc: 0.945312]  [G loss: 1.676248, acc: 0.554688]\n",
            "3479: [D loss: 0.125627, acc: 0.941406]  [G loss: 2.120790, acc: 0.507812]\n",
            "3480: [D loss: 0.074792, acc: 0.968750]  [G loss: 2.911885, acc: 0.437500]\n",
            "3481: [D loss: 0.072891, acc: 0.972656]  [G loss: 3.995701, acc: 0.289062]\n",
            "3482: [D loss: 0.126338, acc: 0.941406]  [G loss: 3.188161, acc: 0.367188]\n",
            "3483: [D loss: 0.099897, acc: 0.972656]  [G loss: 1.587913, acc: 0.601562]\n",
            "3484: [D loss: 0.134896, acc: 0.941406]  [G loss: 1.408695, acc: 0.632812]\n",
            "3485: [D loss: 0.215800, acc: 0.914062]  [G loss: 2.459637, acc: 0.437500]\n",
            "3486: [D loss: 0.072495, acc: 0.980469]  [G loss: 4.136581, acc: 0.226562]\n",
            "3487: [D loss: 0.151207, acc: 0.937500]  [G loss: 3.947127, acc: 0.265625]\n",
            "3488: [D loss: 0.142666, acc: 0.957031]  [G loss: 1.935590, acc: 0.484375]\n",
            "3489: [D loss: 0.031330, acc: 0.996094]  [G loss: 0.821587, acc: 0.703125]\n",
            "3490: [D loss: 0.133778, acc: 0.957031]  [G loss: 1.207770, acc: 0.609375]\n",
            "3491: [D loss: 0.118623, acc: 0.949219]  [G loss: 3.223660, acc: 0.343750]\n",
            "3492: [D loss: 0.134403, acc: 0.964844]  [G loss: 4.014396, acc: 0.242188]\n",
            "3493: [D loss: 0.082207, acc: 0.976562]  [G loss: 3.680929, acc: 0.289062]\n",
            "3494: [D loss: 0.150448, acc: 0.949219]  [G loss: 2.824839, acc: 0.375000]\n",
            "3495: [D loss: 0.028897, acc: 0.992188]  [G loss: 1.452505, acc: 0.539062]\n",
            "3496: [D loss: 0.076443, acc: 0.980469]  [G loss: 1.117436, acc: 0.679688]\n",
            "3497: [D loss: 0.111857, acc: 0.949219]  [G loss: 2.272301, acc: 0.546875]\n",
            "3498: [D loss: 0.096813, acc: 0.968750]  [G loss: 2.568084, acc: 0.500000]\n",
            "3499: [D loss: 0.070496, acc: 0.980469]  [G loss: 2.800064, acc: 0.468750]\n",
            "3500: [D loss: 0.067662, acc: 0.976562]  [G loss: 2.703617, acc: 0.460938]\n",
            "3501: [D loss: 0.067235, acc: 0.960938]  [G loss: 2.039155, acc: 0.500000]\n",
            "3502: [D loss: 0.051106, acc: 0.980469]  [G loss: 1.368709, acc: 0.593750]\n",
            "3503: [D loss: 0.071462, acc: 0.968750]  [G loss: 1.444764, acc: 0.640625]\n",
            "3504: [D loss: 0.146660, acc: 0.957031]  [G loss: 2.381126, acc: 0.523438]\n",
            "3505: [D loss: 0.072081, acc: 0.972656]  [G loss: 3.500130, acc: 0.390625]\n",
            "3506: [D loss: 0.138528, acc: 0.980469]  [G loss: 3.642612, acc: 0.406250]\n",
            "3507: [D loss: 0.085895, acc: 0.972656]  [G loss: 3.447949, acc: 0.453125]\n",
            "3508: [D loss: 0.101150, acc: 0.972656]  [G loss: 2.666220, acc: 0.523438]\n",
            "3509: [D loss: 0.072621, acc: 0.972656]  [G loss: 0.886635, acc: 0.773438]\n",
            "3510: [D loss: 0.081119, acc: 0.964844]  [G loss: 1.197484, acc: 0.757812]\n",
            "3511: [D loss: 0.148844, acc: 0.949219]  [G loss: 1.863155, acc: 0.648438]\n",
            "3512: [D loss: 0.051370, acc: 0.984375]  [G loss: 1.736097, acc: 0.632812]\n",
            "3513: [D loss: 0.122675, acc: 0.957031]  [G loss: 2.723638, acc: 0.492188]\n",
            "3514: [D loss: 0.168508, acc: 0.953125]  [G loss: 1.786064, acc: 0.625000]\n",
            "3515: [D loss: 0.058887, acc: 0.976562]  [G loss: 1.298818, acc: 0.718750]\n",
            "3516: [D loss: 0.038892, acc: 0.988281]  [G loss: 0.625159, acc: 0.812500]\n",
            "3517: [D loss: 0.063337, acc: 0.968750]  [G loss: 0.340015, acc: 0.898438]\n",
            "3518: [D loss: 0.111554, acc: 0.953125]  [G loss: 0.661713, acc: 0.875000]\n",
            "3519: [D loss: 0.081694, acc: 0.972656]  [G loss: 1.585239, acc: 0.703125]\n",
            "3520: [D loss: 0.005114, acc: 1.000000]  [G loss: 1.779877, acc: 0.664062]\n",
            "3521: [D loss: 0.027407, acc: 0.988281]  [G loss: 2.326583, acc: 0.695312]\n",
            "3522: [D loss: 0.096074, acc: 0.984375]  [G loss: 1.551747, acc: 0.726562]\n",
            "3523: [D loss: 0.018847, acc: 0.992188]  [G loss: 1.550578, acc: 0.734375]\n",
            "3524: [D loss: 0.039618, acc: 0.988281]  [G loss: 1.600459, acc: 0.742188]\n",
            "3525: [D loss: 0.071560, acc: 0.984375]  [G loss: 1.149141, acc: 0.781250]\n",
            "3526: [D loss: 0.067068, acc: 0.976562]  [G loss: 1.149396, acc: 0.789062]\n",
            "3527: [D loss: 0.040351, acc: 0.988281]  [G loss: 1.548752, acc: 0.710938]\n",
            "3528: [D loss: 0.059289, acc: 0.980469]  [G loss: 1.856624, acc: 0.718750]\n",
            "3529: [D loss: 0.073974, acc: 0.972656]  [G loss: 1.345790, acc: 0.789062]\n",
            "3530: [D loss: 0.040098, acc: 0.984375]  [G loss: 1.321705, acc: 0.710938]\n",
            "3531: [D loss: 0.044601, acc: 0.984375]  [G loss: 1.340868, acc: 0.695312]\n",
            "3532: [D loss: 0.064633, acc: 0.980469]  [G loss: 0.968522, acc: 0.734375]\n",
            "3533: [D loss: 0.095532, acc: 0.964844]  [G loss: 1.178007, acc: 0.718750]\n",
            "3534: [D loss: 0.053706, acc: 0.972656]  [G loss: 1.546959, acc: 0.679688]\n",
            "3535: [D loss: 0.059638, acc: 0.988281]  [G loss: 2.379789, acc: 0.484375]\n",
            "3536: [D loss: 0.030849, acc: 0.984375]  [G loss: 3.186463, acc: 0.421875]\n",
            "3537: [D loss: 0.112917, acc: 0.964844]  [G loss: 3.511812, acc: 0.484375]\n",
            "3538: [D loss: 0.062103, acc: 0.984375]  [G loss: 2.768316, acc: 0.554688]\n",
            "3539: [D loss: 0.070280, acc: 0.984375]  [G loss: 2.709273, acc: 0.554688]\n",
            "3540: [D loss: 0.028693, acc: 0.992188]  [G loss: 3.078689, acc: 0.554688]\n",
            "3541: [D loss: 0.020065, acc: 0.996094]  [G loss: 2.526047, acc: 0.593750]\n",
            "3542: [D loss: 0.050496, acc: 0.980469]  [G loss: 2.717930, acc: 0.492188]\n",
            "3543: [D loss: 0.065444, acc: 0.976562]  [G loss: 2.895891, acc: 0.476562]\n",
            "3544: [D loss: 0.047705, acc: 0.972656]  [G loss: 2.560792, acc: 0.437500]\n",
            "3545: [D loss: 0.016423, acc: 0.996094]  [G loss: 3.049371, acc: 0.429688]\n",
            "3546: [D loss: 0.011289, acc: 0.996094]  [G loss: 2.377434, acc: 0.492188]\n",
            "3547: [D loss: 0.040859, acc: 0.988281]  [G loss: 2.328177, acc: 0.539062]\n",
            "3548: [D loss: 0.055260, acc: 0.984375]  [G loss: 1.705149, acc: 0.640625]\n",
            "3549: [D loss: 0.039713, acc: 0.988281]  [G loss: 1.610835, acc: 0.578125]\n",
            "3550: [D loss: 0.025699, acc: 0.988281]  [G loss: 1.388822, acc: 0.664062]\n",
            "3551: [D loss: 0.036109, acc: 0.988281]  [G loss: 2.132456, acc: 0.601562]\n",
            "3552: [D loss: 0.022214, acc: 0.996094]  [G loss: 2.453488, acc: 0.484375]\n",
            "3553: [D loss: 0.036395, acc: 0.980469]  [G loss: 2.498810, acc: 0.554688]\n",
            "3554: [D loss: 0.030346, acc: 0.992188]  [G loss: 2.263373, acc: 0.562500]\n",
            "3555: [D loss: 0.027462, acc: 0.996094]  [G loss: 2.964604, acc: 0.484375]\n",
            "3556: [D loss: 0.037418, acc: 0.984375]  [G loss: 3.738076, acc: 0.359375]\n",
            "3557: [D loss: 0.016210, acc: 0.996094]  [G loss: 5.005055, acc: 0.304688]\n",
            "3558: [D loss: 0.043250, acc: 0.992188]  [G loss: 5.978418, acc: 0.164062]\n",
            "3559: [D loss: 0.022951, acc: 0.988281]  [G loss: 6.408226, acc: 0.242188]\n",
            "3560: [D loss: 0.062681, acc: 0.992188]  [G loss: 6.706686, acc: 0.179688]\n",
            "3561: [D loss: 0.045408, acc: 0.984375]  [G loss: 6.149882, acc: 0.257812]\n",
            "3562: [D loss: 0.052581, acc: 0.984375]  [G loss: 4.164701, acc: 0.453125]\n",
            "3563: [D loss: 0.141223, acc: 0.964844]  [G loss: 2.595307, acc: 0.546875]\n",
            "3564: [D loss: 0.063774, acc: 0.980469]  [G loss: 2.845971, acc: 0.468750]\n",
            "3565: [D loss: 0.024016, acc: 0.988281]  [G loss: 3.966499, acc: 0.375000]\n",
            "3566: [D loss: 0.016310, acc: 0.992188]  [G loss: 5.396662, acc: 0.257812]\n",
            "3567: [D loss: 0.013859, acc: 0.996094]  [G loss: 5.715752, acc: 0.265625]\n",
            "3568: [D loss: 0.048852, acc: 0.992188]  [G loss: 6.033612, acc: 0.250000]\n",
            "3569: [D loss: 0.064832, acc: 0.984375]  [G loss: 5.105453, acc: 0.234375]\n",
            "3570: [D loss: 0.097977, acc: 0.976562]  [G loss: 3.603617, acc: 0.406250]\n",
            "3571: [D loss: 0.085477, acc: 0.972656]  [G loss: 2.763289, acc: 0.507812]\n",
            "3572: [D loss: 0.117206, acc: 0.960938]  [G loss: 4.337175, acc: 0.343750]\n",
            "3573: [D loss: 0.024699, acc: 0.988281]  [G loss: 6.485422, acc: 0.148438]\n",
            "3574: [D loss: 0.154958, acc: 0.980469]  [G loss: 7.646535, acc: 0.039062]\n",
            "3575: [D loss: 0.031209, acc: 0.980469]  [G loss: 7.733261, acc: 0.062500]\n",
            "3576: [D loss: 0.071097, acc: 0.980469]  [G loss: 5.717693, acc: 0.125000]\n",
            "3577: [D loss: 0.081755, acc: 0.968750]  [G loss: 4.648056, acc: 0.296875]\n",
            "3578: [D loss: 0.021906, acc: 0.992188]  [G loss: 3.918446, acc: 0.382812]\n",
            "3579: [D loss: 0.086687, acc: 0.968750]  [G loss: 4.305977, acc: 0.359375]\n",
            "3580: [D loss: 0.028354, acc: 0.992188]  [G loss: 4.841093, acc: 0.273438]\n",
            "3581: [D loss: 0.026892, acc: 0.988281]  [G loss: 4.584304, acc: 0.382812]\n",
            "3582: [D loss: 0.020243, acc: 0.988281]  [G loss: 5.178144, acc: 0.375000]\n",
            "3583: [D loss: 0.048512, acc: 0.992188]  [G loss: 4.438554, acc: 0.437500]\n",
            "3584: [D loss: 0.060729, acc: 0.980469]  [G loss: 4.512126, acc: 0.437500]\n",
            "3585: [D loss: 0.111636, acc: 0.976562]  [G loss: 4.313406, acc: 0.484375]\n",
            "3586: [D loss: 0.068544, acc: 0.972656]  [G loss: 4.805556, acc: 0.460938]\n",
            "3587: [D loss: 0.029602, acc: 0.992188]  [G loss: 5.513256, acc: 0.359375]\n",
            "3588: [D loss: 0.076860, acc: 0.988281]  [G loss: 5.250647, acc: 0.375000]\n",
            "3589: [D loss: 0.039906, acc: 0.988281]  [G loss: 3.844531, acc: 0.406250]\n",
            "3590: [D loss: 0.081729, acc: 0.980469]  [G loss: 2.555587, acc: 0.601562]\n",
            "3591: [D loss: 0.112761, acc: 0.957031]  [G loss: 3.994375, acc: 0.437500]\n",
            "3592: [D loss: 0.046449, acc: 0.976562]  [G loss: 5.643846, acc: 0.226562]\n",
            "3593: [D loss: 0.036562, acc: 0.996094]  [G loss: 7.092361, acc: 0.062500]\n",
            "3594: [D loss: 0.110507, acc: 0.972656]  [G loss: 5.755464, acc: 0.210938]\n",
            "3595: [D loss: 0.105901, acc: 0.984375]  [G loss: 4.171521, acc: 0.507812]\n",
            "3596: [D loss: 0.083264, acc: 0.988281]  [G loss: 2.347906, acc: 0.726562]\n",
            "3597: [D loss: 0.009718, acc: 0.996094]  [G loss: 1.294016, acc: 0.828125]\n",
            "3598: [D loss: 0.081403, acc: 0.976562]  [G loss: 1.321615, acc: 0.812500]\n",
            "3599: [D loss: 0.055832, acc: 0.984375]  [G loss: 0.795544, acc: 0.867188]\n",
            "3600: [D loss: 0.070756, acc: 0.988281]  [G loss: 0.675228, acc: 0.851562]\n",
            "3601: [D loss: 0.038212, acc: 0.984375]  [G loss: 0.319660, acc: 0.882812]\n",
            "3602: [D loss: 0.021272, acc: 0.988281]  [G loss: 0.304090, acc: 0.851562]\n",
            "3603: [D loss: 0.014502, acc: 0.992188]  [G loss: 0.633669, acc: 0.820312]\n",
            "3604: [D loss: 0.121398, acc: 0.984375]  [G loss: 0.845254, acc: 0.734375]\n",
            "3605: [D loss: 0.177011, acc: 0.953125]  [G loss: 1.820062, acc: 0.484375]\n",
            "3606: [D loss: 0.151670, acc: 0.964844]  [G loss: 1.194855, acc: 0.601562]\n",
            "3607: [D loss: 0.214552, acc: 0.953125]  [G loss: 0.268504, acc: 0.890625]\n",
            "3608: [D loss: 0.104132, acc: 0.953125]  [G loss: 0.168139, acc: 0.937500]\n",
            "3609: [D loss: 0.158901, acc: 0.937500]  [G loss: 0.545312, acc: 0.820312]\n",
            "3610: [D loss: 0.247128, acc: 0.941406]  [G loss: 0.796568, acc: 0.781250]\n",
            "3611: [D loss: 0.254081, acc: 0.945312]  [G loss: 0.853344, acc: 0.789062]\n",
            "3612: [D loss: 0.188699, acc: 0.949219]  [G loss: 0.502084, acc: 0.867188]\n",
            "3613: [D loss: 0.259671, acc: 0.921875]  [G loss: 0.650819, acc: 0.789062]\n",
            "3614: [D loss: 0.137357, acc: 0.960938]  [G loss: 1.449159, acc: 0.429688]\n",
            "3615: [D loss: 0.115862, acc: 0.964844]  [G loss: 2.247462, acc: 0.312500]\n",
            "3616: [D loss: 0.171648, acc: 0.933594]  [G loss: 2.055016, acc: 0.320312]\n",
            "3617: [D loss: 0.092091, acc: 0.972656]  [G loss: 2.033093, acc: 0.296875]\n",
            "3618: [D loss: 0.056509, acc: 0.980469]  [G loss: 2.186334, acc: 0.226562]\n",
            "3619: [D loss: 0.044235, acc: 0.988281]  [G loss: 2.745283, acc: 0.179688]\n",
            "3620: [D loss: 0.031619, acc: 0.996094]  [G loss: 3.343170, acc: 0.062500]\n",
            "3621: [D loss: 0.026531, acc: 0.996094]  [G loss: 4.415296, acc: 0.070312]\n",
            "3622: [D loss: 0.013571, acc: 0.992188]  [G loss: 4.366976, acc: 0.062500]\n",
            "3623: [D loss: 0.006477, acc: 1.000000]  [G loss: 4.472915, acc: 0.078125]\n",
            "3624: [D loss: 0.017562, acc: 0.992188]  [G loss: 4.065255, acc: 0.109375]\n",
            "3625: [D loss: 0.014311, acc: 0.996094]  [G loss: 4.180448, acc: 0.085938]\n",
            "3626: [D loss: 0.041563, acc: 0.984375]  [G loss: 4.171115, acc: 0.101562]\n",
            "3627: [D loss: 0.067108, acc: 0.972656]  [G loss: 4.249685, acc: 0.054688]\n",
            "3628: [D loss: 0.034137, acc: 0.988281]  [G loss: 4.323041, acc: 0.054688]\n",
            "3629: [D loss: 0.034166, acc: 0.984375]  [G loss: 4.688669, acc: 0.039062]\n",
            "3630: [D loss: 0.061638, acc: 0.968750]  [G loss: 4.142995, acc: 0.093750]\n",
            "3631: [D loss: 0.053358, acc: 0.988281]  [G loss: 3.889504, acc: 0.101562]\n",
            "3632: [D loss: 0.052648, acc: 0.980469]  [G loss: 3.705431, acc: 0.203125]\n",
            "3633: [D loss: 0.064606, acc: 0.972656]  [G loss: 4.070203, acc: 0.117188]\n",
            "3634: [D loss: 0.063735, acc: 0.976562]  [G loss: 4.378814, acc: 0.125000]\n",
            "3635: [D loss: 0.033688, acc: 0.988281]  [G loss: 4.634149, acc: 0.101562]\n",
            "3636: [D loss: 0.125851, acc: 0.957031]  [G loss: 4.307715, acc: 0.164062]\n",
            "3637: [D loss: 0.071816, acc: 0.976562]  [G loss: 3.440373, acc: 0.148438]\n",
            "3638: [D loss: 0.095760, acc: 0.960938]  [G loss: 4.348716, acc: 0.148438]\n",
            "3639: [D loss: 0.068622, acc: 0.980469]  [G loss: 4.193705, acc: 0.117188]\n",
            "3640: [D loss: 0.067564, acc: 0.988281]  [G loss: 3.299915, acc: 0.226562]\n",
            "3641: [D loss: 0.043298, acc: 0.992188]  [G loss: 3.139166, acc: 0.265625]\n",
            "3642: [D loss: 0.083047, acc: 0.964844]  [G loss: 2.421765, acc: 0.359375]\n",
            "3643: [D loss: 0.077620, acc: 0.972656]  [G loss: 2.052650, acc: 0.445312]\n",
            "3644: [D loss: 0.117487, acc: 0.949219]  [G loss: 1.310794, acc: 0.531250]\n",
            "3645: [D loss: 0.196650, acc: 0.929688]  [G loss: 1.587773, acc: 0.507812]\n",
            "3646: [D loss: 0.083503, acc: 0.984375]  [G loss: 1.842104, acc: 0.359375]\n",
            "3647: [D loss: 0.116338, acc: 0.957031]  [G loss: 2.042246, acc: 0.453125]\n",
            "3648: [D loss: 0.112394, acc: 0.968750]  [G loss: 2.270885, acc: 0.421875]\n",
            "3649: [D loss: 0.192598, acc: 0.953125]  [G loss: 1.350818, acc: 0.585938]\n",
            "3650: [D loss: 0.178480, acc: 0.933594]  [G loss: 0.771667, acc: 0.757812]\n",
            "3651: [D loss: 0.148700, acc: 0.949219]  [G loss: 1.795579, acc: 0.468750]\n",
            "3652: [D loss: 0.104521, acc: 0.968750]  [G loss: 2.426649, acc: 0.273438]\n",
            "3653: [D loss: 0.153170, acc: 0.949219]  [G loss: 1.855388, acc: 0.414062]\n",
            "3654: [D loss: 0.114624, acc: 0.949219]  [G loss: 0.968108, acc: 0.687500]\n",
            "3655: [D loss: 0.160331, acc: 0.933594]  [G loss: 0.737339, acc: 0.781250]\n",
            "3656: [D loss: 0.124114, acc: 0.949219]  [G loss: 1.181941, acc: 0.632812]\n",
            "3657: [D loss: 0.105030, acc: 0.964844]  [G loss: 1.887376, acc: 0.515625]\n",
            "3658: [D loss: 0.051710, acc: 0.984375]  [G loss: 2.088534, acc: 0.429688]\n",
            "3659: [D loss: 0.055560, acc: 0.980469]  [G loss: 2.115389, acc: 0.460938]\n",
            "3660: [D loss: 0.105958, acc: 0.960938]  [G loss: 1.000394, acc: 0.617188]\n",
            "3661: [D loss: 0.042981, acc: 0.988281]  [G loss: 0.764883, acc: 0.671875]\n",
            "3662: [D loss: 0.083040, acc: 0.980469]  [G loss: 0.591219, acc: 0.757812]\n",
            "3663: [D loss: 0.090263, acc: 0.957031]  [G loss: 0.780205, acc: 0.718750]\n",
            "3664: [D loss: 0.037509, acc: 0.992188]  [G loss: 1.062330, acc: 0.578125]\n",
            "3665: [D loss: 0.060655, acc: 0.988281]  [G loss: 1.008554, acc: 0.609375]\n",
            "3666: [D loss: 0.050626, acc: 0.984375]  [G loss: 1.720211, acc: 0.476562]\n",
            "3667: [D loss: 0.060553, acc: 0.980469]  [G loss: 2.333809, acc: 0.343750]\n",
            "3668: [D loss: 0.114803, acc: 0.960938]  [G loss: 1.267271, acc: 0.562500]\n",
            "3669: [D loss: 0.089341, acc: 0.972656]  [G loss: 1.545700, acc: 0.570312]\n",
            "3670: [D loss: 0.082211, acc: 0.957031]  [G loss: 1.540319, acc: 0.515625]\n",
            "3671: [D loss: 0.055428, acc: 0.976562]  [G loss: 1.625476, acc: 0.484375]\n",
            "3672: [D loss: 0.091785, acc: 0.964844]  [G loss: 1.682121, acc: 0.429688]\n",
            "3673: [D loss: 0.085419, acc: 0.976562]  [G loss: 1.396773, acc: 0.546875]\n",
            "3674: [D loss: 0.087365, acc: 0.976562]  [G loss: 1.347045, acc: 0.546875]\n",
            "3675: [D loss: 0.097043, acc: 0.972656]  [G loss: 1.609079, acc: 0.484375]\n",
            "3676: [D loss: 0.141004, acc: 0.960938]  [G loss: 1.994401, acc: 0.390625]\n",
            "3677: [D loss: 0.152007, acc: 0.949219]  [G loss: 2.245614, acc: 0.390625]\n",
            "3678: [D loss: 0.074264, acc: 0.972656]  [G loss: 2.285904, acc: 0.382812]\n",
            "3679: [D loss: 0.186897, acc: 0.941406]  [G loss: 1.830256, acc: 0.382812]\n",
            "3680: [D loss: 0.095808, acc: 0.964844]  [G loss: 1.748761, acc: 0.484375]\n",
            "3681: [D loss: 0.122226, acc: 0.957031]  [G loss: 1.823411, acc: 0.445312]\n",
            "3682: [D loss: 0.048339, acc: 0.984375]  [G loss: 2.511586, acc: 0.296875]\n",
            "3683: [D loss: 0.062984, acc: 0.972656]  [G loss: 2.805125, acc: 0.210938]\n",
            "3684: [D loss: 0.051590, acc: 0.980469]  [G loss: 2.748760, acc: 0.296875]\n",
            "3685: [D loss: 0.089338, acc: 0.957031]  [G loss: 2.494007, acc: 0.250000]\n",
            "3686: [D loss: 0.074377, acc: 0.972656]  [G loss: 2.005090, acc: 0.375000]\n",
            "3687: [D loss: 0.106442, acc: 0.949219]  [G loss: 2.915549, acc: 0.171875]\n",
            "3688: [D loss: 0.074279, acc: 0.984375]  [G loss: 3.250329, acc: 0.203125]\n",
            "3689: [D loss: 0.069495, acc: 0.976562]  [G loss: 3.059196, acc: 0.250000]\n",
            "3690: [D loss: 0.053533, acc: 0.984375]  [G loss: 3.183508, acc: 0.179688]\n",
            "3691: [D loss: 0.067334, acc: 0.980469]  [G loss: 2.784744, acc: 0.250000]\n",
            "3692: [D loss: 0.053015, acc: 0.980469]  [G loss: 3.253662, acc: 0.226562]\n",
            "3693: [D loss: 0.068951, acc: 0.976562]  [G loss: 3.614935, acc: 0.179688]\n",
            "3694: [D loss: 0.019797, acc: 0.992188]  [G loss: 3.725872, acc: 0.203125]\n",
            "3695: [D loss: 0.018298, acc: 0.996094]  [G loss: 4.230770, acc: 0.195312]\n",
            "3696: [D loss: 0.043055, acc: 0.988281]  [G loss: 4.137942, acc: 0.195312]\n",
            "3697: [D loss: 0.060245, acc: 0.980469]  [G loss: 3.925671, acc: 0.203125]\n",
            "3698: [D loss: 0.039861, acc: 0.976562]  [G loss: 3.867342, acc: 0.148438]\n",
            "3699: [D loss: 0.064338, acc: 0.972656]  [G loss: 3.377021, acc: 0.281250]\n",
            "3700: [D loss: 0.033915, acc: 0.992188]  [G loss: 2.475624, acc: 0.398438]\n",
            "3701: [D loss: 0.082911, acc: 0.960938]  [G loss: 3.445339, acc: 0.375000]\n",
            "3702: [D loss: 0.039200, acc: 0.988281]  [G loss: 3.329651, acc: 0.343750]\n",
            "3703: [D loss: 0.081242, acc: 0.984375]  [G loss: 3.801878, acc: 0.328125]\n",
            "3704: [D loss: 0.036946, acc: 0.992188]  [G loss: 3.419611, acc: 0.453125]\n",
            "3705: [D loss: 0.091579, acc: 0.972656]  [G loss: 2.665917, acc: 0.500000]\n",
            "3706: [D loss: 0.061919, acc: 0.984375]  [G loss: 2.628310, acc: 0.523438]\n",
            "3707: [D loss: 0.055385, acc: 0.972656]  [G loss: 2.712711, acc: 0.476562]\n",
            "3708: [D loss: 0.048281, acc: 0.976562]  [G loss: 1.891453, acc: 0.593750]\n",
            "3709: [D loss: 0.031597, acc: 0.988281]  [G loss: 1.615110, acc: 0.593750]\n",
            "3710: [D loss: 0.029872, acc: 0.992188]  [G loss: 1.436077, acc: 0.625000]\n",
            "3711: [D loss: 0.038375, acc: 0.988281]  [G loss: 1.195286, acc: 0.632812]\n",
            "3712: [D loss: 0.021751, acc: 0.992188]  [G loss: 1.284446, acc: 0.640625]\n",
            "3713: [D loss: 0.044669, acc: 0.984375]  [G loss: 1.474135, acc: 0.656250]\n",
            "3714: [D loss: 0.077468, acc: 0.968750]  [G loss: 1.389781, acc: 0.617188]\n",
            "3715: [D loss: 0.021979, acc: 0.992188]  [G loss: 1.388803, acc: 0.656250]\n",
            "3716: [D loss: 0.113709, acc: 0.964844]  [G loss: 0.931152, acc: 0.781250]\n",
            "3717: [D loss: 0.031191, acc: 0.988281]  [G loss: 0.991291, acc: 0.734375]\n",
            "3718: [D loss: 0.049426, acc: 0.992188]  [G loss: 1.040004, acc: 0.734375]\n",
            "3719: [D loss: 0.036828, acc: 0.984375]  [G loss: 0.884776, acc: 0.773438]\n",
            "3720: [D loss: 0.050139, acc: 0.988281]  [G loss: 1.191051, acc: 0.742188]\n",
            "3721: [D loss: 0.036010, acc: 0.984375]  [G loss: 1.476873, acc: 0.679688]\n",
            "3722: [D loss: 0.038421, acc: 0.984375]  [G loss: 1.551630, acc: 0.679688]\n",
            "3723: [D loss: 0.034839, acc: 0.988281]  [G loss: 1.340791, acc: 0.671875]\n",
            "3724: [D loss: 0.036182, acc: 0.984375]  [G loss: 1.331448, acc: 0.671875]\n",
            "3725: [D loss: 0.109056, acc: 0.976562]  [G loss: 0.789060, acc: 0.765625]\n",
            "3726: [D loss: 0.017466, acc: 0.996094]  [G loss: 0.455699, acc: 0.859375]\n",
            "3727: [D loss: 0.065431, acc: 0.980469]  [G loss: 0.401154, acc: 0.882812]\n",
            "3728: [D loss: 0.086064, acc: 0.976562]  [G loss: 0.479293, acc: 0.835938]\n",
            "3729: [D loss: 0.049591, acc: 0.980469]  [G loss: 0.512849, acc: 0.804688]\n",
            "3730: [D loss: 0.067025, acc: 0.980469]  [G loss: 0.475618, acc: 0.796875]\n",
            "3731: [D loss: 0.011638, acc: 0.996094]  [G loss: 0.718640, acc: 0.703125]\n",
            "3732: [D loss: 0.016268, acc: 0.992188]  [G loss: 0.776241, acc: 0.718750]\n",
            "3733: [D loss: 0.027312, acc: 0.996094]  [G loss: 0.690978, acc: 0.726562]\n",
            "3734: [D loss: 0.045338, acc: 0.988281]  [G loss: 0.453849, acc: 0.789062]\n",
            "3735: [D loss: 0.010136, acc: 1.000000]  [G loss: 0.436111, acc: 0.820312]\n",
            "3736: [D loss: 0.031708, acc: 0.992188]  [G loss: 0.344339, acc: 0.851562]\n",
            "3737: [D loss: 0.059702, acc: 0.984375]  [G loss: 0.261550, acc: 0.875000]\n",
            "3738: [D loss: 0.021527, acc: 0.992188]  [G loss: 0.374536, acc: 0.851562]\n",
            "3739: [D loss: 0.017184, acc: 0.996094]  [G loss: 0.364219, acc: 0.890625]\n",
            "3740: [D loss: 0.057076, acc: 0.980469]  [G loss: 0.499397, acc: 0.835938]\n",
            "3741: [D loss: 0.021729, acc: 0.992188]  [G loss: 0.511396, acc: 0.828125]\n",
            "3742: [D loss: 0.017482, acc: 0.992188]  [G loss: 0.477070, acc: 0.859375]\n",
            "3743: [D loss: 0.017602, acc: 0.992188]  [G loss: 0.365464, acc: 0.867188]\n",
            "3744: [D loss: 0.034840, acc: 0.988281]  [G loss: 0.401833, acc: 0.875000]\n",
            "3745: [D loss: 0.013071, acc: 1.000000]  [G loss: 0.309328, acc: 0.882812]\n",
            "3746: [D loss: 0.020696, acc: 0.992188]  [G loss: 0.435736, acc: 0.812500]\n",
            "3747: [D loss: 0.024939, acc: 0.992188]  [G loss: 0.589129, acc: 0.781250]\n",
            "3748: [D loss: 0.024717, acc: 0.996094]  [G loss: 0.455402, acc: 0.812500]\n",
            "3749: [D loss: 0.013755, acc: 0.992188]  [G loss: 0.461645, acc: 0.835938]\n",
            "3750: [D loss: 0.010702, acc: 0.996094]  [G loss: 0.397778, acc: 0.828125]\n",
            "3751: [D loss: 0.012010, acc: 0.996094]  [G loss: 0.337440, acc: 0.875000]\n",
            "3752: [D loss: 0.009310, acc: 0.996094]  [G loss: 0.141373, acc: 0.945312]\n",
            "3753: [D loss: 0.005643, acc: 1.000000]  [G loss: 0.203877, acc: 0.898438]\n",
            "3754: [D loss: 0.008472, acc: 1.000000]  [G loss: 0.231673, acc: 0.875000]\n",
            "3755: [D loss: 0.008883, acc: 1.000000]  [G loss: 0.260241, acc: 0.882812]\n",
            "3756: [D loss: 0.015706, acc: 0.992188]  [G loss: 0.307059, acc: 0.875000]\n",
            "3757: [D loss: 0.009356, acc: 1.000000]  [G loss: 0.343635, acc: 0.867188]\n",
            "3758: [D loss: 0.004166, acc: 1.000000]  [G loss: 0.459404, acc: 0.835938]\n",
            "3759: [D loss: 0.003023, acc: 1.000000]  [G loss: 0.407075, acc: 0.851562]\n",
            "3760: [D loss: 0.062256, acc: 0.984375]  [G loss: 0.282220, acc: 0.875000]\n",
            "3761: [D loss: 0.009984, acc: 0.996094]  [G loss: 0.188710, acc: 0.921875]\n",
            "3762: [D loss: 0.027001, acc: 0.992188]  [G loss: 0.232110, acc: 0.898438]\n",
            "3763: [D loss: 0.021779, acc: 0.996094]  [G loss: 0.275177, acc: 0.906250]\n",
            "3764: [D loss: 0.023503, acc: 0.992188]  [G loss: 0.229964, acc: 0.898438]\n",
            "3765: [D loss: 0.017626, acc: 0.988281]  [G loss: 0.572184, acc: 0.820312]\n",
            "3766: [D loss: 0.039330, acc: 0.996094]  [G loss: 0.949072, acc: 0.765625]\n",
            "3767: [D loss: 0.021069, acc: 0.996094]  [G loss: 1.160218, acc: 0.695312]\n",
            "3768: [D loss: 0.081433, acc: 0.972656]  [G loss: 0.875220, acc: 0.742188]\n",
            "3769: [D loss: 0.046651, acc: 0.984375]  [G loss: 0.387529, acc: 0.843750]\n",
            "3770: [D loss: 0.019219, acc: 0.992188]  [G loss: 0.139887, acc: 0.960938]\n",
            "3771: [D loss: 0.027048, acc: 0.988281]  [G loss: 0.133438, acc: 0.937500]\n",
            "3772: [D loss: 0.064648, acc: 0.968750]  [G loss: 0.293422, acc: 0.882812]\n",
            "3773: [D loss: 0.013623, acc: 1.000000]  [G loss: 0.676203, acc: 0.820312]\n",
            "3774: [D loss: 0.012172, acc: 0.996094]  [G loss: 1.362197, acc: 0.585938]\n",
            "3775: [D loss: 0.057014, acc: 0.984375]  [G loss: 1.316136, acc: 0.640625]\n",
            "3776: [D loss: 0.059618, acc: 0.980469]  [G loss: 1.003588, acc: 0.695312]\n",
            "3777: [D loss: 0.022707, acc: 0.992188]  [G loss: 0.925508, acc: 0.726562]\n",
            "3778: [D loss: 0.040594, acc: 0.992188]  [G loss: 0.506384, acc: 0.835938]\n",
            "3779: [D loss: 0.033929, acc: 0.992188]  [G loss: 0.307571, acc: 0.882812]\n",
            "3780: [D loss: 0.022627, acc: 0.996094]  [G loss: 0.258851, acc: 0.867188]\n",
            "3781: [D loss: 0.035583, acc: 0.992188]  [G loss: 0.372613, acc: 0.859375]\n",
            "3782: [D loss: 0.018473, acc: 0.996094]  [G loss: 0.583679, acc: 0.796875]\n",
            "3783: [D loss: 0.014411, acc: 0.996094]  [G loss: 0.907999, acc: 0.695312]\n",
            "3784: [D loss: 0.004815, acc: 1.000000]  [G loss: 0.893688, acc: 0.710938]\n",
            "3785: [D loss: 0.039285, acc: 0.984375]  [G loss: 0.890522, acc: 0.695312]\n",
            "3786: [D loss: 0.023821, acc: 0.988281]  [G loss: 0.723263, acc: 0.726562]\n",
            "3787: [D loss: 0.032962, acc: 0.988281]  [G loss: 0.382506, acc: 0.835938]\n",
            "3788: [D loss: 0.034592, acc: 0.988281]  [G loss: 0.330206, acc: 0.835938]\n",
            "3789: [D loss: 0.026543, acc: 0.988281]  [G loss: 0.339864, acc: 0.875000]\n",
            "3790: [D loss: 0.018022, acc: 0.992188]  [G loss: 0.288056, acc: 0.882812]\n",
            "3791: [D loss: 0.041867, acc: 0.988281]  [G loss: 0.391717, acc: 0.812500]\n",
            "3792: [D loss: 0.020738, acc: 0.992188]  [G loss: 0.562395, acc: 0.734375]\n",
            "3793: [D loss: 0.020777, acc: 0.992188]  [G loss: 0.594423, acc: 0.750000]\n",
            "3794: [D loss: 0.021801, acc: 0.992188]  [G loss: 0.542152, acc: 0.742188]\n",
            "3795: [D loss: 0.021233, acc: 0.988281]  [G loss: 0.585569, acc: 0.757812]\n",
            "3796: [D loss: 0.035455, acc: 0.992188]  [G loss: 0.308088, acc: 0.890625]\n",
            "3797: [D loss: 0.044574, acc: 0.992188]  [G loss: 0.505980, acc: 0.781250]\n",
            "3798: [D loss: 0.019389, acc: 0.992188]  [G loss: 0.577179, acc: 0.695312]\n",
            "3799: [D loss: 0.019253, acc: 1.000000]  [G loss: 0.770907, acc: 0.625000]\n",
            "3800: [D loss: 0.021203, acc: 0.992188]  [G loss: 0.710556, acc: 0.687500]\n",
            "3801: [D loss: 0.041415, acc: 0.980469]  [G loss: 0.534703, acc: 0.765625]\n",
            "3802: [D loss: 0.008546, acc: 0.996094]  [G loss: 0.728977, acc: 0.710938]\n",
            "3803: [D loss: 0.007860, acc: 1.000000]  [G loss: 0.812268, acc: 0.648438]\n",
            "3804: [D loss: 0.018092, acc: 0.992188]  [G loss: 0.840278, acc: 0.679688]\n",
            "3805: [D loss: 0.024389, acc: 0.988281]  [G loss: 0.753190, acc: 0.757812]\n",
            "3806: [D loss: 0.026116, acc: 0.996094]  [G loss: 0.777920, acc: 0.656250]\n",
            "3807: [D loss: 0.047108, acc: 0.984375]  [G loss: 0.618019, acc: 0.710938]\n",
            "3808: [D loss: 0.015084, acc: 0.996094]  [G loss: 0.338009, acc: 0.851562]\n",
            "3809: [D loss: 0.036967, acc: 0.980469]  [G loss: 0.438031, acc: 0.781250]\n",
            "3810: [D loss: 0.043721, acc: 0.988281]  [G loss: 0.591562, acc: 0.679688]\n",
            "3811: [D loss: 0.020058, acc: 0.988281]  [G loss: 0.667877, acc: 0.671875]\n",
            "3812: [D loss: 0.012011, acc: 0.996094]  [G loss: 0.750698, acc: 0.656250]\n",
            "3813: [D loss: 0.010707, acc: 0.996094]  [G loss: 0.938340, acc: 0.593750]\n",
            "3814: [D loss: 0.011229, acc: 0.996094]  [G loss: 1.105677, acc: 0.570312]\n",
            "3815: [D loss: 0.033857, acc: 0.984375]  [G loss: 1.170287, acc: 0.492188]\n",
            "3816: [D loss: 0.053117, acc: 0.980469]  [G loss: 0.759857, acc: 0.648438]\n",
            "3817: [D loss: 0.010156, acc: 1.000000]  [G loss: 0.605376, acc: 0.710938]\n",
            "3818: [D loss: 0.032604, acc: 0.988281]  [G loss: 0.571177, acc: 0.726562]\n",
            "3819: [D loss: 0.052962, acc: 0.984375]  [G loss: 0.982603, acc: 0.585938]\n",
            "3820: [D loss: 0.025842, acc: 0.996094]  [G loss: 1.735822, acc: 0.335938]\n",
            "3821: [D loss: 0.039040, acc: 0.984375]  [G loss: 2.410099, acc: 0.203125]\n",
            "3822: [D loss: 0.015674, acc: 0.996094]  [G loss: 2.087800, acc: 0.171875]\n",
            "3823: [D loss: 0.023625, acc: 0.992188]  [G loss: 2.488782, acc: 0.164062]\n",
            "3824: [D loss: 0.017215, acc: 0.996094]  [G loss: 2.260245, acc: 0.179688]\n",
            "3825: [D loss: 0.037965, acc: 0.984375]  [G loss: 1.990477, acc: 0.265625]\n",
            "3826: [D loss: 0.058324, acc: 0.984375]  [G loss: 2.315101, acc: 0.156250]\n",
            "3827: [D loss: 0.030533, acc: 0.988281]  [G loss: 2.614271, acc: 0.109375]\n",
            "3828: [D loss: 0.028990, acc: 0.996094]  [G loss: 2.611897, acc: 0.164062]\n",
            "3829: [D loss: 0.036320, acc: 0.988281]  [G loss: 3.174974, acc: 0.117188]\n",
            "3830: [D loss: 0.028495, acc: 0.996094]  [G loss: 3.608177, acc: 0.070312]\n",
            "3831: [D loss: 0.029527, acc: 0.988281]  [G loss: 3.927497, acc: 0.046875]\n",
            "3832: [D loss: 0.050246, acc: 0.984375]  [G loss: 4.412444, acc: 0.046875]\n",
            "3833: [D loss: 0.040838, acc: 0.988281]  [G loss: 3.842042, acc: 0.062500]\n",
            "3834: [D loss: 0.035199, acc: 0.984375]  [G loss: 3.569712, acc: 0.148438]\n",
            "3835: [D loss: 0.041550, acc: 0.988281]  [G loss: 3.313130, acc: 0.132812]\n",
            "3836: [D loss: 0.036341, acc: 0.988281]  [G loss: 3.779417, acc: 0.148438]\n",
            "3837: [D loss: 0.024052, acc: 0.992188]  [G loss: 4.185763, acc: 0.078125]\n",
            "3838: [D loss: 0.017503, acc: 1.000000]  [G loss: 4.848245, acc: 0.046875]\n",
            "3839: [D loss: 0.057259, acc: 0.980469]  [G loss: 6.268831, acc: 0.007812]\n",
            "3840: [D loss: 0.031593, acc: 0.992188]  [G loss: 6.769249, acc: 0.015625]\n",
            "3841: [D loss: 0.083083, acc: 0.968750]  [G loss: 5.406239, acc: 0.062500]\n",
            "3842: [D loss: 0.043207, acc: 0.984375]  [G loss: 3.935031, acc: 0.140625]\n",
            "3843: [D loss: 0.115939, acc: 0.976562]  [G loss: 4.487872, acc: 0.117188]\n",
            "3844: [D loss: 0.106131, acc: 0.980469]  [G loss: 5.435278, acc: 0.085938]\n",
            "3845: [D loss: 0.026214, acc: 0.992188]  [G loss: 6.256146, acc: 0.078125]\n",
            "3846: [D loss: 0.070596, acc: 0.976562]  [G loss: 5.736081, acc: 0.109375]\n",
            "3847: [D loss: 0.097070, acc: 0.976562]  [G loss: 4.741968, acc: 0.226562]\n",
            "3848: [D loss: 0.071027, acc: 0.984375]  [G loss: 3.509485, acc: 0.320312]\n",
            "3849: [D loss: 0.048608, acc: 0.980469]  [G loss: 3.115410, acc: 0.390625]\n",
            "3850: [D loss: 0.123106, acc: 0.960938]  [G loss: 3.869656, acc: 0.257812]\n",
            "3851: [D loss: 0.018384, acc: 0.992188]  [G loss: 4.234417, acc: 0.226562]\n",
            "3852: [D loss: 0.031636, acc: 0.988281]  [G loss: 5.355419, acc: 0.164062]\n",
            "3853: [D loss: 0.049659, acc: 0.988281]  [G loss: 5.661026, acc: 0.140625]\n",
            "3854: [D loss: 0.113014, acc: 0.976562]  [G loss: 4.256558, acc: 0.250000]\n",
            "3855: [D loss: 0.020854, acc: 0.996094]  [G loss: 3.056225, acc: 0.304688]\n",
            "3856: [D loss: 0.014807, acc: 0.996094]  [G loss: 1.871155, acc: 0.484375]\n",
            "3857: [D loss: 0.045577, acc: 0.988281]  [G loss: 1.573228, acc: 0.554688]\n",
            "3858: [D loss: 0.027783, acc: 0.988281]  [G loss: 1.728732, acc: 0.500000]\n",
            "3859: [D loss: 0.023475, acc: 0.988281]  [G loss: 1.738737, acc: 0.554688]\n",
            "3860: [D loss: 0.027898, acc: 0.988281]  [G loss: 1.540199, acc: 0.554688]\n",
            "3861: [D loss: 0.007674, acc: 0.996094]  [G loss: 1.624322, acc: 0.632812]\n",
            "3862: [D loss: 0.020216, acc: 0.992188]  [G loss: 1.533470, acc: 0.656250]\n",
            "3863: [D loss: 0.033589, acc: 0.980469]  [G loss: 1.203037, acc: 0.750000]\n",
            "3864: [D loss: 0.051091, acc: 0.980469]  [G loss: 1.164742, acc: 0.773438]\n",
            "3865: [D loss: 0.050462, acc: 0.984375]  [G loss: 0.951496, acc: 0.820312]\n",
            "3866: [D loss: 0.025910, acc: 0.992188]  [G loss: 0.625464, acc: 0.843750]\n",
            "3867: [D loss: 0.037113, acc: 0.980469]  [G loss: 0.715809, acc: 0.820312]\n",
            "3868: [D loss: 0.015848, acc: 0.992188]  [G loss: 0.562003, acc: 0.820312]\n",
            "3869: [D loss: 0.048669, acc: 0.976562]  [G loss: 0.655090, acc: 0.835938]\n",
            "3870: [D loss: 0.026520, acc: 0.992188]  [G loss: 0.824884, acc: 0.781250]\n",
            "3871: [D loss: 0.037415, acc: 0.984375]  [G loss: 0.763489, acc: 0.804688]\n",
            "3872: [D loss: 0.044150, acc: 0.992188]  [G loss: 0.642558, acc: 0.812500]\n",
            "3873: [D loss: 0.045212, acc: 0.980469]  [G loss: 0.434453, acc: 0.875000]\n",
            "3874: [D loss: 0.018798, acc: 0.992188]  [G loss: 0.455543, acc: 0.890625]\n",
            "3875: [D loss: 0.088726, acc: 0.972656]  [G loss: 0.572599, acc: 0.882812]\n",
            "3876: [D loss: 0.094977, acc: 0.964844]  [G loss: 0.756401, acc: 0.828125]\n",
            "3877: [D loss: 0.044941, acc: 0.980469]  [G loss: 0.711080, acc: 0.859375]\n",
            "3878: [D loss: 0.109019, acc: 0.960938]  [G loss: 0.578142, acc: 0.851562]\n",
            "3879: [D loss: 0.048606, acc: 0.968750]  [G loss: 0.701006, acc: 0.812500]\n",
            "3880: [D loss: 0.074518, acc: 0.968750]  [G loss: 1.191997, acc: 0.710938]\n",
            "3881: [D loss: 0.080653, acc: 0.976562]  [G loss: 1.343636, acc: 0.695312]\n",
            "3882: [D loss: 0.083703, acc: 0.976562]  [G loss: 1.342331, acc: 0.718750]\n",
            "3883: [D loss: 0.203414, acc: 0.964844]  [G loss: 0.717077, acc: 0.859375]\n",
            "3884: [D loss: 0.055264, acc: 0.984375]  [G loss: 0.272056, acc: 0.906250]\n",
            "3885: [D loss: 0.153532, acc: 0.937500]  [G loss: 0.993873, acc: 0.703125]\n",
            "3886: [D loss: 0.030286, acc: 0.992188]  [G loss: 2.316585, acc: 0.429688]\n",
            "3887: [D loss: 0.117452, acc: 0.972656]  [G loss: 2.499639, acc: 0.351562]\n",
            "3888: [D loss: 0.053506, acc: 0.980469]  [G loss: 1.452471, acc: 0.562500]\n",
            "3889: [D loss: 0.039759, acc: 0.988281]  [G loss: 0.657083, acc: 0.765625]\n",
            "3890: [D loss: 0.057314, acc: 0.972656]  [G loss: 0.480857, acc: 0.804688]\n",
            "3891: [D loss: 0.023401, acc: 0.996094]  [G loss: 0.813831, acc: 0.687500]\n",
            "3892: [D loss: 0.035276, acc: 0.988281]  [G loss: 2.112043, acc: 0.546875]\n",
            "3893: [D loss: 0.018554, acc: 0.996094]  [G loss: 2.013918, acc: 0.585938]\n",
            "3894: [D loss: 0.047922, acc: 0.972656]  [G loss: 1.828838, acc: 0.625000]\n",
            "3895: [D loss: 0.007799, acc: 0.996094]  [G loss: 1.575132, acc: 0.687500]\n",
            "3896: [D loss: 0.011931, acc: 0.992188]  [G loss: 2.101265, acc: 0.656250]\n",
            "3897: [D loss: 0.009576, acc: 0.996094]  [G loss: 2.155811, acc: 0.710938]\n",
            "3898: [D loss: 0.014231, acc: 0.992188]  [G loss: 1.927948, acc: 0.695312]\n",
            "3899: [D loss: 0.014749, acc: 0.992188]  [G loss: 2.516863, acc: 0.640625]\n",
            "3900: [D loss: 0.053015, acc: 0.992188]  [G loss: 2.292094, acc: 0.648438]\n",
            "3901: [D loss: 0.005687, acc: 1.000000]  [G loss: 2.302584, acc: 0.679688]\n",
            "3902: [D loss: 0.003806, acc: 1.000000]  [G loss: 2.051497, acc: 0.687500]\n",
            "3903: [D loss: 0.032093, acc: 0.988281]  [G loss: 2.165099, acc: 0.671875]\n",
            "3904: [D loss: 0.059687, acc: 0.980469]  [G loss: 1.518191, acc: 0.750000]\n",
            "3905: [D loss: 0.063315, acc: 0.980469]  [G loss: 1.100494, acc: 0.773438]\n",
            "3906: [D loss: 0.051827, acc: 0.972656]  [G loss: 1.025957, acc: 0.718750]\n",
            "3907: [D loss: 0.053223, acc: 0.980469]  [G loss: 1.310675, acc: 0.710938]\n",
            "3908: [D loss: 0.011260, acc: 1.000000]  [G loss: 1.505987, acc: 0.726562]\n",
            "3909: [D loss: 0.031003, acc: 0.996094]  [G loss: 1.498638, acc: 0.734375]\n",
            "3910: [D loss: 0.029249, acc: 0.988281]  [G loss: 1.419811, acc: 0.734375]\n",
            "3911: [D loss: 0.021995, acc: 0.988281]  [G loss: 1.566981, acc: 0.742188]\n",
            "3912: [D loss: 0.043058, acc: 0.992188]  [G loss: 1.222342, acc: 0.781250]\n",
            "3913: [D loss: 0.067377, acc: 0.980469]  [G loss: 1.597208, acc: 0.765625]\n",
            "3914: [D loss: 0.041420, acc: 0.976562]  [G loss: 1.696110, acc: 0.757812]\n",
            "3915: [D loss: 0.014380, acc: 0.992188]  [G loss: 1.404062, acc: 0.757812]\n",
            "3916: [D loss: 0.112084, acc: 0.984375]  [G loss: 1.032179, acc: 0.828125]\n",
            "3917: [D loss: 0.038921, acc: 0.980469]  [G loss: 1.204176, acc: 0.804688]\n",
            "3918: [D loss: 0.026992, acc: 0.996094]  [G loss: 0.857245, acc: 0.804688]\n",
            "3919: [D loss: 0.060160, acc: 0.976562]  [G loss: 1.064362, acc: 0.781250]\n",
            "3920: [D loss: 0.026878, acc: 0.988281]  [G loss: 1.185321, acc: 0.734375]\n",
            "3921: [D loss: 0.013591, acc: 0.996094]  [G loss: 1.679021, acc: 0.664062]\n",
            "3922: [D loss: 0.024083, acc: 0.988281]  [G loss: 1.842929, acc: 0.632812]\n",
            "3923: [D loss: 0.035363, acc: 0.988281]  [G loss: 1.731858, acc: 0.648438]\n",
            "3924: [D loss: 0.013504, acc: 0.996094]  [G loss: 1.503301, acc: 0.718750]\n",
            "3925: [D loss: 0.012778, acc: 0.996094]  [G loss: 1.590139, acc: 0.648438]\n",
            "3926: [D loss: 0.016860, acc: 0.992188]  [G loss: 1.396851, acc: 0.679688]\n",
            "3927: [D loss: 0.022578, acc: 0.992188]  [G loss: 1.021510, acc: 0.734375]\n",
            "3928: [D loss: 0.045019, acc: 0.984375]  [G loss: 0.990435, acc: 0.757812]\n",
            "3929: [D loss: 0.060544, acc: 0.984375]  [G loss: 1.522732, acc: 0.640625]\n",
            "3930: [D loss: 0.025328, acc: 0.992188]  [G loss: 1.779167, acc: 0.570312]\n",
            "3931: [D loss: 0.019314, acc: 0.992188]  [G loss: 2.265475, acc: 0.523438]\n",
            "3932: [D loss: 0.088892, acc: 0.988281]  [G loss: 2.219803, acc: 0.507812]\n",
            "3933: [D loss: 0.055185, acc: 0.984375]  [G loss: 1.926522, acc: 0.617188]\n",
            "3934: [D loss: 0.064234, acc: 0.976562]  [G loss: 1.571936, acc: 0.648438]\n",
            "3935: [D loss: 0.035603, acc: 0.988281]  [G loss: 1.352210, acc: 0.718750]\n",
            "3936: [D loss: 0.030333, acc: 0.988281]  [G loss: 1.738770, acc: 0.617188]\n",
            "3937: [D loss: 0.023420, acc: 0.988281]  [G loss: 2.081124, acc: 0.578125]\n",
            "3938: [D loss: 0.066082, acc: 0.976562]  [G loss: 1.518197, acc: 0.671875]\n",
            "3939: [D loss: 0.051102, acc: 0.984375]  [G loss: 1.049376, acc: 0.726562]\n",
            "3940: [D loss: 0.039732, acc: 0.984375]  [G loss: 1.402797, acc: 0.656250]\n",
            "3941: [D loss: 0.032840, acc: 0.992188]  [G loss: 1.669595, acc: 0.601562]\n",
            "3942: [D loss: 0.035388, acc: 0.988281]  [G loss: 1.685274, acc: 0.585938]\n",
            "3943: [D loss: 0.025716, acc: 0.992188]  [G loss: 1.560864, acc: 0.640625]\n",
            "3944: [D loss: 0.034080, acc: 0.988281]  [G loss: 1.123978, acc: 0.734375]\n",
            "3945: [D loss: 0.032671, acc: 0.984375]  [G loss: 1.515780, acc: 0.664062]\n",
            "3946: [D loss: 0.008007, acc: 1.000000]  [G loss: 1.491657, acc: 0.656250]\n",
            "3947: [D loss: 0.040911, acc: 0.988281]  [G loss: 1.827605, acc: 0.609375]\n",
            "3948: [D loss: 0.028121, acc: 0.992188]  [G loss: 1.719300, acc: 0.562500]\n",
            "3949: [D loss: 0.017605, acc: 0.992188]  [G loss: 1.951957, acc: 0.507812]\n",
            "3950: [D loss: 0.028071, acc: 0.988281]  [G loss: 2.062807, acc: 0.531250]\n",
            "3951: [D loss: 0.024782, acc: 0.992188]  [G loss: 2.503993, acc: 0.429688]\n",
            "3952: [D loss: 0.031225, acc: 0.992188]  [G loss: 2.301923, acc: 0.523438]\n",
            "3953: [D loss: 0.069189, acc: 0.988281]  [G loss: 1.375273, acc: 0.601562]\n",
            "3954: [D loss: 0.032552, acc: 0.988281]  [G loss: 1.319396, acc: 0.570312]\n",
            "3955: [D loss: 0.038773, acc: 0.988281]  [G loss: 1.312020, acc: 0.617188]\n",
            "3956: [D loss: 0.033259, acc: 0.988281]  [G loss: 1.550288, acc: 0.578125]\n",
            "3957: [D loss: 0.032585, acc: 0.988281]  [G loss: 1.586154, acc: 0.531250]\n",
            "3958: [D loss: 0.025265, acc: 0.988281]  [G loss: 1.735678, acc: 0.484375]\n",
            "3959: [D loss: 0.074425, acc: 0.972656]  [G loss: 1.309572, acc: 0.640625]\n",
            "3960: [D loss: 0.025036, acc: 0.984375]  [G loss: 1.138694, acc: 0.687500]\n",
            "3961: [D loss: 0.012245, acc: 0.996094]  [G loss: 0.732645, acc: 0.781250]\n",
            "3962: [D loss: 0.041291, acc: 0.980469]  [G loss: 1.035392, acc: 0.718750]\n",
            "3963: [D loss: 0.065309, acc: 0.992188]  [G loss: 1.560284, acc: 0.531250]\n",
            "3964: [D loss: 0.031297, acc: 0.984375]  [G loss: 1.767405, acc: 0.531250]\n",
            "3965: [D loss: 0.036077, acc: 0.988281]  [G loss: 1.582914, acc: 0.554688]\n",
            "3966: [D loss: 0.124223, acc: 0.976562]  [G loss: 0.664449, acc: 0.757812]\n",
            "3967: [D loss: 0.049016, acc: 0.984375]  [G loss: 0.553774, acc: 0.765625]\n",
            "3968: [D loss: 0.050506, acc: 0.984375]  [G loss: 1.290682, acc: 0.632812]\n",
            "3969: [D loss: 0.028086, acc: 0.992188]  [G loss: 2.669020, acc: 0.265625]\n",
            "3970: [D loss: 0.045390, acc: 0.976562]  [G loss: 3.223090, acc: 0.195312]\n",
            "3971: [D loss: 0.070913, acc: 0.984375]  [G loss: 3.085168, acc: 0.242188]\n",
            "3972: [D loss: 0.039783, acc: 0.980469]  [G loss: 2.112232, acc: 0.406250]\n",
            "3973: [D loss: 0.037688, acc: 0.988281]  [G loss: 2.284253, acc: 0.398438]\n",
            "3974: [D loss: 0.043463, acc: 0.980469]  [G loss: 3.106104, acc: 0.242188]\n",
            "3975: [D loss: 0.011188, acc: 0.996094]  [G loss: 4.316675, acc: 0.156250]\n",
            "3976: [D loss: 0.035386, acc: 0.988281]  [G loss: 5.023449, acc: 0.085938]\n",
            "3977: [D loss: 0.040344, acc: 0.984375]  [G loss: 4.424206, acc: 0.171875]\n",
            "3978: [D loss: 0.024120, acc: 0.988281]  [G loss: 3.761333, acc: 0.328125]\n",
            "3979: [D loss: 0.015509, acc: 1.000000]  [G loss: 3.552405, acc: 0.203125]\n",
            "3980: [D loss: 0.016409, acc: 0.996094]  [G loss: 3.592221, acc: 0.265625]\n",
            "3981: [D loss: 0.039855, acc: 0.988281]  [G loss: 4.585534, acc: 0.218750]\n",
            "3982: [D loss: 0.061782, acc: 0.972656]  [G loss: 4.256358, acc: 0.234375]\n",
            "3983: [D loss: 0.037276, acc: 0.980469]  [G loss: 3.577267, acc: 0.289062]\n",
            "3984: [D loss: 0.028186, acc: 0.996094]  [G loss: 3.958213, acc: 0.273438]\n",
            "3985: [D loss: 0.013438, acc: 1.000000]  [G loss: 4.630841, acc: 0.218750]\n",
            "3986: [D loss: 0.008808, acc: 0.996094]  [G loss: 5.097876, acc: 0.187500]\n",
            "3987: [D loss: 0.044170, acc: 0.988281]  [G loss: 4.487378, acc: 0.203125]\n",
            "3988: [D loss: 0.019393, acc: 0.996094]  [G loss: 4.202334, acc: 0.250000]\n",
            "3989: [D loss: 0.025578, acc: 0.992188]  [G loss: 4.105289, acc: 0.242188]\n",
            "3990: [D loss: 0.051573, acc: 0.984375]  [G loss: 3.642077, acc: 0.296875]\n",
            "3991: [D loss: 0.030260, acc: 0.992188]  [G loss: 3.807713, acc: 0.328125]\n",
            "3992: [D loss: 0.111274, acc: 0.968750]  [G loss: 5.933709, acc: 0.171875]\n",
            "3993: [D loss: 0.141392, acc: 0.960938]  [G loss: 6.159021, acc: 0.195312]\n",
            "3994: [D loss: 0.061030, acc: 0.984375]  [G loss: 4.316818, acc: 0.335938]\n",
            "3995: [D loss: 0.013581, acc: 0.992188]  [G loss: 3.043558, acc: 0.414062]\n",
            "3996: [D loss: 0.061102, acc: 0.972656]  [G loss: 2.643080, acc: 0.468750]\n",
            "3997: [D loss: 0.057532, acc: 0.980469]  [G loss: 3.534291, acc: 0.343750]\n",
            "3998: [D loss: 0.036408, acc: 0.988281]  [G loss: 4.115727, acc: 0.281250]\n",
            "3999: [D loss: 0.050301, acc: 0.984375]  [G loss: 4.936616, acc: 0.265625]\n",
            "4000: [D loss: 0.135891, acc: 0.976562]  [G loss: 3.610781, acc: 0.257812]\n",
            "4001: [D loss: 0.040436, acc: 0.988281]  [G loss: 3.852638, acc: 0.273438]\n",
            "4002: [D loss: 0.070045, acc: 0.964844]  [G loss: 3.505595, acc: 0.203125]\n",
            "4003: [D loss: 0.074879, acc: 0.972656]  [G loss: 5.228016, acc: 0.117188]\n",
            "4004: [D loss: 0.101325, acc: 0.984375]  [G loss: 6.011264, acc: 0.054688]\n",
            "4005: [D loss: 0.062000, acc: 0.980469]  [G loss: 5.575682, acc: 0.093750]\n",
            "4006: [D loss: 0.028989, acc: 0.988281]  [G loss: 4.572698, acc: 0.117188]\n",
            "4007: [D loss: 0.059730, acc: 0.984375]  [G loss: 3.441681, acc: 0.218750]\n",
            "4008: [D loss: 0.107762, acc: 0.968750]  [G loss: 3.877013, acc: 0.250000]\n",
            "4009: [D loss: 0.091964, acc: 0.972656]  [G loss: 3.415722, acc: 0.257812]\n",
            "4010: [D loss: 0.057719, acc: 0.972656]  [G loss: 4.105506, acc: 0.218750]\n",
            "4011: [D loss: 0.092421, acc: 0.968750]  [G loss: 5.549231, acc: 0.187500]\n",
            "4012: [D loss: 0.051239, acc: 0.992188]  [G loss: 6.027313, acc: 0.085938]\n",
            "4013: [D loss: 0.076621, acc: 0.972656]  [G loss: 5.223321, acc: 0.101562]\n",
            "4014: [D loss: 0.027879, acc: 0.992188]  [G loss: 3.798199, acc: 0.203125]\n",
            "4015: [D loss: 0.144886, acc: 0.968750]  [G loss: 3.362829, acc: 0.226562]\n",
            "4016: [D loss: 0.110395, acc: 0.964844]  [G loss: 2.380811, acc: 0.390625]\n",
            "4017: [D loss: 0.057604, acc: 0.968750]  [G loss: 1.378006, acc: 0.578125]\n",
            "4018: [D loss: 0.088809, acc: 0.964844]  [G loss: 1.306566, acc: 0.632812]\n",
            "4019: [D loss: 0.070676, acc: 0.980469]  [G loss: 1.721092, acc: 0.570312]\n",
            "4020: [D loss: 0.012970, acc: 0.996094]  [G loss: 1.876877, acc: 0.562500]\n",
            "4021: [D loss: 0.121900, acc: 0.972656]  [G loss: 1.329589, acc: 0.648438]\n",
            "4022: [D loss: 0.052263, acc: 0.984375]  [G loss: 1.072933, acc: 0.726562]\n",
            "4023: [D loss: 0.031128, acc: 0.988281]  [G loss: 0.549409, acc: 0.828125]\n",
            "4024: [D loss: 0.112531, acc: 0.964844]  [G loss: 0.529449, acc: 0.804688]\n",
            "4025: [D loss: 0.069869, acc: 0.964844]  [G loss: 0.672884, acc: 0.765625]\n",
            "4026: [D loss: 0.106777, acc: 0.976562]  [G loss: 0.839324, acc: 0.710938]\n",
            "4027: [D loss: 0.076722, acc: 0.976562]  [G loss: 0.585867, acc: 0.796875]\n",
            "4028: [D loss: 0.062458, acc: 0.984375]  [G loss: 0.516800, acc: 0.796875]\n",
            "4029: [D loss: 0.028727, acc: 0.992188]  [G loss: 0.285601, acc: 0.875000]\n",
            "4030: [D loss: 0.049030, acc: 0.984375]  [G loss: 0.303056, acc: 0.859375]\n",
            "4031: [D loss: 0.038479, acc: 0.980469]  [G loss: 0.363295, acc: 0.820312]\n",
            "4032: [D loss: 0.029743, acc: 0.992188]  [G loss: 0.582186, acc: 0.765625]\n",
            "4033: [D loss: 0.007429, acc: 1.000000]  [G loss: 0.801075, acc: 0.750000]\n",
            "4034: [D loss: 0.078897, acc: 0.988281]  [G loss: 0.925706, acc: 0.750000]\n",
            "4035: [D loss: 0.108405, acc: 0.968750]  [G loss: 0.384467, acc: 0.820312]\n",
            "4036: [D loss: 0.042586, acc: 0.988281]  [G loss: 0.197330, acc: 0.929688]\n",
            "4037: [D loss: 0.046180, acc: 0.992188]  [G loss: 0.151415, acc: 0.929688]\n",
            "4038: [D loss: 0.118777, acc: 0.953125]  [G loss: 0.594724, acc: 0.789062]\n",
            "4039: [D loss: 0.045204, acc: 0.988281]  [G loss: 1.338241, acc: 0.671875]\n",
            "4040: [D loss: 0.102546, acc: 0.972656]  [G loss: 1.493733, acc: 0.664062]\n",
            "4041: [D loss: 0.133101, acc: 0.968750]  [G loss: 1.213966, acc: 0.695312]\n",
            "4042: [D loss: 0.059468, acc: 0.984375]  [G loss: 0.519877, acc: 0.765625]\n",
            "4043: [D loss: 0.025440, acc: 0.992188]  [G loss: 0.287962, acc: 0.898438]\n",
            "4044: [D loss: 0.081181, acc: 0.972656]  [G loss: 0.596379, acc: 0.781250]\n",
            "4045: [D loss: 0.060627, acc: 0.972656]  [G loss: 1.086847, acc: 0.671875]\n",
            "4046: [D loss: 0.032896, acc: 0.984375]  [G loss: 1.380854, acc: 0.632812]\n",
            "4047: [D loss: 0.062525, acc: 0.984375]  [G loss: 1.580458, acc: 0.632812]\n",
            "4048: [D loss: 0.079305, acc: 0.980469]  [G loss: 1.306991, acc: 0.640625]\n",
            "4049: [D loss: 0.019221, acc: 0.996094]  [G loss: 1.209824, acc: 0.601562]\n",
            "4050: [D loss: 0.096548, acc: 0.976562]  [G loss: 0.697127, acc: 0.781250]\n",
            "4051: [D loss: 0.071365, acc: 0.980469]  [G loss: 0.775603, acc: 0.718750]\n",
            "4052: [D loss: 0.063214, acc: 0.980469]  [G loss: 1.152678, acc: 0.679688]\n",
            "4053: [D loss: 0.073745, acc: 0.980469]  [G loss: 1.713358, acc: 0.515625]\n",
            "4054: [D loss: 0.028986, acc: 0.988281]  [G loss: 2.101542, acc: 0.570312]\n",
            "4055: [D loss: 0.062646, acc: 0.980469]  [G loss: 2.488474, acc: 0.484375]\n",
            "4056: [D loss: 0.039771, acc: 0.984375]  [G loss: 2.014329, acc: 0.585938]\n",
            "4057: [D loss: 0.049007, acc: 0.984375]  [G loss: 1.667784, acc: 0.609375]\n",
            "4058: [D loss: 0.084319, acc: 0.988281]  [G loss: 1.468188, acc: 0.671875]\n",
            "4059: [D loss: 0.049945, acc: 0.980469]  [G loss: 0.906146, acc: 0.710938]\n",
            "4060: [D loss: 0.080750, acc: 0.972656]  [G loss: 1.184356, acc: 0.687500]\n",
            "4061: [D loss: 0.052854, acc: 0.984375]  [G loss: 1.300011, acc: 0.687500]\n",
            "4062: [D loss: 0.026785, acc: 0.996094]  [G loss: 1.312516, acc: 0.640625]\n",
            "4063: [D loss: 0.040543, acc: 0.984375]  [G loss: 1.636373, acc: 0.695312]\n",
            "4064: [D loss: 0.038945, acc: 0.992188]  [G loss: 1.806373, acc: 0.554688]\n",
            "4065: [D loss: 0.048670, acc: 0.988281]  [G loss: 1.931477, acc: 0.523438]\n",
            "4066: [D loss: 0.076191, acc: 0.976562]  [G loss: 1.469971, acc: 0.578125]\n",
            "4067: [D loss: 0.038347, acc: 0.984375]  [G loss: 1.691141, acc: 0.601562]\n",
            "4068: [D loss: 0.043428, acc: 0.988281]  [G loss: 1.431867, acc: 0.687500]\n",
            "4069: [D loss: 0.038440, acc: 0.992188]  [G loss: 1.513038, acc: 0.570312]\n",
            "4070: [D loss: 0.045461, acc: 0.992188]  [G loss: 1.787988, acc: 0.554688]\n",
            "4071: [D loss: 0.048198, acc: 0.988281]  [G loss: 1.850733, acc: 0.484375]\n",
            "4072: [D loss: 0.024451, acc: 0.992188]  [G loss: 2.465655, acc: 0.375000]\n",
            "4073: [D loss: 0.027118, acc: 0.992188]  [G loss: 2.645603, acc: 0.273438]\n",
            "4074: [D loss: 0.082434, acc: 0.980469]  [G loss: 1.919919, acc: 0.414062]\n",
            "4075: [D loss: 0.056258, acc: 0.976562]  [G loss: 1.466835, acc: 0.476562]\n",
            "4076: [D loss: 0.033695, acc: 0.992188]  [G loss: 1.390430, acc: 0.523438]\n",
            "4077: [D loss: 0.066352, acc: 0.968750]  [G loss: 1.342295, acc: 0.492188]\n",
            "4078: [D loss: 0.059188, acc: 0.984375]  [G loss: 1.981092, acc: 0.296875]\n",
            "4079: [D loss: 0.037611, acc: 0.988281]  [G loss: 2.747292, acc: 0.195312]\n",
            "4080: [D loss: 0.041931, acc: 0.988281]  [G loss: 3.303402, acc: 0.101562]\n",
            "4081: [D loss: 0.060850, acc: 0.988281]  [G loss: 3.868570, acc: 0.109375]\n",
            "4082: [D loss: 0.070028, acc: 0.976562]  [G loss: 3.285561, acc: 0.171875]\n",
            "4083: [D loss: 0.047676, acc: 0.980469]  [G loss: 2.436601, acc: 0.289062]\n",
            "4084: [D loss: 0.029263, acc: 0.988281]  [G loss: 2.151448, acc: 0.281250]\n",
            "4085: [D loss: 0.039531, acc: 0.988281]  [G loss: 1.921918, acc: 0.398438]\n",
            "4086: [D loss: 0.084066, acc: 0.976562]  [G loss: 2.791467, acc: 0.187500]\n",
            "4087: [D loss: 0.030647, acc: 0.996094]  [G loss: 3.672886, acc: 0.078125]\n",
            "4088: [D loss: 0.027573, acc: 0.988281]  [G loss: 4.458055, acc: 0.093750]\n",
            "4089: [D loss: 0.030930, acc: 0.988281]  [G loss: 4.675601, acc: 0.039062]\n",
            "4090: [D loss: 0.068530, acc: 0.980469]  [G loss: 4.371387, acc: 0.054688]\n",
            "4091: [D loss: 0.017238, acc: 1.000000]  [G loss: 3.824295, acc: 0.070312]\n",
            "4092: [D loss: 0.043219, acc: 0.992188]  [G loss: 3.842425, acc: 0.132812]\n",
            "4093: [D loss: 0.055738, acc: 0.984375]  [G loss: 3.148253, acc: 0.125000]\n",
            "4094: [D loss: 0.047142, acc: 0.984375]  [G loss: 3.286557, acc: 0.140625]\n",
            "4095: [D loss: 0.031422, acc: 0.996094]  [G loss: 3.650661, acc: 0.093750]\n",
            "4096: [D loss: 0.038618, acc: 0.988281]  [G loss: 4.141283, acc: 0.078125]\n",
            "4097: [D loss: 0.039966, acc: 0.984375]  [G loss: 4.864512, acc: 0.078125]\n",
            "4098: [D loss: 0.023534, acc: 0.992188]  [G loss: 5.098211, acc: 0.078125]\n",
            "4099: [D loss: 0.055543, acc: 0.976562]  [G loss: 4.699507, acc: 0.070312]\n",
            "4100: [D loss: 0.016831, acc: 0.992188]  [G loss: 4.567803, acc: 0.062500]\n",
            "4101: [D loss: 0.053513, acc: 0.980469]  [G loss: 4.374526, acc: 0.156250]\n",
            "4102: [D loss: 0.021142, acc: 0.996094]  [G loss: 4.310841, acc: 0.125000]\n",
            "4103: [D loss: 0.025497, acc: 0.988281]  [G loss: 4.130107, acc: 0.109375]\n",
            "4104: [D loss: 0.050671, acc: 0.984375]  [G loss: 4.588718, acc: 0.109375]\n",
            "4105: [D loss: 0.026268, acc: 0.992188]  [G loss: 4.377253, acc: 0.148438]\n",
            "4106: [D loss: 0.044659, acc: 0.980469]  [G loss: 4.640519, acc: 0.132812]\n",
            "4107: [D loss: 0.022307, acc: 0.992188]  [G loss: 3.867433, acc: 0.148438]\n",
            "4108: [D loss: 0.028835, acc: 0.988281]  [G loss: 3.530632, acc: 0.242188]\n",
            "4109: [D loss: 0.019384, acc: 0.992188]  [G loss: 3.508445, acc: 0.250000]\n",
            "4110: [D loss: 0.024442, acc: 0.992188]  [G loss: 2.836254, acc: 0.335938]\n",
            "4111: [D loss: 0.007880, acc: 1.000000]  [G loss: 2.489154, acc: 0.367188]\n",
            "4112: [D loss: 0.029527, acc: 0.984375]  [G loss: 2.058913, acc: 0.523438]\n",
            "4113: [D loss: 0.027298, acc: 0.992188]  [G loss: 1.913320, acc: 0.554688]\n",
            "4114: [D loss: 0.021570, acc: 0.996094]  [G loss: 1.468163, acc: 0.625000]\n",
            "4115: [D loss: 0.023219, acc: 0.992188]  [G loss: 1.295746, acc: 0.695312]\n",
            "4116: [D loss: 0.031012, acc: 0.992188]  [G loss: 1.247722, acc: 0.703125]\n",
            "4117: [D loss: 0.048342, acc: 0.980469]  [G loss: 1.119425, acc: 0.718750]\n",
            "4118: [D loss: 0.006565, acc: 1.000000]  [G loss: 0.918798, acc: 0.726562]\n",
            "4119: [D loss: 0.014012, acc: 0.996094]  [G loss: 1.104227, acc: 0.734375]\n",
            "4120: [D loss: 0.018591, acc: 0.992188]  [G loss: 0.722699, acc: 0.773438]\n",
            "4121: [D loss: 0.005963, acc: 1.000000]  [G loss: 0.649579, acc: 0.804688]\n",
            "4122: [D loss: 0.005417, acc: 1.000000]  [G loss: 0.648030, acc: 0.812500]\n",
            "4123: [D loss: 0.005762, acc: 1.000000]  [G loss: 0.428672, acc: 0.867188]\n",
            "4124: [D loss: 0.009139, acc: 0.996094]  [G loss: 0.321702, acc: 0.875000]\n",
            "4125: [D loss: 0.004847, acc: 1.000000]  [G loss: 0.385499, acc: 0.875000]\n",
            "4126: [D loss: 0.007375, acc: 1.000000]  [G loss: 0.285014, acc: 0.914062]\n",
            "4127: [D loss: 0.023982, acc: 0.988281]  [G loss: 0.286398, acc: 0.914062]\n",
            "4128: [D loss: 0.023797, acc: 0.988281]  [G loss: 0.467353, acc: 0.843750]\n",
            "4129: [D loss: 0.048136, acc: 0.980469]  [G loss: 0.488935, acc: 0.859375]\n",
            "4130: [D loss: 0.013020, acc: 0.996094]  [G loss: 0.508845, acc: 0.875000]\n",
            "4131: [D loss: 0.034519, acc: 0.988281]  [G loss: 0.578972, acc: 0.890625]\n",
            "4132: [D loss: 0.048874, acc: 0.984375]  [G loss: 0.702228, acc: 0.867188]\n",
            "4133: [D loss: 0.003028, acc: 1.000000]  [G loss: 0.715628, acc: 0.875000]\n",
            "4134: [D loss: 0.023955, acc: 0.988281]  [G loss: 0.530726, acc: 0.898438]\n",
            "4135: [D loss: 0.035704, acc: 0.988281]  [G loss: 0.502051, acc: 0.906250]\n",
            "4136: [D loss: 0.022553, acc: 0.988281]  [G loss: 0.417350, acc: 0.929688]\n",
            "4137: [D loss: 0.046908, acc: 0.980469]  [G loss: 0.563142, acc: 0.898438]\n",
            "4138: [D loss: 0.041306, acc: 0.988281]  [G loss: 0.628522, acc: 0.929688]\n",
            "4139: [D loss: 0.030436, acc: 0.988281]  [G loss: 0.883125, acc: 0.882812]\n",
            "4140: [D loss: 0.040089, acc: 0.980469]  [G loss: 0.896452, acc: 0.796875]\n",
            "4141: [D loss: 0.035500, acc: 0.984375]  [G loss: 1.240223, acc: 0.796875]\n",
            "4142: [D loss: 0.087096, acc: 0.972656]  [G loss: 1.469376, acc: 0.710938]\n",
            "4143: [D loss: 0.015707, acc: 0.996094]  [G loss: 1.503713, acc: 0.656250]\n",
            "4144: [D loss: 0.027671, acc: 0.988281]  [G loss: 1.922874, acc: 0.664062]\n",
            "4145: [D loss: 0.089047, acc: 0.968750]  [G loss: 2.099778, acc: 0.625000]\n",
            "4146: [D loss: 0.079049, acc: 0.976562]  [G loss: 2.003956, acc: 0.539062]\n",
            "4147: [D loss: 0.135409, acc: 0.957031]  [G loss: 1.442576, acc: 0.640625]\n",
            "4148: [D loss: 0.045157, acc: 0.988281]  [G loss: 1.722062, acc: 0.578125]\n",
            "4149: [D loss: 0.084018, acc: 0.980469]  [G loss: 1.732877, acc: 0.585938]\n",
            "4150: [D loss: 0.094404, acc: 0.976562]  [G loss: 2.356151, acc: 0.445312]\n",
            "4151: [D loss: 0.069049, acc: 0.968750]  [G loss: 2.216115, acc: 0.484375]\n",
            "4152: [D loss: 0.053022, acc: 0.976562]  [G loss: 1.648266, acc: 0.609375]\n",
            "4153: [D loss: 0.037393, acc: 0.988281]  [G loss: 1.055845, acc: 0.703125]\n",
            "4154: [D loss: 0.037268, acc: 0.984375]  [G loss: 1.218611, acc: 0.734375]\n",
            "4155: [D loss: 0.070628, acc: 0.976562]  [G loss: 1.844860, acc: 0.578125]\n",
            "4156: [D loss: 0.070801, acc: 0.976562]  [G loss: 1.774647, acc: 0.554688]\n",
            "4157: [D loss: 0.012455, acc: 1.000000]  [G loss: 1.714689, acc: 0.570312]\n",
            "4158: [D loss: 0.008754, acc: 1.000000]  [G loss: 1.674928, acc: 0.570312]\n",
            "4159: [D loss: 0.027262, acc: 0.984375]  [G loss: 1.374568, acc: 0.656250]\n",
            "4160: [D loss: 0.044631, acc: 0.980469]  [G loss: 0.496497, acc: 0.796875]\n",
            "4161: [D loss: 0.030669, acc: 0.992188]  [G loss: 0.770942, acc: 0.757812]\n",
            "4162: [D loss: 0.033554, acc: 0.992188]  [G loss: 0.955395, acc: 0.726562]\n",
            "4163: [D loss: 0.040123, acc: 0.992188]  [G loss: 1.441208, acc: 0.570312]\n",
            "4164: [D loss: 0.053523, acc: 0.976562]  [G loss: 2.176630, acc: 0.421875]\n",
            "4165: [D loss: 0.024099, acc: 0.996094]  [G loss: 2.376577, acc: 0.390625]\n",
            "4166: [D loss: 0.048381, acc: 0.992188]  [G loss: 3.209544, acc: 0.273438]\n",
            "4167: [D loss: 0.021082, acc: 0.992188]  [G loss: 2.748971, acc: 0.359375]\n",
            "4168: [D loss: 0.047575, acc: 0.988281]  [G loss: 1.746660, acc: 0.531250]\n",
            "4169: [D loss: 0.044106, acc: 0.984375]  [G loss: 1.272241, acc: 0.554688]\n",
            "4170: [D loss: 0.101968, acc: 0.968750]  [G loss: 3.113295, acc: 0.320312]\n",
            "4171: [D loss: 0.044818, acc: 0.992188]  [G loss: 3.461594, acc: 0.210938]\n",
            "4172: [D loss: 0.056777, acc: 0.980469]  [G loss: 3.518528, acc: 0.210938]\n",
            "4173: [D loss: 0.056663, acc: 0.980469]  [G loss: 1.473598, acc: 0.562500]\n",
            "4174: [D loss: 0.047126, acc: 0.988281]  [G loss: 1.103456, acc: 0.664062]\n",
            "4175: [D loss: 0.080235, acc: 0.972656]  [G loss: 0.817118, acc: 0.703125]\n",
            "4176: [D loss: 0.146425, acc: 0.941406]  [G loss: 4.087262, acc: 0.156250]\n",
            "4177: [D loss: 0.123090, acc: 0.964844]  [G loss: 4.930542, acc: 0.062500]\n",
            "4178: [D loss: 0.167501, acc: 0.945312]  [G loss: 3.381322, acc: 0.226562]\n",
            "4179: [D loss: 0.080066, acc: 0.972656]  [G loss: 2.095526, acc: 0.429688]\n",
            "4180: [D loss: 0.083103, acc: 0.984375]  [G loss: 1.045107, acc: 0.585938]\n",
            "4181: [D loss: 0.108861, acc: 0.964844]  [G loss: 1.559201, acc: 0.492188]\n",
            "4182: [D loss: 0.037284, acc: 0.984375]  [G loss: 2.508192, acc: 0.398438]\n",
            "4183: [D loss: 0.035358, acc: 0.984375]  [G loss: 3.730120, acc: 0.265625]\n",
            "4184: [D loss: 0.054911, acc: 0.980469]  [G loss: 3.874867, acc: 0.210938]\n",
            "4185: [D loss: 0.047396, acc: 0.984375]  [G loss: 3.081389, acc: 0.328125]\n",
            "4186: [D loss: 0.079860, acc: 0.984375]  [G loss: 2.661353, acc: 0.414062]\n",
            "4187: [D loss: 0.046127, acc: 0.992188]  [G loss: 1.935637, acc: 0.578125]\n",
            "4188: [D loss: 0.083379, acc: 0.957031]  [G loss: 2.570036, acc: 0.515625]\n",
            "4189: [D loss: 0.088323, acc: 0.960938]  [G loss: 3.616767, acc: 0.320312]\n",
            "4190: [D loss: 0.128651, acc: 0.960938]  [G loss: 3.206624, acc: 0.328125]\n",
            "4191: [D loss: 0.081340, acc: 0.968750]  [G loss: 2.400437, acc: 0.398438]\n",
            "4192: [D loss: 0.047790, acc: 0.984375]  [G loss: 1.774441, acc: 0.507812]\n",
            "4193: [D loss: 0.019157, acc: 0.992188]  [G loss: 1.800390, acc: 0.500000]\n",
            "4194: [D loss: 0.081478, acc: 0.964844]  [G loss: 1.465408, acc: 0.500000]\n",
            "4195: [D loss: 0.036776, acc: 0.988281]  [G loss: 1.807086, acc: 0.445312]\n",
            "4196: [D loss: 0.087684, acc: 0.992188]  [G loss: 2.478891, acc: 0.351562]\n",
            "4197: [D loss: 0.017265, acc: 1.000000]  [G loss: 2.713062, acc: 0.304688]\n",
            "4198: [D loss: 0.017821, acc: 0.992188]  [G loss: 3.769130, acc: 0.210938]\n",
            "4199: [D loss: 0.037841, acc: 0.988281]  [G loss: 4.321480, acc: 0.109375]\n",
            "4200: [D loss: 0.019911, acc: 0.992188]  [G loss: 4.997778, acc: 0.070312]\n",
            "4201: [D loss: 0.097760, acc: 0.980469]  [G loss: 4.610965, acc: 0.117188]\n",
            "4202: [D loss: 0.020209, acc: 0.992188]  [G loss: 3.656802, acc: 0.156250]\n",
            "4203: [D loss: 0.044288, acc: 0.992188]  [G loss: 3.323925, acc: 0.242188]\n",
            "4204: [D loss: 0.043177, acc: 0.976562]  [G loss: 3.575144, acc: 0.195312]\n",
            "4205: [D loss: 0.011072, acc: 0.996094]  [G loss: 4.613952, acc: 0.093750]\n",
            "4206: [D loss: 0.014189, acc: 0.996094]  [G loss: 5.216205, acc: 0.070312]\n",
            "4207: [D loss: 0.066678, acc: 0.984375]  [G loss: 6.568869, acc: 0.031250]\n",
            "4208: [D loss: 0.077799, acc: 0.984375]  [G loss: 5.813055, acc: 0.039062]\n",
            "4209: [D loss: 0.049626, acc: 0.988281]  [G loss: 4.199314, acc: 0.171875]\n",
            "4210: [D loss: 0.076217, acc: 0.976562]  [G loss: 2.789679, acc: 0.296875]\n",
            "4211: [D loss: 0.104070, acc: 0.960938]  [G loss: 3.250549, acc: 0.234375]\n",
            "4212: [D loss: 0.071712, acc: 0.976562]  [G loss: 4.313942, acc: 0.132812]\n",
            "4213: [D loss: 0.021868, acc: 0.996094]  [G loss: 6.051398, acc: 0.046875]\n",
            "4214: [D loss: 0.113368, acc: 0.976562]  [G loss: 5.976369, acc: 0.031250]\n",
            "4215: [D loss: 0.030005, acc: 0.992188]  [G loss: 5.262053, acc: 0.085938]\n",
            "4216: [D loss: 0.058682, acc: 0.980469]  [G loss: 4.255525, acc: 0.117188]\n",
            "4217: [D loss: 0.109911, acc: 0.964844]  [G loss: 3.024527, acc: 0.265625]\n",
            "4218: [D loss: 0.058017, acc: 0.980469]  [G loss: 4.042948, acc: 0.109375]\n",
            "4219: [D loss: 0.029449, acc: 0.992188]  [G loss: 4.970588, acc: 0.101562]\n",
            "4220: [D loss: 0.125958, acc: 0.968750]  [G loss: 5.091829, acc: 0.070312]\n",
            "4221: [D loss: 0.057300, acc: 0.980469]  [G loss: 3.891212, acc: 0.203125]\n",
            "4222: [D loss: 0.080522, acc: 0.968750]  [G loss: 3.163284, acc: 0.265625]\n",
            "4223: [D loss: 0.039564, acc: 0.984375]  [G loss: 3.271718, acc: 0.210938]\n",
            "4224: [D loss: 0.033305, acc: 0.988281]  [G loss: 5.433552, acc: 0.085938]\n",
            "4225: [D loss: 0.124757, acc: 0.960938]  [G loss: 5.220999, acc: 0.062500]\n",
            "4226: [D loss: 0.065057, acc: 0.984375]  [G loss: 4.173291, acc: 0.156250]\n",
            "4227: [D loss: 0.119019, acc: 0.972656]  [G loss: 1.659636, acc: 0.476562]\n",
            "4228: [D loss: 0.119409, acc: 0.968750]  [G loss: 1.675017, acc: 0.437500]\n",
            "4229: [D loss: 0.071575, acc: 0.984375]  [G loss: 3.054885, acc: 0.273438]\n",
            "4230: [D loss: 0.009590, acc: 0.992188]  [G loss: 4.746156, acc: 0.125000]\n",
            "4231: [D loss: 0.067361, acc: 0.992188]  [G loss: 5.473022, acc: 0.164062]\n",
            "4232: [D loss: 0.137025, acc: 0.972656]  [G loss: 3.437253, acc: 0.265625]\n",
            "4233: [D loss: 0.066321, acc: 0.980469]  [G loss: 1.055751, acc: 0.632812]\n",
            "4234: [D loss: 0.073612, acc: 0.972656]  [G loss: 0.167148, acc: 0.921875]\n",
            "4235: [D loss: 0.213424, acc: 0.929688]  [G loss: 1.138869, acc: 0.601562]\n",
            "4236: [D loss: 0.019167, acc: 0.996094]  [G loss: 2.866572, acc: 0.437500]\n",
            "4237: [D loss: 0.079221, acc: 0.980469]  [G loss: 4.076957, acc: 0.328125]\n",
            "4238: [D loss: 0.068316, acc: 0.980469]  [G loss: 4.136244, acc: 0.343750]\n",
            "4239: [D loss: 0.167048, acc: 0.960938]  [G loss: 2.541697, acc: 0.500000]\n",
            "4240: [D loss: 0.110467, acc: 0.960938]  [G loss: 0.600228, acc: 0.804688]\n",
            "4241: [D loss: 0.044599, acc: 0.976562]  [G loss: 0.113241, acc: 0.945312]\n",
            "4242: [D loss: 0.182568, acc: 0.937500]  [G loss: 0.300900, acc: 0.906250]\n",
            "4243: [D loss: 0.033090, acc: 0.984375]  [G loss: 0.814314, acc: 0.750000]\n",
            "4244: [D loss: 0.033192, acc: 0.992188]  [G loss: 1.173581, acc: 0.687500]\n",
            "4245: [D loss: 0.018531, acc: 0.996094]  [G loss: 1.591777, acc: 0.601562]\n",
            "4246: [D loss: 0.129195, acc: 0.968750]  [G loss: 1.143765, acc: 0.695312]\n",
            "4247: [D loss: 0.046853, acc: 0.980469]  [G loss: 0.729786, acc: 0.773438]\n",
            "4248: [D loss: 0.023223, acc: 0.992188]  [G loss: 0.392306, acc: 0.898438]\n",
            "4249: [D loss: 0.047372, acc: 0.980469]  [G loss: 0.252680, acc: 0.914062]\n",
            "4250: [D loss: 0.063291, acc: 0.980469]  [G loss: 0.252291, acc: 0.890625]\n",
            "4251: [D loss: 0.046756, acc: 0.992188]  [G loss: 0.445607, acc: 0.796875]\n",
            "4252: [D loss: 0.025290, acc: 0.996094]  [G loss: 0.516841, acc: 0.789062]\n",
            "4253: [D loss: 0.007238, acc: 0.996094]  [G loss: 0.872369, acc: 0.679688]\n",
            "4254: [D loss: 0.040120, acc: 0.988281]  [G loss: 0.710553, acc: 0.726562]\n",
            "4255: [D loss: 0.048630, acc: 0.988281]  [G loss: 0.551779, acc: 0.734375]\n",
            "4256: [D loss: 0.076162, acc: 0.976562]  [G loss: 0.407382, acc: 0.835938]\n",
            "4257: [D loss: 0.051981, acc: 0.980469]  [G loss: 0.325747, acc: 0.859375]\n",
            "4258: [D loss: 0.060797, acc: 0.988281]  [G loss: 0.136270, acc: 0.937500]\n",
            "4259: [D loss: 0.041401, acc: 0.984375]  [G loss: 0.124913, acc: 0.937500]\n",
            "4260: [D loss: 0.092668, acc: 0.968750]  [G loss: 0.323785, acc: 0.843750]\n",
            "4261: [D loss: 0.023730, acc: 0.992188]  [G loss: 0.735759, acc: 0.718750]\n",
            "4262: [D loss: 0.048292, acc: 0.984375]  [G loss: 0.955242, acc: 0.664062]\n",
            "4263: [D loss: 0.004114, acc: 1.000000]  [G loss: 1.110005, acc: 0.617188]\n",
            "4264: [D loss: 0.044081, acc: 0.984375]  [G loss: 1.710243, acc: 0.476562]\n",
            "4265: [D loss: 0.013268, acc: 0.996094]  [G loss: 1.338919, acc: 0.554688]\n",
            "4266: [D loss: 0.060691, acc: 0.984375]  [G loss: 1.497756, acc: 0.476562]\n",
            "4267: [D loss: 0.037320, acc: 0.984375]  [G loss: 1.037933, acc: 0.578125]\n",
            "4268: [D loss: 0.019640, acc: 0.996094]  [G loss: 0.633390, acc: 0.734375]\n",
            "4269: [D loss: 0.028569, acc: 0.992188]  [G loss: 0.342834, acc: 0.851562]\n",
            "4270: [D loss: 0.023265, acc: 0.996094]  [G loss: 0.521956, acc: 0.820312]\n",
            "4271: [D loss: 0.103394, acc: 0.972656]  [G loss: 0.544592, acc: 0.726562]\n",
            "4272: [D loss: 0.047699, acc: 0.984375]  [G loss: 0.913079, acc: 0.593750]\n",
            "4273: [D loss: 0.014341, acc: 0.996094]  [G loss: 1.439315, acc: 0.492188]\n",
            "4274: [D loss: 0.037980, acc: 0.988281]  [G loss: 1.319866, acc: 0.523438]\n",
            "4275: [D loss: 0.013779, acc: 0.992188]  [G loss: 1.642056, acc: 0.445312]\n",
            "4276: [D loss: 0.009123, acc: 1.000000]  [G loss: 1.517808, acc: 0.437500]\n",
            "4277: [D loss: 0.037722, acc: 0.988281]  [G loss: 1.513515, acc: 0.460938]\n",
            "4278: [D loss: 0.010022, acc: 0.996094]  [G loss: 1.327006, acc: 0.523438]\n",
            "4279: [D loss: 0.020774, acc: 0.992188]  [G loss: 1.100932, acc: 0.593750]\n",
            "4280: [D loss: 0.039146, acc: 0.988281]  [G loss: 0.887956, acc: 0.625000]\n",
            "4281: [D loss: 0.020620, acc: 0.996094]  [G loss: 0.691836, acc: 0.750000]\n",
            "4282: [D loss: 0.078659, acc: 0.984375]  [G loss: 0.480791, acc: 0.789062]\n",
            "4283: [D loss: 0.021925, acc: 0.992188]  [G loss: 0.539486, acc: 0.757812]\n",
            "4284: [D loss: 0.019804, acc: 0.996094]  [G loss: 0.855315, acc: 0.570312]\n",
            "4285: [D loss: 0.016046, acc: 0.992188]  [G loss: 1.135058, acc: 0.593750]\n",
            "4286: [D loss: 0.020834, acc: 0.996094]  [G loss: 1.454467, acc: 0.468750]\n",
            "4287: [D loss: 0.029022, acc: 0.992188]  [G loss: 1.359150, acc: 0.500000]\n",
            "4288: [D loss: 0.013998, acc: 0.992188]  [G loss: 1.035866, acc: 0.570312]\n",
            "4289: [D loss: 0.022785, acc: 0.988281]  [G loss: 0.823315, acc: 0.671875]\n",
            "4290: [D loss: 0.012721, acc: 0.992188]  [G loss: 0.603022, acc: 0.726562]\n",
            "4291: [D loss: 0.013147, acc: 1.000000]  [G loss: 0.570010, acc: 0.773438]\n",
            "4292: [D loss: 0.029489, acc: 0.988281]  [G loss: 0.304517, acc: 0.882812]\n",
            "4293: [D loss: 0.019846, acc: 1.000000]  [G loss: 0.542119, acc: 0.789062]\n",
            "4294: [D loss: 0.029967, acc: 0.988281]  [G loss: 0.769191, acc: 0.750000]\n",
            "4295: [D loss: 0.014013, acc: 0.996094]  [G loss: 0.922801, acc: 0.734375]\n",
            "4296: [D loss: 0.008159, acc: 1.000000]  [G loss: 0.875117, acc: 0.734375]\n",
            "4297: [D loss: 0.017798, acc: 0.992188]  [G loss: 0.471358, acc: 0.804688]\n",
            "4298: [D loss: 0.013053, acc: 1.000000]  [G loss: 0.681079, acc: 0.765625]\n",
            "4299: [D loss: 0.024873, acc: 0.992188]  [G loss: 0.617088, acc: 0.750000]\n",
            "4300: [D loss: 0.018419, acc: 0.988281]  [G loss: 1.209700, acc: 0.601562]\n",
            "4301: [D loss: 0.026971, acc: 0.996094]  [G loss: 0.857244, acc: 0.710938]\n",
            "4302: [D loss: 0.056069, acc: 0.980469]  [G loss: 1.167176, acc: 0.648438]\n",
            "4303: [D loss: 0.028015, acc: 0.988281]  [G loss: 1.233390, acc: 0.625000]\n",
            "4304: [D loss: 0.024299, acc: 0.992188]  [G loss: 1.092109, acc: 0.656250]\n",
            "4305: [D loss: 0.046186, acc: 0.984375]  [G loss: 1.789779, acc: 0.539062]\n",
            "4306: [D loss: 0.023997, acc: 0.996094]  [G loss: 1.772469, acc: 0.476562]\n",
            "4307: [D loss: 0.031569, acc: 0.992188]  [G loss: 2.116490, acc: 0.445312]\n",
            "4308: [D loss: 0.041757, acc: 0.988281]  [G loss: 2.168250, acc: 0.398438]\n",
            "4309: [D loss: 0.015817, acc: 0.996094]  [G loss: 1.904230, acc: 0.437500]\n",
            "4310: [D loss: 0.016112, acc: 0.996094]  [G loss: 2.320619, acc: 0.343750]\n",
            "4311: [D loss: 0.052846, acc: 0.968750]  [G loss: 2.697059, acc: 0.281250]\n",
            "4312: [D loss: 0.025778, acc: 0.988281]  [G loss: 3.762919, acc: 0.125000]\n",
            "4313: [D loss: 0.011541, acc: 0.996094]  [G loss: 3.954796, acc: 0.148438]\n",
            "4314: [D loss: 0.064661, acc: 0.972656]  [G loss: 4.185534, acc: 0.117188]\n",
            "4315: [D loss: 0.035058, acc: 0.992188]  [G loss: 4.220360, acc: 0.078125]\n",
            "4316: [D loss: 0.017264, acc: 0.992188]  [G loss: 3.955477, acc: 0.078125]\n",
            "4317: [D loss: 0.026508, acc: 0.996094]  [G loss: 3.996618, acc: 0.125000]\n",
            "4318: [D loss: 0.030163, acc: 0.988281]  [G loss: 4.701369, acc: 0.070312]\n",
            "4319: [D loss: 0.010480, acc: 1.000000]  [G loss: 4.734645, acc: 0.023438]\n",
            "4320: [D loss: 0.024528, acc: 0.988281]  [G loss: 5.248029, acc: 0.007812]\n",
            "4321: [D loss: 0.050833, acc: 0.992188]  [G loss: 5.151433, acc: 0.015625]\n",
            "4322: [D loss: 0.035683, acc: 0.988281]  [G loss: 5.077190, acc: 0.023438]\n",
            "4323: [D loss: 0.011837, acc: 1.000000]  [G loss: 4.764759, acc: 0.015625]\n",
            "4324: [D loss: 0.007345, acc: 1.000000]  [G loss: 5.061284, acc: 0.023438]\n",
            "4325: [D loss: 0.018742, acc: 0.992188]  [G loss: 5.096722, acc: 0.015625]\n",
            "4326: [D loss: 0.012730, acc: 1.000000]  [G loss: 5.409632, acc: 0.007812]\n",
            "4327: [D loss: 0.010652, acc: 1.000000]  [G loss: 5.430257, acc: 0.007812]\n",
            "4328: [D loss: 0.010786, acc: 1.000000]  [G loss: 6.051994, acc: 0.000000]\n",
            "4329: [D loss: 0.021339, acc: 0.992188]  [G loss: 6.240141, acc: 0.007812]\n",
            "4330: [D loss: 0.063295, acc: 0.984375]  [G loss: 6.100954, acc: 0.015625]\n",
            "4331: [D loss: 0.033458, acc: 0.992188]  [G loss: 5.641624, acc: 0.023438]\n",
            "4332: [D loss: 0.031742, acc: 0.988281]  [G loss: 4.943736, acc: 0.046875]\n",
            "4333: [D loss: 0.019168, acc: 0.992188]  [G loss: 5.161510, acc: 0.093750]\n",
            "4334: [D loss: 0.017894, acc: 0.996094]  [G loss: 5.638119, acc: 0.015625]\n",
            "4335: [D loss: 0.033122, acc: 0.984375]  [G loss: 6.389708, acc: 0.007812]\n",
            "4336: [D loss: 0.014480, acc: 0.996094]  [G loss: 6.452896, acc: 0.007812]\n",
            "4337: [D loss: 0.008680, acc: 0.996094]  [G loss: 6.488182, acc: 0.015625]\n",
            "4338: [D loss: 0.007534, acc: 1.000000]  [G loss: 6.938064, acc: 0.015625]\n",
            "4339: [D loss: 0.054077, acc: 0.984375]  [G loss: 5.650493, acc: 0.039062]\n",
            "4340: [D loss: 0.014497, acc: 0.996094]  [G loss: 4.776146, acc: 0.046875]\n",
            "4341: [D loss: 0.004022, acc: 1.000000]  [G loss: 4.913715, acc: 0.062500]\n",
            "4342: [D loss: 0.008038, acc: 1.000000]  [G loss: 3.968303, acc: 0.125000]\n",
            "4343: [D loss: 0.043091, acc: 0.988281]  [G loss: 3.684356, acc: 0.148438]\n",
            "4344: [D loss: 0.017731, acc: 0.996094]  [G loss: 3.505362, acc: 0.187500]\n",
            "4345: [D loss: 0.004666, acc: 1.000000]  [G loss: 3.509519, acc: 0.226562]\n",
            "4346: [D loss: 0.040848, acc: 0.980469]  [G loss: 4.415079, acc: 0.085938]\n",
            "4347: [D loss: 0.020383, acc: 0.992188]  [G loss: 4.492123, acc: 0.109375]\n",
            "4348: [D loss: 0.034236, acc: 0.988281]  [G loss: 4.564075, acc: 0.148438]\n",
            "4349: [D loss: 0.077517, acc: 0.976562]  [G loss: 3.508067, acc: 0.234375]\n",
            "4350: [D loss: 0.040448, acc: 0.988281]  [G loss: 2.381575, acc: 0.437500]\n",
            "4351: [D loss: 0.053152, acc: 0.976562]  [G loss: 1.194506, acc: 0.664062]\n",
            "4352: [D loss: 0.041582, acc: 0.988281]  [G loss: 0.667443, acc: 0.765625]\n",
            "4353: [D loss: 0.045338, acc: 0.984375]  [G loss: 0.608181, acc: 0.781250]\n",
            "4354: [D loss: 0.029993, acc: 0.980469]  [G loss: 0.713880, acc: 0.734375]\n",
            "4355: [D loss: 0.066535, acc: 0.988281]  [G loss: 0.870739, acc: 0.718750]\n",
            "4356: [D loss: 0.006103, acc: 1.000000]  [G loss: 1.118392, acc: 0.664062]\n",
            "4357: [D loss: 0.015345, acc: 0.992188]  [G loss: 1.279959, acc: 0.632812]\n",
            "4358: [D loss: 0.051482, acc: 0.984375]  [G loss: 1.024441, acc: 0.656250]\n",
            "4359: [D loss: 0.067652, acc: 0.976562]  [G loss: 0.531803, acc: 0.789062]\n",
            "4360: [D loss: 0.031517, acc: 0.980469]  [G loss: 0.193269, acc: 0.929688]\n",
            "4361: [D loss: 0.058608, acc: 0.976562]  [G loss: 0.233634, acc: 0.898438]\n",
            "4362: [D loss: 0.094947, acc: 0.964844]  [G loss: 0.197838, acc: 0.945312]\n",
            "4363: [D loss: 0.057118, acc: 0.976562]  [G loss: 0.504493, acc: 0.843750]\n",
            "4364: [D loss: 0.083213, acc: 0.968750]  [G loss: 0.623983, acc: 0.789062]\n",
            "4365: [D loss: 0.034035, acc: 0.988281]  [G loss: 0.819980, acc: 0.742188]\n",
            "4366: [D loss: 0.052551, acc: 0.988281]  [G loss: 0.742986, acc: 0.773438]\n",
            "4367: [D loss: 0.064412, acc: 0.984375]  [G loss: 0.419929, acc: 0.851562]\n",
            "4368: [D loss: 0.035070, acc: 0.984375]  [G loss: 0.303032, acc: 0.898438]\n",
            "4369: [D loss: 0.039967, acc: 0.984375]  [G loss: 0.184493, acc: 0.890625]\n",
            "4370: [D loss: 0.056199, acc: 0.976562]  [G loss: 0.358738, acc: 0.875000]\n",
            "4371: [D loss: 0.079912, acc: 0.984375]  [G loss: 0.593758, acc: 0.828125]\n",
            "4372: [D loss: 0.036904, acc: 0.996094]  [G loss: 0.523812, acc: 0.843750]\n",
            "4373: [D loss: 0.035000, acc: 0.988281]  [G loss: 0.527875, acc: 0.835938]\n",
            "4374: [D loss: 0.061837, acc: 0.984375]  [G loss: 0.475591, acc: 0.843750]\n",
            "4375: [D loss: 0.015571, acc: 0.996094]  [G loss: 0.492833, acc: 0.812500]\n",
            "4376: [D loss: 0.031386, acc: 0.992188]  [G loss: 0.406479, acc: 0.882812]\n",
            "4377: [D loss: 0.042456, acc: 0.988281]  [G loss: 0.517938, acc: 0.867188]\n",
            "4378: [D loss: 0.028470, acc: 0.988281]  [G loss: 0.673330, acc: 0.843750]\n",
            "4379: [D loss: 0.021336, acc: 0.992188]  [G loss: 0.517491, acc: 0.828125]\n",
            "4380: [D loss: 0.048577, acc: 0.980469]  [G loss: 0.775193, acc: 0.804688]\n",
            "4381: [D loss: 0.042531, acc: 0.988281]  [G loss: 0.924596, acc: 0.773438]\n",
            "4382: [D loss: 0.016656, acc: 0.992188]  [G loss: 0.892692, acc: 0.734375]\n",
            "4383: [D loss: 0.032198, acc: 0.988281]  [G loss: 0.918504, acc: 0.765625]\n",
            "4384: [D loss: 0.020415, acc: 0.992188]  [G loss: 0.919767, acc: 0.750000]\n",
            "4385: [D loss: 0.027237, acc: 0.992188]  [G loss: 0.879488, acc: 0.734375]\n",
            "4386: [D loss: 0.011364, acc: 0.996094]  [G loss: 0.979791, acc: 0.695312]\n",
            "4387: [D loss: 0.041026, acc: 0.984375]  [G loss: 1.248682, acc: 0.671875]\n",
            "4388: [D loss: 0.020636, acc: 0.992188]  [G loss: 1.429019, acc: 0.640625]\n",
            "4389: [D loss: 0.043597, acc: 0.984375]  [G loss: 1.557456, acc: 0.601562]\n",
            "4390: [D loss: 0.023013, acc: 0.988281]  [G loss: 2.025838, acc: 0.523438]\n",
            "4391: [D loss: 0.027880, acc: 0.992188]  [G loss: 2.538728, acc: 0.468750]\n",
            "4392: [D loss: 0.071376, acc: 0.980469]  [G loss: 1.596317, acc: 0.617188]\n",
            "4393: [D loss: 0.008931, acc: 0.996094]  [G loss: 1.119663, acc: 0.679688]\n",
            "4394: [D loss: 0.030926, acc: 0.988281]  [G loss: 1.092017, acc: 0.679688]\n",
            "4395: [D loss: 0.061815, acc: 0.972656]  [G loss: 1.740702, acc: 0.578125]\n",
            "4396: [D loss: 0.080254, acc: 0.976562]  [G loss: 2.541068, acc: 0.421875]\n",
            "4397: [D loss: 0.023806, acc: 0.984375]  [G loss: 2.480633, acc: 0.507812]\n",
            "4398: [D loss: 0.033044, acc: 0.980469]  [G loss: 2.167745, acc: 0.585938]\n",
            "4399: [D loss: 0.042958, acc: 0.980469]  [G loss: 1.794667, acc: 0.617188]\n",
            "4400: [D loss: 0.057910, acc: 0.972656]  [G loss: 2.603734, acc: 0.507812]\n",
            "4401: [D loss: 0.046666, acc: 0.984375]  [G loss: 2.800906, acc: 0.421875]\n",
            "4402: [D loss: 0.016688, acc: 0.992188]  [G loss: 3.360494, acc: 0.265625]\n",
            "4403: [D loss: 0.046437, acc: 0.980469]  [G loss: 3.191357, acc: 0.328125]\n",
            "4404: [D loss: 0.029650, acc: 0.984375]  [G loss: 3.318838, acc: 0.367188]\n",
            "4405: [D loss: 0.026592, acc: 0.988281]  [G loss: 2.383729, acc: 0.476562]\n",
            "4406: [D loss: 0.008668, acc: 1.000000]  [G loss: 2.010418, acc: 0.554688]\n",
            "4407: [D loss: 0.052136, acc: 0.988281]  [G loss: 2.334079, acc: 0.468750]\n",
            "4408: [D loss: 0.125447, acc: 0.957031]  [G loss: 4.470681, acc: 0.242188]\n",
            "4409: [D loss: 0.069228, acc: 0.988281]  [G loss: 6.064070, acc: 0.109375]\n",
            "4410: [D loss: 0.172095, acc: 0.960938]  [G loss: 4.359206, acc: 0.226562]\n",
            "4411: [D loss: 0.057489, acc: 0.988281]  [G loss: 2.413302, acc: 0.546875]\n",
            "4412: [D loss: 0.100610, acc: 0.960938]  [G loss: 1.971566, acc: 0.679688]\n",
            "4413: [D loss: 0.144743, acc: 0.941406]  [G loss: 3.180897, acc: 0.351562]\n",
            "4414: [D loss: 0.013355, acc: 1.000000]  [G loss: 5.254612, acc: 0.101562]\n",
            "4415: [D loss: 0.067503, acc: 0.976562]  [G loss: 6.506901, acc: 0.023438]\n",
            "4416: [D loss: 0.106351, acc: 0.976562]  [G loss: 5.628649, acc: 0.132812]\n",
            "4417: [D loss: 0.101208, acc: 0.960938]  [G loss: 3.173170, acc: 0.335938]\n",
            "4418: [D loss: 0.086140, acc: 0.960938]  [G loss: 2.263540, acc: 0.421875]\n",
            "4419: [D loss: 0.114010, acc: 0.949219]  [G loss: 2.778074, acc: 0.398438]\n",
            "4420: [D loss: 0.104126, acc: 0.960938]  [G loss: 4.278934, acc: 0.195312]\n",
            "4421: [D loss: 0.077206, acc: 0.972656]  [G loss: 6.567414, acc: 0.070312]\n",
            "4422: [D loss: 0.111511, acc: 0.968750]  [G loss: 5.888113, acc: 0.085938]\n",
            "4423: [D loss: 0.021551, acc: 0.996094]  [G loss: 5.859543, acc: 0.109375]\n",
            "4424: [D loss: 0.021678, acc: 0.988281]  [G loss: 4.758856, acc: 0.179688]\n",
            "4425: [D loss: 0.025867, acc: 0.988281]  [G loss: 3.874423, acc: 0.273438]\n",
            "4426: [D loss: 0.084191, acc: 0.968750]  [G loss: 3.332072, acc: 0.375000]\n",
            "4427: [D loss: 0.024742, acc: 0.988281]  [G loss: 2.598506, acc: 0.484375]\n",
            "4428: [D loss: 0.020434, acc: 0.996094]  [G loss: 2.845044, acc: 0.382812]\n",
            "4429: [D loss: 0.037684, acc: 0.980469]  [G loss: 2.821363, acc: 0.398438]\n",
            "4430: [D loss: 0.031089, acc: 0.992188]  [G loss: 2.736734, acc: 0.406250]\n",
            "4431: [D loss: 0.062632, acc: 0.976562]  [G loss: 2.406953, acc: 0.453125]\n",
            "4432: [D loss: 0.054828, acc: 0.992188]  [G loss: 2.321471, acc: 0.468750]\n",
            "4433: [D loss: 0.047644, acc: 0.980469]  [G loss: 2.185875, acc: 0.507812]\n",
            "4434: [D loss: 0.033710, acc: 0.984375]  [G loss: 2.123298, acc: 0.617188]\n",
            "4435: [D loss: 0.061453, acc: 0.976562]  [G loss: 2.582127, acc: 0.429688]\n",
            "4436: [D loss: 0.022260, acc: 0.992188]  [G loss: 2.567045, acc: 0.484375]\n",
            "4437: [D loss: 0.040166, acc: 0.988281]  [G loss: 3.431088, acc: 0.406250]\n",
            "4438: [D loss: 0.041595, acc: 0.980469]  [G loss: 3.202448, acc: 0.382812]\n",
            "4439: [D loss: 0.038089, acc: 0.992188]  [G loss: 3.789993, acc: 0.289062]\n",
            "4440: [D loss: 0.076284, acc: 0.980469]  [G loss: 4.178081, acc: 0.234375]\n",
            "4441: [D loss: 0.047167, acc: 0.988281]  [G loss: 4.608378, acc: 0.210938]\n",
            "4442: [D loss: 0.016593, acc: 0.996094]  [G loss: 4.967219, acc: 0.164062]\n",
            "4443: [D loss: 0.021340, acc: 0.992188]  [G loss: 4.895266, acc: 0.195312]\n",
            "4444: [D loss: 0.061004, acc: 0.972656]  [G loss: 4.918252, acc: 0.218750]\n",
            "4445: [D loss: 0.092960, acc: 0.960938]  [G loss: 5.958135, acc: 0.179688]\n",
            "4446: [D loss: 0.038635, acc: 0.992188]  [G loss: 6.651903, acc: 0.117188]\n",
            "4447: [D loss: 0.036834, acc: 0.988281]  [G loss: 6.548457, acc: 0.125000]\n",
            "4448: [D loss: 0.050568, acc: 0.984375]  [G loss: 6.650491, acc: 0.125000]\n",
            "4449: [D loss: 0.050206, acc: 0.984375]  [G loss: 5.981498, acc: 0.156250]\n",
            "4450: [D loss: 0.058391, acc: 0.980469]  [G loss: 4.675968, acc: 0.226562]\n",
            "4451: [D loss: 0.054999, acc: 0.984375]  [G loss: 4.402528, acc: 0.234375]\n",
            "4452: [D loss: 0.037077, acc: 0.984375]  [G loss: 4.534803, acc: 0.265625]\n",
            "4453: [D loss: 0.031134, acc: 0.984375]  [G loss: 4.801168, acc: 0.203125]\n",
            "4454: [D loss: 0.036214, acc: 0.988281]  [G loss: 4.954650, acc: 0.210938]\n",
            "4455: [D loss: 0.016320, acc: 0.996094]  [G loss: 5.309633, acc: 0.210938]\n",
            "4456: [D loss: 0.026949, acc: 0.992188]  [G loss: 4.965713, acc: 0.242188]\n",
            "4457: [D loss: 0.039809, acc: 0.984375]  [G loss: 4.749992, acc: 0.257812]\n",
            "4458: [D loss: 0.020900, acc: 0.992188]  [G loss: 5.158929, acc: 0.250000]\n",
            "4459: [D loss: 0.009434, acc: 1.000000]  [G loss: 5.538507, acc: 0.187500]\n",
            "4460: [D loss: 0.024521, acc: 0.988281]  [G loss: 6.469639, acc: 0.109375]\n",
            "4461: [D loss: 0.011070, acc: 0.996094]  [G loss: 7.500052, acc: 0.054688]\n",
            "4462: [D loss: 0.061891, acc: 0.984375]  [G loss: 7.023526, acc: 0.101562]\n",
            "4463: [D loss: 0.059013, acc: 0.996094]  [G loss: 7.433539, acc: 0.085938]\n",
            "4464: [D loss: 0.030897, acc: 0.996094]  [G loss: 6.066077, acc: 0.148438]\n",
            "4465: [D loss: 0.073860, acc: 0.976562]  [G loss: 4.268074, acc: 0.273438]\n",
            "4466: [D loss: 0.034137, acc: 0.988281]  [G loss: 4.738201, acc: 0.218750]\n",
            "4467: [D loss: 0.084049, acc: 0.980469]  [G loss: 5.289426, acc: 0.195312]\n",
            "4468: [D loss: 0.024127, acc: 0.992188]  [G loss: 6.649180, acc: 0.171875]\n",
            "4469: [D loss: 0.046578, acc: 0.988281]  [G loss: 6.658331, acc: 0.179688]\n",
            "4470: [D loss: 0.021169, acc: 0.992188]  [G loss: 6.609521, acc: 0.125000]\n",
            "4471: [D loss: 0.044510, acc: 0.992188]  [G loss: 5.214490, acc: 0.226562]\n",
            "4472: [D loss: 0.037632, acc: 0.996094]  [G loss: 3.785380, acc: 0.351562]\n",
            "4473: [D loss: 0.134351, acc: 0.976562]  [G loss: 2.296721, acc: 0.398438]\n",
            "4474: [D loss: 0.092392, acc: 0.968750]  [G loss: 4.516305, acc: 0.281250]\n",
            "4475: [D loss: 0.033385, acc: 0.996094]  [G loss: 5.241931, acc: 0.242188]\n",
            "4476: [D loss: 0.124031, acc: 0.976562]  [G loss: 5.237191, acc: 0.257812]\n",
            "4477: [D loss: 0.016429, acc: 0.992188]  [G loss: 5.297781, acc: 0.226562]\n",
            "4478: [D loss: 0.104562, acc: 0.972656]  [G loss: 4.599027, acc: 0.257812]\n",
            "4479: [D loss: 0.038919, acc: 0.996094]  [G loss: 3.388334, acc: 0.367188]\n",
            "4480: [D loss: 0.065312, acc: 0.976562]  [G loss: 2.761976, acc: 0.390625]\n",
            "4481: [D loss: 0.085928, acc: 0.972656]  [G loss: 2.847987, acc: 0.453125]\n",
            "4482: [D loss: 0.017294, acc: 0.996094]  [G loss: 2.625496, acc: 0.484375]\n",
            "4483: [D loss: 0.015089, acc: 0.996094]  [G loss: 2.237292, acc: 0.515625]\n",
            "4484: [D loss: 0.022578, acc: 0.992188]  [G loss: 2.089363, acc: 0.531250]\n",
            "4485: [D loss: 0.016627, acc: 0.992188]  [G loss: 2.320402, acc: 0.539062]\n",
            "4486: [D loss: 0.019709, acc: 0.988281]  [G loss: 2.586839, acc: 0.453125]\n",
            "4487: [D loss: 0.009654, acc: 0.992188]  [G loss: 2.285674, acc: 0.453125]\n",
            "4488: [D loss: 0.026456, acc: 0.992188]  [G loss: 1.697712, acc: 0.562500]\n",
            "4489: [D loss: 0.074943, acc: 0.980469]  [G loss: 0.515246, acc: 0.789062]\n",
            "4490: [D loss: 0.052361, acc: 0.980469]  [G loss: 0.300812, acc: 0.859375]\n",
            "4491: [D loss: 0.058672, acc: 0.976562]  [G loss: 0.544155, acc: 0.812500]\n",
            "4492: [D loss: 0.050151, acc: 0.976562]  [G loss: 1.330849, acc: 0.640625]\n",
            "4493: [D loss: 0.044379, acc: 0.984375]  [G loss: 1.352766, acc: 0.578125]\n",
            "4494: [D loss: 0.098858, acc: 0.976562]  [G loss: 0.664453, acc: 0.757812]\n",
            "4495: [D loss: 0.108067, acc: 0.972656]  [G loss: 0.147238, acc: 0.953125]\n",
            "4496: [D loss: 0.102581, acc: 0.957031]  [G loss: 0.289875, acc: 0.882812]\n",
            "4497: [D loss: 0.039338, acc: 0.988281]  [G loss: 0.716828, acc: 0.773438]\n",
            "4498: [D loss: 0.021323, acc: 0.988281]  [G loss: 0.918824, acc: 0.640625]\n",
            "4499: [D loss: 0.027352, acc: 0.992188]  [G loss: 0.889724, acc: 0.695312]\n",
            "4500: [D loss: 0.107776, acc: 0.980469]  [G loss: 0.368359, acc: 0.828125]\n",
            "4501: [D loss: 0.016741, acc: 0.992188]  [G loss: 0.144994, acc: 0.945312]\n",
            "4502: [D loss: 0.062869, acc: 0.980469]  [G loss: 0.049096, acc: 0.976562]\n",
            "4503: [D loss: 0.089447, acc: 0.957031]  [G loss: 0.254329, acc: 0.906250]\n",
            "4504: [D loss: 0.059708, acc: 0.996094]  [G loss: 0.757629, acc: 0.687500]\n",
            "4505: [D loss: 0.010322, acc: 0.996094]  [G loss: 1.204308, acc: 0.484375]\n",
            "4506: [D loss: 0.179007, acc: 0.964844]  [G loss: 0.560400, acc: 0.765625]\n",
            "4507: [D loss: 0.014482, acc: 0.992188]  [G loss: 0.255832, acc: 0.898438]\n",
            "4508: [D loss: 0.016689, acc: 0.992188]  [G loss: 0.155663, acc: 0.921875]\n",
            "4509: [D loss: 0.020103, acc: 0.992188]  [G loss: 0.129684, acc: 0.960938]\n",
            "4510: [D loss: 0.037630, acc: 0.984375]  [G loss: 0.200851, acc: 0.929688]\n",
            "4511: [D loss: 0.021728, acc: 0.996094]  [G loss: 0.368661, acc: 0.851562]\n",
            "4512: [D loss: 0.033487, acc: 0.992188]  [G loss: 0.399876, acc: 0.835938]\n",
            "4513: [D loss: 0.018416, acc: 0.996094]  [G loss: 0.336655, acc: 0.835938]\n",
            "4514: [D loss: 0.012668, acc: 0.996094]  [G loss: 0.289770, acc: 0.867188]\n",
            "4515: [D loss: 0.032539, acc: 0.988281]  [G loss: 0.275674, acc: 0.859375]\n",
            "4516: [D loss: 0.041818, acc: 0.992188]  [G loss: 0.135634, acc: 0.960938]\n",
            "4517: [D loss: 0.016067, acc: 0.992188]  [G loss: 0.029154, acc: 0.992188]\n",
            "4518: [D loss: 0.020145, acc: 0.992188]  [G loss: 0.070565, acc: 0.968750]\n",
            "4519: [D loss: 0.010104, acc: 1.000000]  [G loss: 0.136247, acc: 0.953125]\n",
            "4520: [D loss: 0.010821, acc: 0.996094]  [G loss: 0.207326, acc: 0.921875]\n",
            "4521: [D loss: 0.015363, acc: 0.992188]  [G loss: 0.280392, acc: 0.859375]\n",
            "4522: [D loss: 0.040216, acc: 0.976562]  [G loss: 0.410589, acc: 0.828125]\n",
            "4523: [D loss: 0.058033, acc: 0.988281]  [G loss: 0.494770, acc: 0.828125]\n",
            "4524: [D loss: 0.035754, acc: 0.988281]  [G loss: 0.364692, acc: 0.843750]\n",
            "4525: [D loss: 0.009929, acc: 0.996094]  [G loss: 0.455443, acc: 0.851562]\n",
            "4526: [D loss: 0.008615, acc: 1.000000]  [G loss: 0.442434, acc: 0.828125]\n",
            "4527: [D loss: 0.006702, acc: 1.000000]  [G loss: 0.260479, acc: 0.882812]\n",
            "4528: [D loss: 0.019194, acc: 0.996094]  [G loss: 0.195634, acc: 0.898438]\n",
            "4529: [D loss: 0.011612, acc: 1.000000]  [G loss: 0.262880, acc: 0.859375]\n",
            "4530: [D loss: 0.022130, acc: 0.992188]  [G loss: 0.158748, acc: 0.914062]\n",
            "4531: [D loss: 0.012310, acc: 1.000000]  [G loss: 0.241280, acc: 0.914062]\n",
            "4532: [D loss: 0.019738, acc: 0.992188]  [G loss: 0.190456, acc: 0.937500]\n",
            "4533: [D loss: 0.022679, acc: 0.996094]  [G loss: 0.183425, acc: 0.929688]\n",
            "4534: [D loss: 0.011953, acc: 0.996094]  [G loss: 0.244419, acc: 0.906250]\n",
            "4535: [D loss: 0.033253, acc: 0.988281]  [G loss: 0.259599, acc: 0.890625]\n",
            "4536: [D loss: 0.018381, acc: 0.988281]  [G loss: 0.336489, acc: 0.859375]\n",
            "4537: [D loss: 0.007446, acc: 1.000000]  [G loss: 0.594736, acc: 0.796875]\n",
            "4538: [D loss: 0.010704, acc: 0.996094]  [G loss: 0.591756, acc: 0.789062]\n",
            "4539: [D loss: 0.018436, acc: 0.996094]  [G loss: 0.617902, acc: 0.859375]\n",
            "4540: [D loss: 0.022610, acc: 0.992188]  [G loss: 0.483953, acc: 0.851562]\n",
            "4541: [D loss: 0.031435, acc: 0.996094]  [G loss: 0.254770, acc: 0.898438]\n",
            "4542: [D loss: 0.012591, acc: 0.996094]  [G loss: 0.082153, acc: 0.968750]\n",
            "4543: [D loss: 0.024295, acc: 0.988281]  [G loss: 0.146034, acc: 0.945312]\n",
            "4544: [D loss: 0.020844, acc: 0.996094]  [G loss: 0.159747, acc: 0.921875]\n",
            "4545: [D loss: 0.028994, acc: 0.992188]  [G loss: 0.129064, acc: 0.945312]\n",
            "4546: [D loss: 0.036175, acc: 0.992188]  [G loss: 0.308679, acc: 0.906250]\n",
            "4547: [D loss: 0.059529, acc: 0.984375]  [G loss: 0.327100, acc: 0.882812]\n",
            "4548: [D loss: 0.042751, acc: 0.984375]  [G loss: 0.066201, acc: 0.976562]\n",
            "4549: [D loss: 0.032824, acc: 0.996094]  [G loss: 0.146818, acc: 0.937500]\n",
            "4550: [D loss: 0.026838, acc: 0.992188]  [G loss: 0.097700, acc: 0.968750]\n",
            "4551: [D loss: 0.020858, acc: 0.992188]  [G loss: 0.329530, acc: 0.882812]\n",
            "4552: [D loss: 0.057054, acc: 0.996094]  [G loss: 0.513793, acc: 0.851562]\n",
            "4553: [D loss: 0.005501, acc: 1.000000]  [G loss: 0.672786, acc: 0.757812]\n",
            "4554: [D loss: 0.007046, acc: 1.000000]  [G loss: 0.952786, acc: 0.679688]\n",
            "4555: [D loss: 0.006991, acc: 1.000000]  [G loss: 1.096998, acc: 0.687500]\n",
            "4556: [D loss: 0.003361, acc: 1.000000]  [G loss: 1.368113, acc: 0.570312]\n",
            "4557: [D loss: 0.027204, acc: 0.988281]  [G loss: 0.918936, acc: 0.695312]\n",
            "4558: [D loss: 0.003591, acc: 1.000000]  [G loss: 1.184351, acc: 0.632812]\n",
            "4559: [D loss: 0.007022, acc: 1.000000]  [G loss: 0.860569, acc: 0.742188]\n",
            "4560: [D loss: 0.009791, acc: 1.000000]  [G loss: 0.693544, acc: 0.781250]\n",
            "4561: [D loss: 0.007368, acc: 1.000000]  [G loss: 0.744791, acc: 0.734375]\n",
            "4562: [D loss: 0.009030, acc: 0.992188]  [G loss: 0.523754, acc: 0.796875]\n",
            "4563: [D loss: 0.014227, acc: 0.996094]  [G loss: 0.644603, acc: 0.750000]\n",
            "4564: [D loss: 0.008339, acc: 0.996094]  [G loss: 0.728718, acc: 0.757812]\n",
            "4565: [D loss: 0.021599, acc: 0.996094]  [G loss: 0.831207, acc: 0.718750]\n",
            "4566: [D loss: 0.010249, acc: 1.000000]  [G loss: 0.887299, acc: 0.687500]\n",
            "4567: [D loss: 0.004942, acc: 1.000000]  [G loss: 1.039206, acc: 0.703125]\n",
            "4568: [D loss: 0.021269, acc: 0.988281]  [G loss: 1.168291, acc: 0.625000]\n",
            "4569: [D loss: 0.030856, acc: 0.988281]  [G loss: 1.206244, acc: 0.648438]\n",
            "4570: [D loss: 0.055813, acc: 0.984375]  [G loss: 1.241572, acc: 0.664062]\n",
            "4571: [D loss: 0.009963, acc: 0.992188]  [G loss: 1.378150, acc: 0.593750]\n",
            "4572: [D loss: 0.017678, acc: 0.992188]  [G loss: 1.349876, acc: 0.640625]\n",
            "4573: [D loss: 0.011270, acc: 0.996094]  [G loss: 1.302846, acc: 0.679688]\n",
            "4574: [D loss: 0.027729, acc: 0.984375]  [G loss: 1.382880, acc: 0.640625]\n",
            "4575: [D loss: 0.010253, acc: 0.996094]  [G loss: 1.682342, acc: 0.593750]\n",
            "4576: [D loss: 0.010811, acc: 1.000000]  [G loss: 1.820644, acc: 0.554688]\n",
            "4577: [D loss: 0.026194, acc: 0.996094]  [G loss: 1.755231, acc: 0.570312]\n",
            "4578: [D loss: 0.012510, acc: 1.000000]  [G loss: 1.551699, acc: 0.593750]\n",
            "4579: [D loss: 0.015755, acc: 0.996094]  [G loss: 2.023708, acc: 0.554688]\n",
            "4580: [D loss: 0.017061, acc: 0.996094]  [G loss: 1.826159, acc: 0.531250]\n",
            "4581: [D loss: 0.012373, acc: 0.996094]  [G loss: 1.590244, acc: 0.656250]\n",
            "4582: [D loss: 0.006192, acc: 1.000000]  [G loss: 1.671383, acc: 0.640625]\n",
            "4583: [D loss: 0.036253, acc: 0.988281]  [G loss: 1.424343, acc: 0.664062]\n",
            "4584: [D loss: 0.019571, acc: 0.992188]  [G loss: 1.710788, acc: 0.593750]\n",
            "4585: [D loss: 0.031766, acc: 0.984375]  [G loss: 1.843638, acc: 0.617188]\n",
            "4586: [D loss: 0.007941, acc: 0.996094]  [G loss: 2.016021, acc: 0.578125]\n",
            "4587: [D loss: 0.012696, acc: 1.000000]  [G loss: 1.787665, acc: 0.601562]\n",
            "4588: [D loss: 0.041919, acc: 0.984375]  [G loss: 2.369481, acc: 0.523438]\n",
            "4589: [D loss: 0.042264, acc: 0.984375]  [G loss: 1.916066, acc: 0.578125]\n",
            "4590: [D loss: 0.049065, acc: 0.984375]  [G loss: 1.758349, acc: 0.539062]\n",
            "4591: [D loss: 0.027977, acc: 0.988281]  [G loss: 1.926331, acc: 0.601562]\n",
            "4592: [D loss: 0.078269, acc: 0.976562]  [G loss: 1.742048, acc: 0.585938]\n",
            "4593: [D loss: 0.022613, acc: 0.996094]  [G loss: 1.823376, acc: 0.562500]\n",
            "4594: [D loss: 0.051678, acc: 0.980469]  [G loss: 2.165241, acc: 0.468750]\n",
            "4595: [D loss: 0.035417, acc: 0.976562]  [G loss: 2.896471, acc: 0.351562]\n",
            "4596: [D loss: 0.020633, acc: 0.988281]  [G loss: 3.491808, acc: 0.335938]\n",
            "4597: [D loss: 0.070842, acc: 0.968750]  [G loss: 3.201383, acc: 0.367188]\n",
            "4598: [D loss: 0.039881, acc: 0.980469]  [G loss: 2.331329, acc: 0.523438]\n",
            "4599: [D loss: 0.060434, acc: 0.980469]  [G loss: 2.581717, acc: 0.484375]\n",
            "4600: [D loss: 0.079653, acc: 0.968750]  [G loss: 4.565856, acc: 0.210938]\n",
            "4601: [D loss: 0.040730, acc: 0.984375]  [G loss: 5.918531, acc: 0.085938]\n",
            "4602: [D loss: 0.216120, acc: 0.921875]  [G loss: 4.682893, acc: 0.171875]\n",
            "4603: [D loss: 0.017243, acc: 0.996094]  [G loss: 2.961198, acc: 0.257812]\n",
            "4604: [D loss: 0.033173, acc: 0.996094]  [G loss: 2.607284, acc: 0.390625]\n",
            "4605: [D loss: 0.145176, acc: 0.941406]  [G loss: 4.527107, acc: 0.109375]\n",
            "4606: [D loss: 0.058806, acc: 0.980469]  [G loss: 5.781813, acc: 0.039062]\n",
            "4607: [D loss: 0.033557, acc: 0.988281]  [G loss: 6.202760, acc: 0.039062]\n",
            "4608: [D loss: 0.162166, acc: 0.960938]  [G loss: 5.265674, acc: 0.070312]\n",
            "4609: [D loss: 0.065220, acc: 0.968750]  [G loss: 2.866806, acc: 0.257812]\n",
            "4610: [D loss: 0.029747, acc: 0.988281]  [G loss: 1.694721, acc: 0.476562]\n",
            "4611: [D loss: 0.204113, acc: 0.917969]  [G loss: 3.637880, acc: 0.140625]\n",
            "4612: [D loss: 0.044680, acc: 0.984375]  [G loss: 6.702264, acc: 0.023438]\n",
            "4613: [D loss: 0.022358, acc: 0.996094]  [G loss: 7.932267, acc: 0.007812]\n",
            "4614: [D loss: 0.105240, acc: 0.972656]  [G loss: 7.996210, acc: 0.000000]\n",
            "4615: [D loss: 0.067701, acc: 0.976562]  [G loss: 7.273278, acc: 0.000000]\n",
            "4616: [D loss: 0.111898, acc: 0.980469]  [G loss: 6.819737, acc: 0.015625]\n",
            "4617: [D loss: 0.041248, acc: 0.980469]  [G loss: 4.871946, acc: 0.093750]\n",
            "4618: [D loss: 0.076816, acc: 0.953125]  [G loss: 4.969544, acc: 0.070312]\n",
            "4619: [D loss: 0.058409, acc: 0.972656]  [G loss: 5.421514, acc: 0.031250]\n",
            "4620: [D loss: 0.024047, acc: 0.992188]  [G loss: 5.497914, acc: 0.031250]\n",
            "4621: [D loss: 0.022396, acc: 0.996094]  [G loss: 5.215069, acc: 0.031250]\n",
            "4622: [D loss: 0.067921, acc: 0.972656]  [G loss: 5.490805, acc: 0.070312]\n",
            "4623: [D loss: 0.045141, acc: 0.984375]  [G loss: 5.573586, acc: 0.101562]\n",
            "4624: [D loss: 0.043187, acc: 0.988281]  [G loss: 4.677680, acc: 0.156250]\n",
            "4625: [D loss: 0.023647, acc: 0.992188]  [G loss: 4.398693, acc: 0.164062]\n",
            "4626: [D loss: 0.062969, acc: 0.972656]  [G loss: 3.820410, acc: 0.289062]\n",
            "4627: [D loss: 0.055039, acc: 0.984375]  [G loss: 3.795888, acc: 0.281250]\n",
            "4628: [D loss: 0.059690, acc: 0.988281]  [G loss: 4.210779, acc: 0.242188]\n",
            "4629: [D loss: 0.024810, acc: 0.988281]  [G loss: 4.495318, acc: 0.250000]\n",
            "4630: [D loss: 0.048379, acc: 0.976562]  [G loss: 4.645443, acc: 0.195312]\n",
            "4631: [D loss: 0.035168, acc: 0.988281]  [G loss: 4.637146, acc: 0.265625]\n",
            "4632: [D loss: 0.114658, acc: 0.957031]  [G loss: 3.874452, acc: 0.296875]\n",
            "4633: [D loss: 0.075231, acc: 0.976562]  [G loss: 3.406549, acc: 0.382812]\n",
            "4634: [D loss: 0.138642, acc: 0.972656]  [G loss: 3.513832, acc: 0.390625]\n",
            "4635: [D loss: 0.078386, acc: 0.976562]  [G loss: 3.188599, acc: 0.406250]\n",
            "4636: [D loss: 0.031915, acc: 0.992188]  [G loss: 3.045684, acc: 0.406250]\n",
            "4637: [D loss: 0.049125, acc: 0.988281]  [G loss: 3.161573, acc: 0.421875]\n",
            "4638: [D loss: 0.174035, acc: 0.937500]  [G loss: 2.695328, acc: 0.492188]\n",
            "4639: [D loss: 0.081945, acc: 0.972656]  [G loss: 1.900166, acc: 0.625000]\n",
            "4640: [D loss: 0.104868, acc: 0.960938]  [G loss: 2.195339, acc: 0.507812]\n",
            "4641: [D loss: 0.045450, acc: 0.980469]  [G loss: 2.594636, acc: 0.507812]\n",
            "4642: [D loss: 0.099179, acc: 0.976562]  [G loss: 2.475755, acc: 0.476562]\n",
            "4643: [D loss: 0.075127, acc: 0.960938]  [G loss: 1.880813, acc: 0.562500]\n",
            "4644: [D loss: 0.086157, acc: 0.972656]  [G loss: 1.368719, acc: 0.617188]\n",
            "4645: [D loss: 0.136062, acc: 0.960938]  [G loss: 1.205226, acc: 0.687500]\n",
            "4646: [D loss: 0.077736, acc: 0.964844]  [G loss: 1.422328, acc: 0.601562]\n",
            "4647: [D loss: 0.061651, acc: 0.976562]  [G loss: 1.564844, acc: 0.601562]\n",
            "4648: [D loss: 0.061013, acc: 0.980469]  [G loss: 1.468762, acc: 0.585938]\n",
            "4649: [D loss: 0.074475, acc: 0.980469]  [G loss: 1.125221, acc: 0.640625]\n",
            "4650: [D loss: 0.069788, acc: 0.964844]  [G loss: 1.249788, acc: 0.710938]\n",
            "4651: [D loss: 0.081306, acc: 0.972656]  [G loss: 0.763955, acc: 0.796875]\n",
            "4652: [D loss: 0.111295, acc: 0.972656]  [G loss: 0.786843, acc: 0.765625]\n",
            "4653: [D loss: 0.089111, acc: 0.972656]  [G loss: 0.562461, acc: 0.835938]\n",
            "4654: [D loss: 0.065321, acc: 0.968750]  [G loss: 0.605292, acc: 0.843750]\n",
            "4655: [D loss: 0.044316, acc: 0.976562]  [G loss: 0.776186, acc: 0.789062]\n",
            "4656: [D loss: 0.022788, acc: 0.996094]  [G loss: 1.073882, acc: 0.757812]\n",
            "4657: [D loss: 0.027844, acc: 0.988281]  [G loss: 1.216011, acc: 0.687500]\n",
            "4658: [D loss: 0.070429, acc: 0.972656]  [G loss: 1.140558, acc: 0.695312]\n",
            "4659: [D loss: 0.066130, acc: 0.968750]  [G loss: 0.794635, acc: 0.710938]\n",
            "4660: [D loss: 0.079843, acc: 0.976562]  [G loss: 0.631164, acc: 0.804688]\n",
            "4661: [D loss: 0.068435, acc: 0.980469]  [G loss: 0.816159, acc: 0.742188]\n",
            "4662: [D loss: 0.064253, acc: 0.976562]  [G loss: 0.734338, acc: 0.750000]\n",
            "4663: [D loss: 0.065030, acc: 0.968750]  [G loss: 1.255997, acc: 0.695312]\n",
            "4664: [D loss: 0.024899, acc: 0.988281]  [G loss: 1.630745, acc: 0.632812]\n",
            "4665: [D loss: 0.095837, acc: 0.972656]  [G loss: 1.491473, acc: 0.562500]\n",
            "4666: [D loss: 0.074720, acc: 0.972656]  [G loss: 1.003431, acc: 0.703125]\n",
            "4667: [D loss: 0.078553, acc: 0.972656]  [G loss: 0.497435, acc: 0.812500]\n",
            "4668: [D loss: 0.125456, acc: 0.960938]  [G loss: 0.454084, acc: 0.835938]\n",
            "4669: [D loss: 0.102172, acc: 0.976562]  [G loss: 0.517296, acc: 0.828125]\n",
            "4670: [D loss: 0.136812, acc: 0.945312]  [G loss: 1.077113, acc: 0.750000]\n",
            "4671: [D loss: 0.147979, acc: 0.960938]  [G loss: 1.201646, acc: 0.687500]\n",
            "4672: [D loss: 0.100209, acc: 0.957031]  [G loss: 1.157648, acc: 0.695312]\n",
            "4673: [D loss: 0.078259, acc: 0.972656]  [G loss: 0.968714, acc: 0.812500]\n",
            "4674: [D loss: 0.062772, acc: 0.980469]  [G loss: 0.816444, acc: 0.820312]\n",
            "4675: [D loss: 0.089970, acc: 0.964844]  [G loss: 0.987177, acc: 0.750000]\n",
            "4676: [D loss: 0.088705, acc: 0.976562]  [G loss: 1.576516, acc: 0.671875]\n",
            "4677: [D loss: 0.055554, acc: 0.976562]  [G loss: 2.015401, acc: 0.500000]\n",
            "4678: [D loss: 0.085551, acc: 0.976562]  [G loss: 2.351621, acc: 0.476562]\n",
            "4679: [D loss: 0.009587, acc: 0.996094]  [G loss: 1.750947, acc: 0.632812]\n",
            "4680: [D loss: 0.022422, acc: 0.992188]  [G loss: 1.833302, acc: 0.593750]\n",
            "4681: [D loss: 0.050080, acc: 0.988281]  [G loss: 1.992261, acc: 0.539062]\n",
            "4682: [D loss: 0.034586, acc: 0.980469]  [G loss: 2.163578, acc: 0.562500]\n",
            "4683: [D loss: 0.085393, acc: 0.976562]  [G loss: 2.179580, acc: 0.507812]\n",
            "4684: [D loss: 0.066867, acc: 0.984375]  [G loss: 2.059080, acc: 0.570312]\n",
            "4685: [D loss: 0.063120, acc: 0.984375]  [G loss: 1.870823, acc: 0.625000]\n",
            "4686: [D loss: 0.136662, acc: 0.960938]  [G loss: 1.929282, acc: 0.609375]\n",
            "4687: [D loss: 0.081815, acc: 0.964844]  [G loss: 2.499571, acc: 0.507812]\n",
            "4688: [D loss: 0.111901, acc: 0.949219]  [G loss: 3.280527, acc: 0.304688]\n",
            "4689: [D loss: 0.054343, acc: 0.976562]  [G loss: 3.651251, acc: 0.265625]\n",
            "4690: [D loss: 0.091292, acc: 0.960938]  [G loss: 3.170377, acc: 0.210938]\n",
            "4691: [D loss: 0.062891, acc: 0.972656]  [G loss: 3.466154, acc: 0.265625]\n",
            "4692: [D loss: 0.111172, acc: 0.972656]  [G loss: 2.907789, acc: 0.281250]\n",
            "4693: [D loss: 0.091127, acc: 0.976562]  [G loss: 2.950373, acc: 0.289062]\n",
            "4694: [D loss: 0.066272, acc: 0.980469]  [G loss: 2.779618, acc: 0.351562]\n",
            "4695: [D loss: 0.099440, acc: 0.960938]  [G loss: 2.756105, acc: 0.421875]\n",
            "4696: [D loss: 0.069656, acc: 0.972656]  [G loss: 3.250062, acc: 0.367188]\n",
            "4697: [D loss: 0.098062, acc: 0.964844]  [G loss: 3.728754, acc: 0.328125]\n",
            "4698: [D loss: 0.117810, acc: 0.964844]  [G loss: 4.658411, acc: 0.281250]\n",
            "4699: [D loss: 0.165239, acc: 0.949219]  [G loss: 3.647135, acc: 0.304688]\n",
            "4700: [D loss: 0.129571, acc: 0.957031]  [G loss: 2.699192, acc: 0.390625]\n",
            "4701: [D loss: 0.104454, acc: 0.968750]  [G loss: 2.431404, acc: 0.492188]\n",
            "4702: [D loss: 0.206879, acc: 0.914062]  [G loss: 3.344895, acc: 0.304688]\n",
            "4703: [D loss: 0.134264, acc: 0.949219]  [G loss: 4.936370, acc: 0.156250]\n",
            "4704: [D loss: 0.276613, acc: 0.945312]  [G loss: 3.866064, acc: 0.257812]\n",
            "4705: [D loss: 0.154616, acc: 0.964844]  [G loss: 2.855688, acc: 0.414062]\n",
            "4706: [D loss: 0.102828, acc: 0.968750]  [G loss: 1.987343, acc: 0.578125]\n",
            "4707: [D loss: 0.164776, acc: 0.917969]  [G loss: 2.726486, acc: 0.429688]\n",
            "4708: [D loss: 0.097210, acc: 0.957031]  [G loss: 3.781586, acc: 0.226562]\n",
            "4709: [D loss: 0.141771, acc: 0.953125]  [G loss: 4.463594, acc: 0.109375]\n",
            "4710: [D loss: 0.076903, acc: 0.960938]  [G loss: 3.693541, acc: 0.109375]\n",
            "4711: [D loss: 0.141713, acc: 0.968750]  [G loss: 2.761227, acc: 0.289062]\n",
            "4712: [D loss: 0.063042, acc: 0.988281]  [G loss: 2.004373, acc: 0.328125]\n",
            "4713: [D loss: 0.056656, acc: 0.984375]  [G loss: 1.465488, acc: 0.429688]\n",
            "4714: [D loss: 0.076820, acc: 0.976562]  [G loss: 1.150696, acc: 0.539062]\n",
            "4715: [D loss: 0.129303, acc: 0.960938]  [G loss: 1.765585, acc: 0.414062]\n",
            "4716: [D loss: 0.056277, acc: 0.984375]  [G loss: 2.539415, acc: 0.335938]\n",
            "4717: [D loss: 0.065697, acc: 0.976562]  [G loss: 3.235631, acc: 0.289062]\n",
            "4718: [D loss: 0.079160, acc: 0.972656]  [G loss: 2.738567, acc: 0.390625]\n",
            "4719: [D loss: 0.034746, acc: 0.984375]  [G loss: 1.883077, acc: 0.539062]\n",
            "4720: [D loss: 0.023514, acc: 0.996094]  [G loss: 1.539585, acc: 0.609375]\n",
            "4721: [D loss: 0.078553, acc: 0.976562]  [G loss: 1.896959, acc: 0.593750]\n",
            "4722: [D loss: 0.042175, acc: 0.980469]  [G loss: 1.897567, acc: 0.562500]\n",
            "4723: [D loss: 0.027715, acc: 0.992188]  [G loss: 2.376820, acc: 0.554688]\n",
            "4724: [D loss: 0.090223, acc: 0.964844]  [G loss: 2.747529, acc: 0.468750]\n",
            "4725: [D loss: 0.038302, acc: 0.988281]  [G loss: 2.146188, acc: 0.492188]\n",
            "4726: [D loss: 0.059708, acc: 0.980469]  [G loss: 2.194126, acc: 0.507812]\n",
            "4727: [D loss: 0.060271, acc: 0.980469]  [G loss: 1.947208, acc: 0.554688]\n",
            "4728: [D loss: 0.048454, acc: 0.976562]  [G loss: 2.048809, acc: 0.539062]\n",
            "4729: [D loss: 0.042258, acc: 0.992188]  [G loss: 1.843683, acc: 0.585938]\n",
            "4730: [D loss: 0.061771, acc: 0.984375]  [G loss: 1.789348, acc: 0.585938]\n",
            "4731: [D loss: 0.048019, acc: 0.988281]  [G loss: 1.799623, acc: 0.554688]\n",
            "4732: [D loss: 0.058911, acc: 0.972656]  [G loss: 1.928765, acc: 0.578125]\n",
            "4733: [D loss: 0.047242, acc: 0.980469]  [G loss: 1.524892, acc: 0.648438]\n",
            "4734: [D loss: 0.041369, acc: 0.992188]  [G loss: 1.624780, acc: 0.640625]\n",
            "4735: [D loss: 0.029203, acc: 0.996094]  [G loss: 1.839340, acc: 0.601562]\n",
            "4736: [D loss: 0.069560, acc: 0.980469]  [G loss: 1.720368, acc: 0.632812]\n",
            "4737: [D loss: 0.040002, acc: 0.984375]  [G loss: 1.699798, acc: 0.656250]\n",
            "4738: [D loss: 0.045573, acc: 0.996094]  [G loss: 1.367218, acc: 0.718750]\n",
            "4739: [D loss: 0.053960, acc: 0.980469]  [G loss: 1.721395, acc: 0.695312]\n",
            "4740: [D loss: 0.039759, acc: 0.984375]  [G loss: 1.456618, acc: 0.679688]\n",
            "4741: [D loss: 0.031726, acc: 0.988281]  [G loss: 1.243085, acc: 0.734375]\n",
            "4742: [D loss: 0.070225, acc: 0.972656]  [G loss: 1.080958, acc: 0.789062]\n",
            "4743: [D loss: 0.041741, acc: 0.980469]  [G loss: 1.460935, acc: 0.695312]\n",
            "4744: [D loss: 0.023188, acc: 0.992188]  [G loss: 1.221801, acc: 0.687500]\n",
            "4745: [D loss: 0.041843, acc: 0.988281]  [G loss: 1.291322, acc: 0.671875]\n",
            "4746: [D loss: 0.027109, acc: 0.992188]  [G loss: 1.503633, acc: 0.617188]\n",
            "4747: [D loss: 0.023126, acc: 0.996094]  [G loss: 1.156181, acc: 0.718750]\n",
            "4748: [D loss: 0.029560, acc: 0.988281]  [G loss: 0.975218, acc: 0.671875]\n",
            "4749: [D loss: 0.039532, acc: 0.984375]  [G loss: 1.297324, acc: 0.656250]\n",
            "4750: [D loss: 0.023040, acc: 0.996094]  [G loss: 1.256711, acc: 0.718750]\n",
            "4751: [D loss: 0.026505, acc: 0.984375]  [G loss: 1.312174, acc: 0.671875]\n",
            "4752: [D loss: 0.074693, acc: 0.980469]  [G loss: 1.466532, acc: 0.718750]\n",
            "4753: [D loss: 0.090442, acc: 0.960938]  [G loss: 0.840686, acc: 0.804688]\n",
            "4754: [D loss: 0.080371, acc: 0.980469]  [G loss: 0.792640, acc: 0.796875]\n",
            "4755: [D loss: 0.066960, acc: 0.984375]  [G loss: 1.057433, acc: 0.796875]\n",
            "4756: [D loss: 0.058787, acc: 0.980469]  [G loss: 1.706408, acc: 0.664062]\n",
            "4757: [D loss: 0.064092, acc: 0.984375]  [G loss: 2.166419, acc: 0.539062]\n",
            "4758: [D loss: 0.081001, acc: 0.984375]  [G loss: 2.121166, acc: 0.570312]\n",
            "4759: [D loss: 0.091473, acc: 0.980469]  [G loss: 1.113991, acc: 0.734375]\n",
            "4760: [D loss: 0.050460, acc: 0.984375]  [G loss: 0.763730, acc: 0.789062]\n",
            "4761: [D loss: 0.105308, acc: 0.953125]  [G loss: 0.925584, acc: 0.695312]\n",
            "4762: [D loss: 0.112921, acc: 0.968750]  [G loss: 1.242589, acc: 0.664062]\n",
            "4763: [D loss: 0.055218, acc: 0.976562]  [G loss: 1.782912, acc: 0.640625]\n",
            "4764: [D loss: 0.060158, acc: 0.984375]  [G loss: 1.708465, acc: 0.632812]\n",
            "4765: [D loss: 0.068982, acc: 0.976562]  [G loss: 1.539862, acc: 0.664062]\n",
            "4766: [D loss: 0.086255, acc: 0.968750]  [G loss: 1.549122, acc: 0.679688]\n",
            "4767: [D loss: 0.067287, acc: 0.968750]  [G loss: 1.439120, acc: 0.718750]\n",
            "4768: [D loss: 0.094918, acc: 0.972656]  [G loss: 1.387662, acc: 0.664062]\n",
            "4769: [D loss: 0.075988, acc: 0.980469]  [G loss: 1.414186, acc: 0.664062]\n",
            "4770: [D loss: 0.133492, acc: 0.972656]  [G loss: 0.827147, acc: 0.703125]\n",
            "4771: [D loss: 0.060627, acc: 0.984375]  [G loss: 1.299909, acc: 0.640625]\n",
            "4772: [D loss: 0.107773, acc: 0.945312]  [G loss: 1.850157, acc: 0.570312]\n",
            "4773: [D loss: 0.058366, acc: 0.972656]  [G loss: 2.030321, acc: 0.500000]\n",
            "4774: [D loss: 0.138157, acc: 0.957031]  [G loss: 1.270108, acc: 0.664062]\n",
            "4775: [D loss: 0.065779, acc: 0.984375]  [G loss: 1.102471, acc: 0.710938]\n",
            "4776: [D loss: 0.104043, acc: 0.953125]  [G loss: 1.797562, acc: 0.554688]\n",
            "4777: [D loss: 0.078447, acc: 0.976562]  [G loss: 2.973311, acc: 0.406250]\n",
            "4778: [D loss: 0.076298, acc: 0.968750]  [G loss: 2.725656, acc: 0.492188]\n",
            "4779: [D loss: 0.065176, acc: 0.980469]  [G loss: 2.943288, acc: 0.492188]\n",
            "4780: [D loss: 0.041801, acc: 0.984375]  [G loss: 2.346519, acc: 0.531250]\n",
            "4781: [D loss: 0.069309, acc: 0.984375]  [G loss: 2.936719, acc: 0.429688]\n",
            "4782: [D loss: 0.068088, acc: 0.980469]  [G loss: 3.545339, acc: 0.296875]\n",
            "4783: [D loss: 0.096192, acc: 0.960938]  [G loss: 4.315291, acc: 0.156250]\n",
            "4784: [D loss: 0.041446, acc: 0.980469]  [G loss: 3.915971, acc: 0.171875]\n",
            "4785: [D loss: 0.028018, acc: 0.992188]  [G loss: 3.682793, acc: 0.203125]\n",
            "4786: [D loss: 0.073914, acc: 0.980469]  [G loss: 3.105346, acc: 0.312500]\n",
            "4787: [D loss: 0.050276, acc: 0.976562]  [G loss: 2.543022, acc: 0.453125]\n",
            "4788: [D loss: 0.046720, acc: 0.972656]  [G loss: 3.000319, acc: 0.367188]\n",
            "4789: [D loss: 0.030446, acc: 0.988281]  [G loss: 3.134564, acc: 0.320312]\n",
            "4790: [D loss: 0.035960, acc: 0.988281]  [G loss: 3.665424, acc: 0.312500]\n",
            "4791: [D loss: 0.049159, acc: 0.984375]  [G loss: 3.376164, acc: 0.351562]\n",
            "4792: [D loss: 0.097225, acc: 0.964844]  [G loss: 2.427370, acc: 0.546875]\n",
            "4793: [D loss: 0.065404, acc: 0.980469]  [G loss: 2.089993, acc: 0.523438]\n",
            "4794: [D loss: 0.081735, acc: 0.976562]  [G loss: 1.711741, acc: 0.507812]\n",
            "4795: [D loss: 0.040129, acc: 0.980469]  [G loss: 2.244663, acc: 0.539062]\n",
            "4796: [D loss: 0.029864, acc: 0.984375]  [G loss: 2.685834, acc: 0.476562]\n",
            "4797: [D loss: 0.055323, acc: 0.976562]  [G loss: 2.661144, acc: 0.421875]\n",
            "4798: [D loss: 0.053226, acc: 0.976562]  [G loss: 2.604279, acc: 0.437500]\n",
            "4799: [D loss: 0.053714, acc: 0.976562]  [G loss: 2.309327, acc: 0.460938]\n",
            "4800: [D loss: 0.020317, acc: 0.992188]  [G loss: 2.333737, acc: 0.484375]\n",
            "4801: [D loss: 0.051866, acc: 0.972656]  [G loss: 2.660955, acc: 0.421875]\n",
            "4802: [D loss: 0.012777, acc: 0.996094]  [G loss: 3.186578, acc: 0.312500]\n",
            "4803: [D loss: 0.027339, acc: 0.992188]  [G loss: 3.797619, acc: 0.265625]\n",
            "4804: [D loss: 0.092482, acc: 0.972656]  [G loss: 3.398849, acc: 0.335938]\n",
            "4805: [D loss: 0.044902, acc: 0.980469]  [G loss: 1.929497, acc: 0.460938]\n",
            "4806: [D loss: 0.025203, acc: 0.984375]  [G loss: 1.988286, acc: 0.546875]\n",
            "4807: [D loss: 0.092129, acc: 0.964844]  [G loss: 3.657687, acc: 0.312500]\n",
            "4808: [D loss: 0.021690, acc: 0.996094]  [G loss: 4.668632, acc: 0.250000]\n",
            "4809: [D loss: 0.035367, acc: 0.988281]  [G loss: 5.231790, acc: 0.257812]\n",
            "4810: [D loss: 0.072328, acc: 0.976562]  [G loss: 4.568905, acc: 0.312500]\n",
            "4811: [D loss: 0.107353, acc: 0.968750]  [G loss: 3.360733, acc: 0.515625]\n",
            "4812: [D loss: 0.089002, acc: 0.972656]  [G loss: 2.980464, acc: 0.554688]\n",
            "4813: [D loss: 0.081869, acc: 0.980469]  [G loss: 2.471243, acc: 0.546875]\n",
            "4814: [D loss: 0.102532, acc: 0.945312]  [G loss: 2.767981, acc: 0.453125]\n",
            "4815: [D loss: 0.038182, acc: 0.988281]  [G loss: 3.936946, acc: 0.375000]\n",
            "4816: [D loss: 0.083906, acc: 0.980469]  [G loss: 4.031407, acc: 0.320312]\n",
            "4817: [D loss: 0.103553, acc: 0.976562]  [G loss: 3.708496, acc: 0.367188]\n",
            "4818: [D loss: 0.065747, acc: 0.988281]  [G loss: 3.333500, acc: 0.421875]\n",
            "4819: [D loss: 0.063531, acc: 0.984375]  [G loss: 3.066560, acc: 0.507812]\n",
            "4820: [D loss: 0.070746, acc: 0.984375]  [G loss: 2.955327, acc: 0.523438]\n",
            "4821: [D loss: 0.156872, acc: 0.964844]  [G loss: 2.713203, acc: 0.507812]\n",
            "4822: [D loss: 0.066854, acc: 0.980469]  [G loss: 2.556034, acc: 0.546875]\n",
            "4823: [D loss: 0.061853, acc: 0.980469]  [G loss: 2.407199, acc: 0.578125]\n",
            "4824: [D loss: 0.041013, acc: 0.976562]  [G loss: 2.984467, acc: 0.500000]\n",
            "4825: [D loss: 0.041745, acc: 0.980469]  [G loss: 3.170922, acc: 0.406250]\n",
            "4826: [D loss: 0.077312, acc: 0.976562]  [G loss: 3.142154, acc: 0.367188]\n",
            "4827: [D loss: 0.093252, acc: 0.972656]  [G loss: 2.645448, acc: 0.468750]\n",
            "4828: [D loss: 0.041528, acc: 0.980469]  [G loss: 2.053390, acc: 0.523438]\n",
            "4829: [D loss: 0.072569, acc: 0.968750]  [G loss: 2.516110, acc: 0.492188]\n",
            "4830: [D loss: 0.077418, acc: 0.972656]  [G loss: 3.671420, acc: 0.281250]\n",
            "4831: [D loss: 0.051306, acc: 0.976562]  [G loss: 6.002353, acc: 0.101562]\n",
            "4832: [D loss: 0.033246, acc: 0.992188]  [G loss: 6.457664, acc: 0.054688]\n",
            "4833: [D loss: 0.052464, acc: 0.988281]  [G loss: 5.787614, acc: 0.031250]\n",
            "4834: [D loss: 0.088508, acc: 0.984375]  [G loss: 5.510617, acc: 0.078125]\n",
            "4835: [D loss: 0.110260, acc: 0.972656]  [G loss: 4.046174, acc: 0.195312]\n",
            "4836: [D loss: 0.045314, acc: 0.972656]  [G loss: 2.994229, acc: 0.375000]\n",
            "4837: [D loss: 0.162558, acc: 0.960938]  [G loss: 3.391131, acc: 0.359375]\n",
            "4838: [D loss: 0.054007, acc: 0.976562]  [G loss: 3.424055, acc: 0.367188]\n",
            "4839: [D loss: 0.049707, acc: 0.976562]  [G loss: 4.307873, acc: 0.203125]\n",
            "4840: [D loss: 0.087856, acc: 0.964844]  [G loss: 4.761671, acc: 0.203125]\n",
            "4841: [D loss: 0.129855, acc: 0.980469]  [G loss: 4.297457, acc: 0.179688]\n",
            "4842: [D loss: 0.035806, acc: 0.984375]  [G loss: 3.291742, acc: 0.312500]\n",
            "4843: [D loss: 0.053772, acc: 0.984375]  [G loss: 3.147409, acc: 0.343750]\n",
            "4844: [D loss: 0.036304, acc: 0.984375]  [G loss: 3.301432, acc: 0.312500]\n",
            "4845: [D loss: 0.056975, acc: 0.980469]  [G loss: 2.863734, acc: 0.335938]\n",
            "4846: [D loss: 0.050421, acc: 0.968750]  [G loss: 3.376035, acc: 0.296875]\n",
            "4847: [D loss: 0.050375, acc: 0.972656]  [G loss: 3.644028, acc: 0.289062]\n",
            "4848: [D loss: 0.013138, acc: 0.996094]  [G loss: 4.387105, acc: 0.218750]\n",
            "4849: [D loss: 0.085330, acc: 0.968750]  [G loss: 3.393587, acc: 0.257812]\n",
            "4850: [D loss: 0.017137, acc: 0.996094]  [G loss: 2.644098, acc: 0.296875]\n",
            "4851: [D loss: 0.063881, acc: 0.980469]  [G loss: 2.395758, acc: 0.398438]\n",
            "4852: [D loss: 0.040136, acc: 0.984375]  [G loss: 1.865988, acc: 0.492188]\n",
            "4853: [D loss: 0.036309, acc: 0.984375]  [G loss: 1.669430, acc: 0.531250]\n",
            "4854: [D loss: 0.031673, acc: 0.988281]  [G loss: 1.716733, acc: 0.460938]\n",
            "4855: [D loss: 0.046791, acc: 0.980469]  [G loss: 2.455729, acc: 0.367188]\n",
            "4856: [D loss: 0.070490, acc: 0.968750]  [G loss: 2.408512, acc: 0.398438]\n",
            "4857: [D loss: 0.047866, acc: 0.984375]  [G loss: 2.624222, acc: 0.320312]\n",
            "4858: [D loss: 0.031120, acc: 0.988281]  [G loss: 3.154044, acc: 0.210938]\n",
            "4859: [D loss: 0.044441, acc: 0.996094]  [G loss: 3.856642, acc: 0.195312]\n",
            "4860: [D loss: 0.040919, acc: 0.988281]  [G loss: 4.219443, acc: 0.117188]\n",
            "4861: [D loss: 0.088375, acc: 0.968750]  [G loss: 3.306672, acc: 0.210938]\n",
            "4862: [D loss: 0.107071, acc: 0.960938]  [G loss: 2.486646, acc: 0.304688]\n",
            "4863: [D loss: 0.074351, acc: 0.960938]  [G loss: 3.395828, acc: 0.234375]\n",
            "4864: [D loss: 0.047930, acc: 0.980469]  [G loss: 4.092561, acc: 0.125000]\n",
            "4865: [D loss: 0.085243, acc: 0.976562]  [G loss: 4.602825, acc: 0.117188]\n",
            "4866: [D loss: 0.094561, acc: 0.976562]  [G loss: 4.233244, acc: 0.132812]\n",
            "4867: [D loss: 0.064683, acc: 0.972656]  [G loss: 3.280586, acc: 0.312500]\n",
            "4868: [D loss: 0.203896, acc: 0.953125]  [G loss: 2.958829, acc: 0.234375]\n",
            "4869: [D loss: 0.085006, acc: 0.968750]  [G loss: 3.856894, acc: 0.132812]\n",
            "4870: [D loss: 0.061835, acc: 0.980469]  [G loss: 4.348670, acc: 0.101562]\n",
            "4871: [D loss: 0.089410, acc: 0.968750]  [G loss: 4.361219, acc: 0.085938]\n",
            "4872: [D loss: 0.059090, acc: 0.972656]  [G loss: 4.047628, acc: 0.125000]\n",
            "4873: [D loss: 0.068472, acc: 0.984375]  [G loss: 3.111964, acc: 0.226562]\n",
            "4874: [D loss: 0.095288, acc: 0.960938]  [G loss: 2.372301, acc: 0.320312]\n",
            "4875: [D loss: 0.035661, acc: 0.984375]  [G loss: 2.928074, acc: 0.273438]\n",
            "4876: [D loss: 0.058847, acc: 0.976562]  [G loss: 2.755395, acc: 0.320312]\n",
            "4877: [D loss: 0.053200, acc: 0.976562]  [G loss: 2.935957, acc: 0.289062]\n",
            "4878: [D loss: 0.067448, acc: 0.972656]  [G loss: 2.207279, acc: 0.367188]\n",
            "4879: [D loss: 0.038790, acc: 0.988281]  [G loss: 1.655968, acc: 0.476562]\n",
            "4880: [D loss: 0.041661, acc: 0.984375]  [G loss: 1.448999, acc: 0.578125]\n",
            "4881: [D loss: 0.036764, acc: 0.988281]  [G loss: 1.524233, acc: 0.539062]\n",
            "4882: [D loss: 0.021765, acc: 0.992188]  [G loss: 1.790110, acc: 0.531250]\n",
            "4883: [D loss: 0.021236, acc: 0.992188]  [G loss: 1.519934, acc: 0.585938]\n",
            "4884: [D loss: 0.013619, acc: 0.996094]  [G loss: 1.482595, acc: 0.601562]\n",
            "4885: [D loss: 0.065586, acc: 0.980469]  [G loss: 1.091467, acc: 0.664062]\n",
            "4886: [D loss: 0.036876, acc: 0.992188]  [G loss: 0.711592, acc: 0.718750]\n",
            "4887: [D loss: 0.080067, acc: 0.964844]  [G loss: 1.306647, acc: 0.671875]\n",
            "4888: [D loss: 0.041360, acc: 0.984375]  [G loss: 1.574157, acc: 0.664062]\n",
            "4889: [D loss: 0.044755, acc: 0.984375]  [G loss: 1.408792, acc: 0.679688]\n",
            "4890: [D loss: 0.080964, acc: 0.976562]  [G loss: 0.957120, acc: 0.695312]\n",
            "4891: [D loss: 0.039654, acc: 0.988281]  [G loss: 0.788745, acc: 0.789062]\n",
            "4892: [D loss: 0.064802, acc: 0.972656]  [G loss: 1.064798, acc: 0.734375]\n",
            "4893: [D loss: 0.028434, acc: 0.996094]  [G loss: 1.119133, acc: 0.726562]\n",
            "4894: [D loss: 0.066630, acc: 0.980469]  [G loss: 1.344757, acc: 0.734375]\n",
            "4895: [D loss: 0.067849, acc: 0.976562]  [G loss: 0.929161, acc: 0.773438]\n",
            "4896: [D loss: 0.050677, acc: 0.988281]  [G loss: 0.806769, acc: 0.843750]\n",
            "4897: [D loss: 0.086459, acc: 0.968750]  [G loss: 0.670162, acc: 0.835938]\n",
            "4898: [D loss: 0.053836, acc: 0.980469]  [G loss: 1.136976, acc: 0.765625]\n",
            "4899: [D loss: 0.056880, acc: 0.976562]  [G loss: 1.355482, acc: 0.726562]\n",
            "4900: [D loss: 0.073982, acc: 0.976562]  [G loss: 1.032723, acc: 0.812500]\n",
            "4901: [D loss: 0.020590, acc: 0.992188]  [G loss: 0.717868, acc: 0.851562]\n",
            "4902: [D loss: 0.039032, acc: 0.984375]  [G loss: 0.586578, acc: 0.828125]\n",
            "4903: [D loss: 0.051113, acc: 0.972656]  [G loss: 0.824929, acc: 0.804688]\n",
            "4904: [D loss: 0.045632, acc: 0.988281]  [G loss: 1.285466, acc: 0.703125]\n",
            "4905: [D loss: 0.051541, acc: 0.988281]  [G loss: 1.573730, acc: 0.648438]\n",
            "4906: [D loss: 0.051831, acc: 0.980469]  [G loss: 1.281321, acc: 0.679688]\n",
            "4907: [D loss: 0.036628, acc: 0.996094]  [G loss: 0.972881, acc: 0.734375]\n",
            "4908: [D loss: 0.053996, acc: 0.980469]  [G loss: 0.948729, acc: 0.781250]\n",
            "4909: [D loss: 0.050112, acc: 0.980469]  [G loss: 0.981675, acc: 0.750000]\n",
            "4910: [D loss: 0.044750, acc: 0.976562]  [G loss: 1.720963, acc: 0.609375]\n",
            "4911: [D loss: 0.128108, acc: 0.968750]  [G loss: 1.818709, acc: 0.617188]\n",
            "4912: [D loss: 0.072837, acc: 0.980469]  [G loss: 1.457549, acc: 0.656250]\n",
            "4913: [D loss: 0.083396, acc: 0.964844]  [G loss: 1.310392, acc: 0.718750]\n",
            "4914: [D loss: 0.127262, acc: 0.960938]  [G loss: 0.878443, acc: 0.773438]\n",
            "4915: [D loss: 0.052983, acc: 0.980469]  [G loss: 1.008455, acc: 0.695312]\n",
            "4916: [D loss: 0.088138, acc: 0.988281]  [G loss: 0.833030, acc: 0.773438]\n",
            "4917: [D loss: 0.075973, acc: 0.972656]  [G loss: 1.013712, acc: 0.726562]\n",
            "4918: [D loss: 0.095908, acc: 0.964844]  [G loss: 1.710145, acc: 0.476562]\n",
            "4919: [D loss: 0.037520, acc: 0.988281]  [G loss: 2.069191, acc: 0.476562]\n",
            "4920: [D loss: 0.065949, acc: 0.980469]  [G loss: 2.125784, acc: 0.406250]\n",
            "4921: [D loss: 0.042243, acc: 0.984375]  [G loss: 2.018837, acc: 0.500000]\n",
            "4922: [D loss: 0.098339, acc: 0.949219]  [G loss: 1.760116, acc: 0.460938]\n",
            "4923: [D loss: 0.083905, acc: 0.968750]  [G loss: 1.076633, acc: 0.671875]\n",
            "4924: [D loss: 0.079797, acc: 0.964844]  [G loss: 0.797884, acc: 0.710938]\n",
            "4925: [D loss: 0.065039, acc: 0.980469]  [G loss: 1.182542, acc: 0.656250]\n",
            "4926: [D loss: 0.067989, acc: 0.980469]  [G loss: 1.210212, acc: 0.609375]\n",
            "4927: [D loss: 0.057522, acc: 0.964844]  [G loss: 1.791928, acc: 0.562500]\n",
            "4928: [D loss: 0.031930, acc: 0.984375]  [G loss: 2.275199, acc: 0.492188]\n",
            "4929: [D loss: 0.080487, acc: 0.972656]  [G loss: 2.779732, acc: 0.382812]\n",
            "4930: [D loss: 0.065127, acc: 0.980469]  [G loss: 1.984416, acc: 0.500000]\n",
            "4931: [D loss: 0.012477, acc: 0.996094]  [G loss: 2.012990, acc: 0.554688]\n",
            "4932: [D loss: 0.029605, acc: 0.988281]  [G loss: 1.679995, acc: 0.570312]\n",
            "4933: [D loss: 0.039134, acc: 0.984375]  [G loss: 1.384158, acc: 0.632812]\n",
            "4934: [D loss: 0.021182, acc: 0.996094]  [G loss: 1.512658, acc: 0.570312]\n",
            "4935: [D loss: 0.029172, acc: 0.988281]  [G loss: 1.997450, acc: 0.523438]\n",
            "4936: [D loss: 0.081426, acc: 0.964844]  [G loss: 2.709104, acc: 0.382812]\n",
            "4937: [D loss: 0.056186, acc: 0.980469]  [G loss: 2.951783, acc: 0.335938]\n",
            "4938: [D loss: 0.051408, acc: 0.980469]  [G loss: 2.205517, acc: 0.445312]\n",
            "4939: [D loss: 0.038700, acc: 0.984375]  [G loss: 2.063379, acc: 0.468750]\n",
            "4940: [D loss: 0.030836, acc: 0.988281]  [G loss: 2.144889, acc: 0.429688]\n",
            "4941: [D loss: 0.079700, acc: 0.976562]  [G loss: 1.891492, acc: 0.453125]\n",
            "4942: [D loss: 0.138609, acc: 0.945312]  [G loss: 2.502449, acc: 0.296875]\n",
            "4943: [D loss: 0.074989, acc: 0.972656]  [G loss: 2.324285, acc: 0.359375]\n",
            "4944: [D loss: 0.086536, acc: 0.957031]  [G loss: 2.011191, acc: 0.453125]\n",
            "4945: [D loss: 0.071296, acc: 0.980469]  [G loss: 1.319639, acc: 0.562500]\n",
            "4946: [D loss: 0.073845, acc: 0.972656]  [G loss: 1.346551, acc: 0.625000]\n",
            "4947: [D loss: 0.082897, acc: 0.980469]  [G loss: 2.373734, acc: 0.382812]\n",
            "4948: [D loss: 0.075203, acc: 0.964844]  [G loss: 2.227999, acc: 0.453125]\n",
            "4949: [D loss: 0.072100, acc: 0.976562]  [G loss: 1.958730, acc: 0.484375]\n",
            "4950: [D loss: 0.096665, acc: 0.964844]  [G loss: 1.019591, acc: 0.664062]\n",
            "4951: [D loss: 0.076302, acc: 0.972656]  [G loss: 1.079340, acc: 0.695312]\n",
            "4952: [D loss: 0.114268, acc: 0.968750]  [G loss: 2.123109, acc: 0.484375]\n",
            "4953: [D loss: 0.046687, acc: 0.980469]  [G loss: 3.335453, acc: 0.359375]\n",
            "4954: [D loss: 0.067018, acc: 0.976562]  [G loss: 3.699176, acc: 0.304688]\n",
            "4955: [D loss: 0.014628, acc: 0.996094]  [G loss: 2.781056, acc: 0.414062]\n",
            "4956: [D loss: 0.136872, acc: 0.972656]  [G loss: 2.074771, acc: 0.507812]\n",
            "4957: [D loss: 0.034130, acc: 0.988281]  [G loss: 1.448800, acc: 0.656250]\n",
            "4958: [D loss: 0.059198, acc: 0.976562]  [G loss: 1.571218, acc: 0.609375]\n",
            "4959: [D loss: 0.033358, acc: 0.984375]  [G loss: 1.729761, acc: 0.570312]\n",
            "4960: [D loss: 0.078982, acc: 0.964844]  [G loss: 2.452092, acc: 0.429688]\n",
            "4961: [D loss: 0.107865, acc: 0.964844]  [G loss: 3.425315, acc: 0.265625]\n",
            "4962: [D loss: 0.106679, acc: 0.960938]  [G loss: 2.908227, acc: 0.375000]\n",
            "4963: [D loss: 0.020300, acc: 0.996094]  [G loss: 2.049704, acc: 0.468750]\n",
            "4964: [D loss: 0.041456, acc: 0.980469]  [G loss: 1.548988, acc: 0.546875]\n",
            "4965: [D loss: 0.109421, acc: 0.968750]  [G loss: 1.434878, acc: 0.601562]\n",
            "4966: [D loss: 0.039096, acc: 0.984375]  [G loss: 2.052976, acc: 0.453125]\n",
            "4967: [D loss: 0.033026, acc: 0.988281]  [G loss: 2.948187, acc: 0.187500]\n",
            "4968: [D loss: 0.094860, acc: 0.976562]  [G loss: 2.948730, acc: 0.234375]\n",
            "4969: [D loss: 0.098319, acc: 0.972656]  [G loss: 2.796158, acc: 0.320312]\n",
            "4970: [D loss: 0.058916, acc: 0.980469]  [G loss: 1.882202, acc: 0.429688]\n",
            "4971: [D loss: 0.021220, acc: 0.992188]  [G loss: 2.207367, acc: 0.375000]\n",
            "4972: [D loss: 0.041760, acc: 0.976562]  [G loss: 2.015268, acc: 0.445312]\n",
            "4973: [D loss: 0.033718, acc: 0.992188]  [G loss: 2.604742, acc: 0.328125]\n",
            "4974: [D loss: 0.035335, acc: 0.988281]  [G loss: 3.409893, acc: 0.179688]\n",
            "4975: [D loss: 0.032618, acc: 0.984375]  [G loss: 4.029102, acc: 0.132812]\n",
            "4976: [D loss: 0.036826, acc: 0.988281]  [G loss: 3.847484, acc: 0.140625]\n",
            "4977: [D loss: 0.049976, acc: 0.980469]  [G loss: 3.543747, acc: 0.265625]\n",
            "4978: [D loss: 0.061534, acc: 0.972656]  [G loss: 2.932103, acc: 0.281250]\n",
            "4979: [D loss: 0.112088, acc: 0.964844]  [G loss: 2.326216, acc: 0.382812]\n",
            "4980: [D loss: 0.092881, acc: 0.968750]  [G loss: 3.172895, acc: 0.242188]\n",
            "4981: [D loss: 0.069768, acc: 0.976562]  [G loss: 3.814957, acc: 0.203125]\n",
            "4982: [D loss: 0.129007, acc: 0.949219]  [G loss: 2.906415, acc: 0.351562]\n",
            "4983: [D loss: 0.099802, acc: 0.960938]  [G loss: 3.618103, acc: 0.304688]\n",
            "4984: [D loss: 0.102893, acc: 0.957031]  [G loss: 5.250424, acc: 0.195312]\n",
            "4985: [D loss: 0.166513, acc: 0.953125]  [G loss: 5.024442, acc: 0.226562]\n",
            "4986: [D loss: 0.141272, acc: 0.945312]  [G loss: 4.135985, acc: 0.320312]\n",
            "4987: [D loss: 0.202548, acc: 0.953125]  [G loss: 2.866082, acc: 0.476562]\n",
            "4988: [D loss: 0.160464, acc: 0.960938]  [G loss: 3.104740, acc: 0.460938]\n",
            "4989: [D loss: 0.045297, acc: 0.980469]  [G loss: 2.679486, acc: 0.484375]\n",
            "4990: [D loss: 0.117809, acc: 0.960938]  [G loss: 2.173757, acc: 0.523438]\n",
            "4991: [D loss: 0.095267, acc: 0.964844]  [G loss: 2.073468, acc: 0.593750]\n",
            "4992: [D loss: 0.034324, acc: 0.988281]  [G loss: 1.913584, acc: 0.570312]\n",
            "4993: [D loss: 0.055474, acc: 0.984375]  [G loss: 1.811396, acc: 0.515625]\n",
            "4994: [D loss: 0.053428, acc: 0.984375]  [G loss: 2.035049, acc: 0.554688]\n",
            "4995: [D loss: 0.078304, acc: 0.968750]  [G loss: 1.337469, acc: 0.656250]\n",
            "4996: [D loss: 0.060808, acc: 0.968750]  [G loss: 2.231292, acc: 0.554688]\n",
            "4997: [D loss: 0.065682, acc: 0.972656]  [G loss: 2.246736, acc: 0.570312]\n",
            "4998: [D loss: 0.058650, acc: 0.992188]  [G loss: 1.594406, acc: 0.609375]\n",
            "4999: [D loss: 0.082856, acc: 0.972656]  [G loss: 2.627854, acc: 0.554688]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8OZt5dMiWCET",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_discriminador = [] \n",
        "acc_discriminador = [] \n",
        "loss_generador = []\n",
        "acc_generador = []\n",
        "\n",
        "for loss, acc in hist['d']:\n",
        "  loss_discriminador.append(loss)\n",
        "  acc_discriminador.append(acc)\n",
        "\n",
        "for loss, acc in hist['g']:\n",
        "  loss_generador.append(loss)\n",
        "  acc_generador.append(acc)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdEXnRvIS2eR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "6366fe91-d248-4171-b0ff-fd3484c46eed"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(6, figsize=(16,8))\n",
        "plt.suptitle(\"Loss y Acc para redes discriminadora y generadora (batch = 128)\", y=1.03)\n",
        "plt.subplot(121)\n",
        "plt.title(\"Loss para distintas redes\")\n",
        "plt.plot(loss_discriminador, label = \"Discriminador\")\n",
        "plt.plot(loss_generador, label=\"Generador\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"N de epoch\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.subplot(122)\n",
        "plt.title(\"Acc para distintas redes\")\n",
        "plt.plot(acc_discriminador, label = \"Discriminador\")\n",
        "plt.plot(acc_generador, label=\"Generador\")\n",
        "plt.ylabel(\"Acc\")\n",
        "plt.xlabel(\"N de epoch\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show();"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "RQscMx8CS-Jz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "5b262428-4539-431e-98fe-450403555e9f"
      },
      "cell_type": "code",
      "source": [
        "N = 10\n",
        "noise = np.random.uniform(-1.0, 1.0, size=[N, input_dim]) \n",
        "images = G.predict(noise)\n",
        "import matplotlib.pyplot as plt\n",
        "for i in range(images.shape[0]):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    image = images[i, :, :, :]\n",
        "    image = np.reshape(image, [img_rows, img_cols])\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fwNFz6WbZInR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
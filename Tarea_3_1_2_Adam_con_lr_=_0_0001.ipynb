{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea 3 - 1.2 - Adam con lr = 0.0001.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/gonzalezf/Tarea3-INF395/blob/master/Tarea_3_1_2_Adam_con_lr_=_0_0001.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "OduUJ0FnVYP6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input,Conv2D,Flatten,Dense,MaxPool2D\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.layers import Reshape,Conv2DTranspose,Activation\n",
        "from keras import backend as K\n",
        "from keras.layers import Lambda\n",
        "from keras import utils\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "from scipy.stats import norm\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDSl4qUjVchu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "10d084ee-6135-4d66-e27f-6fd2fcc0ef48"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JspDUiSOVeaM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vS1w3Oh8Vfs-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0NjIC1b_Vhp6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_rows, img_cols, channel = X_train.shape[1:]\n",
        "# color channels (1 = grayscale, 3 = RGB) \n",
        "#si fueran imagenes a color, se utilizaria channel = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A29FZz7OVlMn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lXaBkY4fVozz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# GAN"
      ]
    },
    {
      "metadata": {
        "id": "1y2hDhluVpR7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model,Sequential\n",
        "from keras.layers import LeakyReLU,Conv2D,Dropout,Flatten,Dense\n",
        "from keras.layers import BatchNormalization,Reshape,UpSampling2D,Conv2DTranspose,Activation\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Model,Sequential\n",
        "from keras.layers import LeakyReLU,Conv2D,Dropout,Flatten,Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZXRraSQxVq0J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "1eba52ab-0e6a-487c-a0db-5f1aefcb2c2d"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "## Discriminator\n",
        "D = Sequential()\n",
        "depth = 64\n",
        "dropout = 0.4\n",
        "input_shape = (img_rows, img_cols, channel)\n",
        "D.add(Conv2D(depth*1, (5,5), strides=2, input_shape=input_shape,padding='same', activation=LeakyReLU(alpha=0.2)))\n",
        "D.add(Dropout(dropout))\n",
        "D.add(Conv2D(depth*2, (5,5), strides=2, padding='same',activation=LeakyReLU(alpha=0.2)))\n",
        "D.add(Dropout(dropout))\n",
        "D.add(Conv2D(depth*4, (5,5), strides=2, padding='same',activation=LeakyReLU(alpha=0.2)))\n",
        "D.add(Dropout(dropout))\n",
        "D.add(Flatten())\n",
        "D.add(Dense(1024,activation=LeakyReLU(alpha=0.2)))\n",
        "D.add(Dense(1,activation='sigmoid'))\n",
        "D.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              4195328   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 5,222,401\n",
            "Trainable params: 5,222,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/activations.py:115: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
            "  identifier=identifier.__class__.__name__))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9gbhPus9Vwnk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "d2ef96b2-7500-4cb0-d864-9e4b14d3fff3"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.layers import BatchNormalization,Reshape,UpSampling2D,Conv2DTranspose,Activation\n",
        "## Generator\n",
        "G = Sequential()\n",
        "dim = 14\n",
        "input_dim= 2 #para que sea similar al vAE\n",
        "G.add(Dense(128, input_dim=input_dim))\n",
        "G.add(BatchNormalization())\n",
        "G.add(Activation('relu'))\n",
        "G.add(Dense(dim*dim*depth))\n",
        "G.add(BatchNormalization())\n",
        "G.add(Activation('relu'))\n",
        "G.add(Reshape((dim, dim, depth)))\n",
        "G.add(Conv2DTranspose(int(depth/2), (3,3), padding='same',strides=(2,2)))\n",
        "G.add(BatchNormalization())\n",
        "G.add(Activation('relu'))\n",
        "G.add(Conv2DTranspose(int(depth/2), (3,3), padding='same'))\n",
        "G.add(BatchNormalization())\n",
        "G.add(Activation('relu'))\n",
        "G.add(Conv2DTranspose(channel, (3,3), padding='same')) \n",
        "G.add(Activation('sigmoid')) \n",
        "G.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 128)               384       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 12544)             1618176   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 12544)             50176     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         289       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,697,505\n",
            "Trainable params: 1,672,033\n",
            "Non-trainable params: 25,472\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OqNVdOPmV1V2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "## Discriminator model (police)\n",
        "#optimizer = RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8)\n",
        "optimizer = Adam(0.001)\n",
        "DM = Sequential()\n",
        "DM.add(D)\n",
        "DM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
        "## Adversarial model (Generator->Discriminator)\n",
        "D.trainable=False #set the discriminator freeze  (fixed params)\n",
        "#optimizer = RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8)\n",
        "optimizer = Adam(lr = 0.0001)\n",
        "AM = Sequential()\n",
        "AM.add(G)\n",
        "AM.add(D)\n",
        "AM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KqxiO0utV7wP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90053
        },
        "outputId": "dab2f175-893d-4806-b8fb-f3b13296e85d"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def train_on_steps(X_train,DM,AM,G,steps,batch_size):\n",
        "    history = {\"d\":[],\"g\":[]}\n",
        "    for e in range(train_steps):\n",
        "        # Make generative images\n",
        "        image_batch = X_train[np.random.randint(0,X_train.shape[0],size=batch_size),:,:,:] #sample images from real data\n",
        "        noise_gen = np.random.uniform(-1,1,size=[batch_size,input_dim]) #sample image from generated data\n",
        "        generated_images = G.predict(noise_gen) #fake images\n",
        "        # Train discriminator on generated images\n",
        "        X = np.concatenate((image_batch, generated_images))\n",
        "        #create labels\n",
        "        y = np.ones([2*batch_size,1])\n",
        "        y[batch_size:,:] = 0\n",
        "        d_loss  = DM.train_on_batch(X,y)\n",
        "        history[\"d\"].append(d_loss)\n",
        "        # train Generator-Discriminator stack on input noise to non-generated output class\n",
        "        noise_tr = np.random.uniform(-1,1,size=[batch_size,input_dim])\n",
        "        y = np.ones([batch_size, 1])\n",
        "        g_loss = AM.train_on_batch(noise_tr, y)\n",
        "        history[\"g\"].append(g_loss)\n",
        "        log_mesg = \"%d: [D loss: %f, acc: %f]\" % (e, d_loss[0], d_loss[1])\n",
        "        log_mesg = \"%s  [G loss: %f, acc: %f]\" % (log_mesg, g_loss[0], g_loss[1])\n",
        "        print(log_mesg)\n",
        "    return history\n",
        "train_steps = 5000 #or few if  you want\n",
        "hist = train_on_steps(X_train,DM,AM,G,train_steps,64)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: [D loss: 0.686838, acc: 0.671875]  [G loss: 1.384352, acc: 0.000000]\n",
            "1: [D loss: 0.518227, acc: 0.570312]  [G loss: 3.843259, acc: 0.000000]\n",
            "2: [D loss: 0.280287, acc: 0.953125]  [G loss: 0.094210, acc: 1.000000]\n",
            "3: [D loss: 1.417555, acc: 0.500000]  [G loss: 8.325933, acc: 0.000000]\n",
            "4: [D loss: 0.144066, acc: 0.960938]  [G loss: 12.545464, acc: 0.000000]\n",
            "5: [D loss: 0.475059, acc: 0.757812]  [G loss: 11.654345, acc: 0.000000]\n",
            "6: [D loss: 0.401079, acc: 0.765625]  [G loss: 8.609407, acc: 0.000000]\n",
            "7: [D loss: 0.190151, acc: 0.929688]  [G loss: 5.636868, acc: 0.000000]\n",
            "8: [D loss: 0.076134, acc: 0.984375]  [G loss: 3.023290, acc: 0.000000]\n",
            "9: [D loss: 0.053312, acc: 1.000000]  [G loss: 1.841708, acc: 0.000000]\n",
            "10: [D loss: 0.152195, acc: 0.953125]  [G loss: 2.516591, acc: 0.000000]\n",
            "11: [D loss: 0.097051, acc: 0.984375]  [G loss: 4.352693, acc: 0.000000]\n",
            "12: [D loss: 0.020598, acc: 1.000000]  [G loss: 5.919382, acc: 0.000000]\n",
            "13: [D loss: 0.005941, acc: 1.000000]  [G loss: 7.761530, acc: 0.000000]\n",
            "14: [D loss: 0.004200, acc: 1.000000]  [G loss: 9.519296, acc: 0.000000]\n",
            "15: [D loss: 0.002284, acc: 1.000000]  [G loss: 11.237960, acc: 0.000000]\n",
            "16: [D loss: 0.007324, acc: 1.000000]  [G loss: 12.602200, acc: 0.000000]\n",
            "17: [D loss: 0.002388, acc: 1.000000]  [G loss: 13.706352, acc: 0.000000]\n",
            "18: [D loss: 0.001978, acc: 1.000000]  [G loss: 14.735401, acc: 0.000000]\n",
            "19: [D loss: 0.002964, acc: 1.000000]  [G loss: 15.441692, acc: 0.000000]\n",
            "20: [D loss: 0.000592, acc: 1.000000]  [G loss: 15.696644, acc: 0.000000]\n",
            "21: [D loss: 0.001135, acc: 1.000000]  [G loss: 15.769171, acc: 0.000000]\n",
            "22: [D loss: 0.000243, acc: 1.000000]  [G loss: 15.827503, acc: 0.000000]\n",
            "23: [D loss: 0.000165, acc: 1.000000]  [G loss: 15.888485, acc: 0.000000]\n",
            "24: [D loss: 0.000777, acc: 1.000000]  [G loss: 15.748134, acc: 0.000000]\n",
            "25: [D loss: 0.000419, acc: 1.000000]  [G loss: 15.599087, acc: 0.000000]\n",
            "26: [D loss: 0.000013, acc: 1.000000]  [G loss: 15.218003, acc: 0.000000]\n",
            "27: [D loss: 0.000003, acc: 1.000000]  [G loss: 15.074982, acc: 0.000000]\n",
            "28: [D loss: 0.000014, acc: 1.000000]  [G loss: 14.274805, acc: 0.000000]\n",
            "29: [D loss: 0.000017, acc: 1.000000]  [G loss: 13.994737, acc: 0.000000]\n",
            "30: [D loss: 0.000085, acc: 1.000000]  [G loss: 13.397491, acc: 0.000000]\n",
            "31: [D loss: 0.000241, acc: 1.000000]  [G loss: 12.924664, acc: 0.000000]\n",
            "32: [D loss: 0.002823, acc: 1.000000]  [G loss: 12.292114, acc: 0.000000]\n",
            "33: [D loss: 0.005198, acc: 1.000000]  [G loss: 12.368267, acc: 0.000000]\n",
            "34: [D loss: 0.012161, acc: 0.992188]  [G loss: 12.743474, acc: 0.000000]\n",
            "35: [D loss: 0.002893, acc: 1.000000]  [G loss: 14.029673, acc: 0.000000]\n",
            "36: [D loss: 0.000121, acc: 1.000000]  [G loss: 14.148682, acc: 0.000000]\n",
            "37: [D loss: 0.000454, acc: 1.000000]  [G loss: 14.651782, acc: 0.000000]\n",
            "38: [D loss: 0.050736, acc: 0.992188]  [G loss: 14.695976, acc: 0.000000]\n",
            "39: [D loss: 0.000341, acc: 1.000000]  [G loss: 14.677945, acc: 0.000000]\n",
            "40: [D loss: 0.016179, acc: 0.992188]  [G loss: 13.813027, acc: 0.000000]\n",
            "41: [D loss: 0.003214, acc: 1.000000]  [G loss: 13.721662, acc: 0.000000]\n",
            "42: [D loss: 0.008483, acc: 0.992188]  [G loss: 13.580441, acc: 0.000000]\n",
            "43: [D loss: 0.016637, acc: 0.992188]  [G loss: 14.472204, acc: 0.000000]\n",
            "44: [D loss: 0.006779, acc: 1.000000]  [G loss: 14.746797, acc: 0.000000]\n",
            "45: [D loss: 0.009229, acc: 0.992188]  [G loss: 14.876952, acc: 0.000000]\n",
            "46: [D loss: 0.019395, acc: 0.992188]  [G loss: 14.397041, acc: 0.000000]\n",
            "47: [D loss: 0.038821, acc: 0.984375]  [G loss: 14.358242, acc: 0.000000]\n",
            "48: [D loss: 0.055458, acc: 0.984375]  [G loss: 15.416252, acc: 0.000000]\n",
            "49: [D loss: 0.008910, acc: 1.000000]  [G loss: 15.553122, acc: 0.000000]\n",
            "50: [D loss: 0.018210, acc: 0.992188]  [G loss: 15.516446, acc: 0.000000]\n",
            "51: [D loss: 0.020854, acc: 0.984375]  [G loss: 15.488674, acc: 0.000000]\n",
            "52: [D loss: 0.058160, acc: 0.984375]  [G loss: 15.435715, acc: 0.000000]\n",
            "53: [D loss: 0.049403, acc: 0.968750]  [G loss: 15.036625, acc: 0.000000]\n",
            "54: [D loss: 0.099912, acc: 0.953125]  [G loss: 15.878233, acc: 0.000000]\n",
            "55: [D loss: 0.415671, acc: 0.898438]  [G loss: 15.219679, acc: 0.000000]\n",
            "56: [D loss: 0.095036, acc: 0.968750]  [G loss: 13.867792, acc: 0.000000]\n",
            "57: [D loss: 0.278232, acc: 0.921875]  [G loss: 13.954920, acc: 0.015625]\n",
            "58: [D loss: 0.231211, acc: 0.898438]  [G loss: 13.916567, acc: 0.000000]\n",
            "59: [D loss: 0.066121, acc: 0.976562]  [G loss: 13.927666, acc: 0.000000]\n",
            "60: [D loss: 0.023831, acc: 1.000000]  [G loss: 14.048878, acc: 0.000000]\n",
            "61: [D loss: 0.090015, acc: 0.984375]  [G loss: 13.854259, acc: 0.000000]\n",
            "62: [D loss: 0.103090, acc: 0.953125]  [G loss: 12.955099, acc: 0.000000]\n",
            "63: [D loss: 0.058235, acc: 0.992188]  [G loss: 12.808742, acc: 0.000000]\n",
            "64: [D loss: 0.031885, acc: 1.000000]  [G loss: 12.394136, acc: 0.000000]\n",
            "65: [D loss: 0.054612, acc: 0.984375]  [G loss: 11.721654, acc: 0.000000]\n",
            "66: [D loss: 0.142309, acc: 0.937500]  [G loss: 11.313021, acc: 0.000000]\n",
            "67: [D loss: 0.045518, acc: 0.992188]  [G loss: 12.186031, acc: 0.000000]\n",
            "68: [D loss: 0.067131, acc: 0.976562]  [G loss: 11.500046, acc: 0.046875]\n",
            "69: [D loss: 0.119114, acc: 0.953125]  [G loss: 11.991243, acc: 0.000000]\n",
            "70: [D loss: 0.092269, acc: 0.976562]  [G loss: 12.351419, acc: 0.000000]\n",
            "71: [D loss: 0.085610, acc: 0.968750]  [G loss: 12.280567, acc: 0.000000]\n",
            "72: [D loss: 0.055916, acc: 0.984375]  [G loss: 12.904238, acc: 0.000000]\n",
            "73: [D loss: 0.098562, acc: 0.960938]  [G loss: 12.424458, acc: 0.000000]\n",
            "74: [D loss: 0.083401, acc: 0.984375]  [G loss: 12.162402, acc: 0.000000]\n",
            "75: [D loss: 0.048543, acc: 0.984375]  [G loss: 11.769075, acc: 0.000000]\n",
            "76: [D loss: 0.083081, acc: 0.960938]  [G loss: 12.113237, acc: 0.000000]\n",
            "77: [D loss: 0.028957, acc: 0.984375]  [G loss: 11.920877, acc: 0.000000]\n",
            "78: [D loss: 0.056934, acc: 0.976562]  [G loss: 12.651641, acc: 0.000000]\n",
            "79: [D loss: 0.033566, acc: 0.976562]  [G loss: 12.732405, acc: 0.000000]\n",
            "80: [D loss: 0.004563, acc: 1.000000]  [G loss: 12.972456, acc: 0.000000]\n",
            "81: [D loss: 0.022902, acc: 0.984375]  [G loss: 13.089743, acc: 0.000000]\n",
            "82: [D loss: 0.115305, acc: 0.976562]  [G loss: 12.560057, acc: 0.000000]\n",
            "83: [D loss: 0.048454, acc: 0.976562]  [G loss: 12.284571, acc: 0.000000]\n",
            "84: [D loss: 0.068732, acc: 0.968750]  [G loss: 11.783766, acc: 0.015625]\n",
            "85: [D loss: 0.085696, acc: 0.953125]  [G loss: 12.221490, acc: 0.000000]\n",
            "86: [D loss: 0.062192, acc: 0.968750]  [G loss: 12.018354, acc: 0.000000]\n",
            "87: [D loss: 0.052629, acc: 0.976562]  [G loss: 12.322803, acc: 0.000000]\n",
            "88: [D loss: 0.070357, acc: 0.976562]  [G loss: 11.852337, acc: 0.000000]\n",
            "89: [D loss: 0.026980, acc: 0.992188]  [G loss: 11.846901, acc: 0.000000]\n",
            "90: [D loss: 0.153929, acc: 0.937500]  [G loss: 11.503759, acc: 0.000000]\n",
            "91: [D loss: 0.096862, acc: 0.960938]  [G loss: 12.486115, acc: 0.000000]\n",
            "92: [D loss: 0.042667, acc: 0.976562]  [G loss: 13.087160, acc: 0.000000]\n",
            "93: [D loss: 0.011665, acc: 1.000000]  [G loss: 13.404013, acc: 0.000000]\n",
            "94: [D loss: 0.012380, acc: 0.992188]  [G loss: 13.910601, acc: 0.000000]\n",
            "95: [D loss: 0.015723, acc: 0.992188]  [G loss: 13.245071, acc: 0.000000]\n",
            "96: [D loss: 0.004942, acc: 1.000000]  [G loss: 13.022355, acc: 0.000000]\n",
            "97: [D loss: 0.005475, acc: 1.000000]  [G loss: 12.565227, acc: 0.000000]\n",
            "98: [D loss: 0.005237, acc: 1.000000]  [G loss: 13.271881, acc: 0.000000]\n",
            "99: [D loss: 0.042289, acc: 0.976562]  [G loss: 13.689610, acc: 0.000000]\n",
            "100: [D loss: 0.008171, acc: 0.992188]  [G loss: 14.908268, acc: 0.000000]\n",
            "101: [D loss: 0.006571, acc: 1.000000]  [G loss: 15.441092, acc: 0.000000]\n",
            "102: [D loss: 0.083924, acc: 0.984375]  [G loss: 14.696136, acc: 0.000000]\n",
            "103: [D loss: 0.033412, acc: 0.984375]  [G loss: 14.173246, acc: 0.000000]\n",
            "104: [D loss: 0.012645, acc: 0.992188]  [G loss: 13.451708, acc: 0.015625]\n",
            "105: [D loss: 0.092989, acc: 0.984375]  [G loss: 13.921370, acc: 0.031250]\n",
            "106: [D loss: 0.066548, acc: 0.976562]  [G loss: 15.547599, acc: 0.000000]\n",
            "107: [D loss: 0.170543, acc: 0.945312]  [G loss: 14.979666, acc: 0.000000]\n",
            "108: [D loss: 0.130849, acc: 0.960938]  [G loss: 13.577383, acc: 0.015625]\n",
            "109: [D loss: 0.215297, acc: 0.929688]  [G loss: 13.382471, acc: 0.031250]\n",
            "110: [D loss: 0.131772, acc: 0.953125]  [G loss: 14.610619, acc: 0.000000]\n",
            "111: [D loss: 0.116543, acc: 0.976562]  [G loss: 14.732170, acc: 0.000000]\n",
            "112: [D loss: 0.051129, acc: 0.976562]  [G loss: 14.524832, acc: 0.000000]\n",
            "113: [D loss: 0.053108, acc: 0.976562]  [G loss: 13.752941, acc: 0.000000]\n",
            "114: [D loss: 0.042017, acc: 0.984375]  [G loss: 13.277454, acc: 0.000000]\n",
            "115: [D loss: 0.054960, acc: 0.984375]  [G loss: 14.021593, acc: 0.000000]\n",
            "116: [D loss: 0.029095, acc: 0.984375]  [G loss: 14.077331, acc: 0.000000]\n",
            "117: [D loss: 0.064916, acc: 0.984375]  [G loss: 14.006832, acc: 0.000000]\n",
            "118: [D loss: 0.018924, acc: 1.000000]  [G loss: 13.869437, acc: 0.000000]\n",
            "119: [D loss: 0.068547, acc: 0.968750]  [G loss: 13.456327, acc: 0.000000]\n",
            "120: [D loss: 0.060139, acc: 0.968750]  [G loss: 13.124313, acc: 0.000000]\n",
            "121: [D loss: 0.037159, acc: 0.984375]  [G loss: 13.127215, acc: 0.000000]\n",
            "122: [D loss: 0.043506, acc: 0.984375]  [G loss: 13.763814, acc: 0.000000]\n",
            "123: [D loss: 0.009412, acc: 1.000000]  [G loss: 13.692516, acc: 0.000000]\n",
            "124: [D loss: 0.066785, acc: 0.984375]  [G loss: 13.724680, acc: 0.000000]\n",
            "125: [D loss: 0.027762, acc: 1.000000]  [G loss: 13.265580, acc: 0.000000]\n",
            "126: [D loss: 0.038175, acc: 0.984375]  [G loss: 12.677008, acc: 0.000000]\n",
            "127: [D loss: 0.040611, acc: 0.984375]  [G loss: 12.418314, acc: 0.000000]\n",
            "128: [D loss: 0.056294, acc: 0.976562]  [G loss: 12.378198, acc: 0.000000]\n",
            "129: [D loss: 0.069628, acc: 0.960938]  [G loss: 13.243700, acc: 0.000000]\n",
            "130: [D loss: 0.057452, acc: 0.976562]  [G loss: 13.740683, acc: 0.000000]\n",
            "131: [D loss: 0.098922, acc: 0.960938]  [G loss: 12.988478, acc: 0.000000]\n",
            "132: [D loss: 0.015591, acc: 1.000000]  [G loss: 12.698977, acc: 0.000000]\n",
            "133: [D loss: 0.089707, acc: 0.960938]  [G loss: 11.805191, acc: 0.000000]\n",
            "134: [D loss: 0.115021, acc: 0.953125]  [G loss: 11.728584, acc: 0.000000]\n",
            "135: [D loss: 0.059625, acc: 0.984375]  [G loss: 12.228684, acc: 0.000000]\n",
            "136: [D loss: 0.090730, acc: 0.968750]  [G loss: 12.984131, acc: 0.000000]\n",
            "137: [D loss: 0.043063, acc: 0.984375]  [G loss: 12.070460, acc: 0.000000]\n",
            "138: [D loss: 0.042520, acc: 0.984375]  [G loss: 11.652042, acc: 0.000000]\n",
            "139: [D loss: 0.015109, acc: 0.992188]  [G loss: 11.245611, acc: 0.000000]\n",
            "140: [D loss: 0.024417, acc: 0.992188]  [G loss: 9.868325, acc: 0.109375]\n",
            "141: [D loss: 0.025319, acc: 0.992188]  [G loss: 10.777695, acc: 0.062500]\n",
            "142: [D loss: 0.023387, acc: 0.992188]  [G loss: 12.190271, acc: 0.000000]\n",
            "143: [D loss: 0.001224, acc: 1.000000]  [G loss: 13.371594, acc: 0.000000]\n",
            "144: [D loss: 0.000378, acc: 1.000000]  [G loss: 14.256442, acc: 0.000000]\n",
            "145: [D loss: 0.010011, acc: 0.992188]  [G loss: 14.832792, acc: 0.000000]\n",
            "146: [D loss: 0.000482, acc: 1.000000]  [G loss: 15.016008, acc: 0.000000]\n",
            "147: [D loss: 0.002400, acc: 1.000000]  [G loss: 14.237764, acc: 0.000000]\n",
            "148: [D loss: 0.005196, acc: 1.000000]  [G loss: 13.427496, acc: 0.000000]\n",
            "149: [D loss: 0.002111, acc: 1.000000]  [G loss: 12.090684, acc: 0.000000]\n",
            "150: [D loss: 0.001362, acc: 1.000000]  [G loss: 11.594514, acc: 0.078125]\n",
            "151: [D loss: 0.050266, acc: 0.968750]  [G loss: 12.607940, acc: 0.000000]\n",
            "152: [D loss: 0.027219, acc: 0.984375]  [G loss: 12.767399, acc: 0.015625]\n",
            "153: [D loss: 0.008230, acc: 1.000000]  [G loss: 12.388814, acc: 0.031250]\n",
            "154: [D loss: 0.055874, acc: 0.984375]  [G loss: 11.074209, acc: 0.125000]\n",
            "155: [D loss: 0.035441, acc: 0.976562]  [G loss: 12.654310, acc: 0.000000]\n",
            "156: [D loss: 0.027493, acc: 0.984375]  [G loss: 12.184606, acc: 0.031250]\n",
            "157: [D loss: 0.049890, acc: 0.984375]  [G loss: 11.375898, acc: 0.109375]\n",
            "158: [D loss: 0.160953, acc: 0.914062]  [G loss: 9.286469, acc: 0.312500]\n",
            "159: [D loss: 0.647393, acc: 0.875000]  [G loss: 15.984277, acc: 0.000000]\n",
            "160: [D loss: 1.333491, acc: 0.726562]  [G loss: 11.725950, acc: 0.031250]\n",
            "161: [D loss: 0.051121, acc: 0.984375]  [G loss: 9.240832, acc: 0.265625]\n",
            "162: [D loss: 0.950752, acc: 0.843750]  [G loss: 10.880678, acc: 0.046875]\n",
            "163: [D loss: 0.036441, acc: 0.984375]  [G loss: 13.061878, acc: 0.000000]\n",
            "164: [D loss: 0.003200, acc: 1.000000]  [G loss: 14.132710, acc: 0.000000]\n",
            "165: [D loss: 0.002479, acc: 1.000000]  [G loss: 15.113855, acc: 0.000000]\n",
            "166: [D loss: 0.016214, acc: 1.000000]  [G loss: 15.331238, acc: 0.000000]\n",
            "167: [D loss: 0.082575, acc: 0.960938]  [G loss: 15.459812, acc: 0.000000]\n",
            "168: [D loss: 0.101920, acc: 0.984375]  [G loss: 15.178738, acc: 0.000000]\n",
            "169: [D loss: 0.049799, acc: 0.976562]  [G loss: 14.148670, acc: 0.000000]\n",
            "170: [D loss: 0.019370, acc: 0.992188]  [G loss: 13.887503, acc: 0.000000]\n",
            "171: [D loss: 0.002861, acc: 1.000000]  [G loss: 12.805391, acc: 0.000000]\n",
            "172: [D loss: 0.001807, acc: 1.000000]  [G loss: 12.174072, acc: 0.000000]\n",
            "173: [D loss: 0.007566, acc: 1.000000]  [G loss: 11.814465, acc: 0.000000]\n",
            "174: [D loss: 0.007755, acc: 1.000000]  [G loss: 11.349886, acc: 0.000000]\n",
            "175: [D loss: 0.024358, acc: 1.000000]  [G loss: 11.117124, acc: 0.000000]\n",
            "176: [D loss: 0.043552, acc: 0.992188]  [G loss: 11.467319, acc: 0.015625]\n",
            "177: [D loss: 0.035417, acc: 1.000000]  [G loss: 10.986096, acc: 0.000000]\n",
            "178: [D loss: 0.022803, acc: 1.000000]  [G loss: 11.099838, acc: 0.000000]\n",
            "179: [D loss: 0.012077, acc: 1.000000]  [G loss: 11.188072, acc: 0.000000]\n",
            "180: [D loss: 0.005272, acc: 1.000000]  [G loss: 11.486775, acc: 0.000000]\n",
            "181: [D loss: 0.003310, acc: 1.000000]  [G loss: 11.983829, acc: 0.000000]\n",
            "182: [D loss: 0.001401, acc: 1.000000]  [G loss: 12.103077, acc: 0.000000]\n",
            "183: [D loss: 0.001481, acc: 1.000000]  [G loss: 12.098612, acc: 0.000000]\n",
            "184: [D loss: 0.001058, acc: 1.000000]  [G loss: 12.708405, acc: 0.000000]\n",
            "185: [D loss: 0.003922, acc: 1.000000]  [G loss: 12.492948, acc: 0.000000]\n",
            "186: [D loss: 0.000403, acc: 1.000000]  [G loss: 12.341402, acc: 0.000000]\n",
            "187: [D loss: 0.001541, acc: 1.000000]  [G loss: 12.361795, acc: 0.000000]\n",
            "188: [D loss: 0.004311, acc: 1.000000]  [G loss: 12.532769, acc: 0.000000]\n",
            "189: [D loss: 0.000951, acc: 1.000000]  [G loss: 12.304502, acc: 0.000000]\n",
            "190: [D loss: 0.001170, acc: 1.000000]  [G loss: 11.852966, acc: 0.000000]\n",
            "191: [D loss: 0.005253, acc: 1.000000]  [G loss: 11.909149, acc: 0.000000]\n",
            "192: [D loss: 0.005805, acc: 1.000000]  [G loss: 11.089405, acc: 0.000000]\n",
            "193: [D loss: 0.016497, acc: 0.992188]  [G loss: 11.213793, acc: 0.000000]\n",
            "194: [D loss: 0.016631, acc: 0.992188]  [G loss: 10.730488, acc: 0.015625]\n",
            "195: [D loss: 0.025096, acc: 1.000000]  [G loss: 10.590902, acc: 0.000000]\n",
            "196: [D loss: 0.043420, acc: 0.992188]  [G loss: 10.407175, acc: 0.000000]\n",
            "197: [D loss: 0.019642, acc: 1.000000]  [G loss: 10.992774, acc: 0.000000]\n",
            "198: [D loss: 0.031606, acc: 0.992188]  [G loss: 10.737880, acc: 0.015625]\n",
            "199: [D loss: 0.031324, acc: 0.984375]  [G loss: 11.324965, acc: 0.015625]\n",
            "200: [D loss: 0.051255, acc: 0.976562]  [G loss: 11.051268, acc: 0.000000]\n",
            "201: [D loss: 0.038485, acc: 0.992188]  [G loss: 10.640999, acc: 0.000000]\n",
            "202: [D loss: 0.124079, acc: 0.953125]  [G loss: 10.443134, acc: 0.031250]\n",
            "203: [D loss: 0.028682, acc: 1.000000]  [G loss: 10.258511, acc: 0.046875]\n",
            "204: [D loss: 0.082939, acc: 0.953125]  [G loss: 10.255367, acc: 0.109375]\n",
            "205: [D loss: 0.044602, acc: 0.976562]  [G loss: 9.582514, acc: 0.000000]\n",
            "206: [D loss: 0.027485, acc: 0.992188]  [G loss: 10.168667, acc: 0.015625]\n",
            "207: [D loss: 0.100154, acc: 0.960938]  [G loss: 10.569408, acc: 0.000000]\n",
            "208: [D loss: 0.127760, acc: 0.953125]  [G loss: 10.122957, acc: 0.031250]\n",
            "209: [D loss: 0.105627, acc: 0.968750]  [G loss: 8.725362, acc: 0.156250]\n",
            "210: [D loss: 0.084471, acc: 0.953125]  [G loss: 8.476839, acc: 0.281250]\n",
            "211: [D loss: 0.111828, acc: 0.960938]  [G loss: 8.607862, acc: 0.234375]\n",
            "212: [D loss: 0.089707, acc: 0.953125]  [G loss: 9.202748, acc: 0.078125]\n",
            "213: [D loss: 0.090004, acc: 0.960938]  [G loss: 9.808743, acc: 0.015625]\n",
            "214: [D loss: 0.080633, acc: 0.984375]  [G loss: 9.694736, acc: 0.000000]\n",
            "215: [D loss: 0.077762, acc: 0.960938]  [G loss: 8.398441, acc: 0.015625]\n",
            "216: [D loss: 0.035959, acc: 0.992188]  [G loss: 8.794283, acc: 0.015625]\n",
            "217: [D loss: 0.033815, acc: 0.984375]  [G loss: 8.345148, acc: 0.171875]\n",
            "218: [D loss: 0.043472, acc: 0.976562]  [G loss: 8.529170, acc: 0.140625]\n",
            "219: [D loss: 0.033431, acc: 0.984375]  [G loss: 8.341149, acc: 0.062500]\n",
            "220: [D loss: 0.044749, acc: 0.992188]  [G loss: 9.070962, acc: 0.031250]\n",
            "221: [D loss: 0.064053, acc: 0.984375]  [G loss: 9.036236, acc: 0.000000]\n",
            "222: [D loss: 0.022115, acc: 0.992188]  [G loss: 9.195759, acc: 0.031250]\n",
            "223: [D loss: 0.011187, acc: 1.000000]  [G loss: 8.563026, acc: 0.093750]\n",
            "224: [D loss: 0.014528, acc: 0.992188]  [G loss: 9.973654, acc: 0.109375]\n",
            "225: [D loss: 0.047530, acc: 0.992188]  [G loss: 8.395248, acc: 0.218750]\n",
            "226: [D loss: 0.008052, acc: 1.000000]  [G loss: 8.949559, acc: 0.140625]\n",
            "227: [D loss: 0.057491, acc: 0.960938]  [G loss: 8.921261, acc: 0.203125]\n",
            "228: [D loss: 0.009206, acc: 1.000000]  [G loss: 8.520039, acc: 0.171875]\n",
            "229: [D loss: 0.019734, acc: 1.000000]  [G loss: 8.737808, acc: 0.218750]\n",
            "230: [D loss: 0.029912, acc: 0.984375]  [G loss: 8.597307, acc: 0.093750]\n",
            "231: [D loss: 0.025246, acc: 0.992188]  [G loss: 8.279641, acc: 0.109375]\n",
            "232: [D loss: 0.051111, acc: 0.992188]  [G loss: 8.935902, acc: 0.093750]\n",
            "233: [D loss: 0.021280, acc: 0.992188]  [G loss: 9.245704, acc: 0.109375]\n",
            "234: [D loss: 0.022973, acc: 1.000000]  [G loss: 8.642389, acc: 0.156250]\n",
            "235: [D loss: 0.028024, acc: 0.992188]  [G loss: 9.126305, acc: 0.140625]\n",
            "236: [D loss: 0.011595, acc: 1.000000]  [G loss: 9.127085, acc: 0.015625]\n",
            "237: [D loss: 0.048484, acc: 0.976562]  [G loss: 9.319201, acc: 0.015625]\n",
            "238: [D loss: 0.019810, acc: 0.992188]  [G loss: 9.456830, acc: 0.031250]\n",
            "239: [D loss: 0.013563, acc: 0.992188]  [G loss: 9.993435, acc: 0.000000]\n",
            "240: [D loss: 0.058789, acc: 0.976562]  [G loss: 9.923264, acc: 0.031250]\n",
            "241: [D loss: 0.005203, acc: 1.000000]  [G loss: 9.563527, acc: 0.015625]\n",
            "242: [D loss: 0.019245, acc: 0.992188]  [G loss: 9.538332, acc: 0.093750]\n",
            "243: [D loss: 0.057191, acc: 0.992188]  [G loss: 8.508877, acc: 0.062500]\n",
            "244: [D loss: 0.031008, acc: 0.992188]  [G loss: 8.980913, acc: 0.046875]\n",
            "245: [D loss: 0.006705, acc: 1.000000]  [G loss: 8.927732, acc: 0.031250]\n",
            "246: [D loss: 0.013231, acc: 0.992188]  [G loss: 9.712427, acc: 0.000000]\n",
            "247: [D loss: 0.000477, acc: 1.000000]  [G loss: 10.155434, acc: 0.000000]\n",
            "248: [D loss: 0.002820, acc: 1.000000]  [G loss: 10.965822, acc: 0.000000]\n",
            "249: [D loss: 0.000438, acc: 1.000000]  [G loss: 11.039492, acc: 0.000000]\n",
            "250: [D loss: 0.000426, acc: 1.000000]  [G loss: 12.060773, acc: 0.000000]\n",
            "251: [D loss: 0.001289, acc: 1.000000]  [G loss: 11.950580, acc: 0.000000]\n",
            "252: [D loss: 0.018745, acc: 0.992188]  [G loss: 11.436381, acc: 0.015625]\n",
            "253: [D loss: 0.000474, acc: 1.000000]  [G loss: 11.048152, acc: 0.000000]\n",
            "254: [D loss: 0.037999, acc: 0.992188]  [G loss: 10.602406, acc: 0.000000]\n",
            "255: [D loss: 0.000545, acc: 1.000000]  [G loss: 9.407106, acc: 0.046875]\n",
            "256: [D loss: 0.006040, acc: 1.000000]  [G loss: 8.608432, acc: 0.062500]\n",
            "257: [D loss: 0.031753, acc: 0.984375]  [G loss: 9.350564, acc: 0.078125]\n",
            "258: [D loss: 0.032884, acc: 0.984375]  [G loss: 10.687435, acc: 0.000000]\n",
            "259: [D loss: 0.000526, acc: 1.000000]  [G loss: 11.335567, acc: 0.000000]\n",
            "260: [D loss: 0.000341, acc: 1.000000]  [G loss: 11.744367, acc: 0.000000]\n",
            "261: [D loss: 0.041250, acc: 0.992188]  [G loss: 12.717546, acc: 0.000000]\n",
            "262: [D loss: 0.030595, acc: 0.976562]  [G loss: 11.882982, acc: 0.000000]\n",
            "263: [D loss: 0.000330, acc: 1.000000]  [G loss: 10.819277, acc: 0.000000]\n",
            "264: [D loss: 0.000228, acc: 1.000000]  [G loss: 10.900331, acc: 0.000000]\n",
            "265: [D loss: 0.000141, acc: 1.000000]  [G loss: 9.557048, acc: 0.031250]\n",
            "266: [D loss: 0.001418, acc: 1.000000]  [G loss: 9.811217, acc: 0.031250]\n",
            "267: [D loss: 0.003689, acc: 1.000000]  [G loss: 8.798052, acc: 0.078125]\n",
            "268: [D loss: 0.016110, acc: 0.992188]  [G loss: 9.218620, acc: 0.046875]\n",
            "269: [D loss: 0.003095, acc: 1.000000]  [G loss: 9.157911, acc: 0.031250]\n",
            "270: [D loss: 0.025554, acc: 0.992188]  [G loss: 9.621745, acc: 0.015625]\n",
            "271: [D loss: 0.000903, acc: 1.000000]  [G loss: 10.455700, acc: 0.046875]\n",
            "272: [D loss: 0.003455, acc: 1.000000]  [G loss: 11.371449, acc: 0.000000]\n",
            "273: [D loss: 0.000937, acc: 1.000000]  [G loss: 11.563805, acc: 0.000000]\n",
            "274: [D loss: 0.003290, acc: 1.000000]  [G loss: 12.046005, acc: 0.015625]\n",
            "275: [D loss: 0.008904, acc: 1.000000]  [G loss: 11.560627, acc: 0.015625]\n",
            "276: [D loss: 0.070029, acc: 0.968750]  [G loss: 10.387260, acc: 0.015625]\n",
            "277: [D loss: 0.017016, acc: 0.992188]  [G loss: 9.212825, acc: 0.031250]\n",
            "278: [D loss: 0.014292, acc: 0.992188]  [G loss: 8.912260, acc: 0.062500]\n",
            "279: [D loss: 0.001519, acc: 1.000000]  [G loss: 8.674961, acc: 0.125000]\n",
            "280: [D loss: 0.002566, acc: 1.000000]  [G loss: 8.584538, acc: 0.156250]\n",
            "281: [D loss: 0.026143, acc: 0.984375]  [G loss: 9.081591, acc: 0.078125]\n",
            "282: [D loss: 0.016732, acc: 0.992188]  [G loss: 9.670685, acc: 0.015625]\n",
            "283: [D loss: 0.001142, acc: 1.000000]  [G loss: 10.500082, acc: 0.015625]\n",
            "284: [D loss: 0.011078, acc: 0.992188]  [G loss: 10.725427, acc: 0.031250]\n",
            "285: [D loss: 0.015530, acc: 0.992188]  [G loss: 10.626801, acc: 0.000000]\n",
            "286: [D loss: 0.045857, acc: 0.976562]  [G loss: 9.380644, acc: 0.031250]\n",
            "287: [D loss: 0.000628, acc: 1.000000]  [G loss: 7.736389, acc: 0.250000]\n",
            "288: [D loss: 0.005435, acc: 1.000000]  [G loss: 8.006243, acc: 0.390625]\n",
            "289: [D loss: 0.058067, acc: 0.984375]  [G loss: 7.490485, acc: 0.359375]\n",
            "290: [D loss: 0.029637, acc: 0.984375]  [G loss: 8.577411, acc: 0.125000]\n",
            "291: [D loss: 0.006110, acc: 1.000000]  [G loss: 9.472772, acc: 0.046875]\n",
            "292: [D loss: 0.044164, acc: 0.992188]  [G loss: 9.765539, acc: 0.046875]\n",
            "293: [D loss: 0.043747, acc: 0.984375]  [G loss: 9.512241, acc: 0.000000]\n",
            "294: [D loss: 0.002361, acc: 1.000000]  [G loss: 9.170155, acc: 0.093750]\n",
            "295: [D loss: 0.016189, acc: 0.992188]  [G loss: 8.399414, acc: 0.031250]\n",
            "296: [D loss: 0.012393, acc: 0.992188]  [G loss: 8.516130, acc: 0.234375]\n",
            "297: [D loss: 0.014559, acc: 1.000000]  [G loss: 7.552018, acc: 0.359375]\n",
            "298: [D loss: 0.025368, acc: 0.992188]  [G loss: 7.855759, acc: 0.203125]\n",
            "299: [D loss: 0.006132, acc: 1.000000]  [G loss: 7.836801, acc: 0.312500]\n",
            "300: [D loss: 0.012407, acc: 0.992188]  [G loss: 8.308678, acc: 0.140625]\n",
            "301: [D loss: 0.001036, acc: 1.000000]  [G loss: 8.431124, acc: 0.156250]\n",
            "302: [D loss: 0.004844, acc: 1.000000]  [G loss: 9.165291, acc: 0.031250]\n",
            "303: [D loss: 0.002026, acc: 1.000000]  [G loss: 9.499942, acc: 0.078125]\n",
            "304: [D loss: 0.001654, acc: 1.000000]  [G loss: 9.602171, acc: 0.078125]\n",
            "305: [D loss: 0.001516, acc: 1.000000]  [G loss: 9.437454, acc: 0.046875]\n",
            "306: [D loss: 0.000610, acc: 1.000000]  [G loss: 9.148428, acc: 0.062500]\n",
            "307: [D loss: 0.018650, acc: 0.992188]  [G loss: 9.426369, acc: 0.015625]\n",
            "308: [D loss: 0.004053, acc: 1.000000]  [G loss: 9.963176, acc: 0.015625]\n",
            "309: [D loss: 0.012527, acc: 1.000000]  [G loss: 9.057409, acc: 0.031250]\n",
            "310: [D loss: 0.010019, acc: 1.000000]  [G loss: 9.572325, acc: 0.000000]\n",
            "311: [D loss: 0.021116, acc: 0.992188]  [G loss: 9.905958, acc: 0.031250]\n",
            "312: [D loss: 0.194359, acc: 0.945312]  [G loss: 8.267661, acc: 0.062500]\n",
            "313: [D loss: 0.160769, acc: 0.937500]  [G loss: 9.027809, acc: 0.187500]\n",
            "314: [D loss: 0.013662, acc: 1.000000]  [G loss: 10.506922, acc: 0.000000]\n",
            "315: [D loss: 0.003548, acc: 1.000000]  [G loss: 11.284426, acc: 0.000000]\n",
            "316: [D loss: 0.042615, acc: 0.968750]  [G loss: 11.105347, acc: 0.000000]\n",
            "317: [D loss: 0.112268, acc: 0.960938]  [G loss: 9.059156, acc: 0.000000]\n",
            "318: [D loss: 0.041071, acc: 0.976562]  [G loss: 8.626642, acc: 0.265625]\n",
            "319: [D loss: 0.041938, acc: 0.992188]  [G loss: 8.441096, acc: 0.265625]\n",
            "320: [D loss: 0.064561, acc: 0.984375]  [G loss: 9.306204, acc: 0.000000]\n",
            "321: [D loss: 0.041450, acc: 0.992188]  [G loss: 9.355558, acc: 0.031250]\n",
            "322: [D loss: 0.008186, acc: 1.000000]  [G loss: 10.408724, acc: 0.000000]\n",
            "323: [D loss: 0.001408, acc: 1.000000]  [G loss: 11.379575, acc: 0.000000]\n",
            "324: [D loss: 0.041299, acc: 0.984375]  [G loss: 11.071314, acc: 0.000000]\n",
            "325: [D loss: 0.010634, acc: 0.992188]  [G loss: 10.681490, acc: 0.000000]\n",
            "326: [D loss: 0.033182, acc: 0.992188]  [G loss: 9.394936, acc: 0.031250]\n",
            "327: [D loss: 0.035842, acc: 0.984375]  [G loss: 9.632395, acc: 0.046875]\n",
            "328: [D loss: 0.031776, acc: 0.976562]  [G loss: 9.921532, acc: 0.000000]\n",
            "329: [D loss: 0.014950, acc: 0.992188]  [G loss: 10.449909, acc: 0.000000]\n",
            "330: [D loss: 0.067291, acc: 0.976562]  [G loss: 10.979038, acc: 0.000000]\n",
            "331: [D loss: 0.027492, acc: 0.992188]  [G loss: 8.896416, acc: 0.281250]\n",
            "332: [D loss: 0.057988, acc: 0.984375]  [G loss: 9.064953, acc: 0.000000]\n",
            "333: [D loss: 0.035688, acc: 0.976562]  [G loss: 9.456230, acc: 0.046875]\n",
            "334: [D loss: 0.094498, acc: 0.992188]  [G loss: 10.214784, acc: 0.000000]\n",
            "335: [D loss: 0.024893, acc: 1.000000]  [G loss: 9.529734, acc: 0.000000]\n",
            "336: [D loss: 0.052501, acc: 0.992188]  [G loss: 9.238794, acc: 0.015625]\n",
            "337: [D loss: 0.011038, acc: 1.000000]  [G loss: 9.454493, acc: 0.000000]\n",
            "338: [D loss: 0.033413, acc: 0.984375]  [G loss: 10.006672, acc: 0.000000]\n",
            "339: [D loss: 0.005372, acc: 1.000000]  [G loss: 10.647388, acc: 0.000000]\n",
            "340: [D loss: 0.042024, acc: 0.992188]  [G loss: 10.549000, acc: 0.000000]\n",
            "341: [D loss: 0.003574, acc: 1.000000]  [G loss: 10.927542, acc: 0.000000]\n",
            "342: [D loss: 0.014338, acc: 0.992188]  [G loss: 10.308148, acc: 0.000000]\n",
            "343: [D loss: 0.005669, acc: 1.000000]  [G loss: 10.424118, acc: 0.000000]\n",
            "344: [D loss: 0.006962, acc: 1.000000]  [G loss: 10.370512, acc: 0.000000]\n",
            "345: [D loss: 0.010655, acc: 1.000000]  [G loss: 10.777390, acc: 0.000000]\n",
            "346: [D loss: 0.011841, acc: 1.000000]  [G loss: 11.151384, acc: 0.000000]\n",
            "347: [D loss: 0.001995, acc: 1.000000]  [G loss: 11.281775, acc: 0.000000]\n",
            "348: [D loss: 0.007967, acc: 1.000000]  [G loss: 11.307864, acc: 0.000000]\n",
            "349: [D loss: 0.001309, acc: 1.000000]  [G loss: 12.564532, acc: 0.000000]\n",
            "350: [D loss: 0.001600, acc: 1.000000]  [G loss: 12.669596, acc: 0.000000]\n",
            "351: [D loss: 0.002949, acc: 1.000000]  [G loss: 12.768829, acc: 0.000000]\n",
            "352: [D loss: 0.004026, acc: 1.000000]  [G loss: 12.759417, acc: 0.000000]\n",
            "353: [D loss: 0.006011, acc: 1.000000]  [G loss: 12.489830, acc: 0.000000]\n",
            "354: [D loss: 0.034601, acc: 0.992188]  [G loss: 11.139101, acc: 0.000000]\n",
            "355: [D loss: 0.050972, acc: 0.984375]  [G loss: 11.051655, acc: 0.031250]\n",
            "356: [D loss: 0.113828, acc: 0.960938]  [G loss: 11.549445, acc: 0.015625]\n",
            "357: [D loss: 0.023039, acc: 0.992188]  [G loss: 10.997921, acc: 0.000000]\n",
            "358: [D loss: 0.010896, acc: 0.992188]  [G loss: 11.368910, acc: 0.015625]\n",
            "359: [D loss: 0.007486, acc: 1.000000]  [G loss: 10.992893, acc: 0.000000]\n",
            "360: [D loss: 0.014938, acc: 0.992188]  [G loss: 10.921673, acc: 0.000000]\n",
            "361: [D loss: 0.008593, acc: 1.000000]  [G loss: 10.473645, acc: 0.000000]\n",
            "362: [D loss: 0.080461, acc: 0.968750]  [G loss: 9.658278, acc: 0.015625]\n",
            "363: [D loss: 0.069812, acc: 0.968750]  [G loss: 8.764254, acc: 0.031250]\n",
            "364: [D loss: 0.076060, acc: 0.984375]  [G loss: 8.881735, acc: 0.031250]\n",
            "365: [D loss: 0.033819, acc: 0.984375]  [G loss: 10.019788, acc: 0.015625]\n",
            "366: [D loss: 0.006048, acc: 1.000000]  [G loss: 11.419327, acc: 0.000000]\n",
            "367: [D loss: 0.004514, acc: 1.000000]  [G loss: 12.052696, acc: 0.000000]\n",
            "368: [D loss: 0.006908, acc: 1.000000]  [G loss: 12.221334, acc: 0.000000]\n",
            "369: [D loss: 0.016973, acc: 0.992188]  [G loss: 11.694532, acc: 0.015625]\n",
            "370: [D loss: 0.020553, acc: 0.984375]  [G loss: 11.593694, acc: 0.031250]\n",
            "371: [D loss: 0.034126, acc: 0.984375]  [G loss: 10.609598, acc: 0.031250]\n",
            "372: [D loss: 0.036004, acc: 0.976562]  [G loss: 9.549477, acc: 0.109375]\n",
            "373: [D loss: 0.003180, acc: 1.000000]  [G loss: 9.903761, acc: 0.078125]\n",
            "374: [D loss: 0.009101, acc: 0.992188]  [G loss: 10.114781, acc: 0.078125]\n",
            "375: [D loss: 0.008126, acc: 0.992188]  [G loss: 9.604747, acc: 0.078125]\n",
            "376: [D loss: 0.002613, acc: 1.000000]  [G loss: 9.927857, acc: 0.078125]\n",
            "377: [D loss: 0.038929, acc: 0.992188]  [G loss: 8.681308, acc: 0.109375]\n",
            "378: [D loss: 0.004244, acc: 1.000000]  [G loss: 8.725657, acc: 0.234375]\n",
            "379: [D loss: 0.002452, acc: 1.000000]  [G loss: 8.101109, acc: 0.281250]\n",
            "380: [D loss: 0.013802, acc: 0.992188]  [G loss: 7.661604, acc: 0.281250]\n",
            "381: [D loss: 0.018260, acc: 0.992188]  [G loss: 8.183828, acc: 0.203125]\n",
            "382: [D loss: 0.003411, acc: 1.000000]  [G loss: 8.764169, acc: 0.156250]\n",
            "383: [D loss: 0.002256, acc: 1.000000]  [G loss: 8.872705, acc: 0.109375]\n",
            "384: [D loss: 0.002679, acc: 1.000000]  [G loss: 9.581358, acc: 0.078125]\n",
            "385: [D loss: 0.028691, acc: 0.992188]  [G loss: 8.768423, acc: 0.109375]\n",
            "386: [D loss: 0.016681, acc: 0.992188]  [G loss: 8.163011, acc: 0.187500]\n",
            "387: [D loss: 0.011814, acc: 0.992188]  [G loss: 7.947573, acc: 0.218750]\n",
            "388: [D loss: 0.014886, acc: 1.000000]  [G loss: 7.766195, acc: 0.187500]\n",
            "389: [D loss: 0.019219, acc: 0.992188]  [G loss: 8.090151, acc: 0.218750]\n",
            "390: [D loss: 0.173890, acc: 0.960938]  [G loss: 7.460112, acc: 0.390625]\n",
            "391: [D loss: 0.054111, acc: 0.984375]  [G loss: 6.767556, acc: 0.375000]\n",
            "392: [D loss: 0.028560, acc: 0.992188]  [G loss: 8.468410, acc: 0.203125]\n",
            "393: [D loss: 0.031837, acc: 0.992188]  [G loss: 8.239684, acc: 0.281250]\n",
            "394: [D loss: 0.019940, acc: 0.992188]  [G loss: 8.885506, acc: 0.093750]\n",
            "395: [D loss: 0.176631, acc: 0.953125]  [G loss: 6.327432, acc: 0.468750]\n",
            "396: [D loss: 0.174714, acc: 0.953125]  [G loss: 7.509327, acc: 0.375000]\n",
            "397: [D loss: 0.080085, acc: 0.968750]  [G loss: 9.008709, acc: 0.093750]\n",
            "398: [D loss: 0.107335, acc: 0.960938]  [G loss: 9.505011, acc: 0.015625]\n",
            "399: [D loss: 0.016795, acc: 1.000000]  [G loss: 9.547937, acc: 0.000000]\n",
            "400: [D loss: 0.022471, acc: 0.992188]  [G loss: 8.947865, acc: 0.109375]\n",
            "401: [D loss: 0.028871, acc: 0.992188]  [G loss: 7.567835, acc: 0.125000]\n",
            "402: [D loss: 0.023261, acc: 0.992188]  [G loss: 7.173120, acc: 0.343750]\n",
            "403: [D loss: 0.055780, acc: 0.968750]  [G loss: 7.784392, acc: 0.187500]\n",
            "404: [D loss: 0.015369, acc: 0.992188]  [G loss: 8.348809, acc: 0.093750]\n",
            "405: [D loss: 0.038463, acc: 0.976562]  [G loss: 9.607622, acc: 0.000000]\n",
            "406: [D loss: 0.137597, acc: 0.968750]  [G loss: 8.455562, acc: 0.015625]\n",
            "407: [D loss: 0.023613, acc: 0.992188]  [G loss: 7.884427, acc: 0.078125]\n",
            "408: [D loss: 0.012449, acc: 1.000000]  [G loss: 7.097843, acc: 0.187500]\n",
            "409: [D loss: 0.041200, acc: 0.984375]  [G loss: 8.108179, acc: 0.171875]\n",
            "410: [D loss: 0.028391, acc: 0.992188]  [G loss: 8.704443, acc: 0.062500]\n",
            "411: [D loss: 0.033533, acc: 0.992188]  [G loss: 10.222355, acc: 0.000000]\n",
            "412: [D loss: 0.015122, acc: 0.992188]  [G loss: 11.053114, acc: 0.000000]\n",
            "413: [D loss: 0.022000, acc: 0.992188]  [G loss: 10.695955, acc: 0.000000]\n",
            "414: [D loss: 0.061937, acc: 0.984375]  [G loss: 9.570652, acc: 0.015625]\n",
            "415: [D loss: 0.043983, acc: 0.984375]  [G loss: 7.828599, acc: 0.203125]\n",
            "416: [D loss: 0.109558, acc: 0.960938]  [G loss: 7.559572, acc: 0.203125]\n",
            "417: [D loss: 0.156716, acc: 0.945312]  [G loss: 9.937181, acc: 0.015625]\n",
            "418: [D loss: 0.006984, acc: 1.000000]  [G loss: 12.411384, acc: 0.000000]\n",
            "419: [D loss: 0.021361, acc: 1.000000]  [G loss: 14.227413, acc: 0.000000]\n",
            "420: [D loss: 0.214263, acc: 0.929688]  [G loss: 11.773344, acc: 0.000000]\n",
            "421: [D loss: 0.042018, acc: 0.992188]  [G loss: 9.550586, acc: 0.015625]\n",
            "422: [D loss: 0.011182, acc: 1.000000]  [G loss: 8.407154, acc: 0.187500]\n",
            "423: [D loss: 0.109201, acc: 0.945312]  [G loss: 8.351941, acc: 0.078125]\n",
            "424: [D loss: 0.127885, acc: 0.937500]  [G loss: 10.633573, acc: 0.015625]\n",
            "425: [D loss: 0.004601, acc: 1.000000]  [G loss: 13.239284, acc: 0.000000]\n",
            "426: [D loss: 0.056272, acc: 0.976562]  [G loss: 14.332169, acc: 0.000000]\n",
            "427: [D loss: 0.146705, acc: 0.953125]  [G loss: 12.548248, acc: 0.015625]\n",
            "428: [D loss: 0.059231, acc: 0.976562]  [G loss: 10.363153, acc: 0.015625]\n",
            "429: [D loss: 0.006295, acc: 1.000000]  [G loss: 8.711435, acc: 0.156250]\n",
            "430: [D loss: 0.022131, acc: 0.984375]  [G loss: 7.862822, acc: 0.250000]\n",
            "431: [D loss: 0.040965, acc: 0.992188]  [G loss: 6.670316, acc: 0.312500]\n",
            "432: [D loss: 0.107349, acc: 0.960938]  [G loss: 8.111335, acc: 0.140625]\n",
            "433: [D loss: 0.061412, acc: 0.984375]  [G loss: 9.873121, acc: 0.062500]\n",
            "434: [D loss: 0.015516, acc: 0.992188]  [G loss: 12.014545, acc: 0.031250]\n",
            "435: [D loss: 0.035536, acc: 0.976562]  [G loss: 12.668814, acc: 0.015625]\n",
            "436: [D loss: 0.119632, acc: 0.945312]  [G loss: 12.640760, acc: 0.031250]\n",
            "437: [D loss: 0.126759, acc: 0.945312]  [G loss: 10.663483, acc: 0.031250]\n",
            "438: [D loss: 0.008615, acc: 1.000000]  [G loss: 8.336078, acc: 0.109375]\n",
            "439: [D loss: 0.040498, acc: 0.984375]  [G loss: 7.558288, acc: 0.187500]\n",
            "440: [D loss: 0.060353, acc: 0.968750]  [G loss: 7.518288, acc: 0.140625]\n",
            "441: [D loss: 0.034968, acc: 0.992188]  [G loss: 8.615658, acc: 0.078125]\n",
            "442: [D loss: 0.007991, acc: 1.000000]  [G loss: 9.429873, acc: 0.015625]\n",
            "443: [D loss: 0.004033, acc: 1.000000]  [G loss: 9.636307, acc: 0.000000]\n",
            "444: [D loss: 0.002627, acc: 1.000000]  [G loss: 11.079546, acc: 0.000000]\n",
            "445: [D loss: 0.057128, acc: 0.976562]  [G loss: 10.591806, acc: 0.000000]\n",
            "446: [D loss: 0.036407, acc: 0.984375]  [G loss: 9.612413, acc: 0.000000]\n",
            "447: [D loss: 0.033728, acc: 0.984375]  [G loss: 8.890426, acc: 0.000000]\n",
            "448: [D loss: 0.058192, acc: 0.976562]  [G loss: 8.470888, acc: 0.031250]\n",
            "449: [D loss: 0.107961, acc: 0.960938]  [G loss: 10.758537, acc: 0.000000]\n",
            "450: [D loss: 0.016212, acc: 0.992188]  [G loss: 12.296417, acc: 0.000000]\n",
            "451: [D loss: 0.130814, acc: 0.953125]  [G loss: 11.902292, acc: 0.000000]\n",
            "452: [D loss: 0.054707, acc: 0.960938]  [G loss: 10.854850, acc: 0.000000]\n",
            "453: [D loss: 0.036432, acc: 0.992188]  [G loss: 9.138334, acc: 0.000000]\n",
            "454: [D loss: 0.018813, acc: 1.000000]  [G loss: 8.142829, acc: 0.000000]\n",
            "455: [D loss: 0.064358, acc: 0.968750]  [G loss: 8.346779, acc: 0.031250]\n",
            "456: [D loss: 0.053507, acc: 0.984375]  [G loss: 8.431606, acc: 0.046875]\n",
            "457: [D loss: 0.025900, acc: 0.992188]  [G loss: 9.288534, acc: 0.000000]\n",
            "458: [D loss: 0.017357, acc: 0.992188]  [G loss: 9.778981, acc: 0.000000]\n",
            "459: [D loss: 0.029243, acc: 0.984375]  [G loss: 9.187257, acc: 0.000000]\n",
            "460: [D loss: 0.042903, acc: 0.984375]  [G loss: 8.889864, acc: 0.031250]\n",
            "461: [D loss: 0.010175, acc: 1.000000]  [G loss: 8.082781, acc: 0.046875]\n",
            "462: [D loss: 0.043537, acc: 0.984375]  [G loss: 7.587626, acc: 0.062500]\n",
            "463: [D loss: 0.041940, acc: 0.984375]  [G loss: 7.807637, acc: 0.140625]\n",
            "464: [D loss: 0.062924, acc: 0.984375]  [G loss: 7.942138, acc: 0.203125]\n",
            "465: [D loss: 0.069860, acc: 0.953125]  [G loss: 6.966729, acc: 0.093750]\n",
            "466: [D loss: 0.050654, acc: 0.984375]  [G loss: 7.821940, acc: 0.062500]\n",
            "467: [D loss: 0.021670, acc: 1.000000]  [G loss: 7.622117, acc: 0.109375]\n",
            "468: [D loss: 0.052017, acc: 0.968750]  [G loss: 8.348939, acc: 0.015625]\n",
            "469: [D loss: 0.066430, acc: 0.960938]  [G loss: 8.770628, acc: 0.031250]\n",
            "470: [D loss: 0.057133, acc: 0.984375]  [G loss: 9.045889, acc: 0.000000]\n",
            "471: [D loss: 0.042301, acc: 0.976562]  [G loss: 9.256636, acc: 0.015625]\n",
            "472: [D loss: 0.137304, acc: 0.953125]  [G loss: 8.316971, acc: 0.078125]\n",
            "473: [D loss: 0.036852, acc: 0.984375]  [G loss: 7.002046, acc: 0.062500]\n",
            "474: [D loss: 0.068423, acc: 0.968750]  [G loss: 7.116481, acc: 0.078125]\n",
            "475: [D loss: 0.063605, acc: 0.976562]  [G loss: 8.318945, acc: 0.015625]\n",
            "476: [D loss: 0.013721, acc: 1.000000]  [G loss: 9.207678, acc: 0.000000]\n",
            "477: [D loss: 0.073775, acc: 0.976562]  [G loss: 9.404211, acc: 0.000000]\n",
            "478: [D loss: 0.030165, acc: 0.984375]  [G loss: 8.304550, acc: 0.046875]\n",
            "479: [D loss: 0.062768, acc: 0.976562]  [G loss: 7.418584, acc: 0.109375]\n",
            "480: [D loss: 0.069416, acc: 0.968750]  [G loss: 8.567820, acc: 0.062500]\n",
            "481: [D loss: 0.039540, acc: 0.984375]  [G loss: 9.697713, acc: 0.000000]\n",
            "482: [D loss: 0.009596, acc: 1.000000]  [G loss: 10.715933, acc: 0.000000]\n",
            "483: [D loss: 0.084104, acc: 0.968750]  [G loss: 10.553200, acc: 0.015625]\n",
            "484: [D loss: 0.034158, acc: 0.984375]  [G loss: 9.133429, acc: 0.015625]\n",
            "485: [D loss: 0.015010, acc: 1.000000]  [G loss: 8.432785, acc: 0.015625]\n",
            "486: [D loss: 0.048103, acc: 0.984375]  [G loss: 7.798367, acc: 0.078125]\n",
            "487: [D loss: 0.044730, acc: 0.992188]  [G loss: 8.881229, acc: 0.015625]\n",
            "488: [D loss: 0.012165, acc: 1.000000]  [G loss: 9.057421, acc: 0.046875]\n",
            "489: [D loss: 0.074407, acc: 0.976562]  [G loss: 9.277995, acc: 0.015625]\n",
            "490: [D loss: 0.033074, acc: 0.984375]  [G loss: 8.661770, acc: 0.062500]\n",
            "491: [D loss: 0.021463, acc: 0.984375]  [G loss: 8.051764, acc: 0.078125]\n",
            "492: [D loss: 0.022842, acc: 0.992188]  [G loss: 7.287317, acc: 0.078125]\n",
            "493: [D loss: 0.022710, acc: 0.992188]  [G loss: 8.784855, acc: 0.046875]\n",
            "494: [D loss: 0.005260, acc: 1.000000]  [G loss: 7.775715, acc: 0.125000]\n",
            "495: [D loss: 0.021467, acc: 0.984375]  [G loss: 8.426427, acc: 0.015625]\n",
            "496: [D loss: 0.002090, acc: 1.000000]  [G loss: 10.251121, acc: 0.000000]\n",
            "497: [D loss: 0.030215, acc: 0.992188]  [G loss: 9.740053, acc: 0.000000]\n",
            "498: [D loss: 0.014029, acc: 0.992188]  [G loss: 9.584374, acc: 0.015625]\n",
            "499: [D loss: 0.013006, acc: 1.000000]  [G loss: 8.588641, acc: 0.062500]\n",
            "500: [D loss: 0.071714, acc: 0.976562]  [G loss: 7.184682, acc: 0.203125]\n",
            "501: [D loss: 0.037663, acc: 0.984375]  [G loss: 5.784314, acc: 0.265625]\n",
            "502: [D loss: 0.085768, acc: 0.968750]  [G loss: 6.680381, acc: 0.281250]\n",
            "503: [D loss: 0.036642, acc: 0.976562]  [G loss: 7.515569, acc: 0.156250]\n",
            "504: [D loss: 0.054046, acc: 0.976562]  [G loss: 8.039358, acc: 0.109375]\n",
            "505: [D loss: 0.032170, acc: 0.984375]  [G loss: 7.831099, acc: 0.140625]\n",
            "506: [D loss: 0.029138, acc: 0.984375]  [G loss: 7.157470, acc: 0.218750]\n",
            "507: [D loss: 0.063727, acc: 0.984375]  [G loss: 6.488241, acc: 0.390625]\n",
            "508: [D loss: 0.070271, acc: 0.976562]  [G loss: 6.865602, acc: 0.281250]\n",
            "509: [D loss: 0.024752, acc: 0.992188]  [G loss: 7.688135, acc: 0.156250]\n",
            "510: [D loss: 0.015042, acc: 1.000000]  [G loss: 8.040766, acc: 0.078125]\n",
            "511: [D loss: 0.012879, acc: 0.992188]  [G loss: 8.084106, acc: 0.109375]\n",
            "512: [D loss: 0.059399, acc: 0.976562]  [G loss: 7.756650, acc: 0.203125]\n",
            "513: [D loss: 0.034560, acc: 0.976562]  [G loss: 6.867214, acc: 0.234375]\n",
            "514: [D loss: 0.027211, acc: 0.992188]  [G loss: 6.612673, acc: 0.390625]\n",
            "515: [D loss: 0.102765, acc: 0.945312]  [G loss: 6.351947, acc: 0.218750]\n",
            "516: [D loss: 0.058449, acc: 0.976562]  [G loss: 7.898949, acc: 0.093750]\n",
            "517: [D loss: 0.037542, acc: 0.984375]  [G loss: 9.629234, acc: 0.046875]\n",
            "518: [D loss: 0.104555, acc: 0.960938]  [G loss: 8.792986, acc: 0.093750]\n",
            "519: [D loss: 0.101414, acc: 0.953125]  [G loss: 6.518558, acc: 0.296875]\n",
            "520: [D loss: 0.022395, acc: 0.992188]  [G loss: 6.521299, acc: 0.421875]\n",
            "521: [D loss: 0.052753, acc: 0.984375]  [G loss: 6.160200, acc: 0.515625]\n",
            "522: [D loss: 0.110681, acc: 0.968750]  [G loss: 7.252039, acc: 0.281250]\n",
            "523: [D loss: 0.011291, acc: 1.000000]  [G loss: 7.949341, acc: 0.140625]\n",
            "524: [D loss: 0.026490, acc: 0.976562]  [G loss: 9.692850, acc: 0.031250]\n",
            "525: [D loss: 0.048035, acc: 0.984375]  [G loss: 10.187654, acc: 0.015625]\n",
            "526: [D loss: 0.006076, acc: 1.000000]  [G loss: 10.653795, acc: 0.015625]\n",
            "527: [D loss: 0.035585, acc: 0.984375]  [G loss: 10.123864, acc: 0.046875]\n",
            "528: [D loss: 0.003954, acc: 1.000000]  [G loss: 9.389599, acc: 0.062500]\n",
            "529: [D loss: 0.024765, acc: 0.984375]  [G loss: 9.117290, acc: 0.062500]\n",
            "530: [D loss: 0.092306, acc: 0.984375]  [G loss: 8.744129, acc: 0.109375]\n",
            "531: [D loss: 0.013210, acc: 0.992188]  [G loss: 7.954394, acc: 0.171875]\n",
            "532: [D loss: 0.016367, acc: 0.992188]  [G loss: 7.850968, acc: 0.140625]\n",
            "533: [D loss: 0.006462, acc: 1.000000]  [G loss: 7.121066, acc: 0.171875]\n",
            "534: [D loss: 0.016372, acc: 0.992188]  [G loss: 7.251088, acc: 0.218750]\n",
            "535: [D loss: 0.015672, acc: 0.992188]  [G loss: 7.717907, acc: 0.140625]\n",
            "536: [D loss: 0.017645, acc: 0.992188]  [G loss: 8.033048, acc: 0.187500]\n",
            "537: [D loss: 0.007763, acc: 1.000000]  [G loss: 7.559950, acc: 0.171875]\n",
            "538: [D loss: 0.003836, acc: 1.000000]  [G loss: 7.865462, acc: 0.171875]\n",
            "539: [D loss: 0.030070, acc: 0.992188]  [G loss: 7.917020, acc: 0.109375]\n",
            "540: [D loss: 0.015861, acc: 0.992188]  [G loss: 7.812629, acc: 0.187500]\n",
            "541: [D loss: 0.008426, acc: 1.000000]  [G loss: 6.931252, acc: 0.250000]\n",
            "542: [D loss: 0.040899, acc: 0.976562]  [G loss: 8.007616, acc: 0.093750]\n",
            "543: [D loss: 0.032493, acc: 0.984375]  [G loss: 9.101566, acc: 0.015625]\n",
            "544: [D loss: 0.022428, acc: 0.984375]  [G loss: 8.376165, acc: 0.046875]\n",
            "545: [D loss: 0.007153, acc: 1.000000]  [G loss: 8.772152, acc: 0.125000]\n",
            "546: [D loss: 0.061114, acc: 0.976562]  [G loss: 7.170005, acc: 0.203125]\n",
            "547: [D loss: 0.019304, acc: 0.992188]  [G loss: 6.900213, acc: 0.375000]\n",
            "548: [D loss: 0.023254, acc: 1.000000]  [G loss: 6.392115, acc: 0.296875]\n",
            "549: [D loss: 0.065228, acc: 0.976562]  [G loss: 6.821900, acc: 0.187500]\n",
            "550: [D loss: 0.055900, acc: 0.968750]  [G loss: 7.477724, acc: 0.140625]\n",
            "551: [D loss: 0.007338, acc: 1.000000]  [G loss: 8.384896, acc: 0.062500]\n",
            "552: [D loss: 0.041902, acc: 0.984375]  [G loss: 8.177086, acc: 0.031250]\n",
            "553: [D loss: 0.128282, acc: 0.960938]  [G loss: 8.989293, acc: 0.031250]\n",
            "554: [D loss: 0.005952, acc: 1.000000]  [G loss: 8.347590, acc: 0.062500]\n",
            "555: [D loss: 0.013740, acc: 1.000000]  [G loss: 7.738372, acc: 0.140625]\n",
            "556: [D loss: 0.014122, acc: 1.000000]  [G loss: 7.416347, acc: 0.281250]\n",
            "557: [D loss: 0.025046, acc: 0.984375]  [G loss: 7.976026, acc: 0.156250]\n",
            "558: [D loss: 0.030231, acc: 0.984375]  [G loss: 8.799408, acc: 0.078125]\n",
            "559: [D loss: 0.003073, acc: 1.000000]  [G loss: 9.794973, acc: 0.015625]\n",
            "560: [D loss: 0.021332, acc: 0.992188]  [G loss: 10.462646, acc: 0.015625]\n",
            "561: [D loss: 0.086417, acc: 0.968750]  [G loss: 9.999561, acc: 0.031250]\n",
            "562: [D loss: 0.027396, acc: 0.992188]  [G loss: 9.012169, acc: 0.015625]\n",
            "563: [D loss: 0.009364, acc: 0.992188]  [G loss: 7.610972, acc: 0.234375]\n",
            "564: [D loss: 0.037726, acc: 0.984375]  [G loss: 6.957636, acc: 0.187500]\n",
            "565: [D loss: 0.179121, acc: 0.937500]  [G loss: 9.634870, acc: 0.015625]\n",
            "566: [D loss: 0.044698, acc: 0.984375]  [G loss: 12.530380, acc: 0.000000]\n",
            "567: [D loss: 0.061104, acc: 0.984375]  [G loss: 11.844507, acc: 0.000000]\n",
            "568: [D loss: 0.085576, acc: 0.968750]  [G loss: 9.734927, acc: 0.000000]\n",
            "569: [D loss: 0.035908, acc: 0.984375]  [G loss: 8.118560, acc: 0.140625]\n",
            "570: [D loss: 0.078995, acc: 0.968750]  [G loss: 7.470877, acc: 0.296875]\n",
            "571: [D loss: 0.222266, acc: 0.945312]  [G loss: 11.043999, acc: 0.000000]\n",
            "572: [D loss: 0.215286, acc: 0.960938]  [G loss: 11.982876, acc: 0.000000]\n",
            "573: [D loss: 0.037421, acc: 0.984375]  [G loss: 12.160967, acc: 0.000000]\n",
            "574: [D loss: 0.205955, acc: 0.953125]  [G loss: 9.750876, acc: 0.000000]\n",
            "575: [D loss: 0.048705, acc: 0.984375]  [G loss: 7.208357, acc: 0.078125]\n",
            "576: [D loss: 0.046186, acc: 0.984375]  [G loss: 6.732819, acc: 0.281250]\n",
            "577: [D loss: 0.076328, acc: 0.976562]  [G loss: 7.569683, acc: 0.109375]\n",
            "578: [D loss: 0.084005, acc: 0.945312]  [G loss: 8.443951, acc: 0.031250]\n",
            "579: [D loss: 0.022784, acc: 0.992188]  [G loss: 8.823479, acc: 0.015625]\n",
            "580: [D loss: 0.027790, acc: 0.984375]  [G loss: 10.235356, acc: 0.000000]\n",
            "581: [D loss: 0.014244, acc: 0.992188]  [G loss: 9.778996, acc: 0.000000]\n",
            "582: [D loss: 0.019681, acc: 0.992188]  [G loss: 10.865331, acc: 0.000000]\n",
            "583: [D loss: 0.022462, acc: 0.984375]  [G loss: 10.249134, acc: 0.000000]\n",
            "584: [D loss: 0.049000, acc: 0.968750]  [G loss: 9.922966, acc: 0.000000]\n",
            "585: [D loss: 0.049293, acc: 0.984375]  [G loss: 8.857925, acc: 0.031250]\n",
            "586: [D loss: 0.034034, acc: 0.984375]  [G loss: 8.040538, acc: 0.109375]\n",
            "587: [D loss: 0.032006, acc: 0.976562]  [G loss: 8.006131, acc: 0.109375]\n",
            "588: [D loss: 0.007383, acc: 1.000000]  [G loss: 7.012868, acc: 0.140625]\n",
            "589: [D loss: 0.018215, acc: 1.000000]  [G loss: 8.004510, acc: 0.125000]\n",
            "590: [D loss: 0.006927, acc: 1.000000]  [G loss: 7.713054, acc: 0.125000]\n",
            "591: [D loss: 0.017514, acc: 0.992188]  [G loss: 7.463879, acc: 0.125000]\n",
            "592: [D loss: 0.022464, acc: 0.992188]  [G loss: 8.426942, acc: 0.015625]\n",
            "593: [D loss: 0.026740, acc: 0.984375]  [G loss: 9.476383, acc: 0.046875]\n",
            "594: [D loss: 0.090687, acc: 0.968750]  [G loss: 8.660957, acc: 0.078125]\n",
            "595: [D loss: 0.006667, acc: 1.000000]  [G loss: 7.792963, acc: 0.078125]\n",
            "596: [D loss: 0.038936, acc: 0.984375]  [G loss: 7.888470, acc: 0.078125]\n",
            "597: [D loss: 0.014028, acc: 1.000000]  [G loss: 7.971140, acc: 0.046875]\n",
            "598: [D loss: 0.023054, acc: 0.984375]  [G loss: 7.267451, acc: 0.062500]\n",
            "599: [D loss: 0.021176, acc: 0.992188]  [G loss: 6.961810, acc: 0.250000]\n",
            "600: [D loss: 0.038375, acc: 0.984375]  [G loss: 7.178833, acc: 0.250000]\n",
            "601: [D loss: 0.021056, acc: 1.000000]  [G loss: 6.819072, acc: 0.171875]\n",
            "602: [D loss: 0.062823, acc: 0.976562]  [G loss: 7.776702, acc: 0.093750]\n",
            "603: [D loss: 0.009256, acc: 1.000000]  [G loss: 7.985775, acc: 0.140625]\n",
            "604: [D loss: 0.042960, acc: 0.992188]  [G loss: 7.597271, acc: 0.078125]\n",
            "605: [D loss: 0.015481, acc: 1.000000]  [G loss: 8.464082, acc: 0.171875]\n",
            "606: [D loss: 0.030616, acc: 0.984375]  [G loss: 7.730430, acc: 0.156250]\n",
            "607: [D loss: 0.005850, acc: 1.000000]  [G loss: 7.224798, acc: 0.078125]\n",
            "608: [D loss: 0.093348, acc: 0.976562]  [G loss: 6.931670, acc: 0.250000]\n",
            "609: [D loss: 0.089225, acc: 0.945312]  [G loss: 6.949555, acc: 0.171875]\n",
            "610: [D loss: 0.020624, acc: 1.000000]  [G loss: 7.136222, acc: 0.218750]\n",
            "611: [D loss: 0.025936, acc: 0.984375]  [G loss: 6.543843, acc: 0.296875]\n",
            "612: [D loss: 0.052534, acc: 0.992188]  [G loss: 6.917996, acc: 0.296875]\n",
            "613: [D loss: 0.050452, acc: 0.968750]  [G loss: 6.593601, acc: 0.265625]\n",
            "614: [D loss: 0.067561, acc: 0.968750]  [G loss: 6.493819, acc: 0.312500]\n",
            "615: [D loss: 0.092352, acc: 0.968750]  [G loss: 7.583921, acc: 0.203125]\n",
            "616: [D loss: 0.017632, acc: 0.992188]  [G loss: 8.274319, acc: 0.093750]\n",
            "617: [D loss: 0.018238, acc: 1.000000]  [G loss: 9.059973, acc: 0.015625]\n",
            "618: [D loss: 0.063706, acc: 0.976562]  [G loss: 9.236273, acc: 0.000000]\n",
            "619: [D loss: 0.037625, acc: 0.976562]  [G loss: 8.467322, acc: 0.015625]\n",
            "620: [D loss: 0.036693, acc: 0.984375]  [G loss: 8.194382, acc: 0.140625]\n",
            "621: [D loss: 0.037595, acc: 0.968750]  [G loss: 7.566574, acc: 0.156250]\n",
            "622: [D loss: 0.014517, acc: 1.000000]  [G loss: 7.942906, acc: 0.156250]\n",
            "623: [D loss: 0.048665, acc: 0.984375]  [G loss: 8.080409, acc: 0.125000]\n",
            "624: [D loss: 0.005408, acc: 1.000000]  [G loss: 8.202276, acc: 0.140625]\n",
            "625: [D loss: 0.004392, acc: 1.000000]  [G loss: 8.519684, acc: 0.062500]\n",
            "626: [D loss: 0.007185, acc: 1.000000]  [G loss: 9.195879, acc: 0.125000]\n",
            "627: [D loss: 0.015589, acc: 0.992188]  [G loss: 8.255539, acc: 0.093750]\n",
            "628: [D loss: 0.007164, acc: 1.000000]  [G loss: 8.245604, acc: 0.171875]\n",
            "629: [D loss: 0.023425, acc: 0.992188]  [G loss: 7.656673, acc: 0.156250]\n",
            "630: [D loss: 0.013028, acc: 1.000000]  [G loss: 8.353556, acc: 0.171875]\n",
            "631: [D loss: 0.020969, acc: 0.992188]  [G loss: 7.622789, acc: 0.234375]\n",
            "632: [D loss: 0.013194, acc: 1.000000]  [G loss: 7.079589, acc: 0.218750]\n",
            "633: [D loss: 0.010928, acc: 1.000000]  [G loss: 7.370646, acc: 0.156250]\n",
            "634: [D loss: 0.012136, acc: 1.000000]  [G loss: 8.057694, acc: 0.125000]\n",
            "635: [D loss: 0.006400, acc: 1.000000]  [G loss: 7.465570, acc: 0.109375]\n",
            "636: [D loss: 0.005784, acc: 1.000000]  [G loss: 7.683637, acc: 0.078125]\n",
            "637: [D loss: 0.006786, acc: 1.000000]  [G loss: 8.119825, acc: 0.078125]\n",
            "638: [D loss: 0.007699, acc: 1.000000]  [G loss: 8.258018, acc: 0.062500]\n",
            "639: [D loss: 0.019993, acc: 0.992188]  [G loss: 7.611241, acc: 0.062500]\n",
            "640: [D loss: 0.013301, acc: 1.000000]  [G loss: 7.129514, acc: 0.093750]\n",
            "641: [D loss: 0.032252, acc: 0.992188]  [G loss: 7.227866, acc: 0.046875]\n",
            "642: [D loss: 0.010514, acc: 1.000000]  [G loss: 7.343382, acc: 0.078125]\n",
            "643: [D loss: 0.008304, acc: 1.000000]  [G loss: 7.707924, acc: 0.078125]\n",
            "644: [D loss: 0.027545, acc: 0.992188]  [G loss: 7.562607, acc: 0.078125]\n",
            "645: [D loss: 0.056480, acc: 0.984375]  [G loss: 7.158343, acc: 0.187500]\n",
            "646: [D loss: 0.006127, acc: 1.000000]  [G loss: 6.349496, acc: 0.265625]\n",
            "647: [D loss: 0.004050, acc: 1.000000]  [G loss: 6.544481, acc: 0.296875]\n",
            "648: [D loss: 0.006467, acc: 1.000000]  [G loss: 5.815223, acc: 0.218750]\n",
            "649: [D loss: 0.005335, acc: 1.000000]  [G loss: 6.339960, acc: 0.281250]\n",
            "650: [D loss: 0.024073, acc: 0.984375]  [G loss: 6.860220, acc: 0.234375]\n",
            "651: [D loss: 0.065956, acc: 0.968750]  [G loss: 6.835817, acc: 0.109375]\n",
            "652: [D loss: 0.018038, acc: 0.984375]  [G loss: 7.298862, acc: 0.093750]\n",
            "653: [D loss: 0.003804, acc: 1.000000]  [G loss: 7.777220, acc: 0.093750]\n",
            "654: [D loss: 0.109351, acc: 0.968750]  [G loss: 7.099792, acc: 0.171875]\n",
            "655: [D loss: 0.002302, acc: 1.000000]  [G loss: 6.385779, acc: 0.359375]\n",
            "656: [D loss: 0.031406, acc: 0.984375]  [G loss: 6.247197, acc: 0.343750]\n",
            "657: [D loss: 0.016280, acc: 1.000000]  [G loss: 6.762730, acc: 0.234375]\n",
            "658: [D loss: 0.014438, acc: 1.000000]  [G loss: 7.372624, acc: 0.203125]\n",
            "659: [D loss: 0.013633, acc: 0.992188]  [G loss: 8.517624, acc: 0.109375]\n",
            "660: [D loss: 0.027057, acc: 0.984375]  [G loss: 8.166542, acc: 0.125000]\n",
            "661: [D loss: 0.002368, acc: 1.000000]  [G loss: 7.953846, acc: 0.187500]\n",
            "662: [D loss: 0.004756, acc: 1.000000]  [G loss: 7.316210, acc: 0.234375]\n",
            "663: [D loss: 0.016146, acc: 0.992188]  [G loss: 7.984998, acc: 0.218750]\n",
            "664: [D loss: 0.117392, acc: 0.976562]  [G loss: 7.363392, acc: 0.250000]\n",
            "665: [D loss: 0.030681, acc: 0.984375]  [G loss: 6.071406, acc: 0.328125]\n",
            "666: [D loss: 0.021984, acc: 0.992188]  [G loss: 6.339179, acc: 0.234375]\n",
            "667: [D loss: 0.004036, acc: 1.000000]  [G loss: 5.916863, acc: 0.171875]\n",
            "668: [D loss: 0.034826, acc: 0.984375]  [G loss: 7.484150, acc: 0.140625]\n",
            "669: [D loss: 0.032998, acc: 0.992188]  [G loss: 8.268601, acc: 0.062500]\n",
            "670: [D loss: 0.005928, acc: 1.000000]  [G loss: 10.576098, acc: 0.015625]\n",
            "671: [D loss: 0.005602, acc: 1.000000]  [G loss: 10.205458, acc: 0.031250]\n",
            "672: [D loss: 0.066368, acc: 0.960938]  [G loss: 8.883522, acc: 0.093750]\n",
            "673: [D loss: 0.022277, acc: 0.984375]  [G loss: 7.502419, acc: 0.171875]\n",
            "674: [D loss: 0.003378, acc: 1.000000]  [G loss: 5.717960, acc: 0.437500]\n",
            "675: [D loss: 0.027969, acc: 1.000000]  [G loss: 6.220972, acc: 0.484375]\n",
            "676: [D loss: 0.128846, acc: 0.945312]  [G loss: 7.181234, acc: 0.265625]\n",
            "677: [D loss: 0.002821, acc: 1.000000]  [G loss: 7.788214, acc: 0.062500]\n",
            "678: [D loss: 0.012698, acc: 1.000000]  [G loss: 9.178607, acc: 0.046875]\n",
            "679: [D loss: 0.036796, acc: 0.984375]  [G loss: 9.942877, acc: 0.078125]\n",
            "680: [D loss: 0.047100, acc: 0.976562]  [G loss: 9.645006, acc: 0.015625]\n",
            "681: [D loss: 0.035992, acc: 0.992188]  [G loss: 8.600379, acc: 0.093750]\n",
            "682: [D loss: 0.021595, acc: 0.992188]  [G loss: 7.638246, acc: 0.265625]\n",
            "683: [D loss: 0.010487, acc: 1.000000]  [G loss: 7.538570, acc: 0.265625]\n",
            "684: [D loss: 0.057062, acc: 0.968750]  [G loss: 7.747777, acc: 0.234375]\n",
            "685: [D loss: 0.054920, acc: 0.984375]  [G loss: 7.131187, acc: 0.234375]\n",
            "686: [D loss: 0.013409, acc: 0.992188]  [G loss: 7.108510, acc: 0.156250]\n",
            "687: [D loss: 0.002063, acc: 1.000000]  [G loss: 8.319606, acc: 0.171875]\n",
            "688: [D loss: 0.032652, acc: 0.992188]  [G loss: 6.544815, acc: 0.203125]\n",
            "689: [D loss: 0.004948, acc: 1.000000]  [G loss: 6.725687, acc: 0.234375]\n",
            "690: [D loss: 0.001474, acc: 1.000000]  [G loss: 5.818000, acc: 0.328125]\n",
            "691: [D loss: 0.003210, acc: 1.000000]  [G loss: 5.754608, acc: 0.296875]\n",
            "692: [D loss: 0.012887, acc: 0.992188]  [G loss: 5.479653, acc: 0.468750]\n",
            "693: [D loss: 0.013099, acc: 1.000000]  [G loss: 5.956366, acc: 0.343750]\n",
            "694: [D loss: 0.013821, acc: 0.992188]  [G loss: 6.392442, acc: 0.265625]\n",
            "695: [D loss: 0.046106, acc: 0.984375]  [G loss: 5.682363, acc: 0.281250]\n",
            "696: [D loss: 0.029799, acc: 0.992188]  [G loss: 6.268281, acc: 0.421875]\n",
            "697: [D loss: 0.050179, acc: 0.976562]  [G loss: 5.511288, acc: 0.500000]\n",
            "698: [D loss: 0.031035, acc: 0.992188]  [G loss: 5.723548, acc: 0.531250]\n",
            "699: [D loss: 0.048232, acc: 0.976562]  [G loss: 6.225327, acc: 0.203125]\n",
            "700: [D loss: 0.028179, acc: 0.984375]  [G loss: 7.017940, acc: 0.140625]\n",
            "701: [D loss: 0.038064, acc: 0.976562]  [G loss: 8.216370, acc: 0.109375]\n",
            "702: [D loss: 0.006512, acc: 1.000000]  [G loss: 7.518328, acc: 0.156250]\n",
            "703: [D loss: 0.112364, acc: 0.968750]  [G loss: 6.675532, acc: 0.250000]\n",
            "704: [D loss: 0.024418, acc: 0.992188]  [G loss: 5.704961, acc: 0.421875]\n",
            "705: [D loss: 0.073204, acc: 0.976562]  [G loss: 5.750711, acc: 0.281250]\n",
            "706: [D loss: 0.037025, acc: 0.992188]  [G loss: 7.043920, acc: 0.093750]\n",
            "707: [D loss: 0.032642, acc: 0.992188]  [G loss: 9.398546, acc: 0.015625]\n",
            "708: [D loss: 0.023549, acc: 0.984375]  [G loss: 11.036004, acc: 0.000000]\n",
            "709: [D loss: 0.018382, acc: 0.984375]  [G loss: 11.723953, acc: 0.000000]\n",
            "710: [D loss: 0.211888, acc: 0.960938]  [G loss: 9.043287, acc: 0.031250]\n",
            "711: [D loss: 0.040755, acc: 0.984375]  [G loss: 6.440625, acc: 0.218750]\n",
            "712: [D loss: 0.035274, acc: 0.984375]  [G loss: 6.691493, acc: 0.265625]\n",
            "713: [D loss: 0.088111, acc: 0.968750]  [G loss: 7.417206, acc: 0.125000]\n",
            "714: [D loss: 0.009429, acc: 1.000000]  [G loss: 8.162817, acc: 0.062500]\n",
            "715: [D loss: 0.008776, acc: 0.992188]  [G loss: 10.393909, acc: 0.000000]\n",
            "716: [D loss: 0.046445, acc: 0.976562]  [G loss: 10.153777, acc: 0.015625]\n",
            "717: [D loss: 0.081467, acc: 0.968750]  [G loss: 8.686428, acc: 0.046875]\n",
            "718: [D loss: 0.000421, acc: 1.000000]  [G loss: 8.671099, acc: 0.140625]\n",
            "719: [D loss: 0.010105, acc: 0.992188]  [G loss: 6.995948, acc: 0.171875]\n",
            "720: [D loss: 0.016408, acc: 0.992188]  [G loss: 7.702545, acc: 0.140625]\n",
            "721: [D loss: 0.100539, acc: 0.984375]  [G loss: 6.602132, acc: 0.218750]\n",
            "722: [D loss: 0.006952, acc: 1.000000]  [G loss: 7.062592, acc: 0.187500]\n",
            "723: [D loss: 0.012163, acc: 1.000000]  [G loss: 7.164620, acc: 0.125000]\n",
            "724: [D loss: 0.017472, acc: 0.992188]  [G loss: 6.194682, acc: 0.109375]\n",
            "725: [D loss: 0.013823, acc: 0.992188]  [G loss: 6.376296, acc: 0.218750]\n",
            "726: [D loss: 0.043858, acc: 0.992188]  [G loss: 5.909034, acc: 0.218750]\n",
            "727: [D loss: 0.068081, acc: 0.968750]  [G loss: 8.032698, acc: 0.015625]\n",
            "728: [D loss: 0.053977, acc: 0.984375]  [G loss: 9.416164, acc: 0.000000]\n",
            "729: [D loss: 0.070537, acc: 0.976562]  [G loss: 8.036430, acc: 0.109375]\n",
            "730: [D loss: 0.039753, acc: 0.984375]  [G loss: 8.037201, acc: 0.109375]\n",
            "731: [D loss: 0.052738, acc: 0.976562]  [G loss: 6.366751, acc: 0.390625]\n",
            "732: [D loss: 0.110690, acc: 0.960938]  [G loss: 6.129591, acc: 0.281250]\n",
            "733: [D loss: 0.099672, acc: 0.976562]  [G loss: 6.551768, acc: 0.171875]\n",
            "734: [D loss: 0.041612, acc: 0.992188]  [G loss: 7.669355, acc: 0.125000]\n",
            "735: [D loss: 0.057328, acc: 0.992188]  [G loss: 7.896261, acc: 0.125000]\n",
            "736: [D loss: 0.016297, acc: 0.992188]  [G loss: 7.034618, acc: 0.046875]\n",
            "737: [D loss: 0.010534, acc: 1.000000]  [G loss: 6.447904, acc: 0.062500]\n",
            "738: [D loss: 0.004421, acc: 1.000000]  [G loss: 6.313804, acc: 0.203125]\n",
            "739: [D loss: 0.014080, acc: 0.992188]  [G loss: 6.479535, acc: 0.218750]\n",
            "740: [D loss: 0.027651, acc: 0.992188]  [G loss: 6.904597, acc: 0.203125]\n",
            "741: [D loss: 0.026488, acc: 0.992188]  [G loss: 6.658842, acc: 0.140625]\n",
            "742: [D loss: 0.059203, acc: 0.960938]  [G loss: 7.953856, acc: 0.031250]\n",
            "743: [D loss: 0.053370, acc: 0.992188]  [G loss: 9.001427, acc: 0.000000]\n",
            "744: [D loss: 0.010739, acc: 1.000000]  [G loss: 8.640915, acc: 0.062500]\n",
            "745: [D loss: 0.049064, acc: 0.976562]  [G loss: 8.684955, acc: 0.093750]\n",
            "746: [D loss: 0.018380, acc: 0.992188]  [G loss: 7.053843, acc: 0.187500]\n",
            "747: [D loss: 0.026734, acc: 0.992188]  [G loss: 6.344555, acc: 0.296875]\n",
            "748: [D loss: 0.040288, acc: 0.984375]  [G loss: 6.198664, acc: 0.343750]\n",
            "749: [D loss: 0.084332, acc: 0.984375]  [G loss: 7.762345, acc: 0.171875]\n",
            "750: [D loss: 0.018619, acc: 0.984375]  [G loss: 8.677031, acc: 0.046875]\n",
            "751: [D loss: 0.081086, acc: 0.968750]  [G loss: 9.467502, acc: 0.015625]\n",
            "752: [D loss: 0.052302, acc: 0.984375]  [G loss: 8.447658, acc: 0.046875]\n",
            "753: [D loss: 0.022897, acc: 0.992188]  [G loss: 6.433792, acc: 0.250000]\n",
            "754: [D loss: 0.010293, acc: 1.000000]  [G loss: 5.991979, acc: 0.203125]\n",
            "755: [D loss: 0.010198, acc: 1.000000]  [G loss: 6.175634, acc: 0.343750]\n",
            "756: [D loss: 0.023540, acc: 0.984375]  [G loss: 7.189269, acc: 0.093750]\n",
            "757: [D loss: 0.001100, acc: 1.000000]  [G loss: 7.338584, acc: 0.109375]\n",
            "758: [D loss: 0.039185, acc: 0.976562]  [G loss: 7.172169, acc: 0.093750]\n",
            "759: [D loss: 0.001619, acc: 1.000000]  [G loss: 6.899506, acc: 0.171875]\n",
            "760: [D loss: 0.019508, acc: 0.992188]  [G loss: 6.917812, acc: 0.203125]\n",
            "761: [D loss: 0.011370, acc: 0.992188]  [G loss: 6.149259, acc: 0.187500]\n",
            "762: [D loss: 0.043436, acc: 0.976562]  [G loss: 7.283401, acc: 0.093750]\n",
            "763: [D loss: 0.019434, acc: 0.984375]  [G loss: 8.726270, acc: 0.046875]\n",
            "764: [D loss: 0.038477, acc: 0.984375]  [G loss: 8.902920, acc: 0.046875]\n",
            "765: [D loss: 0.040034, acc: 0.976562]  [G loss: 7.841826, acc: 0.078125]\n",
            "766: [D loss: 0.019282, acc: 0.992188]  [G loss: 7.089350, acc: 0.109375]\n",
            "767: [D loss: 0.008085, acc: 1.000000]  [G loss: 5.687877, acc: 0.312500]\n",
            "768: [D loss: 0.089382, acc: 0.984375]  [G loss: 6.506092, acc: 0.250000]\n",
            "769: [D loss: 0.008513, acc: 1.000000]  [G loss: 6.943169, acc: 0.078125]\n",
            "770: [D loss: 0.007400, acc: 1.000000]  [G loss: 7.785496, acc: 0.031250]\n",
            "771: [D loss: 0.003558, acc: 1.000000]  [G loss: 9.200298, acc: 0.031250]\n",
            "772: [D loss: 0.108832, acc: 0.976562]  [G loss: 7.391854, acc: 0.234375]\n",
            "773: [D loss: 0.048796, acc: 0.984375]  [G loss: 6.104004, acc: 0.328125]\n",
            "774: [D loss: 0.099288, acc: 0.968750]  [G loss: 6.558340, acc: 0.328125]\n",
            "775: [D loss: 0.061386, acc: 0.976562]  [G loss: 8.328727, acc: 0.109375]\n",
            "776: [D loss: 0.025959, acc: 0.984375]  [G loss: 8.916056, acc: 0.015625]\n",
            "777: [D loss: 0.140108, acc: 0.945312]  [G loss: 7.120350, acc: 0.093750]\n",
            "778: [D loss: 0.093081, acc: 0.976562]  [G loss: 5.625411, acc: 0.359375]\n",
            "779: [D loss: 0.062410, acc: 0.968750]  [G loss: 5.527541, acc: 0.421875]\n",
            "780: [D loss: 0.079095, acc: 0.968750]  [G loss: 6.311914, acc: 0.187500]\n",
            "781: [D loss: 0.026062, acc: 0.992188]  [G loss: 7.903724, acc: 0.125000]\n",
            "782: [D loss: 0.034151, acc: 0.992188]  [G loss: 8.390324, acc: 0.046875]\n",
            "783: [D loss: 0.025444, acc: 0.984375]  [G loss: 8.938484, acc: 0.046875]\n",
            "784: [D loss: 0.082816, acc: 0.968750]  [G loss: 6.200274, acc: 0.156250]\n",
            "785: [D loss: 0.014007, acc: 0.992188]  [G loss: 6.335176, acc: 0.250000]\n",
            "786: [D loss: 0.015642, acc: 0.992188]  [G loss: 5.292775, acc: 0.328125]\n",
            "787: [D loss: 0.036857, acc: 0.984375]  [G loss: 5.716886, acc: 0.421875]\n",
            "788: [D loss: 0.021545, acc: 0.992188]  [G loss: 6.294614, acc: 0.281250]\n",
            "789: [D loss: 0.038580, acc: 0.992188]  [G loss: 5.989497, acc: 0.281250]\n",
            "790: [D loss: 0.086250, acc: 0.976562]  [G loss: 6.506730, acc: 0.281250]\n",
            "791: [D loss: 0.012343, acc: 0.992188]  [G loss: 4.768425, acc: 0.359375]\n",
            "792: [D loss: 0.068057, acc: 0.976562]  [G loss: 6.176634, acc: 0.281250]\n",
            "793: [D loss: 0.058454, acc: 0.992188]  [G loss: 5.792352, acc: 0.265625]\n",
            "794: [D loss: 0.007427, acc: 1.000000]  [G loss: 7.582503, acc: 0.140625]\n",
            "795: [D loss: 0.005526, acc: 1.000000]  [G loss: 7.905587, acc: 0.125000]\n",
            "796: [D loss: 0.052144, acc: 0.984375]  [G loss: 8.314775, acc: 0.046875]\n",
            "797: [D loss: 0.016518, acc: 0.992188]  [G loss: 7.527700, acc: 0.078125]\n",
            "798: [D loss: 0.013891, acc: 1.000000]  [G loss: 6.365788, acc: 0.203125]\n",
            "799: [D loss: 0.053714, acc: 0.968750]  [G loss: 6.745808, acc: 0.203125]\n",
            "800: [D loss: 0.076317, acc: 0.960938]  [G loss: 6.199368, acc: 0.234375]\n",
            "801: [D loss: 0.041050, acc: 0.984375]  [G loss: 5.695454, acc: 0.250000]\n",
            "802: [D loss: 0.017421, acc: 0.992188]  [G loss: 6.223948, acc: 0.140625]\n",
            "803: [D loss: 0.014439, acc: 1.000000]  [G loss: 6.485152, acc: 0.171875]\n",
            "804: [D loss: 0.031372, acc: 0.984375]  [G loss: 6.244184, acc: 0.296875]\n",
            "805: [D loss: 0.085057, acc: 0.968750]  [G loss: 6.008905, acc: 0.390625]\n",
            "806: [D loss: 0.178271, acc: 0.929688]  [G loss: 6.538788, acc: 0.156250]\n",
            "807: [D loss: 0.042257, acc: 0.976562]  [G loss: 6.995374, acc: 0.031250]\n",
            "808: [D loss: 0.181193, acc: 0.945312]  [G loss: 6.808745, acc: 0.078125]\n",
            "809: [D loss: 0.059617, acc: 0.984375]  [G loss: 5.900523, acc: 0.265625]\n",
            "810: [D loss: 0.029689, acc: 0.992188]  [G loss: 6.045728, acc: 0.328125]\n",
            "811: [D loss: 0.049644, acc: 0.968750]  [G loss: 5.845860, acc: 0.312500]\n",
            "812: [D loss: 0.019289, acc: 0.992188]  [G loss: 6.376218, acc: 0.156250]\n",
            "813: [D loss: 0.078219, acc: 0.984375]  [G loss: 5.705467, acc: 0.140625]\n",
            "814: [D loss: 0.017570, acc: 0.992188]  [G loss: 7.311738, acc: 0.062500]\n",
            "815: [D loss: 0.014939, acc: 0.992188]  [G loss: 7.984120, acc: 0.031250]\n",
            "816: [D loss: 0.017328, acc: 0.992188]  [G loss: 7.601836, acc: 0.031250]\n",
            "817: [D loss: 0.144824, acc: 0.960938]  [G loss: 7.083217, acc: 0.078125]\n",
            "818: [D loss: 0.015479, acc: 1.000000]  [G loss: 5.624132, acc: 0.515625]\n",
            "819: [D loss: 0.021888, acc: 1.000000]  [G loss: 5.186793, acc: 0.500000]\n",
            "820: [D loss: 0.101206, acc: 0.976562]  [G loss: 5.992009, acc: 0.328125]\n",
            "821: [D loss: 0.042706, acc: 0.984375]  [G loss: 6.066089, acc: 0.250000]\n",
            "822: [D loss: 0.026402, acc: 0.992188]  [G loss: 6.980430, acc: 0.156250]\n",
            "823: [D loss: 0.064095, acc: 0.976562]  [G loss: 6.235384, acc: 0.234375]\n",
            "824: [D loss: 0.052363, acc: 0.976562]  [G loss: 6.068102, acc: 0.218750]\n",
            "825: [D loss: 0.032057, acc: 0.984375]  [G loss: 5.848517, acc: 0.375000]\n",
            "826: [D loss: 0.001998, acc: 1.000000]  [G loss: 5.086246, acc: 0.421875]\n",
            "827: [D loss: 0.046979, acc: 0.984375]  [G loss: 4.488085, acc: 0.578125]\n",
            "828: [D loss: 0.065741, acc: 0.976562]  [G loss: 4.840705, acc: 0.484375]\n",
            "829: [D loss: 0.010525, acc: 1.000000]  [G loss: 5.642035, acc: 0.312500]\n",
            "830: [D loss: 0.052462, acc: 0.984375]  [G loss: 6.066309, acc: 0.218750]\n",
            "831: [D loss: 0.010788, acc: 1.000000]  [G loss: 6.006928, acc: 0.140625]\n",
            "832: [D loss: 0.028705, acc: 0.992188]  [G loss: 6.580215, acc: 0.156250]\n",
            "833: [D loss: 0.007191, acc: 1.000000]  [G loss: 6.790265, acc: 0.156250]\n",
            "834: [D loss: 0.007477, acc: 1.000000]  [G loss: 6.126282, acc: 0.187500]\n",
            "835: [D loss: 0.052779, acc: 0.984375]  [G loss: 5.993687, acc: 0.171875]\n",
            "836: [D loss: 0.052651, acc: 0.976562]  [G loss: 5.098833, acc: 0.375000]\n",
            "837: [D loss: 0.061330, acc: 0.976562]  [G loss: 5.175097, acc: 0.500000]\n",
            "838: [D loss: 0.021329, acc: 0.992188]  [G loss: 5.791549, acc: 0.359375]\n",
            "839: [D loss: 0.013689, acc: 0.992188]  [G loss: 5.887031, acc: 0.359375]\n",
            "840: [D loss: 0.005001, acc: 1.000000]  [G loss: 6.359757, acc: 0.203125]\n",
            "841: [D loss: 0.083455, acc: 0.976562]  [G loss: 5.041182, acc: 0.281250]\n",
            "842: [D loss: 0.033286, acc: 0.992188]  [G loss: 5.113411, acc: 0.468750]\n",
            "843: [D loss: 0.042462, acc: 0.984375]  [G loss: 4.272470, acc: 0.562500]\n",
            "844: [D loss: 0.026452, acc: 0.984375]  [G loss: 5.235960, acc: 0.296875]\n",
            "845: [D loss: 0.059467, acc: 0.984375]  [G loss: 5.086296, acc: 0.296875]\n",
            "846: [D loss: 0.009186, acc: 1.000000]  [G loss: 5.602003, acc: 0.234375]\n",
            "847: [D loss: 0.033567, acc: 0.992188]  [G loss: 5.546299, acc: 0.250000]\n",
            "848: [D loss: 0.001847, acc: 1.000000]  [G loss: 6.143613, acc: 0.171875]\n",
            "849: [D loss: 0.038855, acc: 0.976562]  [G loss: 5.057087, acc: 0.203125]\n",
            "850: [D loss: 0.060515, acc: 0.984375]  [G loss: 5.519448, acc: 0.453125]\n",
            "851: [D loss: 0.029602, acc: 0.992188]  [G loss: 4.709058, acc: 0.484375]\n",
            "852: [D loss: 0.080806, acc: 0.953125]  [G loss: 5.524800, acc: 0.281250]\n",
            "853: [D loss: 0.002789, acc: 1.000000]  [G loss: 7.634856, acc: 0.062500]\n",
            "854: [D loss: 0.068434, acc: 0.976562]  [G loss: 6.981288, acc: 0.078125]\n",
            "855: [D loss: 0.034630, acc: 0.984375]  [G loss: 7.303448, acc: 0.156250]\n",
            "856: [D loss: 0.075475, acc: 0.968750]  [G loss: 5.904721, acc: 0.156250]\n",
            "857: [D loss: 0.059179, acc: 0.984375]  [G loss: 4.567366, acc: 0.484375]\n",
            "858: [D loss: 0.050511, acc: 0.976562]  [G loss: 4.457861, acc: 0.578125]\n",
            "859: [D loss: 0.057152, acc: 0.984375]  [G loss: 5.176753, acc: 0.421875]\n",
            "860: [D loss: 0.031442, acc: 0.992188]  [G loss: 6.575953, acc: 0.203125]\n",
            "861: [D loss: 0.010956, acc: 1.000000]  [G loss: 7.613154, acc: 0.093750]\n",
            "862: [D loss: 0.019111, acc: 0.992188]  [G loss: 8.604972, acc: 0.031250]\n",
            "863: [D loss: 0.115540, acc: 0.984375]  [G loss: 7.811218, acc: 0.031250]\n",
            "864: [D loss: 0.088025, acc: 0.968750]  [G loss: 6.670581, acc: 0.187500]\n",
            "865: [D loss: 0.026111, acc: 0.992188]  [G loss: 5.783172, acc: 0.328125]\n",
            "866: [D loss: 0.006246, acc: 1.000000]  [G loss: 4.778057, acc: 0.515625]\n",
            "867: [D loss: 0.082749, acc: 0.968750]  [G loss: 5.548567, acc: 0.421875]\n",
            "868: [D loss: 0.033292, acc: 0.984375]  [G loss: 5.263294, acc: 0.281250]\n",
            "869: [D loss: 0.029833, acc: 0.992188]  [G loss: 5.503624, acc: 0.171875]\n",
            "870: [D loss: 0.049909, acc: 0.984375]  [G loss: 6.143384, acc: 0.203125]\n",
            "871: [D loss: 0.005280, acc: 1.000000]  [G loss: 6.159293, acc: 0.187500]\n",
            "872: [D loss: 0.014274, acc: 0.992188]  [G loss: 6.788602, acc: 0.109375]\n",
            "873: [D loss: 0.011377, acc: 0.992188]  [G loss: 6.215781, acc: 0.140625]\n",
            "874: [D loss: 0.002403, acc: 1.000000]  [G loss: 6.305451, acc: 0.218750]\n",
            "875: [D loss: 0.011790, acc: 0.992188]  [G loss: 5.832953, acc: 0.171875]\n",
            "876: [D loss: 0.026046, acc: 0.992188]  [G loss: 5.887197, acc: 0.250000]\n",
            "877: [D loss: 0.004184, acc: 1.000000]  [G loss: 5.598833, acc: 0.281250]\n",
            "878: [D loss: 0.020640, acc: 0.984375]  [G loss: 5.322916, acc: 0.343750]\n",
            "879: [D loss: 0.030006, acc: 0.992188]  [G loss: 5.626368, acc: 0.328125]\n",
            "880: [D loss: 0.006003, acc: 1.000000]  [G loss: 6.169693, acc: 0.281250]\n",
            "881: [D loss: 0.012055, acc: 0.992188]  [G loss: 6.587900, acc: 0.078125]\n",
            "882: [D loss: 0.000835, acc: 1.000000]  [G loss: 6.836652, acc: 0.062500]\n",
            "883: [D loss: 0.004223, acc: 1.000000]  [G loss: 7.398807, acc: 0.046875]\n",
            "884: [D loss: 0.118555, acc: 0.984375]  [G loss: 6.802260, acc: 0.062500]\n",
            "885: [D loss: 0.006311, acc: 0.992188]  [G loss: 6.330194, acc: 0.046875]\n",
            "886: [D loss: 0.024663, acc: 0.992188]  [G loss: 6.020934, acc: 0.078125]\n",
            "887: [D loss: 0.069719, acc: 0.984375]  [G loss: 5.261496, acc: 0.437500]\n",
            "888: [D loss: 0.010261, acc: 1.000000]  [G loss: 4.812202, acc: 0.500000]\n",
            "889: [D loss: 0.014269, acc: 0.992188]  [G loss: 3.999428, acc: 0.625000]\n",
            "890: [D loss: 0.040277, acc: 0.992188]  [G loss: 3.739051, acc: 0.671875]\n",
            "891: [D loss: 0.081109, acc: 0.968750]  [G loss: 4.952353, acc: 0.375000]\n",
            "892: [D loss: 0.025102, acc: 0.992188]  [G loss: 5.500565, acc: 0.203125]\n",
            "893: [D loss: 0.093260, acc: 0.976562]  [G loss: 5.473608, acc: 0.281250]\n",
            "894: [D loss: 0.053305, acc: 0.992188]  [G loss: 5.941081, acc: 0.203125]\n",
            "895: [D loss: 0.072689, acc: 0.976562]  [G loss: 5.705931, acc: 0.328125]\n",
            "896: [D loss: 0.028201, acc: 0.984375]  [G loss: 5.156758, acc: 0.437500]\n",
            "897: [D loss: 0.053533, acc: 0.976562]  [G loss: 4.992462, acc: 0.421875]\n",
            "898: [D loss: 0.063324, acc: 0.976562]  [G loss: 5.162117, acc: 0.390625]\n",
            "899: [D loss: 0.047476, acc: 0.976562]  [G loss: 5.255157, acc: 0.312500]\n",
            "900: [D loss: 0.009962, acc: 1.000000]  [G loss: 6.113626, acc: 0.296875]\n",
            "901: [D loss: 0.042760, acc: 0.984375]  [G loss: 5.486680, acc: 0.265625]\n",
            "902: [D loss: 0.026826, acc: 0.992188]  [G loss: 5.502392, acc: 0.250000]\n",
            "903: [D loss: 0.048363, acc: 0.992188]  [G loss: 5.747262, acc: 0.312500]\n",
            "904: [D loss: 0.058669, acc: 0.984375]  [G loss: 5.242932, acc: 0.390625]\n",
            "905: [D loss: 0.031221, acc: 0.992188]  [G loss: 4.833278, acc: 0.375000]\n",
            "906: [D loss: 0.016905, acc: 1.000000]  [G loss: 4.774524, acc: 0.390625]\n",
            "907: [D loss: 0.155325, acc: 0.953125]  [G loss: 5.138925, acc: 0.281250]\n",
            "908: [D loss: 0.027906, acc: 0.984375]  [G loss: 5.638584, acc: 0.187500]\n",
            "909: [D loss: 0.015621, acc: 0.992188]  [G loss: 6.151321, acc: 0.093750]\n",
            "910: [D loss: 0.189487, acc: 0.968750]  [G loss: 6.228984, acc: 0.125000]\n",
            "911: [D loss: 0.018487, acc: 0.992188]  [G loss: 5.975335, acc: 0.250000]\n",
            "912: [D loss: 0.092858, acc: 0.984375]  [G loss: 4.439633, acc: 0.281250]\n",
            "913: [D loss: 0.053232, acc: 0.984375]  [G loss: 4.430574, acc: 0.484375]\n",
            "914: [D loss: 0.038570, acc: 0.984375]  [G loss: 4.586139, acc: 0.593750]\n",
            "915: [D loss: 0.150904, acc: 0.929688]  [G loss: 5.037105, acc: 0.250000]\n",
            "916: [D loss: 0.024757, acc: 0.992188]  [G loss: 5.679204, acc: 0.140625]\n",
            "917: [D loss: 0.049119, acc: 0.984375]  [G loss: 7.328358, acc: 0.046875]\n",
            "918: [D loss: 0.013867, acc: 0.992188]  [G loss: 7.372281, acc: 0.078125]\n",
            "919: [D loss: 0.034279, acc: 0.984375]  [G loss: 7.434011, acc: 0.031250]\n",
            "920: [D loss: 0.030032, acc: 0.984375]  [G loss: 6.665486, acc: 0.125000]\n",
            "921: [D loss: 0.045654, acc: 0.976562]  [G loss: 5.830451, acc: 0.156250]\n",
            "922: [D loss: 0.040621, acc: 0.984375]  [G loss: 5.337014, acc: 0.328125]\n",
            "923: [D loss: 0.069018, acc: 0.984375]  [G loss: 5.095311, acc: 0.375000]\n",
            "924: [D loss: 0.114790, acc: 0.968750]  [G loss: 4.771333, acc: 0.468750]\n",
            "925: [D loss: 0.031363, acc: 0.984375]  [G loss: 4.599411, acc: 0.468750]\n",
            "926: [D loss: 0.013554, acc: 1.000000]  [G loss: 4.415655, acc: 0.328125]\n",
            "927: [D loss: 0.012974, acc: 1.000000]  [G loss: 5.251387, acc: 0.312500]\n",
            "928: [D loss: 0.010147, acc: 1.000000]  [G loss: 5.752222, acc: 0.187500]\n",
            "929: [D loss: 0.006130, acc: 1.000000]  [G loss: 5.518218, acc: 0.109375]\n",
            "930: [D loss: 0.021132, acc: 0.992188]  [G loss: 6.254883, acc: 0.171875]\n",
            "931: [D loss: 0.029315, acc: 0.984375]  [G loss: 6.135031, acc: 0.125000]\n",
            "932: [D loss: 0.028953, acc: 0.984375]  [G loss: 5.700470, acc: 0.281250]\n",
            "933: [D loss: 0.043373, acc: 0.992188]  [G loss: 5.466922, acc: 0.125000]\n",
            "934: [D loss: 0.015894, acc: 0.992188]  [G loss: 5.184001, acc: 0.234375]\n",
            "935: [D loss: 0.015266, acc: 0.992188]  [G loss: 4.839026, acc: 0.203125]\n",
            "936: [D loss: 0.013205, acc: 1.000000]  [G loss: 4.656431, acc: 0.250000]\n",
            "937: [D loss: 0.033469, acc: 0.992188]  [G loss: 4.353526, acc: 0.453125]\n",
            "938: [D loss: 0.085253, acc: 0.976562]  [G loss: 4.656765, acc: 0.250000]\n",
            "939: [D loss: 0.078585, acc: 0.984375]  [G loss: 4.626770, acc: 0.375000]\n",
            "940: [D loss: 0.095965, acc: 0.953125]  [G loss: 5.724581, acc: 0.187500]\n",
            "941: [D loss: 0.041629, acc: 0.976562]  [G loss: 8.259426, acc: 0.000000]\n",
            "942: [D loss: 0.034217, acc: 0.992188]  [G loss: 7.953952, acc: 0.000000]\n",
            "943: [D loss: 0.242523, acc: 0.937500]  [G loss: 7.439034, acc: 0.031250]\n",
            "944: [D loss: 0.037974, acc: 0.984375]  [G loss: 6.541758, acc: 0.078125]\n",
            "945: [D loss: 0.053132, acc: 0.992188]  [G loss: 5.552005, acc: 0.218750]\n",
            "946: [D loss: 0.099209, acc: 0.953125]  [G loss: 5.271712, acc: 0.234375]\n",
            "947: [D loss: 0.087909, acc: 0.968750]  [G loss: 6.033620, acc: 0.046875]\n",
            "948: [D loss: 0.017579, acc: 0.992188]  [G loss: 7.172241, acc: 0.031250]\n",
            "949: [D loss: 0.008415, acc: 1.000000]  [G loss: 7.978264, acc: 0.000000]\n",
            "950: [D loss: 0.047406, acc: 0.984375]  [G loss: 7.817671, acc: 0.015625]\n",
            "951: [D loss: 0.055403, acc: 0.992188]  [G loss: 8.022486, acc: 0.000000]\n",
            "952: [D loss: 0.054144, acc: 0.976562]  [G loss: 6.712896, acc: 0.062500]\n",
            "953: [D loss: 0.012518, acc: 1.000000]  [G loss: 6.451061, acc: 0.109375]\n",
            "954: [D loss: 0.050440, acc: 0.992188]  [G loss: 5.725418, acc: 0.218750]\n",
            "955: [D loss: 0.039897, acc: 0.976562]  [G loss: 5.680598, acc: 0.156250]\n",
            "956: [D loss: 0.020202, acc: 1.000000]  [G loss: 5.439939, acc: 0.156250]\n",
            "957: [D loss: 0.018145, acc: 1.000000]  [G loss: 6.173841, acc: 0.125000]\n",
            "958: [D loss: 0.012172, acc: 1.000000]  [G loss: 6.517299, acc: 0.125000]\n",
            "959: [D loss: 0.028313, acc: 0.992188]  [G loss: 6.210838, acc: 0.156250]\n",
            "960: [D loss: 0.022028, acc: 0.984375]  [G loss: 6.617018, acc: 0.093750]\n",
            "961: [D loss: 0.013113, acc: 0.992188]  [G loss: 5.812686, acc: 0.203125]\n",
            "962: [D loss: 0.002475, acc: 1.000000]  [G loss: 5.546262, acc: 0.250000]\n",
            "963: [D loss: 0.002649, acc: 1.000000]  [G loss: 5.185966, acc: 0.187500]\n",
            "964: [D loss: 0.026723, acc: 0.992188]  [G loss: 5.356696, acc: 0.187500]\n",
            "965: [D loss: 0.005022, acc: 1.000000]  [G loss: 5.391103, acc: 0.203125]\n",
            "966: [D loss: 0.006078, acc: 1.000000]  [G loss: 4.959911, acc: 0.406250]\n",
            "967: [D loss: 0.012061, acc: 1.000000]  [G loss: 4.947581, acc: 0.375000]\n",
            "968: [D loss: 0.012733, acc: 0.992188]  [G loss: 5.328338, acc: 0.265625]\n",
            "969: [D loss: 0.010704, acc: 1.000000]  [G loss: 5.730364, acc: 0.218750]\n",
            "970: [D loss: 0.039835, acc: 0.984375]  [G loss: 5.716373, acc: 0.171875]\n",
            "971: [D loss: 0.006602, acc: 1.000000]  [G loss: 5.619232, acc: 0.156250]\n",
            "972: [D loss: 0.010713, acc: 0.992188]  [G loss: 5.370363, acc: 0.281250]\n",
            "973: [D loss: 0.016268, acc: 0.992188]  [G loss: 4.836513, acc: 0.343750]\n",
            "974: [D loss: 0.025910, acc: 0.984375]  [G loss: 5.321105, acc: 0.312500]\n",
            "975: [D loss: 0.052526, acc: 0.992188]  [G loss: 5.158795, acc: 0.281250]\n",
            "976: [D loss: 0.018834, acc: 0.992188]  [G loss: 5.551751, acc: 0.328125]\n",
            "977: [D loss: 0.021071, acc: 1.000000]  [G loss: 5.780768, acc: 0.203125]\n",
            "978: [D loss: 0.025097, acc: 0.984375]  [G loss: 5.644634, acc: 0.296875]\n",
            "979: [D loss: 0.064725, acc: 0.976562]  [G loss: 5.405381, acc: 0.328125]\n",
            "980: [D loss: 0.045294, acc: 0.992188]  [G loss: 5.538974, acc: 0.281250]\n",
            "981: [D loss: 0.025823, acc: 0.992188]  [G loss: 4.948339, acc: 0.359375]\n",
            "982: [D loss: 0.021697, acc: 0.992188]  [G loss: 6.330928, acc: 0.171875]\n",
            "983: [D loss: 0.011254, acc: 0.992188]  [G loss: 6.728131, acc: 0.093750]\n",
            "984: [D loss: 0.034447, acc: 0.984375]  [G loss: 6.686090, acc: 0.109375]\n",
            "985: [D loss: 0.063354, acc: 0.976562]  [G loss: 5.754046, acc: 0.328125]\n",
            "986: [D loss: 0.041154, acc: 0.976562]  [G loss: 4.572508, acc: 0.359375]\n",
            "987: [D loss: 0.045740, acc: 0.984375]  [G loss: 5.176056, acc: 0.218750]\n",
            "988: [D loss: 0.011682, acc: 1.000000]  [G loss: 6.000929, acc: 0.156250]\n",
            "989: [D loss: 0.007742, acc: 1.000000]  [G loss: 6.972339, acc: 0.078125]\n",
            "990: [D loss: 0.029422, acc: 0.992188]  [G loss: 6.158351, acc: 0.187500]\n",
            "991: [D loss: 0.013264, acc: 0.992188]  [G loss: 6.189666, acc: 0.312500]\n",
            "992: [D loss: 0.032328, acc: 0.992188]  [G loss: 6.358324, acc: 0.281250]\n",
            "993: [D loss: 0.069556, acc: 0.976562]  [G loss: 6.894872, acc: 0.187500]\n",
            "994: [D loss: 0.009931, acc: 0.992188]  [G loss: 7.703062, acc: 0.187500]\n",
            "995: [D loss: 0.006299, acc: 1.000000]  [G loss: 8.656076, acc: 0.046875]\n",
            "996: [D loss: 0.060582, acc: 0.968750]  [G loss: 7.932892, acc: 0.203125]\n",
            "997: [D loss: 0.008546, acc: 1.000000]  [G loss: 6.578107, acc: 0.265625]\n",
            "998: [D loss: 0.010468, acc: 1.000000]  [G loss: 6.181125, acc: 0.406250]\n",
            "999: [D loss: 0.010953, acc: 1.000000]  [G loss: 5.622437, acc: 0.343750]\n",
            "1000: [D loss: 0.045174, acc: 0.984375]  [G loss: 5.917847, acc: 0.296875]\n",
            "1001: [D loss: 0.018577, acc: 0.992188]  [G loss: 6.858622, acc: 0.093750]\n",
            "1002: [D loss: 0.030614, acc: 0.984375]  [G loss: 6.308321, acc: 0.218750]\n",
            "1003: [D loss: 0.047578, acc: 0.984375]  [G loss: 6.022579, acc: 0.265625]\n",
            "1004: [D loss: 0.026279, acc: 0.992188]  [G loss: 5.178029, acc: 0.390625]\n",
            "1005: [D loss: 0.018004, acc: 0.992188]  [G loss: 5.191174, acc: 0.359375]\n",
            "1006: [D loss: 0.051687, acc: 0.984375]  [G loss: 5.365002, acc: 0.421875]\n",
            "1007: [D loss: 0.034351, acc: 0.992188]  [G loss: 6.145687, acc: 0.156250]\n",
            "1008: [D loss: 0.023036, acc: 0.984375]  [G loss: 6.199877, acc: 0.171875]\n",
            "1009: [D loss: 0.054345, acc: 0.976562]  [G loss: 5.718225, acc: 0.281250]\n",
            "1010: [D loss: 0.020351, acc: 0.984375]  [G loss: 4.726238, acc: 0.328125]\n",
            "1011: [D loss: 0.008483, acc: 1.000000]  [G loss: 5.021044, acc: 0.312500]\n",
            "1012: [D loss: 0.042504, acc: 0.984375]  [G loss: 4.667326, acc: 0.312500]\n",
            "1013: [D loss: 0.054495, acc: 0.984375]  [G loss: 6.519058, acc: 0.125000]\n",
            "1014: [D loss: 0.015818, acc: 0.992188]  [G loss: 6.085495, acc: 0.125000]\n",
            "1015: [D loss: 0.006536, acc: 1.000000]  [G loss: 6.036263, acc: 0.187500]\n",
            "1016: [D loss: 0.018695, acc: 0.992188]  [G loss: 5.873948, acc: 0.250000]\n",
            "1017: [D loss: 0.064762, acc: 0.976562]  [G loss: 4.010406, acc: 0.500000]\n",
            "1018: [D loss: 0.010738, acc: 1.000000]  [G loss: 4.233582, acc: 0.578125]\n",
            "1019: [D loss: 0.048802, acc: 0.976562]  [G loss: 5.236362, acc: 0.500000]\n",
            "1020: [D loss: 0.111401, acc: 0.945312]  [G loss: 6.699155, acc: 0.046875]\n",
            "1021: [D loss: 0.021592, acc: 0.992188]  [G loss: 10.141534, acc: 0.000000]\n",
            "1022: [D loss: 0.291814, acc: 0.921875]  [G loss: 7.499677, acc: 0.015625]\n",
            "1023: [D loss: 0.077413, acc: 0.984375]  [G loss: 5.525155, acc: 0.328125]\n",
            "1024: [D loss: 0.043959, acc: 0.984375]  [G loss: 3.994244, acc: 0.562500]\n",
            "1025: [D loss: 0.096916, acc: 0.960938]  [G loss: 3.915772, acc: 0.515625]\n",
            "1026: [D loss: 0.132317, acc: 0.953125]  [G loss: 5.909204, acc: 0.203125]\n",
            "1027: [D loss: 0.002630, acc: 1.000000]  [G loss: 7.962154, acc: 0.015625]\n",
            "1028: [D loss: 0.021772, acc: 0.992188]  [G loss: 9.179325, acc: 0.015625]\n",
            "1029: [D loss: 0.008013, acc: 1.000000]  [G loss: 10.002547, acc: 0.031250]\n",
            "1030: [D loss: 0.030515, acc: 0.984375]  [G loss: 9.529493, acc: 0.000000]\n",
            "1031: [D loss: 0.049003, acc: 0.992188]  [G loss: 8.532110, acc: 0.078125]\n",
            "1032: [D loss: 0.001912, acc: 1.000000]  [G loss: 8.299452, acc: 0.140625]\n",
            "1033: [D loss: 0.011675, acc: 0.992188]  [G loss: 6.803604, acc: 0.359375]\n",
            "1034: [D loss: 0.001417, acc: 1.000000]  [G loss: 5.203938, acc: 0.515625]\n",
            "1035: [D loss: 0.013148, acc: 1.000000]  [G loss: 4.945969, acc: 0.546875]\n",
            "1036: [D loss: 0.021534, acc: 0.992188]  [G loss: 5.244421, acc: 0.531250]\n",
            "1037: [D loss: 0.014775, acc: 1.000000]  [G loss: 5.973311, acc: 0.390625]\n",
            "1038: [D loss: 0.004201, acc: 1.000000]  [G loss: 5.766349, acc: 0.234375]\n",
            "1039: [D loss: 0.002330, acc: 1.000000]  [G loss: 6.741964, acc: 0.109375]\n",
            "1040: [D loss: 0.004995, acc: 1.000000]  [G loss: 7.795431, acc: 0.031250]\n",
            "1041: [D loss: 0.004386, acc: 1.000000]  [G loss: 8.437044, acc: 0.046875]\n",
            "1042: [D loss: 0.054090, acc: 0.984375]  [G loss: 7.509206, acc: 0.109375]\n",
            "1043: [D loss: 0.013028, acc: 0.992188]  [G loss: 6.425731, acc: 0.203125]\n",
            "1044: [D loss: 0.002224, acc: 1.000000]  [G loss: 4.578304, acc: 0.359375]\n",
            "1045: [D loss: 0.054634, acc: 0.976562]  [G loss: 4.657518, acc: 0.390625]\n",
            "1046: [D loss: 0.004121, acc: 1.000000]  [G loss: 4.251926, acc: 0.625000]\n",
            "1047: [D loss: 0.068430, acc: 0.960938]  [G loss: 5.217586, acc: 0.421875]\n",
            "1048: [D loss: 0.049100, acc: 0.976562]  [G loss: 6.147425, acc: 0.171875]\n",
            "1049: [D loss: 0.008648, acc: 1.000000]  [G loss: 7.508920, acc: 0.093750]\n",
            "1050: [D loss: 0.047707, acc: 0.984375]  [G loss: 8.620741, acc: 0.046875]\n",
            "1051: [D loss: 0.149465, acc: 0.960938]  [G loss: 6.530499, acc: 0.140625]\n",
            "1052: [D loss: 0.093369, acc: 0.976562]  [G loss: 5.140382, acc: 0.500000]\n",
            "1053: [D loss: 0.012451, acc: 1.000000]  [G loss: 4.469165, acc: 0.578125]\n",
            "1054: [D loss: 0.142340, acc: 0.945312]  [G loss: 3.934588, acc: 0.546875]\n",
            "1055: [D loss: 0.009462, acc: 1.000000]  [G loss: 4.727907, acc: 0.421875]\n",
            "1056: [D loss: 0.017642, acc: 0.992188]  [G loss: 4.769930, acc: 0.296875]\n",
            "1057: [D loss: 0.133342, acc: 0.976562]  [G loss: 5.104726, acc: 0.296875]\n",
            "1058: [D loss: 0.024151, acc: 0.992188]  [G loss: 5.239814, acc: 0.296875]\n",
            "1059: [D loss: 0.033595, acc: 0.984375]  [G loss: 4.817891, acc: 0.453125]\n",
            "1060: [D loss: 0.029993, acc: 0.992188]  [G loss: 4.307835, acc: 0.546875]\n",
            "1061: [D loss: 0.091432, acc: 0.960938]  [G loss: 4.322039, acc: 0.546875]\n",
            "1062: [D loss: 0.049417, acc: 0.976562]  [G loss: 4.846473, acc: 0.484375]\n",
            "1063: [D loss: 0.011520, acc: 1.000000]  [G loss: 3.671606, acc: 0.593750]\n",
            "1064: [D loss: 0.038669, acc: 0.984375]  [G loss: 4.912980, acc: 0.390625]\n",
            "1065: [D loss: 0.002868, acc: 1.000000]  [G loss: 4.712214, acc: 0.359375]\n",
            "1066: [D loss: 0.052849, acc: 0.984375]  [G loss: 5.839731, acc: 0.203125]\n",
            "1067: [D loss: 0.006193, acc: 1.000000]  [G loss: 6.443185, acc: 0.187500]\n",
            "1068: [D loss: 0.053122, acc: 0.968750]  [G loss: 6.081285, acc: 0.187500]\n",
            "1069: [D loss: 0.020074, acc: 0.992188]  [G loss: 5.230048, acc: 0.234375]\n",
            "1070: [D loss: 0.029366, acc: 0.992188]  [G loss: 5.754285, acc: 0.250000]\n",
            "1071: [D loss: 0.007155, acc: 1.000000]  [G loss: 5.193830, acc: 0.296875]\n",
            "1072: [D loss: 0.003285, acc: 1.000000]  [G loss: 5.060328, acc: 0.390625]\n",
            "1073: [D loss: 0.010421, acc: 1.000000]  [G loss: 4.681192, acc: 0.406250]\n",
            "1074: [D loss: 0.021622, acc: 0.984375]  [G loss: 4.655486, acc: 0.343750]\n",
            "1075: [D loss: 0.036216, acc: 0.984375]  [G loss: 4.635865, acc: 0.312500]\n",
            "1076: [D loss: 0.021335, acc: 0.984375]  [G loss: 3.795249, acc: 0.343750]\n",
            "1077: [D loss: 0.040001, acc: 0.984375]  [G loss: 4.299944, acc: 0.234375]\n",
            "1078: [D loss: 0.005367, acc: 1.000000]  [G loss: 5.262921, acc: 0.250000]\n",
            "1079: [D loss: 0.038686, acc: 0.992188]  [G loss: 5.408388, acc: 0.265625]\n",
            "1080: [D loss: 0.007066, acc: 1.000000]  [G loss: 5.995523, acc: 0.234375]\n",
            "1081: [D loss: 0.088300, acc: 0.968750]  [G loss: 5.050423, acc: 0.296875]\n",
            "1082: [D loss: 0.024021, acc: 0.984375]  [G loss: 3.955120, acc: 0.453125]\n",
            "1083: [D loss: 0.024535, acc: 0.992188]  [G loss: 4.794484, acc: 0.359375]\n",
            "1084: [D loss: 0.015319, acc: 1.000000]  [G loss: 4.684066, acc: 0.296875]\n",
            "1085: [D loss: 0.019158, acc: 0.992188]  [G loss: 5.600670, acc: 0.171875]\n",
            "1086: [D loss: 0.048898, acc: 0.976562]  [G loss: 5.044274, acc: 0.265625]\n",
            "1087: [D loss: 0.027426, acc: 0.984375]  [G loss: 4.789688, acc: 0.406250]\n",
            "1088: [D loss: 0.031367, acc: 0.992188]  [G loss: 4.263163, acc: 0.406250]\n",
            "1089: [D loss: 0.010112, acc: 1.000000]  [G loss: 4.105816, acc: 0.296875]\n",
            "1090: [D loss: 0.016530, acc: 0.992188]  [G loss: 4.188891, acc: 0.468750]\n",
            "1091: [D loss: 0.033122, acc: 0.984375]  [G loss: 4.719583, acc: 0.296875]\n",
            "1092: [D loss: 0.006697, acc: 1.000000]  [G loss: 5.537046, acc: 0.218750]\n",
            "1093: [D loss: 0.007959, acc: 1.000000]  [G loss: 5.584266, acc: 0.187500]\n",
            "1094: [D loss: 0.044373, acc: 0.976562]  [G loss: 4.737931, acc: 0.375000]\n",
            "1095: [D loss: 0.007802, acc: 1.000000]  [G loss: 3.874401, acc: 0.515625]\n",
            "1096: [D loss: 0.073136, acc: 0.976562]  [G loss: 3.897436, acc: 0.578125]\n",
            "1097: [D loss: 0.098232, acc: 0.960938]  [G loss: 6.028335, acc: 0.078125]\n",
            "1098: [D loss: 0.039124, acc: 0.984375]  [G loss: 7.308083, acc: 0.031250]\n",
            "1099: [D loss: 0.027538, acc: 0.992188]  [G loss: 7.193047, acc: 0.062500]\n",
            "1100: [D loss: 0.187105, acc: 0.953125]  [G loss: 4.440685, acc: 0.359375]\n",
            "1101: [D loss: 0.026990, acc: 0.992188]  [G loss: 3.355414, acc: 0.578125]\n",
            "1102: [D loss: 0.107322, acc: 0.960938]  [G loss: 3.294247, acc: 0.531250]\n",
            "1103: [D loss: 0.060198, acc: 0.960938]  [G loss: 5.435579, acc: 0.234375]\n",
            "1104: [D loss: 0.003696, acc: 1.000000]  [G loss: 7.233998, acc: 0.062500]\n",
            "1105: [D loss: 0.084654, acc: 0.984375]  [G loss: 7.750122, acc: 0.062500]\n",
            "1106: [D loss: 0.056409, acc: 0.968750]  [G loss: 6.848545, acc: 0.125000]\n",
            "1107: [D loss: 0.030685, acc: 0.976562]  [G loss: 6.091100, acc: 0.078125]\n",
            "1108: [D loss: 0.006674, acc: 1.000000]  [G loss: 4.825206, acc: 0.250000]\n",
            "1109: [D loss: 0.031018, acc: 0.992188]  [G loss: 4.636269, acc: 0.250000]\n",
            "1110: [D loss: 0.027764, acc: 0.992188]  [G loss: 4.827754, acc: 0.375000]\n",
            "1111: [D loss: 0.021513, acc: 0.992188]  [G loss: 5.542266, acc: 0.171875]\n",
            "1112: [D loss: 0.016965, acc: 1.000000]  [G loss: 5.552612, acc: 0.187500]\n",
            "1113: [D loss: 0.003406, acc: 1.000000]  [G loss: 5.670758, acc: 0.093750]\n",
            "1114: [D loss: 0.004928, acc: 1.000000]  [G loss: 6.258803, acc: 0.125000]\n",
            "1115: [D loss: 0.006105, acc: 1.000000]  [G loss: 6.736351, acc: 0.109375]\n",
            "1116: [D loss: 0.069887, acc: 0.984375]  [G loss: 5.547974, acc: 0.125000]\n",
            "1117: [D loss: 0.038813, acc: 0.992188]  [G loss: 5.120411, acc: 0.062500]\n",
            "1118: [D loss: 0.014935, acc: 1.000000]  [G loss: 4.893465, acc: 0.171875]\n",
            "1119: [D loss: 0.023660, acc: 0.992188]  [G loss: 5.042304, acc: 0.234375]\n",
            "1120: [D loss: 0.031839, acc: 0.984375]  [G loss: 5.068591, acc: 0.109375]\n",
            "1121: [D loss: 0.005851, acc: 1.000000]  [G loss: 5.620783, acc: 0.109375]\n",
            "1122: [D loss: 0.034452, acc: 0.984375]  [G loss: 4.959889, acc: 0.156250]\n",
            "1123: [D loss: 0.024090, acc: 0.992188]  [G loss: 4.544248, acc: 0.359375]\n",
            "1124: [D loss: 0.015866, acc: 1.000000]  [G loss: 3.829805, acc: 0.343750]\n",
            "1125: [D loss: 0.020848, acc: 1.000000]  [G loss: 3.753827, acc: 0.421875]\n",
            "1126: [D loss: 0.023617, acc: 1.000000]  [G loss: 4.402464, acc: 0.265625]\n",
            "1127: [D loss: 0.010373, acc: 1.000000]  [G loss: 3.794115, acc: 0.312500]\n",
            "1128: [D loss: 0.021367, acc: 0.992188]  [G loss: 4.730173, acc: 0.187500]\n",
            "1129: [D loss: 0.019928, acc: 1.000000]  [G loss: 5.063786, acc: 0.203125]\n",
            "1130: [D loss: 0.021783, acc: 0.992188]  [G loss: 4.409018, acc: 0.218750]\n",
            "1131: [D loss: 0.055240, acc: 0.992188]  [G loss: 4.127625, acc: 0.406250]\n",
            "1132: [D loss: 0.050410, acc: 0.992188]  [G loss: 4.080376, acc: 0.421875]\n",
            "1133: [D loss: 0.064658, acc: 0.984375]  [G loss: 3.650983, acc: 0.468750]\n",
            "1134: [D loss: 0.029374, acc: 1.000000]  [G loss: 3.968300, acc: 0.312500]\n",
            "1135: [D loss: 0.017363, acc: 1.000000]  [G loss: 4.723291, acc: 0.218750]\n",
            "1136: [D loss: 0.006289, acc: 1.000000]  [G loss: 5.673589, acc: 0.171875]\n",
            "1137: [D loss: 0.043206, acc: 0.992188]  [G loss: 5.238829, acc: 0.140625]\n",
            "1138: [D loss: 0.093211, acc: 0.976562]  [G loss: 4.274290, acc: 0.328125]\n",
            "1139: [D loss: 0.018968, acc: 1.000000]  [G loss: 3.921942, acc: 0.406250]\n",
            "1140: [D loss: 0.017875, acc: 0.992188]  [G loss: 3.702553, acc: 0.437500]\n",
            "1141: [D loss: 0.103006, acc: 0.968750]  [G loss: 4.496088, acc: 0.250000]\n",
            "1142: [D loss: 0.025538, acc: 0.984375]  [G loss: 5.004040, acc: 0.156250]\n",
            "1143: [D loss: 0.022498, acc: 0.992188]  [G loss: 5.293071, acc: 0.109375]\n",
            "1144: [D loss: 0.029470, acc: 0.992188]  [G loss: 4.932511, acc: 0.156250]\n",
            "1145: [D loss: 0.026086, acc: 0.992188]  [G loss: 5.818668, acc: 0.156250]\n",
            "1146: [D loss: 0.058126, acc: 0.976562]  [G loss: 4.590919, acc: 0.312500]\n",
            "1147: [D loss: 0.072059, acc: 0.976562]  [G loss: 3.792886, acc: 0.359375]\n",
            "1148: [D loss: 0.091014, acc: 0.960938]  [G loss: 4.671354, acc: 0.312500]\n",
            "1149: [D loss: 0.021164, acc: 0.992188]  [G loss: 5.948081, acc: 0.171875]\n",
            "1150: [D loss: 0.093987, acc: 0.960938]  [G loss: 5.038327, acc: 0.312500]\n",
            "1151: [D loss: 0.010322, acc: 1.000000]  [G loss: 4.294919, acc: 0.343750]\n",
            "1152: [D loss: 0.027858, acc: 0.992188]  [G loss: 4.593748, acc: 0.296875]\n",
            "1153: [D loss: 0.054540, acc: 0.976562]  [G loss: 6.931029, acc: 0.125000]\n",
            "1154: [D loss: 0.013327, acc: 0.992188]  [G loss: 7.169351, acc: 0.062500]\n",
            "1155: [D loss: 0.031194, acc: 0.976562]  [G loss: 7.860475, acc: 0.062500]\n",
            "1156: [D loss: 0.037764, acc: 0.992188]  [G loss: 7.660541, acc: 0.109375]\n",
            "1157: [D loss: 0.055008, acc: 0.968750]  [G loss: 5.392778, acc: 0.281250]\n",
            "1158: [D loss: 0.048575, acc: 0.992188]  [G loss: 4.647287, acc: 0.375000]\n",
            "1159: [D loss: 0.068387, acc: 0.976562]  [G loss: 3.926056, acc: 0.468750]\n",
            "1160: [D loss: 0.127871, acc: 0.945312]  [G loss: 2.820597, acc: 0.671875]\n",
            "1161: [D loss: 0.131560, acc: 0.921875]  [G loss: 3.500589, acc: 0.390625]\n",
            "1162: [D loss: 0.032032, acc: 0.984375]  [G loss: 5.278413, acc: 0.125000]\n",
            "1163: [D loss: 0.006496, acc: 1.000000]  [G loss: 6.150505, acc: 0.109375]\n",
            "1164: [D loss: 0.006848, acc: 1.000000]  [G loss: 7.791966, acc: 0.078125]\n",
            "1165: [D loss: 0.107109, acc: 0.968750]  [G loss: 6.920066, acc: 0.078125]\n",
            "1166: [D loss: 0.033910, acc: 0.992188]  [G loss: 4.926513, acc: 0.296875]\n",
            "1167: [D loss: 0.018265, acc: 1.000000]  [G loss: 5.439770, acc: 0.328125]\n",
            "1168: [D loss: 0.027963, acc: 0.984375]  [G loss: 5.671374, acc: 0.265625]\n",
            "1169: [D loss: 0.020101, acc: 0.992188]  [G loss: 8.089865, acc: 0.203125]\n",
            "1170: [D loss: 0.121039, acc: 0.960938]  [G loss: 4.617037, acc: 0.265625]\n",
            "1171: [D loss: 0.076569, acc: 0.984375]  [G loss: 2.567834, acc: 0.562500]\n",
            "1172: [D loss: 0.066329, acc: 0.968750]  [G loss: 3.046277, acc: 0.406250]\n",
            "1173: [D loss: 0.047332, acc: 0.984375]  [G loss: 4.510400, acc: 0.156250]\n",
            "1174: [D loss: 0.021655, acc: 0.992188]  [G loss: 5.516758, acc: 0.046875]\n",
            "1175: [D loss: 0.119195, acc: 0.984375]  [G loss: 4.276135, acc: 0.140625]\n",
            "1176: [D loss: 0.157082, acc: 0.960938]  [G loss: 1.427246, acc: 0.578125]\n",
            "1177: [D loss: 0.117643, acc: 0.953125]  [G loss: 1.497383, acc: 0.640625]\n",
            "1178: [D loss: 0.198435, acc: 0.937500]  [G loss: 5.584444, acc: 0.156250]\n",
            "1179: [D loss: 0.325123, acc: 0.921875]  [G loss: 2.840288, acc: 0.468750]\n",
            "1180: [D loss: 0.140591, acc: 0.960938]  [G loss: 0.762962, acc: 0.812500]\n",
            "1181: [D loss: 0.259302, acc: 0.882812]  [G loss: 1.150294, acc: 0.734375]\n",
            "1182: [D loss: 0.091478, acc: 0.945312]  [G loss: 2.209899, acc: 0.406250]\n",
            "1183: [D loss: 0.057258, acc: 0.976562]  [G loss: 4.272318, acc: 0.156250]\n",
            "1184: [D loss: 0.273050, acc: 0.914062]  [G loss: 2.320435, acc: 0.437500]\n",
            "1185: [D loss: 0.053708, acc: 0.968750]  [G loss: 0.550329, acc: 0.765625]\n",
            "1186: [D loss: 0.062111, acc: 0.984375]  [G loss: 1.116500, acc: 0.640625]\n",
            "1187: [D loss: 0.218623, acc: 0.906250]  [G loss: 3.355183, acc: 0.406250]\n",
            "1188: [D loss: 0.016202, acc: 1.000000]  [G loss: 6.055674, acc: 0.156250]\n",
            "1189: [D loss: 0.188290, acc: 0.929688]  [G loss: 4.895304, acc: 0.218750]\n",
            "1190: [D loss: 0.072471, acc: 0.968750]  [G loss: 3.962530, acc: 0.343750]\n",
            "1191: [D loss: 0.026621, acc: 0.992188]  [G loss: 3.508167, acc: 0.531250]\n",
            "1192: [D loss: 0.067365, acc: 0.968750]  [G loss: 3.262906, acc: 0.468750]\n",
            "1193: [D loss: 0.071002, acc: 0.968750]  [G loss: 4.055394, acc: 0.328125]\n",
            "1194: [D loss: 0.025210, acc: 0.992188]  [G loss: 3.758523, acc: 0.343750]\n",
            "1195: [D loss: 0.006236, acc: 1.000000]  [G loss: 4.407381, acc: 0.187500]\n",
            "1196: [D loss: 0.152710, acc: 0.945312]  [G loss: 4.009812, acc: 0.203125]\n",
            "1197: [D loss: 0.042756, acc: 0.984375]  [G loss: 3.122224, acc: 0.453125]\n",
            "1198: [D loss: 0.104312, acc: 0.960938]  [G loss: 2.471201, acc: 0.531250]\n",
            "1199: [D loss: 0.101186, acc: 0.968750]  [G loss: 1.667378, acc: 0.531250]\n",
            "1200: [D loss: 0.087252, acc: 0.953125]  [G loss: 2.390253, acc: 0.328125]\n",
            "1201: [D loss: 0.040188, acc: 0.984375]  [G loss: 2.699303, acc: 0.312500]\n",
            "1202: [D loss: 0.084847, acc: 0.953125]  [G loss: 3.123323, acc: 0.312500]\n",
            "1203: [D loss: 0.104753, acc: 0.960938]  [G loss: 4.498568, acc: 0.281250]\n",
            "1204: [D loss: 0.048182, acc: 0.984375]  [G loss: 4.448381, acc: 0.265625]\n",
            "1205: [D loss: 0.075539, acc: 0.976562]  [G loss: 4.078977, acc: 0.328125]\n",
            "1206: [D loss: 0.051512, acc: 0.984375]  [G loss: 3.335200, acc: 0.390625]\n",
            "1207: [D loss: 0.058426, acc: 0.976562]  [G loss: 3.797973, acc: 0.484375]\n",
            "1208: [D loss: 0.033511, acc: 0.984375]  [G loss: 3.744836, acc: 0.390625]\n",
            "1209: [D loss: 0.071553, acc: 0.976562]  [G loss: 5.169406, acc: 0.250000]\n",
            "1210: [D loss: 0.033793, acc: 0.984375]  [G loss: 6.072993, acc: 0.156250]\n",
            "1211: [D loss: 0.104346, acc: 0.976562]  [G loss: 7.647500, acc: 0.046875]\n",
            "1212: [D loss: 0.080085, acc: 0.976562]  [G loss: 5.581381, acc: 0.187500]\n",
            "1213: [D loss: 0.019874, acc: 0.992188]  [G loss: 4.449459, acc: 0.375000]\n",
            "1214: [D loss: 0.120712, acc: 0.960938]  [G loss: 4.778914, acc: 0.359375]\n",
            "1215: [D loss: 0.101770, acc: 0.976562]  [G loss: 3.084194, acc: 0.593750]\n",
            "1216: [D loss: 0.126633, acc: 0.945312]  [G loss: 3.746374, acc: 0.406250]\n",
            "1217: [D loss: 0.016314, acc: 1.000000]  [G loss: 6.104360, acc: 0.125000]\n",
            "1218: [D loss: 0.070970, acc: 0.976562]  [G loss: 6.461771, acc: 0.125000]\n",
            "1219: [D loss: 0.077953, acc: 0.968750]  [G loss: 5.638016, acc: 0.125000]\n",
            "1220: [D loss: 0.004896, acc: 1.000000]  [G loss: 5.552509, acc: 0.109375]\n",
            "1221: [D loss: 0.101582, acc: 0.976562]  [G loss: 3.625996, acc: 0.250000]\n",
            "1222: [D loss: 0.047321, acc: 0.984375]  [G loss: 3.094248, acc: 0.343750]\n",
            "1223: [D loss: 0.110901, acc: 0.976562]  [G loss: 3.582683, acc: 0.171875]\n",
            "1224: [D loss: 0.035564, acc: 0.984375]  [G loss: 4.844903, acc: 0.125000]\n",
            "1225: [D loss: 0.193570, acc: 0.953125]  [G loss: 2.845650, acc: 0.312500]\n",
            "1226: [D loss: 0.087619, acc: 0.953125]  [G loss: 2.781543, acc: 0.375000]\n",
            "1227: [D loss: 0.050498, acc: 0.976562]  [G loss: 3.790905, acc: 0.250000]\n",
            "1228: [D loss: 0.168284, acc: 0.960938]  [G loss: 3.469524, acc: 0.312500]\n",
            "1229: [D loss: 0.078461, acc: 0.968750]  [G loss: 3.603240, acc: 0.281250]\n",
            "1230: [D loss: 0.119728, acc: 0.968750]  [G loss: 4.613974, acc: 0.125000]\n",
            "1231: [D loss: 0.050484, acc: 0.984375]  [G loss: 6.270103, acc: 0.140625]\n",
            "1232: [D loss: 0.160678, acc: 0.960938]  [G loss: 5.993469, acc: 0.093750]\n",
            "1233: [D loss: 0.141313, acc: 0.953125]  [G loss: 2.425368, acc: 0.421875]\n",
            "1234: [D loss: 0.342047, acc: 0.890625]  [G loss: 7.522843, acc: 0.000000]\n",
            "1235: [D loss: 0.356470, acc: 0.906250]  [G loss: 6.951708, acc: 0.000000]\n",
            "1236: [D loss: 0.205862, acc: 0.945312]  [G loss: 3.868022, acc: 0.140625]\n",
            "1237: [D loss: 0.168544, acc: 0.921875]  [G loss: 1.427517, acc: 0.406250]\n",
            "1238: [D loss: 0.291821, acc: 0.906250]  [G loss: 2.398635, acc: 0.171875]\n",
            "1239: [D loss: 0.099508, acc: 0.953125]  [G loss: 4.647816, acc: 0.000000]\n",
            "1240: [D loss: 0.150322, acc: 0.953125]  [G loss: 5.608853, acc: 0.015625]\n",
            "1241: [D loss: 0.182056, acc: 0.937500]  [G loss: 5.466181, acc: 0.015625]\n",
            "1242: [D loss: 0.109696, acc: 0.960938]  [G loss: 4.602784, acc: 0.109375]\n",
            "1243: [D loss: 0.161753, acc: 0.945312]  [G loss: 3.338330, acc: 0.515625]\n",
            "1244: [D loss: 0.116643, acc: 0.960938]  [G loss: 2.994203, acc: 0.468750]\n",
            "1245: [D loss: 0.110365, acc: 0.945312]  [G loss: 3.144086, acc: 0.453125]\n",
            "1246: [D loss: 0.080746, acc: 0.976562]  [G loss: 3.904035, acc: 0.250000]\n",
            "1247: [D loss: 0.045879, acc: 0.992188]  [G loss: 3.854290, acc: 0.234375]\n",
            "1248: [D loss: 0.036846, acc: 0.992188]  [G loss: 4.221662, acc: 0.125000]\n",
            "1249: [D loss: 0.017770, acc: 0.992188]  [G loss: 4.264373, acc: 0.187500]\n",
            "1250: [D loss: 0.084924, acc: 0.953125]  [G loss: 3.164493, acc: 0.250000]\n",
            "1251: [D loss: 0.042557, acc: 0.976562]  [G loss: 2.645639, acc: 0.250000]\n",
            "1252: [D loss: 0.021864, acc: 0.992188]  [G loss: 1.939365, acc: 0.437500]\n",
            "1253: [D loss: 0.098931, acc: 0.960938]  [G loss: 2.221309, acc: 0.312500]\n",
            "1254: [D loss: 0.030977, acc: 0.992188]  [G loss: 2.765159, acc: 0.328125]\n",
            "1255: [D loss: 0.036635, acc: 0.976562]  [G loss: 2.916482, acc: 0.281250]\n",
            "1256: [D loss: 0.073471, acc: 0.968750]  [G loss: 2.322388, acc: 0.546875]\n",
            "1257: [D loss: 0.062973, acc: 0.976562]  [G loss: 2.383980, acc: 0.562500]\n",
            "1258: [D loss: 0.043898, acc: 0.984375]  [G loss: 1.772367, acc: 0.656250]\n",
            "1259: [D loss: 0.047774, acc: 0.976562]  [G loss: 1.816553, acc: 0.687500]\n",
            "1260: [D loss: 0.050454, acc: 0.984375]  [G loss: 1.950416, acc: 0.625000]\n",
            "1261: [D loss: 0.021768, acc: 1.000000]  [G loss: 1.532331, acc: 0.734375]\n",
            "1262: [D loss: 0.035342, acc: 0.984375]  [G loss: 2.219292, acc: 0.593750]\n",
            "1263: [D loss: 0.022073, acc: 0.992188]  [G loss: 2.318120, acc: 0.468750]\n",
            "1264: [D loss: 0.138191, acc: 0.968750]  [G loss: 2.142819, acc: 0.484375]\n",
            "1265: [D loss: 0.034330, acc: 0.992188]  [G loss: 1.871805, acc: 0.562500]\n",
            "1266: [D loss: 0.042485, acc: 0.984375]  [G loss: 2.001735, acc: 0.484375]\n",
            "1267: [D loss: 0.066631, acc: 0.976562]  [G loss: 2.252685, acc: 0.500000]\n",
            "1268: [D loss: 0.023384, acc: 0.992188]  [G loss: 2.570725, acc: 0.359375]\n",
            "1269: [D loss: 0.060261, acc: 0.984375]  [G loss: 2.687037, acc: 0.343750]\n",
            "1270: [D loss: 0.029942, acc: 0.984375]  [G loss: 2.860889, acc: 0.328125]\n",
            "1271: [D loss: 0.034569, acc: 0.992188]  [G loss: 2.553850, acc: 0.453125]\n",
            "1272: [D loss: 0.107726, acc: 0.968750]  [G loss: 2.386317, acc: 0.453125]\n",
            "1273: [D loss: 0.014452, acc: 1.000000]  [G loss: 2.663357, acc: 0.468750]\n",
            "1274: [D loss: 0.037078, acc: 0.992188]  [G loss: 2.124233, acc: 0.515625]\n",
            "1275: [D loss: 0.062161, acc: 0.976562]  [G loss: 2.371810, acc: 0.390625]\n",
            "1276: [D loss: 0.058864, acc: 0.984375]  [G loss: 3.721708, acc: 0.218750]\n",
            "1277: [D loss: 0.045186, acc: 0.976562]  [G loss: 3.308215, acc: 0.250000]\n",
            "1278: [D loss: 0.033597, acc: 0.992188]  [G loss: 3.763279, acc: 0.281250]\n",
            "1279: [D loss: 0.121562, acc: 0.968750]  [G loss: 3.244287, acc: 0.265625]\n",
            "1280: [D loss: 0.092182, acc: 0.960938]  [G loss: 3.258411, acc: 0.328125]\n",
            "1281: [D loss: 0.060202, acc: 0.968750]  [G loss: 5.259138, acc: 0.125000]\n",
            "1282: [D loss: 0.065866, acc: 0.976562]  [G loss: 6.110630, acc: 0.093750]\n",
            "1283: [D loss: 0.093693, acc: 0.945312]  [G loss: 4.522022, acc: 0.187500]\n",
            "1284: [D loss: 0.112920, acc: 0.968750]  [G loss: 3.703435, acc: 0.312500]\n",
            "1285: [D loss: 0.069875, acc: 0.976562]  [G loss: 5.048984, acc: 0.218750]\n",
            "1286: [D loss: 0.088234, acc: 0.976562]  [G loss: 6.274031, acc: 0.156250]\n",
            "1287: [D loss: 0.038566, acc: 0.984375]  [G loss: 5.384742, acc: 0.171875]\n",
            "1288: [D loss: 0.090261, acc: 0.960938]  [G loss: 4.438029, acc: 0.265625]\n",
            "1289: [D loss: 0.149807, acc: 0.968750]  [G loss: 3.343229, acc: 0.218750]\n",
            "1290: [D loss: 0.073627, acc: 0.968750]  [G loss: 7.003482, acc: 0.015625]\n",
            "1291: [D loss: 0.032587, acc: 0.992188]  [G loss: 9.259224, acc: 0.000000]\n",
            "1292: [D loss: 0.107208, acc: 0.968750]  [G loss: 9.380791, acc: 0.000000]\n",
            "1293: [D loss: 0.095832, acc: 0.953125]  [G loss: 6.628319, acc: 0.000000]\n",
            "1294: [D loss: 0.014176, acc: 1.000000]  [G loss: 3.207289, acc: 0.078125]\n",
            "1295: [D loss: 0.137073, acc: 0.945312]  [G loss: 4.829072, acc: 0.062500]\n",
            "1296: [D loss: 0.083868, acc: 0.976562]  [G loss: 6.801813, acc: 0.000000]\n",
            "1297: [D loss: 0.070042, acc: 0.984375]  [G loss: 8.124364, acc: 0.015625]\n",
            "1298: [D loss: 0.036077, acc: 0.992188]  [G loss: 7.425718, acc: 0.015625]\n",
            "1299: [D loss: 0.010717, acc: 1.000000]  [G loss: 6.406820, acc: 0.062500]\n",
            "1300: [D loss: 0.071468, acc: 0.976562]  [G loss: 5.494962, acc: 0.187500]\n",
            "1301: [D loss: 0.014690, acc: 1.000000]  [G loss: 4.424535, acc: 0.250000]\n",
            "1302: [D loss: 0.023662, acc: 0.992188]  [G loss: 3.635167, acc: 0.265625]\n",
            "1303: [D loss: 0.058876, acc: 0.976562]  [G loss: 4.887094, acc: 0.125000]\n",
            "1304: [D loss: 0.017521, acc: 1.000000]  [G loss: 5.511702, acc: 0.062500]\n",
            "1305: [D loss: 0.100061, acc: 0.968750]  [G loss: 5.962802, acc: 0.046875]\n",
            "1306: [D loss: 0.068610, acc: 0.984375]  [G loss: 4.856298, acc: 0.062500]\n",
            "1307: [D loss: 0.101061, acc: 0.953125]  [G loss: 3.725058, acc: 0.140625]\n",
            "1308: [D loss: 0.058441, acc: 0.992188]  [G loss: 3.291832, acc: 0.156250]\n",
            "1309: [D loss: 0.061463, acc: 1.000000]  [G loss: 3.826674, acc: 0.171875]\n",
            "1310: [D loss: 0.085384, acc: 0.968750]  [G loss: 4.594389, acc: 0.062500]\n",
            "1311: [D loss: 0.059193, acc: 0.976562]  [G loss: 4.623051, acc: 0.125000]\n",
            "1312: [D loss: 0.037792, acc: 0.976562]  [G loss: 4.607434, acc: 0.218750]\n",
            "1313: [D loss: 0.014550, acc: 1.000000]  [G loss: 4.039939, acc: 0.171875]\n",
            "1314: [D loss: 0.034617, acc: 0.984375]  [G loss: 3.488140, acc: 0.218750]\n",
            "1315: [D loss: 0.041705, acc: 0.984375]  [G loss: 3.376993, acc: 0.296875]\n",
            "1316: [D loss: 0.031062, acc: 0.992188]  [G loss: 3.903930, acc: 0.140625]\n",
            "1317: [D loss: 0.044122, acc: 0.984375]  [G loss: 4.899131, acc: 0.062500]\n",
            "1318: [D loss: 0.021751, acc: 1.000000]  [G loss: 5.798972, acc: 0.046875]\n",
            "1319: [D loss: 0.084977, acc: 0.960938]  [G loss: 4.660728, acc: 0.062500]\n",
            "1320: [D loss: 0.026932, acc: 0.992188]  [G loss: 3.324162, acc: 0.187500]\n",
            "1321: [D loss: 0.119474, acc: 0.953125]  [G loss: 3.803488, acc: 0.234375]\n",
            "1322: [D loss: 0.115808, acc: 0.953125]  [G loss: 5.139557, acc: 0.093750]\n",
            "1323: [D loss: 0.052541, acc: 0.976562]  [G loss: 6.189335, acc: 0.171875]\n",
            "1324: [D loss: 0.105799, acc: 0.976562]  [G loss: 4.878211, acc: 0.218750]\n",
            "1325: [D loss: 0.144024, acc: 0.945312]  [G loss: 3.495342, acc: 0.296875]\n",
            "1326: [D loss: 0.070944, acc: 0.976562]  [G loss: 3.327287, acc: 0.234375]\n",
            "1327: [D loss: 0.094298, acc: 0.953125]  [G loss: 3.785326, acc: 0.156250]\n",
            "1328: [D loss: 0.083969, acc: 0.968750]  [G loss: 3.583322, acc: 0.125000]\n",
            "1329: [D loss: 0.057781, acc: 0.976562]  [G loss: 3.878677, acc: 0.125000]\n",
            "1330: [D loss: 0.043948, acc: 0.976562]  [G loss: 3.617297, acc: 0.140625]\n",
            "1331: [D loss: 0.056890, acc: 0.984375]  [G loss: 3.003972, acc: 0.218750]\n",
            "1332: [D loss: 0.107928, acc: 0.992188]  [G loss: 2.818995, acc: 0.343750]\n",
            "1333: [D loss: 0.082646, acc: 0.953125]  [G loss: 3.512614, acc: 0.187500]\n",
            "1334: [D loss: 0.032069, acc: 1.000000]  [G loss: 4.461053, acc: 0.109375]\n",
            "1335: [D loss: 0.137191, acc: 0.953125]  [G loss: 3.216608, acc: 0.218750]\n",
            "1336: [D loss: 0.092741, acc: 0.968750]  [G loss: 1.609811, acc: 0.390625]\n",
            "1337: [D loss: 0.084803, acc: 0.968750]  [G loss: 1.583165, acc: 0.453125]\n",
            "1338: [D loss: 0.096980, acc: 0.960938]  [G loss: 3.469014, acc: 0.187500]\n",
            "1339: [D loss: 0.045137, acc: 0.984375]  [G loss: 4.279156, acc: 0.093750]\n",
            "1340: [D loss: 0.143774, acc: 0.960938]  [G loss: 4.875598, acc: 0.125000]\n",
            "1341: [D loss: 0.044322, acc: 0.992188]  [G loss: 3.576337, acc: 0.125000]\n",
            "1342: [D loss: 0.045525, acc: 0.992188]  [G loss: 2.751952, acc: 0.312500]\n",
            "1343: [D loss: 0.078678, acc: 0.976562]  [G loss: 1.873358, acc: 0.437500]\n",
            "1344: [D loss: 0.052963, acc: 0.984375]  [G loss: 2.130852, acc: 0.375000]\n",
            "1345: [D loss: 0.041636, acc: 0.984375]  [G loss: 3.326820, acc: 0.187500]\n",
            "1346: [D loss: 0.028807, acc: 0.992188]  [G loss: 4.147913, acc: 0.125000]\n",
            "1347: [D loss: 0.052942, acc: 0.968750]  [G loss: 4.183540, acc: 0.062500]\n",
            "1348: [D loss: 0.099844, acc: 0.953125]  [G loss: 2.836450, acc: 0.234375]\n",
            "1349: [D loss: 0.079618, acc: 0.984375]  [G loss: 2.657574, acc: 0.312500]\n",
            "1350: [D loss: 0.065430, acc: 0.984375]  [G loss: 3.404669, acc: 0.171875]\n",
            "1351: [D loss: 0.053569, acc: 0.976562]  [G loss: 4.899607, acc: 0.078125]\n",
            "1352: [D loss: 0.053767, acc: 0.984375]  [G loss: 5.675406, acc: 0.046875]\n",
            "1353: [D loss: 0.102002, acc: 0.968750]  [G loss: 5.238103, acc: 0.046875]\n",
            "1354: [D loss: 0.100609, acc: 0.960938]  [G loss: 4.034035, acc: 0.125000]\n",
            "1355: [D loss: 0.096885, acc: 0.968750]  [G loss: 3.725317, acc: 0.140625]\n",
            "1356: [D loss: 0.073029, acc: 0.953125]  [G loss: 5.379674, acc: 0.046875]\n",
            "1357: [D loss: 0.046843, acc: 0.976562]  [G loss: 7.513880, acc: 0.015625]\n",
            "1358: [D loss: 0.088696, acc: 0.960938]  [G loss: 6.934494, acc: 0.031250]\n",
            "1359: [D loss: 0.155941, acc: 0.937500]  [G loss: 3.844918, acc: 0.171875]\n",
            "1360: [D loss: 0.082430, acc: 0.976562]  [G loss: 1.319891, acc: 0.609375]\n",
            "1361: [D loss: 0.232164, acc: 0.937500]  [G loss: 2.979908, acc: 0.312500]\n",
            "1362: [D loss: 0.078450, acc: 0.976562]  [G loss: 5.337787, acc: 0.046875]\n",
            "1363: [D loss: 0.136622, acc: 0.953125]  [G loss: 5.441809, acc: 0.125000]\n",
            "1364: [D loss: 0.016610, acc: 0.984375]  [G loss: 4.401158, acc: 0.234375]\n",
            "1365: [D loss: 0.067074, acc: 0.976562]  [G loss: 3.214648, acc: 0.343750]\n",
            "1366: [D loss: 0.064138, acc: 0.968750]  [G loss: 3.764014, acc: 0.265625]\n",
            "1367: [D loss: 0.083185, acc: 0.960938]  [G loss: 5.362945, acc: 0.031250]\n",
            "1368: [D loss: 0.035283, acc: 0.992188]  [G loss: 5.652325, acc: 0.078125]\n",
            "1369: [D loss: 0.104385, acc: 0.960938]  [G loss: 4.860979, acc: 0.109375]\n",
            "1370: [D loss: 0.279511, acc: 0.929688]  [G loss: 1.990480, acc: 0.468750]\n",
            "1371: [D loss: 0.139196, acc: 0.937500]  [G loss: 1.761852, acc: 0.437500]\n",
            "1372: [D loss: 0.101170, acc: 0.960938]  [G loss: 3.528223, acc: 0.234375]\n",
            "1373: [D loss: 0.031818, acc: 0.992188]  [G loss: 3.829173, acc: 0.171875]\n",
            "1374: [D loss: 0.127448, acc: 0.968750]  [G loss: 3.531487, acc: 0.218750]\n",
            "1375: [D loss: 0.048096, acc: 0.984375]  [G loss: 4.163179, acc: 0.312500]\n",
            "1376: [D loss: 0.117628, acc: 0.960938]  [G loss: 4.651875, acc: 0.250000]\n",
            "1377: [D loss: 0.032133, acc: 0.984375]  [G loss: 5.230192, acc: 0.171875]\n",
            "1378: [D loss: 0.044056, acc: 0.984375]  [G loss: 4.222626, acc: 0.234375]\n",
            "1379: [D loss: 0.042406, acc: 0.984375]  [G loss: 4.060685, acc: 0.296875]\n",
            "1380: [D loss: 0.076998, acc: 0.976562]  [G loss: 3.262115, acc: 0.328125]\n",
            "1381: [D loss: 0.054013, acc: 0.984375]  [G loss: 3.148127, acc: 0.312500]\n",
            "1382: [D loss: 0.085430, acc: 0.960938]  [G loss: 1.906720, acc: 0.500000]\n",
            "1383: [D loss: 0.131204, acc: 0.945312]  [G loss: 3.168448, acc: 0.265625]\n",
            "1384: [D loss: 0.011720, acc: 1.000000]  [G loss: 4.572082, acc: 0.171875]\n",
            "1385: [D loss: 0.046072, acc: 0.984375]  [G loss: 5.416199, acc: 0.062500]\n",
            "1386: [D loss: 0.083925, acc: 0.960938]  [G loss: 5.272806, acc: 0.093750]\n",
            "1387: [D loss: 0.043600, acc: 0.984375]  [G loss: 4.204390, acc: 0.140625]\n",
            "1388: [D loss: 0.034006, acc: 0.984375]  [G loss: 3.243005, acc: 0.234375]\n",
            "1389: [D loss: 0.065670, acc: 0.984375]  [G loss: 2.762221, acc: 0.265625]\n",
            "1390: [D loss: 0.096804, acc: 0.960938]  [G loss: 4.296689, acc: 0.171875]\n",
            "1391: [D loss: 0.072847, acc: 0.960938]  [G loss: 5.342557, acc: 0.156250]\n",
            "1392: [D loss: 0.119704, acc: 0.976562]  [G loss: 5.645817, acc: 0.109375]\n",
            "1393: [D loss: 0.064865, acc: 0.992188]  [G loss: 5.477175, acc: 0.046875]\n",
            "1394: [D loss: 0.248043, acc: 0.953125]  [G loss: 3.482498, acc: 0.125000]\n",
            "1395: [D loss: 0.069320, acc: 0.968750]  [G loss: 3.938208, acc: 0.125000]\n",
            "1396: [D loss: 0.122954, acc: 0.945312]  [G loss: 3.458951, acc: 0.218750]\n",
            "1397: [D loss: 0.142668, acc: 0.968750]  [G loss: 5.676231, acc: 0.078125]\n",
            "1398: [D loss: 0.214429, acc: 0.929688]  [G loss: 5.065985, acc: 0.187500]\n",
            "1399: [D loss: 0.095197, acc: 0.960938]  [G loss: 4.526262, acc: 0.140625]\n",
            "1400: [D loss: 0.050050, acc: 0.968750]  [G loss: 5.339694, acc: 0.109375]\n",
            "1401: [D loss: 0.038055, acc: 0.992188]  [G loss: 5.601991, acc: 0.062500]\n",
            "1402: [D loss: 0.030371, acc: 0.984375]  [G loss: 6.161342, acc: 0.062500]\n",
            "1403: [D loss: 0.021877, acc: 0.984375]  [G loss: 6.258119, acc: 0.046875]\n",
            "1404: [D loss: 0.041012, acc: 0.992188]  [G loss: 5.601416, acc: 0.078125]\n",
            "1405: [D loss: 0.096381, acc: 0.984375]  [G loss: 5.029733, acc: 0.156250]\n",
            "1406: [D loss: 0.051009, acc: 0.976562]  [G loss: 4.168570, acc: 0.125000]\n",
            "1407: [D loss: 0.038897, acc: 0.992188]  [G loss: 4.578448, acc: 0.093750]\n",
            "1408: [D loss: 0.031710, acc: 0.984375]  [G loss: 4.712346, acc: 0.093750]\n",
            "1409: [D loss: 0.039494, acc: 0.984375]  [G loss: 5.186481, acc: 0.062500]\n",
            "1410: [D loss: 0.009953, acc: 1.000000]  [G loss: 5.120422, acc: 0.125000]\n",
            "1411: [D loss: 0.012690, acc: 1.000000]  [G loss: 5.555513, acc: 0.031250]\n",
            "1412: [D loss: 0.034724, acc: 0.984375]  [G loss: 5.620394, acc: 0.046875]\n",
            "1413: [D loss: 0.060006, acc: 0.968750]  [G loss: 6.704284, acc: 0.031250]\n",
            "1414: [D loss: 0.005269, acc: 1.000000]  [G loss: 6.982041, acc: 0.000000]\n",
            "1415: [D loss: 0.021394, acc: 0.992188]  [G loss: 7.182559, acc: 0.000000]\n",
            "1416: [D loss: 0.046001, acc: 0.984375]  [G loss: 5.748285, acc: 0.015625]\n",
            "1417: [D loss: 0.086662, acc: 0.984375]  [G loss: 4.245375, acc: 0.062500]\n",
            "1418: [D loss: 0.080613, acc: 0.992188]  [G loss: 3.331195, acc: 0.171875]\n",
            "1419: [D loss: 0.044466, acc: 0.984375]  [G loss: 2.714255, acc: 0.296875]\n",
            "1420: [D loss: 0.170336, acc: 0.937500]  [G loss: 4.552411, acc: 0.093750]\n",
            "1421: [D loss: 0.014872, acc: 1.000000]  [G loss: 6.970565, acc: 0.015625]\n",
            "1422: [D loss: 0.084025, acc: 0.960938]  [G loss: 7.996951, acc: 0.046875]\n",
            "1423: [D loss: 0.029680, acc: 0.984375]  [G loss: 8.059622, acc: 0.031250]\n",
            "1424: [D loss: 0.158482, acc: 0.921875]  [G loss: 4.208169, acc: 0.234375]\n",
            "1425: [D loss: 0.155548, acc: 0.945312]  [G loss: 2.817329, acc: 0.406250]\n",
            "1426: [D loss: 0.054153, acc: 0.976562]  [G loss: 2.265130, acc: 0.484375]\n",
            "1427: [D loss: 0.091338, acc: 0.960938]  [G loss: 3.247737, acc: 0.218750]\n",
            "1428: [D loss: 0.026567, acc: 1.000000]  [G loss: 4.318482, acc: 0.093750]\n",
            "1429: [D loss: 0.042517, acc: 0.992188]  [G loss: 5.896346, acc: 0.000000]\n",
            "1430: [D loss: 0.114061, acc: 0.968750]  [G loss: 5.664871, acc: 0.015625]\n",
            "1431: [D loss: 0.038685, acc: 0.984375]  [G loss: 5.053837, acc: 0.046875]\n",
            "1432: [D loss: 0.053646, acc: 0.984375]  [G loss: 5.034101, acc: 0.031250]\n",
            "1433: [D loss: 0.054306, acc: 0.984375]  [G loss: 4.796029, acc: 0.000000]\n",
            "1434: [D loss: 0.036678, acc: 0.984375]  [G loss: 4.665970, acc: 0.046875]\n",
            "1435: [D loss: 0.065176, acc: 0.984375]  [G loss: 4.099524, acc: 0.046875]\n",
            "1436: [D loss: 0.037110, acc: 0.992188]  [G loss: 3.774045, acc: 0.140625]\n",
            "1437: [D loss: 0.073101, acc: 0.976562]  [G loss: 4.121573, acc: 0.109375]\n",
            "1438: [D loss: 0.045188, acc: 0.992188]  [G loss: 5.139342, acc: 0.000000]\n",
            "1439: [D loss: 0.144035, acc: 0.976562]  [G loss: 5.147288, acc: 0.015625]\n",
            "1440: [D loss: 0.072724, acc: 0.992188]  [G loss: 3.916080, acc: 0.109375]\n",
            "1441: [D loss: 0.085719, acc: 0.976562]  [G loss: 3.062751, acc: 0.125000]\n",
            "1442: [D loss: 0.039834, acc: 0.992188]  [G loss: 3.122852, acc: 0.187500]\n",
            "1443: [D loss: 0.111100, acc: 0.968750]  [G loss: 1.666655, acc: 0.437500]\n",
            "1444: [D loss: 0.121291, acc: 0.953125]  [G loss: 2.651136, acc: 0.234375]\n",
            "1445: [D loss: 0.043418, acc: 0.976562]  [G loss: 4.509716, acc: 0.093750]\n",
            "1446: [D loss: 0.029576, acc: 0.984375]  [G loss: 5.691569, acc: 0.031250]\n",
            "1447: [D loss: 0.085562, acc: 0.953125]  [G loss: 4.412564, acc: 0.046875]\n",
            "1448: [D loss: 0.191458, acc: 0.937500]  [G loss: 3.229503, acc: 0.187500]\n",
            "1449: [D loss: 0.108830, acc: 0.960938]  [G loss: 1.713053, acc: 0.390625]\n",
            "1450: [D loss: 0.089281, acc: 0.960938]  [G loss: 1.804659, acc: 0.296875]\n",
            "1451: [D loss: 0.036008, acc: 0.984375]  [G loss: 2.346530, acc: 0.296875]\n",
            "1452: [D loss: 0.057501, acc: 0.984375]  [G loss: 2.491232, acc: 0.281250]\n",
            "1453: [D loss: 0.025015, acc: 0.992188]  [G loss: 2.188350, acc: 0.312500]\n",
            "1454: [D loss: 0.049690, acc: 0.976562]  [G loss: 2.356384, acc: 0.296875]\n",
            "1455: [D loss: 0.064138, acc: 0.976562]  [G loss: 1.665321, acc: 0.390625]\n",
            "1456: [D loss: 0.041169, acc: 1.000000]  [G loss: 1.620561, acc: 0.421875]\n",
            "1457: [D loss: 0.061742, acc: 0.984375]  [G loss: 1.588397, acc: 0.468750]\n",
            "1458: [D loss: 0.070820, acc: 0.968750]  [G loss: 2.392132, acc: 0.265625]\n",
            "1459: [D loss: 0.079720, acc: 0.976562]  [G loss: 3.109430, acc: 0.140625]\n",
            "1460: [D loss: 0.125841, acc: 0.960938]  [G loss: 2.337994, acc: 0.187500]\n",
            "1461: [D loss: 0.056526, acc: 0.984375]  [G loss: 1.516348, acc: 0.421875]\n",
            "1462: [D loss: 0.060348, acc: 0.968750]  [G loss: 1.760767, acc: 0.375000]\n",
            "1463: [D loss: 0.073180, acc: 0.968750]  [G loss: 2.745854, acc: 0.250000]\n",
            "1464: [D loss: 0.083171, acc: 0.976562]  [G loss: 3.148588, acc: 0.187500]\n",
            "1465: [D loss: 0.075963, acc: 0.984375]  [G loss: 3.490277, acc: 0.125000]\n",
            "1466: [D loss: 0.119299, acc: 0.960938]  [G loss: 3.073337, acc: 0.125000]\n",
            "1467: [D loss: 0.053092, acc: 0.976562]  [G loss: 1.938495, acc: 0.328125]\n",
            "1468: [D loss: 0.079105, acc: 0.968750]  [G loss: 1.482719, acc: 0.421875]\n",
            "1469: [D loss: 0.089150, acc: 0.968750]  [G loss: 0.932825, acc: 0.625000]\n",
            "1470: [D loss: 0.193741, acc: 0.929688]  [G loss: 2.863280, acc: 0.187500]\n",
            "1471: [D loss: 0.078336, acc: 0.984375]  [G loss: 3.260267, acc: 0.156250]\n",
            "1472: [D loss: 0.140767, acc: 0.937500]  [G loss: 2.845783, acc: 0.140625]\n",
            "1473: [D loss: 0.099562, acc: 0.968750]  [G loss: 1.905170, acc: 0.437500]\n",
            "1474: [D loss: 0.075674, acc: 0.968750]  [G loss: 2.096819, acc: 0.390625]\n",
            "1475: [D loss: 0.105472, acc: 0.960938]  [G loss: 3.194412, acc: 0.171875]\n",
            "1476: [D loss: 0.040142, acc: 0.984375]  [G loss: 3.664943, acc: 0.093750]\n",
            "1477: [D loss: 0.049749, acc: 0.984375]  [G loss: 3.767754, acc: 0.093750]\n",
            "1478: [D loss: 0.099269, acc: 0.968750]  [G loss: 3.227447, acc: 0.109375]\n",
            "1479: [D loss: 0.082067, acc: 0.976562]  [G loss: 2.486677, acc: 0.250000]\n",
            "1480: [D loss: 0.069393, acc: 0.976562]  [G loss: 3.313397, acc: 0.093750]\n",
            "1481: [D loss: 0.049456, acc: 0.992188]  [G loss: 3.736783, acc: 0.078125]\n",
            "1482: [D loss: 0.037576, acc: 0.992188]  [G loss: 4.250178, acc: 0.046875]\n",
            "1483: [D loss: 0.042569, acc: 0.984375]  [G loss: 4.130919, acc: 0.062500]\n",
            "1484: [D loss: 0.034921, acc: 0.984375]  [G loss: 4.024829, acc: 0.031250]\n",
            "1485: [D loss: 0.026068, acc: 1.000000]  [G loss: 3.199757, acc: 0.171875]\n",
            "1486: [D loss: 0.112831, acc: 0.976562]  [G loss: 2.642419, acc: 0.265625]\n",
            "1487: [D loss: 0.078186, acc: 0.968750]  [G loss: 2.598579, acc: 0.218750]\n",
            "1488: [D loss: 0.055747, acc: 0.976562]  [G loss: 3.220590, acc: 0.140625]\n",
            "1489: [D loss: 0.106720, acc: 0.960938]  [G loss: 3.960140, acc: 0.093750]\n",
            "1490: [D loss: 0.026813, acc: 0.992188]  [G loss: 5.103614, acc: 0.046875]\n",
            "1491: [D loss: 0.042498, acc: 0.984375]  [G loss: 4.901395, acc: 0.015625]\n",
            "1492: [D loss: 0.047499, acc: 0.984375]  [G loss: 4.285128, acc: 0.093750]\n",
            "1493: [D loss: 0.019045, acc: 0.992188]  [G loss: 4.475073, acc: 0.140625]\n",
            "1494: [D loss: 0.036783, acc: 0.992188]  [G loss: 3.054046, acc: 0.265625]\n",
            "1495: [D loss: 0.103276, acc: 0.960938]  [G loss: 3.942545, acc: 0.218750]\n",
            "1496: [D loss: 0.055636, acc: 0.976562]  [G loss: 4.371325, acc: 0.171875]\n",
            "1497: [D loss: 0.028965, acc: 0.992188]  [G loss: 4.987357, acc: 0.078125]\n",
            "1498: [D loss: 0.022150, acc: 0.992188]  [G loss: 5.086193, acc: 0.046875]\n",
            "1499: [D loss: 0.031350, acc: 0.984375]  [G loss: 4.994064, acc: 0.078125]\n",
            "1500: [D loss: 0.144603, acc: 0.968750]  [G loss: 4.310779, acc: 0.125000]\n",
            "1501: [D loss: 0.070790, acc: 0.976562]  [G loss: 3.029345, acc: 0.218750]\n",
            "1502: [D loss: 0.063822, acc: 0.984375]  [G loss: 1.759332, acc: 0.328125]\n",
            "1503: [D loss: 0.114079, acc: 0.953125]  [G loss: 2.342610, acc: 0.296875]\n",
            "1504: [D loss: 0.028754, acc: 0.992188]  [G loss: 3.898736, acc: 0.109375]\n",
            "1505: [D loss: 0.036177, acc: 0.992188]  [G loss: 5.122160, acc: 0.078125]\n",
            "1506: [D loss: 0.047980, acc: 0.984375]  [G loss: 5.525827, acc: 0.031250]\n",
            "1507: [D loss: 0.014764, acc: 0.992188]  [G loss: 6.454147, acc: 0.031250]\n",
            "1508: [D loss: 0.020303, acc: 0.984375]  [G loss: 5.067274, acc: 0.062500]\n",
            "1509: [D loss: 0.087063, acc: 0.984375]  [G loss: 4.904311, acc: 0.046875]\n",
            "1510: [D loss: 0.015133, acc: 0.992188]  [G loss: 3.738633, acc: 0.187500]\n",
            "1511: [D loss: 0.058269, acc: 0.976562]  [G loss: 3.441781, acc: 0.140625]\n",
            "1512: [D loss: 0.021978, acc: 1.000000]  [G loss: 3.424201, acc: 0.234375]\n",
            "1513: [D loss: 0.083802, acc: 0.968750]  [G loss: 3.397149, acc: 0.218750]\n",
            "1514: [D loss: 0.014586, acc: 1.000000]  [G loss: 3.758326, acc: 0.171875]\n",
            "1515: [D loss: 0.083441, acc: 0.976562]  [G loss: 3.382006, acc: 0.250000]\n",
            "1516: [D loss: 0.009263, acc: 1.000000]  [G loss: 3.024571, acc: 0.296875]\n",
            "1517: [D loss: 0.034326, acc: 0.976562]  [G loss: 3.080974, acc: 0.312500]\n",
            "1518: [D loss: 0.099485, acc: 0.953125]  [G loss: 3.823210, acc: 0.375000]\n",
            "1519: [D loss: 0.069252, acc: 0.984375]  [G loss: 3.522410, acc: 0.296875]\n",
            "1520: [D loss: 0.021759, acc: 0.992188]  [G loss: 2.797450, acc: 0.328125]\n",
            "1521: [D loss: 0.048262, acc: 0.968750]  [G loss: 2.765769, acc: 0.390625]\n",
            "1522: [D loss: 0.050919, acc: 0.968750]  [G loss: 4.340867, acc: 0.140625]\n",
            "1523: [D loss: 0.048400, acc: 0.976562]  [G loss: 4.673354, acc: 0.156250]\n",
            "1524: [D loss: 0.065936, acc: 0.984375]  [G loss: 3.536663, acc: 0.234375]\n",
            "1525: [D loss: 0.078456, acc: 0.945312]  [G loss: 2.107920, acc: 0.500000]\n",
            "1526: [D loss: 0.129167, acc: 0.929688]  [G loss: 2.599312, acc: 0.328125]\n",
            "1527: [D loss: 0.107593, acc: 0.953125]  [G loss: 4.546193, acc: 0.125000]\n",
            "1528: [D loss: 0.280612, acc: 0.945312]  [G loss: 4.583350, acc: 0.125000]\n",
            "1529: [D loss: 0.160787, acc: 0.960938]  [G loss: 2.637439, acc: 0.328125]\n",
            "1530: [D loss: 0.153031, acc: 0.929688]  [G loss: 3.749251, acc: 0.156250]\n",
            "1531: [D loss: 0.165733, acc: 0.953125]  [G loss: 5.991879, acc: 0.031250]\n",
            "1532: [D loss: 0.218812, acc: 0.945312]  [G loss: 5.370911, acc: 0.015625]\n",
            "1533: [D loss: 0.204131, acc: 0.937500]  [G loss: 4.514619, acc: 0.046875]\n",
            "1534: [D loss: 0.189298, acc: 0.921875]  [G loss: 3.020158, acc: 0.140625]\n",
            "1535: [D loss: 0.342853, acc: 0.914062]  [G loss: 7.441226, acc: 0.000000]\n",
            "1536: [D loss: 0.350721, acc: 0.906250]  [G loss: 7.243950, acc: 0.000000]\n",
            "1537: [D loss: 0.428525, acc: 0.882812]  [G loss: 3.784973, acc: 0.031250]\n",
            "1538: [D loss: 0.127019, acc: 0.945312]  [G loss: 2.068325, acc: 0.265625]\n",
            "1539: [D loss: 0.106198, acc: 0.937500]  [G loss: 2.492669, acc: 0.203125]\n",
            "1540: [D loss: 0.163158, acc: 0.929688]  [G loss: 5.013702, acc: 0.000000]\n",
            "1541: [D loss: 0.006158, acc: 1.000000]  [G loss: 7.412505, acc: 0.000000]\n",
            "1542: [D loss: 0.085166, acc: 0.968750]  [G loss: 8.676795, acc: 0.000000]\n",
            "1543: [D loss: 0.065848, acc: 0.976562]  [G loss: 8.409957, acc: 0.000000]\n",
            "1544: [D loss: 0.135214, acc: 0.968750]  [G loss: 7.110548, acc: 0.000000]\n",
            "1545: [D loss: 0.021380, acc: 0.992188]  [G loss: 4.790778, acc: 0.046875]\n",
            "1546: [D loss: 0.016118, acc: 0.992188]  [G loss: 4.161669, acc: 0.062500]\n",
            "1547: [D loss: 0.014992, acc: 1.000000]  [G loss: 3.257508, acc: 0.125000]\n",
            "1548: [D loss: 0.052744, acc: 0.992188]  [G loss: 3.097074, acc: 0.156250]\n",
            "1549: [D loss: 0.023829, acc: 1.000000]  [G loss: 2.997390, acc: 0.250000]\n",
            "1550: [D loss: 0.028410, acc: 0.992188]  [G loss: 3.402228, acc: 0.140625]\n",
            "1551: [D loss: 0.063699, acc: 0.976562]  [G loss: 4.707095, acc: 0.015625]\n",
            "1552: [D loss: 0.009742, acc: 0.992188]  [G loss: 5.453362, acc: 0.000000]\n",
            "1553: [D loss: 0.003765, acc: 1.000000]  [G loss: 6.653913, acc: 0.000000]\n",
            "1554: [D loss: 0.001422, acc: 1.000000]  [G loss: 7.177249, acc: 0.000000]\n",
            "1555: [D loss: 0.147796, acc: 0.968750]  [G loss: 6.552694, acc: 0.000000]\n",
            "1556: [D loss: 0.002834, acc: 1.000000]  [G loss: 5.882670, acc: 0.000000]\n",
            "1557: [D loss: 0.045477, acc: 0.984375]  [G loss: 5.682981, acc: 0.000000]\n",
            "1558: [D loss: 0.011384, acc: 0.992188]  [G loss: 4.976811, acc: 0.015625]\n",
            "1559: [D loss: 0.043773, acc: 0.992188]  [G loss: 3.856968, acc: 0.093750]\n",
            "1560: [D loss: 0.059730, acc: 0.984375]  [G loss: 3.508369, acc: 0.156250]\n",
            "1561: [D loss: 0.022757, acc: 1.000000]  [G loss: 2.912579, acc: 0.250000]\n",
            "1562: [D loss: 0.032798, acc: 0.984375]  [G loss: 3.680431, acc: 0.234375]\n",
            "1563: [D loss: 0.030889, acc: 0.992188]  [G loss: 3.801731, acc: 0.125000]\n",
            "1564: [D loss: 0.012452, acc: 1.000000]  [G loss: 4.513135, acc: 0.093750]\n",
            "1565: [D loss: 0.023771, acc: 0.992188]  [G loss: 4.648674, acc: 0.078125]\n",
            "1566: [D loss: 0.013325, acc: 1.000000]  [G loss: 4.539242, acc: 0.062500]\n",
            "1567: [D loss: 0.013531, acc: 1.000000]  [G loss: 5.313122, acc: 0.093750]\n",
            "1568: [D loss: 0.093368, acc: 0.976562]  [G loss: 4.616255, acc: 0.062500]\n",
            "1569: [D loss: 0.087430, acc: 0.976562]  [G loss: 4.838270, acc: 0.078125]\n",
            "1570: [D loss: 0.069562, acc: 0.968750]  [G loss: 3.932613, acc: 0.187500]\n",
            "1571: [D loss: 0.032896, acc: 0.992188]  [G loss: 3.563322, acc: 0.187500]\n",
            "1572: [D loss: 0.084019, acc: 0.968750]  [G loss: 3.961445, acc: 0.171875]\n",
            "1573: [D loss: 0.054625, acc: 0.984375]  [G loss: 4.555820, acc: 0.093750]\n",
            "1574: [D loss: 0.038039, acc: 0.992188]  [G loss: 4.720585, acc: 0.078125]\n",
            "1575: [D loss: 0.014213, acc: 1.000000]  [G loss: 6.197018, acc: 0.000000]\n",
            "1576: [D loss: 0.049040, acc: 0.968750]  [G loss: 6.191966, acc: 0.015625]\n",
            "1577: [D loss: 0.082492, acc: 0.976562]  [G loss: 7.021456, acc: 0.015625]\n",
            "1578: [D loss: 0.126206, acc: 0.953125]  [G loss: 6.197733, acc: 0.015625]\n",
            "1579: [D loss: 0.155533, acc: 0.960938]  [G loss: 4.307740, acc: 0.093750]\n",
            "1580: [D loss: 0.058341, acc: 0.976562]  [G loss: 3.148664, acc: 0.156250]\n",
            "1581: [D loss: 0.129962, acc: 0.953125]  [G loss: 5.183526, acc: 0.031250]\n",
            "1582: [D loss: 0.065285, acc: 0.953125]  [G loss: 5.506063, acc: 0.031250]\n",
            "1583: [D loss: 0.160829, acc: 0.929688]  [G loss: 5.680631, acc: 0.015625]\n",
            "1584: [D loss: 0.152784, acc: 0.953125]  [G loss: 4.037612, acc: 0.109375]\n",
            "1585: [D loss: 0.098595, acc: 0.937500]  [G loss: 2.164815, acc: 0.359375]\n",
            "1586: [D loss: 0.137532, acc: 0.968750]  [G loss: 1.861071, acc: 0.453125]\n",
            "1587: [D loss: 0.127808, acc: 0.968750]  [G loss: 2.678745, acc: 0.218750]\n",
            "1588: [D loss: 0.089964, acc: 0.968750]  [G loss: 3.704035, acc: 0.156250]\n",
            "1589: [D loss: 0.024105, acc: 0.992188]  [G loss: 4.992290, acc: 0.062500]\n",
            "1590: [D loss: 0.137767, acc: 0.937500]  [G loss: 4.778234, acc: 0.109375]\n",
            "1591: [D loss: 0.111107, acc: 0.953125]  [G loss: 3.163937, acc: 0.250000]\n",
            "1592: [D loss: 0.066715, acc: 0.976562]  [G loss: 2.921415, acc: 0.234375]\n",
            "1593: [D loss: 0.042899, acc: 0.992188]  [G loss: 2.552925, acc: 0.265625]\n",
            "1594: [D loss: 0.070789, acc: 0.953125]  [G loss: 2.954064, acc: 0.250000]\n",
            "1595: [D loss: 0.045610, acc: 0.976562]  [G loss: 3.305738, acc: 0.203125]\n",
            "1596: [D loss: 0.015562, acc: 1.000000]  [G loss: 4.138672, acc: 0.109375]\n",
            "1597: [D loss: 0.033112, acc: 0.984375]  [G loss: 4.791407, acc: 0.078125]\n",
            "1598: [D loss: 0.030467, acc: 0.992188]  [G loss: 3.903683, acc: 0.093750]\n",
            "1599: [D loss: 0.051240, acc: 0.984375]  [G loss: 4.480805, acc: 0.093750]\n",
            "1600: [D loss: 0.016353, acc: 0.992188]  [G loss: 4.426970, acc: 0.140625]\n",
            "1601: [D loss: 0.062214, acc: 0.976562]  [G loss: 2.880674, acc: 0.203125]\n",
            "1602: [D loss: 0.151716, acc: 0.968750]  [G loss: 4.282874, acc: 0.031250]\n",
            "1603: [D loss: 0.028863, acc: 0.992188]  [G loss: 5.212981, acc: 0.031250]\n",
            "1604: [D loss: 0.072543, acc: 0.968750]  [G loss: 5.070130, acc: 0.046875]\n",
            "1605: [D loss: 0.131221, acc: 0.968750]  [G loss: 4.536074, acc: 0.109375]\n",
            "1606: [D loss: 0.051063, acc: 0.976562]  [G loss: 3.647353, acc: 0.156250]\n",
            "1607: [D loss: 0.045102, acc: 0.976562]  [G loss: 3.396844, acc: 0.109375]\n",
            "1608: [D loss: 0.108514, acc: 0.953125]  [G loss: 3.256908, acc: 0.171875]\n",
            "1609: [D loss: 0.030754, acc: 0.992188]  [G loss: 3.834862, acc: 0.125000]\n",
            "1610: [D loss: 0.020719, acc: 0.992188]  [G loss: 3.610209, acc: 0.156250]\n",
            "1611: [D loss: 0.091071, acc: 0.976562]  [G loss: 2.622738, acc: 0.359375]\n",
            "1612: [D loss: 0.028104, acc: 0.992188]  [G loss: 2.417752, acc: 0.406250]\n",
            "1613: [D loss: 0.026820, acc: 0.984375]  [G loss: 2.350981, acc: 0.453125]\n",
            "1614: [D loss: 0.036023, acc: 0.984375]  [G loss: 2.059163, acc: 0.578125]\n",
            "1615: [D loss: 0.034831, acc: 0.992188]  [G loss: 1.113858, acc: 0.687500]\n",
            "1616: [D loss: 0.034547, acc: 0.984375]  [G loss: 1.433511, acc: 0.656250]\n",
            "1617: [D loss: 0.019109, acc: 1.000000]  [G loss: 1.294074, acc: 0.671875]\n",
            "1618: [D loss: 0.025373, acc: 0.992188]  [G loss: 1.703833, acc: 0.593750]\n",
            "1619: [D loss: 0.002976, acc: 1.000000]  [G loss: 1.379252, acc: 0.578125]\n",
            "1620: [D loss: 0.007591, acc: 1.000000]  [G loss: 1.673039, acc: 0.500000]\n",
            "1621: [D loss: 0.011314, acc: 1.000000]  [G loss: 2.183229, acc: 0.437500]\n",
            "1622: [D loss: 0.060397, acc: 0.976562]  [G loss: 3.172222, acc: 0.328125]\n",
            "1623: [D loss: 0.126073, acc: 0.960938]  [G loss: 3.663178, acc: 0.250000]\n",
            "1624: [D loss: 0.111895, acc: 0.976562]  [G loss: 3.963820, acc: 0.234375]\n",
            "1625: [D loss: 0.060555, acc: 0.984375]  [G loss: 3.557546, acc: 0.265625]\n",
            "1626: [D loss: 0.110370, acc: 0.976562]  [G loss: 2.246562, acc: 0.281250]\n",
            "1627: [D loss: 0.032848, acc: 0.984375]  [G loss: 1.192876, acc: 0.562500]\n",
            "1628: [D loss: 0.097290, acc: 0.976562]  [G loss: 1.419664, acc: 0.562500]\n",
            "1629: [D loss: 0.031479, acc: 0.976562]  [G loss: 1.126001, acc: 0.546875]\n",
            "1630: [D loss: 0.010886, acc: 1.000000]  [G loss: 1.945219, acc: 0.453125]\n",
            "1631: [D loss: 0.003038, acc: 1.000000]  [G loss: 2.694130, acc: 0.250000]\n",
            "1632: [D loss: 0.055828, acc: 0.984375]  [G loss: 2.971561, acc: 0.187500]\n",
            "1633: [D loss: 0.121639, acc: 0.968750]  [G loss: 2.278572, acc: 0.296875]\n",
            "1634: [D loss: 0.028501, acc: 0.992188]  [G loss: 1.524312, acc: 0.406250]\n",
            "1635: [D loss: 0.033511, acc: 0.976562]  [G loss: 1.116722, acc: 0.515625]\n",
            "1636: [D loss: 0.040502, acc: 0.984375]  [G loss: 1.427276, acc: 0.515625]\n",
            "1637: [D loss: 0.081555, acc: 0.992188]  [G loss: 1.611548, acc: 0.515625]\n",
            "1638: [D loss: 0.012251, acc: 1.000000]  [G loss: 2.110227, acc: 0.359375]\n",
            "1639: [D loss: 0.017582, acc: 0.992188]  [G loss: 2.122344, acc: 0.421875]\n",
            "1640: [D loss: 0.007636, acc: 1.000000]  [G loss: 2.132421, acc: 0.359375]\n",
            "1641: [D loss: 0.038743, acc: 0.984375]  [G loss: 2.642209, acc: 0.250000]\n",
            "1642: [D loss: 0.019626, acc: 0.992188]  [G loss: 2.774494, acc: 0.203125]\n",
            "1643: [D loss: 0.043945, acc: 0.984375]  [G loss: 2.607121, acc: 0.281250]\n",
            "1644: [D loss: 0.015547, acc: 0.992188]  [G loss: 2.066879, acc: 0.328125]\n",
            "1645: [D loss: 0.009448, acc: 1.000000]  [G loss: 1.325714, acc: 0.593750]\n",
            "1646: [D loss: 0.064476, acc: 0.976562]  [G loss: 1.257114, acc: 0.546875]\n",
            "1647: [D loss: 0.006788, acc: 1.000000]  [G loss: 0.700505, acc: 0.671875]\n",
            "1648: [D loss: 0.027557, acc: 0.984375]  [G loss: 0.718326, acc: 0.687500]\n",
            "1649: [D loss: 0.020922, acc: 0.992188]  [G loss: 0.857078, acc: 0.609375]\n",
            "1650: [D loss: 0.013378, acc: 1.000000]  [G loss: 1.192926, acc: 0.468750]\n",
            "1651: [D loss: 0.005350, acc: 1.000000]  [G loss: 1.740426, acc: 0.328125]\n",
            "1652: [D loss: 0.018065, acc: 0.992188]  [G loss: 1.984514, acc: 0.390625]\n",
            "1653: [D loss: 0.035159, acc: 0.976562]  [G loss: 2.082984, acc: 0.359375]\n",
            "1654: [D loss: 0.055774, acc: 0.992188]  [G loss: 1.224070, acc: 0.562500]\n",
            "1655: [D loss: 0.036314, acc: 0.992188]  [G loss: 1.143713, acc: 0.484375]\n",
            "1656: [D loss: 0.013827, acc: 1.000000]  [G loss: 0.781803, acc: 0.703125]\n",
            "1657: [D loss: 0.023020, acc: 0.992188]  [G loss: 0.766471, acc: 0.703125]\n",
            "1658: [D loss: 0.049916, acc: 0.992188]  [G loss: 0.652496, acc: 0.703125]\n",
            "1659: [D loss: 0.006436, acc: 1.000000]  [G loss: 1.308335, acc: 0.562500]\n",
            "1660: [D loss: 0.041777, acc: 0.976562]  [G loss: 1.054304, acc: 0.609375]\n",
            "1661: [D loss: 0.017384, acc: 0.992188]  [G loss: 0.844745, acc: 0.718750]\n",
            "1662: [D loss: 0.050589, acc: 0.984375]  [G loss: 0.796275, acc: 0.734375]\n",
            "1663: [D loss: 0.095051, acc: 0.953125]  [G loss: 0.628706, acc: 0.734375]\n",
            "1664: [D loss: 0.044145, acc: 0.976562]  [G loss: 0.993646, acc: 0.609375]\n",
            "1665: [D loss: 0.075650, acc: 0.984375]  [G loss: 1.119147, acc: 0.578125]\n",
            "1666: [D loss: 0.030407, acc: 0.992188]  [G loss: 1.144323, acc: 0.656250]\n",
            "1667: [D loss: 0.082116, acc: 0.984375]  [G loss: 0.793583, acc: 0.734375]\n",
            "1668: [D loss: 0.072493, acc: 0.976562]  [G loss: 0.775183, acc: 0.671875]\n",
            "1669: [D loss: 0.075121, acc: 0.976562]  [G loss: 1.471831, acc: 0.515625]\n",
            "1670: [D loss: 0.025285, acc: 0.992188]  [G loss: 2.189975, acc: 0.390625]\n",
            "1671: [D loss: 0.010010, acc: 1.000000]  [G loss: 3.079833, acc: 0.250000]\n",
            "1672: [D loss: 0.057326, acc: 0.984375]  [G loss: 2.677588, acc: 0.328125]\n",
            "1673: [D loss: 0.012569, acc: 1.000000]  [G loss: 2.535167, acc: 0.296875]\n",
            "1674: [D loss: 0.093919, acc: 0.960938]  [G loss: 1.254871, acc: 0.546875]\n",
            "1675: [D loss: 0.027409, acc: 1.000000]  [G loss: 0.532161, acc: 0.843750]\n",
            "1676: [D loss: 0.084274, acc: 0.960938]  [G loss: 1.057755, acc: 0.593750]\n",
            "1677: [D loss: 0.074844, acc: 0.968750]  [G loss: 2.921979, acc: 0.234375]\n",
            "1678: [D loss: 0.010502, acc: 1.000000]  [G loss: 5.732237, acc: 0.000000]\n",
            "1679: [D loss: 0.067400, acc: 0.960938]  [G loss: 5.661863, acc: 0.031250]\n",
            "1680: [D loss: 0.042748, acc: 0.992188]  [G loss: 5.587981, acc: 0.031250]\n",
            "1681: [D loss: 0.084980, acc: 0.968750]  [G loss: 4.023586, acc: 0.140625]\n",
            "1682: [D loss: 0.017229, acc: 0.992188]  [G loss: 2.630458, acc: 0.218750]\n",
            "1683: [D loss: 0.042584, acc: 0.976562]  [G loss: 3.127833, acc: 0.296875]\n",
            "1684: [D loss: 0.057172, acc: 0.976562]  [G loss: 3.224925, acc: 0.171875]\n",
            "1685: [D loss: 0.030166, acc: 0.992188]  [G loss: 4.986330, acc: 0.015625]\n",
            "1686: [D loss: 0.011355, acc: 1.000000]  [G loss: 6.445992, acc: 0.000000]\n",
            "1687: [D loss: 0.071436, acc: 0.984375]  [G loss: 6.275278, acc: 0.015625]\n",
            "1688: [D loss: 0.106254, acc: 0.960938]  [G loss: 5.661855, acc: 0.000000]\n",
            "1689: [D loss: 0.083476, acc: 0.976562]  [G loss: 3.381758, acc: 0.156250]\n",
            "1690: [D loss: 0.066218, acc: 0.960938]  [G loss: 2.770012, acc: 0.218750]\n",
            "1691: [D loss: 0.111637, acc: 0.937500]  [G loss: 3.754639, acc: 0.109375]\n",
            "1692: [D loss: 0.033970, acc: 0.992188]  [G loss: 5.488499, acc: 0.046875]\n",
            "1693: [D loss: 0.053390, acc: 0.968750]  [G loss: 5.015606, acc: 0.109375]\n",
            "1694: [D loss: 0.037340, acc: 0.984375]  [G loss: 4.654483, acc: 0.062500]\n",
            "1695: [D loss: 0.038277, acc: 0.992188]  [G loss: 3.448008, acc: 0.187500]\n",
            "1696: [D loss: 0.082321, acc: 0.976562]  [G loss: 3.139076, acc: 0.187500]\n",
            "1697: [D loss: 0.042521, acc: 0.984375]  [G loss: 2.941210, acc: 0.234375]\n",
            "1698: [D loss: 0.047221, acc: 0.992188]  [G loss: 2.759046, acc: 0.140625]\n",
            "1699: [D loss: 0.040194, acc: 0.992188]  [G loss: 3.560246, acc: 0.187500]\n",
            "1700: [D loss: 0.052791, acc: 0.968750]  [G loss: 3.848830, acc: 0.156250]\n",
            "1701: [D loss: 0.081094, acc: 0.968750]  [G loss: 3.897498, acc: 0.171875]\n",
            "1702: [D loss: 0.099735, acc: 0.960938]  [G loss: 2.843090, acc: 0.234375]\n",
            "1703: [D loss: 0.171204, acc: 0.921875]  [G loss: 3.312841, acc: 0.234375]\n",
            "1704: [D loss: 0.110601, acc: 0.953125]  [G loss: 4.650290, acc: 0.093750]\n",
            "1705: [D loss: 0.025794, acc: 0.992188]  [G loss: 4.719471, acc: 0.218750]\n",
            "1706: [D loss: 0.133333, acc: 0.968750]  [G loss: 3.915555, acc: 0.187500]\n",
            "1707: [D loss: 0.049730, acc: 0.992188]  [G loss: 3.909623, acc: 0.187500]\n",
            "1708: [D loss: 0.046798, acc: 0.992188]  [G loss: 2.403016, acc: 0.265625]\n",
            "1709: [D loss: 0.056189, acc: 0.984375]  [G loss: 2.999756, acc: 0.250000]\n",
            "1710: [D loss: 0.058987, acc: 0.984375]  [G loss: 2.867323, acc: 0.312500]\n",
            "1711: [D loss: 0.109289, acc: 0.960938]  [G loss: 3.337183, acc: 0.312500]\n",
            "1712: [D loss: 0.033753, acc: 0.992188]  [G loss: 4.310593, acc: 0.156250]\n",
            "1713: [D loss: 0.036463, acc: 0.976562]  [G loss: 5.134877, acc: 0.203125]\n",
            "1714: [D loss: 0.046092, acc: 0.976562]  [G loss: 4.989032, acc: 0.187500]\n",
            "1715: [D loss: 0.052051, acc: 0.984375]  [G loss: 3.783589, acc: 0.234375]\n",
            "1716: [D loss: 0.073641, acc: 0.976562]  [G loss: 3.738013, acc: 0.281250]\n",
            "1717: [D loss: 0.048335, acc: 0.976562]  [G loss: 3.315839, acc: 0.359375]\n",
            "1718: [D loss: 0.076473, acc: 0.976562]  [G loss: 3.560203, acc: 0.312500]\n",
            "1719: [D loss: 0.058421, acc: 0.968750]  [G loss: 2.515412, acc: 0.375000]\n",
            "1720: [D loss: 0.035477, acc: 0.984375]  [G loss: 3.489812, acc: 0.234375]\n",
            "1721: [D loss: 0.032487, acc: 0.992188]  [G loss: 4.473485, acc: 0.187500]\n",
            "1722: [D loss: 0.076694, acc: 0.984375]  [G loss: 4.151417, acc: 0.171875]\n",
            "1723: [D loss: 0.047736, acc: 0.992188]  [G loss: 3.030273, acc: 0.328125]\n",
            "1724: [D loss: 0.094943, acc: 0.968750]  [G loss: 2.869099, acc: 0.250000]\n",
            "1725: [D loss: 0.045091, acc: 0.976562]  [G loss: 3.719174, acc: 0.203125]\n",
            "1726: [D loss: 0.114045, acc: 0.976562]  [G loss: 4.407904, acc: 0.140625]\n",
            "1727: [D loss: 0.179145, acc: 0.953125]  [G loss: 4.521712, acc: 0.140625]\n",
            "1728: [D loss: 0.036978, acc: 0.976562]  [G loss: 4.714540, acc: 0.078125]\n",
            "1729: [D loss: 0.098788, acc: 0.960938]  [G loss: 4.152493, acc: 0.203125]\n",
            "1730: [D loss: 0.031702, acc: 0.992188]  [G loss: 3.067824, acc: 0.390625]\n",
            "1731: [D loss: 0.049873, acc: 0.984375]  [G loss: 2.492073, acc: 0.406250]\n",
            "1732: [D loss: 0.090465, acc: 0.976562]  [G loss: 1.803789, acc: 0.578125]\n",
            "1733: [D loss: 0.057670, acc: 0.976562]  [G loss: 2.447956, acc: 0.484375]\n",
            "1734: [D loss: 0.091562, acc: 0.968750]  [G loss: 3.525117, acc: 0.375000]\n",
            "1735: [D loss: 0.096858, acc: 0.968750]  [G loss: 3.611577, acc: 0.296875]\n",
            "1736: [D loss: 0.054235, acc: 0.968750]  [G loss: 2.654730, acc: 0.500000]\n",
            "1737: [D loss: 0.038024, acc: 0.984375]  [G loss: 2.151148, acc: 0.484375]\n",
            "1738: [D loss: 0.124675, acc: 0.960938]  [G loss: 1.989585, acc: 0.531250]\n",
            "1739: [D loss: 0.128675, acc: 0.976562]  [G loss: 3.043604, acc: 0.328125]\n",
            "1740: [D loss: 0.019093, acc: 1.000000]  [G loss: 3.892292, acc: 0.265625]\n",
            "1741: [D loss: 0.062397, acc: 0.976562]  [G loss: 4.356437, acc: 0.187500]\n",
            "1742: [D loss: 0.187417, acc: 0.929688]  [G loss: 2.897851, acc: 0.343750]\n",
            "1743: [D loss: 0.039608, acc: 0.984375]  [G loss: 1.249191, acc: 0.656250]\n",
            "1744: [D loss: 0.164058, acc: 0.921875]  [G loss: 2.737584, acc: 0.328125]\n",
            "1745: [D loss: 0.106827, acc: 0.953125]  [G loss: 4.470769, acc: 0.218750]\n",
            "1746: [D loss: 0.060789, acc: 0.984375]  [G loss: 5.334503, acc: 0.218750]\n",
            "1747: [D loss: 0.059799, acc: 0.968750]  [G loss: 6.464400, acc: 0.156250]\n",
            "1748: [D loss: 0.048418, acc: 0.976562]  [G loss: 6.112199, acc: 0.203125]\n",
            "1749: [D loss: 0.155130, acc: 0.929688]  [G loss: 5.404963, acc: 0.203125]\n",
            "1750: [D loss: 0.236713, acc: 0.945312]  [G loss: 4.123909, acc: 0.296875]\n",
            "1751: [D loss: 0.456818, acc: 0.890625]  [G loss: 6.738113, acc: 0.140625]\n",
            "1752: [D loss: 0.086524, acc: 0.968750]  [G loss: 7.679786, acc: 0.078125]\n",
            "1753: [D loss: 0.178011, acc: 0.921875]  [G loss: 6.469214, acc: 0.187500]\n",
            "1754: [D loss: 0.236591, acc: 0.929688]  [G loss: 3.365119, acc: 0.421875]\n",
            "1755: [D loss: 0.286571, acc: 0.929688]  [G loss: 4.108090, acc: 0.375000]\n",
            "1756: [D loss: 0.236956, acc: 0.921875]  [G loss: 5.658862, acc: 0.218750]\n",
            "1757: [D loss: 0.124753, acc: 0.968750]  [G loss: 6.174665, acc: 0.218750]\n",
            "1758: [D loss: 0.291061, acc: 0.914062]  [G loss: 3.640620, acc: 0.453125]\n",
            "1759: [D loss: 0.119695, acc: 0.945312]  [G loss: 3.478010, acc: 0.468750]\n",
            "1760: [D loss: 0.130820, acc: 0.945312]  [G loss: 3.328076, acc: 0.437500]\n",
            "1761: [D loss: 0.202813, acc: 0.945312]  [G loss: 4.316658, acc: 0.218750]\n",
            "1762: [D loss: 0.142985, acc: 0.945312]  [G loss: 5.819356, acc: 0.125000]\n",
            "1763: [D loss: 0.088466, acc: 0.953125]  [G loss: 7.298156, acc: 0.062500]\n",
            "1764: [D loss: 0.109294, acc: 0.976562]  [G loss: 6.030982, acc: 0.140625]\n",
            "1765: [D loss: 0.215232, acc: 0.929688]  [G loss: 6.106671, acc: 0.187500]\n",
            "1766: [D loss: 0.133359, acc: 0.960938]  [G loss: 5.897217, acc: 0.140625]\n",
            "1767: [D loss: 0.085373, acc: 0.968750]  [G loss: 5.471447, acc: 0.156250]\n",
            "1768: [D loss: 0.059718, acc: 0.984375]  [G loss: 6.199939, acc: 0.109375]\n",
            "1769: [D loss: 0.082971, acc: 0.976562]  [G loss: 7.466582, acc: 0.062500]\n",
            "1770: [D loss: 0.126631, acc: 0.960938]  [G loss: 7.752353, acc: 0.078125]\n",
            "1771: [D loss: 0.171619, acc: 0.960938]  [G loss: 7.241943, acc: 0.093750]\n",
            "1772: [D loss: 0.141174, acc: 0.976562]  [G loss: 5.072500, acc: 0.156250]\n",
            "1773: [D loss: 0.115062, acc: 0.960938]  [G loss: 3.329058, acc: 0.296875]\n",
            "1774: [D loss: 0.245491, acc: 0.929688]  [G loss: 4.030825, acc: 0.375000]\n",
            "1775: [D loss: 0.092783, acc: 0.953125]  [G loss: 4.497443, acc: 0.265625]\n",
            "1776: [D loss: 0.052595, acc: 0.984375]  [G loss: 4.874561, acc: 0.312500]\n",
            "1777: [D loss: 0.028050, acc: 0.984375]  [G loss: 3.764534, acc: 0.312500]\n",
            "1778: [D loss: 0.135319, acc: 0.953125]  [G loss: 3.030730, acc: 0.343750]\n",
            "1779: [D loss: 0.079695, acc: 0.968750]  [G loss: 1.733531, acc: 0.500000]\n",
            "1780: [D loss: 0.246890, acc: 0.937500]  [G loss: 1.707861, acc: 0.562500]\n",
            "1781: [D loss: 0.085823, acc: 0.968750]  [G loss: 1.885934, acc: 0.468750]\n",
            "1782: [D loss: 0.087365, acc: 0.976562]  [G loss: 1.925146, acc: 0.546875]\n",
            "1783: [D loss: 0.093554, acc: 0.968750]  [G loss: 0.904224, acc: 0.671875]\n",
            "1784: [D loss: 0.064670, acc: 0.976562]  [G loss: 0.750152, acc: 0.687500]\n",
            "1785: [D loss: 0.084646, acc: 0.960938]  [G loss: 0.656567, acc: 0.781250]\n",
            "1786: [D loss: 0.061187, acc: 0.984375]  [G loss: 0.970670, acc: 0.656250]\n",
            "1787: [D loss: 0.088661, acc: 0.968750]  [G loss: 1.164185, acc: 0.578125]\n",
            "1788: [D loss: 0.116254, acc: 0.945312]  [G loss: 0.824433, acc: 0.671875]\n",
            "1789: [D loss: 0.115999, acc: 0.976562]  [G loss: 0.658511, acc: 0.796875]\n",
            "1790: [D loss: 0.059781, acc: 0.976562]  [G loss: 0.359420, acc: 0.875000]\n",
            "1791: [D loss: 0.078705, acc: 0.976562]  [G loss: 0.972499, acc: 0.703125]\n",
            "1792: [D loss: 0.085289, acc: 0.968750]  [G loss: 1.423436, acc: 0.593750]\n",
            "1793: [D loss: 0.095575, acc: 0.968750]  [G loss: 1.665746, acc: 0.500000]\n",
            "1794: [D loss: 0.026645, acc: 1.000000]  [G loss: 1.866763, acc: 0.500000]\n",
            "1795: [D loss: 0.138012, acc: 0.953125]  [G loss: 1.023171, acc: 0.640625]\n",
            "1796: [D loss: 0.132271, acc: 0.984375]  [G loss: 0.970216, acc: 0.718750]\n",
            "1797: [D loss: 0.092487, acc: 0.968750]  [G loss: 0.741953, acc: 0.781250]\n",
            "1798: [D loss: 0.053105, acc: 0.976562]  [G loss: 1.372597, acc: 0.593750]\n",
            "1799: [D loss: 0.058943, acc: 0.992188]  [G loss: 1.648695, acc: 0.484375]\n",
            "1800: [D loss: 0.072258, acc: 0.960938]  [G loss: 2.141070, acc: 0.437500]\n",
            "1801: [D loss: 0.013437, acc: 1.000000]  [G loss: 2.576862, acc: 0.265625]\n",
            "1802: [D loss: 0.033767, acc: 0.984375]  [G loss: 3.005394, acc: 0.218750]\n",
            "1803: [D loss: 0.056812, acc: 0.968750]  [G loss: 2.925767, acc: 0.250000]\n",
            "1804: [D loss: 0.109020, acc: 0.960938]  [G loss: 2.764380, acc: 0.203125]\n",
            "1805: [D loss: 0.127429, acc: 0.968750]  [G loss: 2.751246, acc: 0.203125]\n",
            "1806: [D loss: 0.127601, acc: 0.960938]  [G loss: 2.066844, acc: 0.421875]\n",
            "1807: [D loss: 0.095784, acc: 0.976562]  [G loss: 2.236821, acc: 0.406250]\n",
            "1808: [D loss: 0.079231, acc: 0.960938]  [G loss: 3.454102, acc: 0.203125]\n",
            "1809: [D loss: 0.038728, acc: 0.984375]  [G loss: 4.074476, acc: 0.125000]\n",
            "1810: [D loss: 0.043068, acc: 0.976562]  [G loss: 4.758663, acc: 0.078125]\n",
            "1811: [D loss: 0.065232, acc: 0.984375]  [G loss: 4.896069, acc: 0.109375]\n",
            "1812: [D loss: 0.153173, acc: 0.953125]  [G loss: 4.498030, acc: 0.093750]\n",
            "1813: [D loss: 0.103749, acc: 0.945312]  [G loss: 3.898022, acc: 0.250000]\n",
            "1814: [D loss: 0.110368, acc: 0.976562]  [G loss: 3.234129, acc: 0.390625]\n",
            "1815: [D loss: 0.177419, acc: 0.921875]  [G loss: 5.535143, acc: 0.093750]\n",
            "1816: [D loss: 0.084507, acc: 0.976562]  [G loss: 7.216125, acc: 0.000000]\n",
            "1817: [D loss: 0.202501, acc: 0.945312]  [G loss: 8.133661, acc: 0.015625]\n",
            "1818: [D loss: 0.187674, acc: 0.929688]  [G loss: 7.524874, acc: 0.000000]\n",
            "1819: [D loss: 0.087065, acc: 0.968750]  [G loss: 6.009899, acc: 0.125000]\n",
            "1820: [D loss: 0.084785, acc: 0.976562]  [G loss: 5.784157, acc: 0.203125]\n",
            "1821: [D loss: 0.205014, acc: 0.921875]  [G loss: 6.434188, acc: 0.125000]\n",
            "1822: [D loss: 0.057877, acc: 0.984375]  [G loss: 7.440338, acc: 0.093750]\n",
            "1823: [D loss: 0.047608, acc: 0.992188]  [G loss: 8.186383, acc: 0.093750]\n",
            "1824: [D loss: 0.030963, acc: 0.992188]  [G loss: 6.856781, acc: 0.140625]\n",
            "1825: [D loss: 0.017163, acc: 1.000000]  [G loss: 7.010506, acc: 0.171875]\n",
            "1826: [D loss: 0.007722, acc: 1.000000]  [G loss: 7.872326, acc: 0.156250]\n",
            "1827: [D loss: 0.020746, acc: 0.992188]  [G loss: 8.020127, acc: 0.218750]\n",
            "1828: [D loss: 0.000717, acc: 1.000000]  [G loss: 8.221668, acc: 0.250000]\n",
            "1829: [D loss: 0.007444, acc: 1.000000]  [G loss: 8.249285, acc: 0.234375]\n",
            "1830: [D loss: 0.005524, acc: 1.000000]  [G loss: 8.154607, acc: 0.218750]\n",
            "1831: [D loss: 0.010845, acc: 0.992188]  [G loss: 7.894016, acc: 0.234375]\n",
            "1832: [D loss: 0.024046, acc: 0.992188]  [G loss: 7.746201, acc: 0.265625]\n",
            "1833: [D loss: 0.027431, acc: 0.984375]  [G loss: 7.794111, acc: 0.281250]\n",
            "1834: [D loss: 0.001461, acc: 1.000000]  [G loss: 6.216740, acc: 0.390625]\n",
            "1835: [D loss: 0.001397, acc: 1.000000]  [G loss: 6.275914, acc: 0.375000]\n",
            "1836: [D loss: 0.028678, acc: 0.992188]  [G loss: 5.197210, acc: 0.437500]\n",
            "1837: [D loss: 0.017773, acc: 0.992188]  [G loss: 5.678846, acc: 0.359375]\n",
            "1838: [D loss: 0.033394, acc: 0.992188]  [G loss: 7.182929, acc: 0.343750]\n",
            "1839: [D loss: 0.012231, acc: 0.992188]  [G loss: 7.011590, acc: 0.328125]\n",
            "1840: [D loss: 0.031108, acc: 0.992188]  [G loss: 6.723977, acc: 0.359375]\n",
            "1841: [D loss: 0.038793, acc: 0.992188]  [G loss: 5.996611, acc: 0.406250]\n",
            "1842: [D loss: 0.011425, acc: 0.992188]  [G loss: 5.452776, acc: 0.375000]\n",
            "1843: [D loss: 0.001007, acc: 1.000000]  [G loss: 5.307573, acc: 0.484375]\n",
            "1844: [D loss: 0.006681, acc: 1.000000]  [G loss: 4.325119, acc: 0.484375]\n",
            "1845: [D loss: 0.008333, acc: 1.000000]  [G loss: 3.764173, acc: 0.515625]\n",
            "1846: [D loss: 0.147056, acc: 0.937500]  [G loss: 4.511839, acc: 0.500000]\n",
            "1847: [D loss: 0.106231, acc: 0.984375]  [G loss: 5.087088, acc: 0.421875]\n",
            "1848: [D loss: 0.227218, acc: 0.968750]  [G loss: 5.677389, acc: 0.375000]\n",
            "1849: [D loss: 0.146890, acc: 0.960938]  [G loss: 4.455478, acc: 0.406250]\n",
            "1850: [D loss: 0.177216, acc: 0.960938]  [G loss: 4.044050, acc: 0.437500]\n",
            "1851: [D loss: 0.199291, acc: 0.945312]  [G loss: 3.743635, acc: 0.359375]\n",
            "1852: [D loss: 0.119559, acc: 0.960938]  [G loss: 6.258937, acc: 0.140625]\n",
            "1853: [D loss: 0.158609, acc: 0.960938]  [G loss: 5.606701, acc: 0.109375]\n",
            "1854: [D loss: 0.276738, acc: 0.937500]  [G loss: 4.066148, acc: 0.234375]\n",
            "1855: [D loss: 0.114138, acc: 0.960938]  [G loss: 1.752263, acc: 0.484375]\n",
            "1856: [D loss: 0.258333, acc: 0.906250]  [G loss: 5.348330, acc: 0.109375]\n",
            "1857: [D loss: 0.083635, acc: 0.968750]  [G loss: 7.133280, acc: 0.000000]\n",
            "1858: [D loss: 0.097411, acc: 0.960938]  [G loss: 6.645641, acc: 0.015625]\n",
            "1859: [D loss: 0.120392, acc: 0.960938]  [G loss: 6.201729, acc: 0.000000]\n",
            "1860: [D loss: 0.098191, acc: 0.976562]  [G loss: 3.646550, acc: 0.156250]\n",
            "1861: [D loss: 0.085770, acc: 0.960938]  [G loss: 3.350007, acc: 0.218750]\n",
            "1862: [D loss: 0.208100, acc: 0.929688]  [G loss: 5.121951, acc: 0.062500]\n",
            "1863: [D loss: 0.043738, acc: 0.984375]  [G loss: 6.723846, acc: 0.031250]\n",
            "1864: [D loss: 0.131816, acc: 0.937500]  [G loss: 6.713813, acc: 0.000000]\n",
            "1865: [D loss: 0.139717, acc: 0.945312]  [G loss: 5.153375, acc: 0.078125]\n",
            "1866: [D loss: 0.154685, acc: 0.953125]  [G loss: 2.972781, acc: 0.328125]\n",
            "1867: [D loss: 0.061920, acc: 0.984375]  [G loss: 1.873961, acc: 0.468750]\n",
            "1868: [D loss: 0.126583, acc: 0.945312]  [G loss: 1.892506, acc: 0.406250]\n",
            "1869: [D loss: 0.054776, acc: 0.976562]  [G loss: 3.310357, acc: 0.234375]\n",
            "1870: [D loss: 0.039326, acc: 0.992188]  [G loss: 4.506670, acc: 0.093750]\n",
            "1871: [D loss: 0.045245, acc: 0.976562]  [G loss: 5.733306, acc: 0.015625]\n",
            "1872: [D loss: 0.067895, acc: 0.953125]  [G loss: 5.148456, acc: 0.078125]\n",
            "1873: [D loss: 0.091242, acc: 0.976562]  [G loss: 4.944655, acc: 0.046875]\n",
            "1874: [D loss: 0.015473, acc: 0.992188]  [G loss: 3.765310, acc: 0.187500]\n",
            "1875: [D loss: 0.027606, acc: 0.992188]  [G loss: 2.784739, acc: 0.234375]\n",
            "1876: [D loss: 0.017835, acc: 0.992188]  [G loss: 1.984096, acc: 0.406250]\n",
            "1877: [D loss: 0.048200, acc: 0.976562]  [G loss: 1.589665, acc: 0.328125]\n",
            "1878: [D loss: 0.039130, acc: 0.984375]  [G loss: 1.308846, acc: 0.593750]\n",
            "1879: [D loss: 0.030906, acc: 0.984375]  [G loss: 1.411065, acc: 0.484375]\n",
            "1880: [D loss: 0.014521, acc: 1.000000]  [G loss: 1.521495, acc: 0.500000]\n",
            "1881: [D loss: 0.008997, acc: 1.000000]  [G loss: 1.513838, acc: 0.343750]\n",
            "1882: [D loss: 0.007589, acc: 1.000000]  [G loss: 1.595857, acc: 0.515625]\n",
            "1883: [D loss: 0.017734, acc: 0.984375]  [G loss: 1.328681, acc: 0.421875]\n",
            "1884: [D loss: 0.067184, acc: 0.960938]  [G loss: 1.130474, acc: 0.546875]\n",
            "1885: [D loss: 0.007578, acc: 1.000000]  [G loss: 0.810756, acc: 0.562500]\n",
            "1886: [D loss: 0.011124, acc: 1.000000]  [G loss: 0.635412, acc: 0.687500]\n",
            "1887: [D loss: 0.027475, acc: 0.984375]  [G loss: 0.647216, acc: 0.750000]\n",
            "1888: [D loss: 0.014983, acc: 1.000000]  [G loss: 0.409699, acc: 0.765625]\n",
            "1889: [D loss: 0.011861, acc: 1.000000]  [G loss: 0.620905, acc: 0.750000]\n",
            "1890: [D loss: 0.041672, acc: 0.992188]  [G loss: 0.542952, acc: 0.734375]\n",
            "1891: [D loss: 0.047974, acc: 0.984375]  [G loss: 0.498112, acc: 0.765625]\n",
            "1892: [D loss: 0.031485, acc: 0.984375]  [G loss: 0.409220, acc: 0.812500]\n",
            "1893: [D loss: 0.023856, acc: 0.992188]  [G loss: 0.376787, acc: 0.796875]\n",
            "1894: [D loss: 0.025771, acc: 1.000000]  [G loss: 0.282395, acc: 0.828125]\n",
            "1895: [D loss: 0.012457, acc: 1.000000]  [G loss: 0.377180, acc: 0.890625]\n",
            "1896: [D loss: 0.011595, acc: 1.000000]  [G loss: 0.459566, acc: 0.812500]\n",
            "1897: [D loss: 0.004721, acc: 1.000000]  [G loss: 0.483362, acc: 0.875000]\n",
            "1898: [D loss: 0.009557, acc: 1.000000]  [G loss: 0.599320, acc: 0.812500]\n",
            "1899: [D loss: 0.014308, acc: 0.992188]  [G loss: 0.748856, acc: 0.796875]\n",
            "1900: [D loss: 0.006823, acc: 1.000000]  [G loss: 0.517866, acc: 0.796875]\n",
            "1901: [D loss: 0.033785, acc: 0.992188]  [G loss: 0.535169, acc: 0.812500]\n",
            "1902: [D loss: 0.078316, acc: 0.976562]  [G loss: 0.363667, acc: 0.828125]\n",
            "1903: [D loss: 0.014730, acc: 1.000000]  [G loss: 0.628097, acc: 0.812500]\n",
            "1904: [D loss: 0.081570, acc: 0.976562]  [G loss: 0.646779, acc: 0.796875]\n",
            "1905: [D loss: 0.044416, acc: 0.984375]  [G loss: 0.321841, acc: 0.859375]\n",
            "1906: [D loss: 0.036959, acc: 0.984375]  [G loss: 0.346827, acc: 0.859375]\n",
            "1907: [D loss: 0.063827, acc: 0.960938]  [G loss: 0.440424, acc: 0.796875]\n",
            "1908: [D loss: 0.012770, acc: 1.000000]  [G loss: 0.433083, acc: 0.781250]\n",
            "1909: [D loss: 0.006361, acc: 1.000000]  [G loss: 1.110739, acc: 0.609375]\n",
            "1910: [D loss: 0.006647, acc: 1.000000]  [G loss: 1.130217, acc: 0.640625]\n",
            "1911: [D loss: 0.019241, acc: 0.992188]  [G loss: 1.136253, acc: 0.640625]\n",
            "1912: [D loss: 0.149258, acc: 0.960938]  [G loss: 0.439949, acc: 0.843750]\n",
            "1913: [D loss: 0.033252, acc: 0.992188]  [G loss: 0.608342, acc: 0.796875]\n",
            "1914: [D loss: 0.124739, acc: 0.937500]  [G loss: 0.957221, acc: 0.578125]\n",
            "1915: [D loss: 0.032853, acc: 0.984375]  [G loss: 1.846250, acc: 0.546875]\n",
            "1916: [D loss: 0.040635, acc: 0.984375]  [G loss: 2.308619, acc: 0.468750]\n",
            "1917: [D loss: 0.099661, acc: 0.976562]  [G loss: 2.091629, acc: 0.484375]\n",
            "1918: [D loss: 0.043538, acc: 0.976562]  [G loss: 2.162248, acc: 0.468750]\n",
            "1919: [D loss: 0.022088, acc: 0.992188]  [G loss: 1.249551, acc: 0.578125]\n",
            "1920: [D loss: 0.092288, acc: 0.976562]  [G loss: 1.736436, acc: 0.515625]\n",
            "1921: [D loss: 0.037346, acc: 0.992188]  [G loss: 1.888803, acc: 0.406250]\n",
            "1922: [D loss: 0.030231, acc: 0.992188]  [G loss: 2.766466, acc: 0.218750]\n",
            "1923: [D loss: 0.051311, acc: 0.976562]  [G loss: 1.995493, acc: 0.343750]\n",
            "1924: [D loss: 0.048392, acc: 0.984375]  [G loss: 1.779872, acc: 0.296875]\n",
            "1925: [D loss: 0.090831, acc: 0.992188]  [G loss: 1.330767, acc: 0.390625]\n",
            "1926: [D loss: 0.026072, acc: 0.992188]  [G loss: 1.290034, acc: 0.484375]\n",
            "1927: [D loss: 0.111674, acc: 0.945312]  [G loss: 1.795263, acc: 0.406250]\n",
            "1928: [D loss: 0.010855, acc: 1.000000]  [G loss: 2.506527, acc: 0.265625]\n",
            "1929: [D loss: 0.015956, acc: 0.992188]  [G loss: 2.981846, acc: 0.156250]\n",
            "1930: [D loss: 0.024293, acc: 0.992188]  [G loss: 2.268552, acc: 0.312500]\n",
            "1931: [D loss: 0.041040, acc: 0.984375]  [G loss: 2.384326, acc: 0.375000]\n",
            "1932: [D loss: 0.011927, acc: 1.000000]  [G loss: 1.613693, acc: 0.609375]\n",
            "1933: [D loss: 0.042675, acc: 0.992188]  [G loss: 1.336147, acc: 0.609375]\n",
            "1934: [D loss: 0.014966, acc: 0.992188]  [G loss: 0.729522, acc: 0.765625]\n",
            "1935: [D loss: 0.031655, acc: 0.976562]  [G loss: 1.018831, acc: 0.718750]\n",
            "1936: [D loss: 0.006306, acc: 1.000000]  [G loss: 1.310930, acc: 0.562500]\n",
            "1937: [D loss: 0.005433, acc: 1.000000]  [G loss: 1.123980, acc: 0.656250]\n",
            "1938: [D loss: 0.002992, acc: 1.000000]  [G loss: 1.609724, acc: 0.515625]\n",
            "1939: [D loss: 0.036849, acc: 0.984375]  [G loss: 1.295360, acc: 0.656250]\n",
            "1940: [D loss: 0.045099, acc: 0.992188]  [G loss: 1.121347, acc: 0.578125]\n",
            "1941: [D loss: 0.022952, acc: 0.992188]  [G loss: 0.940225, acc: 0.609375]\n",
            "1942: [D loss: 0.032866, acc: 0.984375]  [G loss: 1.807111, acc: 0.500000]\n",
            "1943: [D loss: 0.003231, acc: 1.000000]  [G loss: 2.453094, acc: 0.265625]\n",
            "1944: [D loss: 0.007211, acc: 1.000000]  [G loss: 3.283276, acc: 0.187500]\n",
            "1945: [D loss: 0.034431, acc: 0.984375]  [G loss: 2.848612, acc: 0.343750]\n",
            "1946: [D loss: 0.035736, acc: 0.984375]  [G loss: 2.098026, acc: 0.359375]\n",
            "1947: [D loss: 0.003697, acc: 1.000000]  [G loss: 1.581640, acc: 0.437500]\n",
            "1948: [D loss: 0.017618, acc: 0.992188]  [G loss: 1.079374, acc: 0.593750]\n",
            "1949: [D loss: 0.017283, acc: 0.992188]  [G loss: 0.880924, acc: 0.671875]\n",
            "1950: [D loss: 0.028296, acc: 0.992188]  [G loss: 1.125200, acc: 0.609375]\n",
            "1951: [D loss: 0.010254, acc: 1.000000]  [G loss: 1.471608, acc: 0.578125]\n",
            "1952: [D loss: 0.007684, acc: 1.000000]  [G loss: 1.908816, acc: 0.437500]\n",
            "1953: [D loss: 0.025473, acc: 0.984375]  [G loss: 2.955039, acc: 0.375000]\n",
            "1954: [D loss: 0.006585, acc: 1.000000]  [G loss: 2.371249, acc: 0.406250]\n",
            "1955: [D loss: 0.033397, acc: 0.992188]  [G loss: 1.671479, acc: 0.515625]\n",
            "1956: [D loss: 0.047725, acc: 0.984375]  [G loss: 1.161692, acc: 0.656250]\n",
            "1957: [D loss: 0.032586, acc: 0.984375]  [G loss: 0.952325, acc: 0.703125]\n",
            "1958: [D loss: 0.016391, acc: 0.992188]  [G loss: 0.710030, acc: 0.750000]\n",
            "1959: [D loss: 0.006709, acc: 1.000000]  [G loss: 1.015405, acc: 0.640625]\n",
            "1960: [D loss: 0.036540, acc: 0.984375]  [G loss: 1.759678, acc: 0.500000]\n",
            "1961: [D loss: 0.007146, acc: 1.000000]  [G loss: 2.377578, acc: 0.359375]\n",
            "1962: [D loss: 0.009822, acc: 1.000000]  [G loss: 1.961322, acc: 0.437500]\n",
            "1963: [D loss: 0.010204, acc: 1.000000]  [G loss: 2.170419, acc: 0.421875]\n",
            "1964: [D loss: 0.021300, acc: 0.992188]  [G loss: 1.222325, acc: 0.562500]\n",
            "1965: [D loss: 0.005285, acc: 1.000000]  [G loss: 0.693742, acc: 0.734375]\n",
            "1966: [D loss: 0.012185, acc: 1.000000]  [G loss: 1.204089, acc: 0.671875]\n",
            "1967: [D loss: 0.036292, acc: 0.984375]  [G loss: 1.205906, acc: 0.656250]\n",
            "1968: [D loss: 0.006730, acc: 1.000000]  [G loss: 1.165082, acc: 0.671875]\n",
            "1969: [D loss: 0.010523, acc: 1.000000]  [G loss: 1.066092, acc: 0.718750]\n",
            "1970: [D loss: 0.012743, acc: 1.000000]  [G loss: 1.809015, acc: 0.515625]\n",
            "1971: [D loss: 0.006501, acc: 1.000000]  [G loss: 1.843348, acc: 0.468750]\n",
            "1972: [D loss: 0.031435, acc: 0.984375]  [G loss: 1.381516, acc: 0.656250]\n",
            "1973: [D loss: 0.053177, acc: 0.992188]  [G loss: 1.109397, acc: 0.687500]\n",
            "1974: [D loss: 0.017198, acc: 1.000000]  [G loss: 1.213160, acc: 0.671875]\n",
            "1975: [D loss: 0.051124, acc: 0.976562]  [G loss: 3.113182, acc: 0.343750]\n",
            "1976: [D loss: 0.049763, acc: 0.984375]  [G loss: 5.896886, acc: 0.171875]\n",
            "1977: [D loss: 0.037137, acc: 0.992188]  [G loss: 5.987496, acc: 0.109375]\n",
            "1978: [D loss: 0.062986, acc: 0.976562]  [G loss: 4.775865, acc: 0.265625]\n",
            "1979: [D loss: 0.025549, acc: 0.984375]  [G loss: 2.999794, acc: 0.421875]\n",
            "1980: [D loss: 0.080233, acc: 0.960938]  [G loss: 3.745559, acc: 0.265625]\n",
            "1981: [D loss: 0.015657, acc: 0.992188]  [G loss: 4.027504, acc: 0.296875]\n",
            "1982: [D loss: 0.012349, acc: 1.000000]  [G loss: 4.595818, acc: 0.296875]\n",
            "1983: [D loss: 0.010102, acc: 0.992188]  [G loss: 4.759066, acc: 0.296875]\n",
            "1984: [D loss: 0.044176, acc: 0.976562]  [G loss: 4.138369, acc: 0.375000]\n",
            "1985: [D loss: 0.008774, acc: 1.000000]  [G loss: 3.256590, acc: 0.421875]\n",
            "1986: [D loss: 0.057505, acc: 0.976562]  [G loss: 2.392891, acc: 0.437500]\n",
            "1987: [D loss: 0.092578, acc: 0.976562]  [G loss: 5.344904, acc: 0.125000]\n",
            "1988: [D loss: 0.094874, acc: 0.976562]  [G loss: 6.039282, acc: 0.250000]\n",
            "1989: [D loss: 0.028676, acc: 0.976562]  [G loss: 5.691670, acc: 0.234375]\n",
            "1990: [D loss: 0.019648, acc: 0.992188]  [G loss: 5.135159, acc: 0.265625]\n",
            "1991: [D loss: 0.014960, acc: 0.992188]  [G loss: 4.814989, acc: 0.312500]\n",
            "1992: [D loss: 0.031238, acc: 0.992188]  [G loss: 5.723372, acc: 0.250000]\n",
            "1993: [D loss: 0.013959, acc: 1.000000]  [G loss: 6.210728, acc: 0.218750]\n",
            "1994: [D loss: 0.060361, acc: 0.984375]  [G loss: 7.420784, acc: 0.093750]\n",
            "1995: [D loss: 0.013490, acc: 1.000000]  [G loss: 6.938383, acc: 0.093750]\n",
            "1996: [D loss: 0.030786, acc: 0.984375]  [G loss: 5.958980, acc: 0.218750]\n",
            "1997: [D loss: 0.035263, acc: 0.984375]  [G loss: 4.587800, acc: 0.328125]\n",
            "1998: [D loss: 0.051055, acc: 0.976562]  [G loss: 5.177512, acc: 0.265625]\n",
            "1999: [D loss: 0.058174, acc: 0.968750]  [G loss: 5.594658, acc: 0.218750]\n",
            "2000: [D loss: 0.054526, acc: 0.984375]  [G loss: 5.882911, acc: 0.171875]\n",
            "2001: [D loss: 0.029408, acc: 0.984375]  [G loss: 5.693958, acc: 0.125000]\n",
            "2002: [D loss: 0.031478, acc: 0.984375]  [G loss: 5.792312, acc: 0.109375]\n",
            "2003: [D loss: 0.063245, acc: 0.984375]  [G loss: 6.514100, acc: 0.140625]\n",
            "2004: [D loss: 0.070367, acc: 0.976562]  [G loss: 4.825981, acc: 0.171875]\n",
            "2005: [D loss: 0.078863, acc: 0.968750]  [G loss: 4.615829, acc: 0.140625]\n",
            "2006: [D loss: 0.042881, acc: 0.968750]  [G loss: 6.341902, acc: 0.031250]\n",
            "2007: [D loss: 0.060580, acc: 0.968750]  [G loss: 7.993021, acc: 0.015625]\n",
            "2008: [D loss: 0.166259, acc: 0.953125]  [G loss: 4.735389, acc: 0.078125]\n",
            "2009: [D loss: 0.013286, acc: 0.992188]  [G loss: 3.577749, acc: 0.250000]\n",
            "2010: [D loss: 0.071655, acc: 0.976562]  [G loss: 3.648054, acc: 0.203125]\n",
            "2011: [D loss: 0.055301, acc: 0.992188]  [G loss: 4.244393, acc: 0.156250]\n",
            "2012: [D loss: 0.053903, acc: 0.976562]  [G loss: 7.138664, acc: 0.000000]\n",
            "2013: [D loss: 0.078756, acc: 0.976562]  [G loss: 8.083542, acc: 0.000000]\n",
            "2014: [D loss: 0.168182, acc: 0.929688]  [G loss: 4.771968, acc: 0.109375]\n",
            "2015: [D loss: 0.042156, acc: 0.976562]  [G loss: 3.749595, acc: 0.250000]\n",
            "2016: [D loss: 0.070084, acc: 0.984375]  [G loss: 2.759777, acc: 0.250000]\n",
            "2017: [D loss: 0.107581, acc: 0.968750]  [G loss: 4.332717, acc: 0.171875]\n",
            "2018: [D loss: 0.046873, acc: 0.968750]  [G loss: 6.789503, acc: 0.046875]\n",
            "2019: [D loss: 0.137968, acc: 0.953125]  [G loss: 6.508025, acc: 0.078125]\n",
            "2020: [D loss: 0.185776, acc: 0.976562]  [G loss: 4.788470, acc: 0.125000]\n",
            "2021: [D loss: 0.021375, acc: 0.992188]  [G loss: 3.942828, acc: 0.156250]\n",
            "2022: [D loss: 0.029563, acc: 0.992188]  [G loss: 2.508561, acc: 0.343750]\n",
            "2023: [D loss: 0.101613, acc: 0.968750]  [G loss: 3.745378, acc: 0.203125]\n",
            "2024: [D loss: 0.079132, acc: 0.960938]  [G loss: 5.972229, acc: 0.093750]\n",
            "2025: [D loss: 0.045477, acc: 0.976562]  [G loss: 6.011010, acc: 0.078125]\n",
            "2026: [D loss: 0.158631, acc: 0.968750]  [G loss: 4.366346, acc: 0.156250]\n",
            "2027: [D loss: 0.040384, acc: 0.984375]  [G loss: 2.725382, acc: 0.359375]\n",
            "2028: [D loss: 0.175902, acc: 0.921875]  [G loss: 3.967564, acc: 0.171875]\n",
            "2029: [D loss: 0.060007, acc: 0.968750]  [G loss: 5.948972, acc: 0.109375]\n",
            "2030: [D loss: 0.186385, acc: 0.968750]  [G loss: 6.034292, acc: 0.078125]\n",
            "2031: [D loss: 0.065631, acc: 0.984375]  [G loss: 4.868850, acc: 0.125000]\n",
            "2032: [D loss: 0.060391, acc: 0.984375]  [G loss: 4.250104, acc: 0.234375]\n",
            "2033: [D loss: 0.038070, acc: 0.992188]  [G loss: 3.424871, acc: 0.375000]\n",
            "2034: [D loss: 0.048781, acc: 0.984375]  [G loss: 4.566909, acc: 0.171875]\n",
            "2035: [D loss: 0.092997, acc: 0.960938]  [G loss: 2.931359, acc: 0.343750]\n",
            "2036: [D loss: 0.045763, acc: 0.984375]  [G loss: 3.787662, acc: 0.281250]\n",
            "2037: [D loss: 0.090044, acc: 0.960938]  [G loss: 4.816663, acc: 0.171875]\n",
            "2038: [D loss: 0.037389, acc: 0.984375]  [G loss: 4.326982, acc: 0.218750]\n",
            "2039: [D loss: 0.044732, acc: 0.992188]  [G loss: 4.609642, acc: 0.187500]\n",
            "2040: [D loss: 0.083575, acc: 0.984375]  [G loss: 4.518843, acc: 0.250000]\n",
            "2041: [D loss: 0.012444, acc: 1.000000]  [G loss: 3.941846, acc: 0.281250]\n",
            "2042: [D loss: 0.165874, acc: 0.960938]  [G loss: 3.891156, acc: 0.281250]\n",
            "2043: [D loss: 0.055244, acc: 0.984375]  [G loss: 4.243446, acc: 0.234375]\n",
            "2044: [D loss: 0.052841, acc: 0.984375]  [G loss: 5.369264, acc: 0.109375]\n",
            "2045: [D loss: 0.038222, acc: 0.976562]  [G loss: 5.750920, acc: 0.109375]\n",
            "2046: [D loss: 0.089980, acc: 0.960938]  [G loss: 4.241636, acc: 0.234375]\n",
            "2047: [D loss: 0.084234, acc: 0.976562]  [G loss: 2.873477, acc: 0.343750]\n",
            "2048: [D loss: 0.127295, acc: 0.937500]  [G loss: 2.953595, acc: 0.250000]\n",
            "2049: [D loss: 0.209555, acc: 0.929688]  [G loss: 6.113932, acc: 0.062500]\n",
            "2050: [D loss: 0.138038, acc: 0.953125]  [G loss: 5.522210, acc: 0.156250]\n",
            "2051: [D loss: 0.248259, acc: 0.945312]  [G loss: 2.316730, acc: 0.593750]\n",
            "2052: [D loss: 0.204029, acc: 0.914062]  [G loss: 2.791298, acc: 0.312500]\n",
            "2053: [D loss: 0.069060, acc: 0.968750]  [G loss: 5.327268, acc: 0.125000]\n",
            "2054: [D loss: 0.157835, acc: 0.968750]  [G loss: 5.806416, acc: 0.046875]\n",
            "2055: [D loss: 0.215999, acc: 0.914062]  [G loss: 3.208827, acc: 0.234375]\n",
            "2056: [D loss: 0.076273, acc: 0.968750]  [G loss: 2.585670, acc: 0.312500]\n",
            "2057: [D loss: 0.194042, acc: 0.929688]  [G loss: 3.866961, acc: 0.125000]\n",
            "2058: [D loss: 0.073225, acc: 0.960938]  [G loss: 5.380021, acc: 0.078125]\n",
            "2059: [D loss: 0.057545, acc: 0.976562]  [G loss: 5.873562, acc: 0.046875]\n",
            "2060: [D loss: 0.031456, acc: 0.976562]  [G loss: 5.306060, acc: 0.078125]\n",
            "2061: [D loss: 0.088594, acc: 0.976562]  [G loss: 5.125558, acc: 0.156250]\n",
            "2062: [D loss: 0.080182, acc: 0.968750]  [G loss: 5.536627, acc: 0.125000]\n",
            "2063: [D loss: 0.161002, acc: 0.953125]  [G loss: 5.303918, acc: 0.078125]\n",
            "2064: [D loss: 0.046495, acc: 0.968750]  [G loss: 6.621594, acc: 0.015625]\n",
            "2065: [D loss: 0.115032, acc: 0.984375]  [G loss: 5.829269, acc: 0.031250]\n",
            "2066: [D loss: 0.089065, acc: 0.960938]  [G loss: 4.922380, acc: 0.140625]\n",
            "2067: [D loss: 0.054797, acc: 0.976562]  [G loss: 4.202091, acc: 0.171875]\n",
            "2068: [D loss: 0.045079, acc: 0.976562]  [G loss: 3.729198, acc: 0.203125]\n",
            "2069: [D loss: 0.071418, acc: 0.968750]  [G loss: 3.562456, acc: 0.203125]\n",
            "2070: [D loss: 0.049185, acc: 0.976562]  [G loss: 3.614031, acc: 0.218750]\n",
            "2071: [D loss: 0.047332, acc: 0.984375]  [G loss: 4.103908, acc: 0.187500]\n",
            "2072: [D loss: 0.131019, acc: 0.960938]  [G loss: 3.690464, acc: 0.218750]\n",
            "2073: [D loss: 0.102294, acc: 0.976562]  [G loss: 3.584738, acc: 0.234375]\n",
            "2074: [D loss: 0.072201, acc: 0.960938]  [G loss: 3.940643, acc: 0.171875]\n",
            "2075: [D loss: 0.032059, acc: 0.984375]  [G loss: 4.261421, acc: 0.203125]\n",
            "2076: [D loss: 0.034983, acc: 0.992188]  [G loss: 4.362216, acc: 0.203125]\n",
            "2077: [D loss: 0.074323, acc: 0.960938]  [G loss: 4.383931, acc: 0.062500]\n",
            "2078: [D loss: 0.065733, acc: 0.968750]  [G loss: 3.862174, acc: 0.203125]\n",
            "2079: [D loss: 0.041991, acc: 0.968750]  [G loss: 3.035436, acc: 0.296875]\n",
            "2080: [D loss: 0.095543, acc: 0.953125]  [G loss: 4.204448, acc: 0.140625]\n",
            "2081: [D loss: 0.032802, acc: 0.984375]  [G loss: 5.863110, acc: 0.078125]\n",
            "2082: [D loss: 0.047028, acc: 0.976562]  [G loss: 6.037745, acc: 0.046875]\n",
            "2083: [D loss: 0.055507, acc: 0.976562]  [G loss: 5.243824, acc: 0.062500]\n",
            "2084: [D loss: 0.142979, acc: 0.960938]  [G loss: 3.538463, acc: 0.218750]\n",
            "2085: [D loss: 0.041076, acc: 0.976562]  [G loss: 2.331230, acc: 0.296875]\n",
            "2086: [D loss: 0.110763, acc: 0.953125]  [G loss: 2.010009, acc: 0.375000]\n",
            "2087: [D loss: 0.046899, acc: 1.000000]  [G loss: 2.699988, acc: 0.406250]\n",
            "2088: [D loss: 0.019848, acc: 0.992188]  [G loss: 4.631078, acc: 0.203125]\n",
            "2089: [D loss: 0.030312, acc: 0.984375]  [G loss: 3.880866, acc: 0.265625]\n",
            "2090: [D loss: 0.014788, acc: 1.000000]  [G loss: 5.168141, acc: 0.156250]\n",
            "2091: [D loss: 0.080282, acc: 0.984375]  [G loss: 4.901657, acc: 0.171875]\n",
            "2092: [D loss: 0.125362, acc: 0.984375]  [G loss: 4.399127, acc: 0.265625]\n",
            "2093: [D loss: 0.052552, acc: 0.968750]  [G loss: 2.608633, acc: 0.421875]\n",
            "2094: [D loss: 0.059845, acc: 0.976562]  [G loss: 2.673477, acc: 0.421875]\n",
            "2095: [D loss: 0.115098, acc: 0.953125]  [G loss: 3.886488, acc: 0.296875]\n",
            "2096: [D loss: 0.045360, acc: 0.976562]  [G loss: 4.612294, acc: 0.171875]\n",
            "2097: [D loss: 0.202674, acc: 0.937500]  [G loss: 4.623492, acc: 0.171875]\n",
            "2098: [D loss: 0.121800, acc: 0.976562]  [G loss: 5.130319, acc: 0.078125]\n",
            "2099: [D loss: 0.039086, acc: 0.984375]  [G loss: 4.722426, acc: 0.109375]\n",
            "2100: [D loss: 0.071097, acc: 0.976562]  [G loss: 3.547719, acc: 0.203125]\n",
            "2101: [D loss: 0.153010, acc: 0.937500]  [G loss: 3.267561, acc: 0.218750]\n",
            "2102: [D loss: 0.064875, acc: 0.968750]  [G loss: 3.910650, acc: 0.093750]\n",
            "2103: [D loss: 0.102835, acc: 0.968750]  [G loss: 4.471538, acc: 0.062500]\n",
            "2104: [D loss: 0.035000, acc: 0.992188]  [G loss: 5.032445, acc: 0.046875]\n",
            "2105: [D loss: 0.047808, acc: 0.984375]  [G loss: 5.833310, acc: 0.031250]\n",
            "2106: [D loss: 0.009188, acc: 1.000000]  [G loss: 5.835092, acc: 0.046875]\n",
            "2107: [D loss: 0.050554, acc: 0.984375]  [G loss: 6.309567, acc: 0.000000]\n",
            "2108: [D loss: 0.090281, acc: 0.968750]  [G loss: 5.673830, acc: 0.031250]\n",
            "2109: [D loss: 0.024052, acc: 0.992188]  [G loss: 5.129975, acc: 0.078125]\n",
            "2110: [D loss: 0.041505, acc: 0.976562]  [G loss: 4.693545, acc: 0.093750]\n",
            "2111: [D loss: 0.097663, acc: 0.953125]  [G loss: 5.205997, acc: 0.062500]\n",
            "2112: [D loss: 0.040243, acc: 0.984375]  [G loss: 5.377253, acc: 0.109375]\n",
            "2113: [D loss: 0.024981, acc: 0.992188]  [G loss: 5.805258, acc: 0.078125]\n",
            "2114: [D loss: 0.036533, acc: 0.984375]  [G loss: 5.687323, acc: 0.078125]\n",
            "2115: [D loss: 0.122869, acc: 0.976562]  [G loss: 5.817735, acc: 0.093750]\n",
            "2116: [D loss: 0.008692, acc: 1.000000]  [G loss: 4.938081, acc: 0.125000]\n",
            "2117: [D loss: 0.017130, acc: 1.000000]  [G loss: 4.644535, acc: 0.234375]\n",
            "2118: [D loss: 0.021282, acc: 1.000000]  [G loss: 5.140160, acc: 0.078125]\n",
            "2119: [D loss: 0.025787, acc: 0.992188]  [G loss: 5.627587, acc: 0.093750]\n",
            "2120: [D loss: 0.056546, acc: 0.976562]  [G loss: 6.220341, acc: 0.093750]\n",
            "2121: [D loss: 0.031311, acc: 0.992188]  [G loss: 7.338172, acc: 0.062500]\n",
            "2122: [D loss: 0.071298, acc: 0.960938]  [G loss: 6.165452, acc: 0.078125]\n",
            "2123: [D loss: 0.034884, acc: 0.984375]  [G loss: 6.701787, acc: 0.093750]\n",
            "2124: [D loss: 0.020941, acc: 1.000000]  [G loss: 5.901383, acc: 0.078125]\n",
            "2125: [D loss: 0.037694, acc: 0.984375]  [G loss: 5.777812, acc: 0.078125]\n",
            "2126: [D loss: 0.086059, acc: 0.968750]  [G loss: 5.608179, acc: 0.109375]\n",
            "2127: [D loss: 0.028318, acc: 0.992188]  [G loss: 5.442985, acc: 0.078125]\n",
            "2128: [D loss: 0.036903, acc: 0.984375]  [G loss: 5.788684, acc: 0.140625]\n",
            "2129: [D loss: 0.063488, acc: 0.984375]  [G loss: 6.462381, acc: 0.046875]\n",
            "2130: [D loss: 0.063356, acc: 0.976562]  [G loss: 6.539721, acc: 0.062500]\n",
            "2131: [D loss: 0.071085, acc: 0.976562]  [G loss: 5.393265, acc: 0.109375]\n",
            "2132: [D loss: 0.033865, acc: 0.984375]  [G loss: 5.619219, acc: 0.109375]\n",
            "2133: [D loss: 0.025196, acc: 0.992188]  [G loss: 4.413392, acc: 0.218750]\n",
            "2134: [D loss: 0.015580, acc: 1.000000]  [G loss: 4.316530, acc: 0.140625]\n",
            "2135: [D loss: 0.053470, acc: 0.976562]  [G loss: 3.709121, acc: 0.296875]\n",
            "2136: [D loss: 0.021356, acc: 1.000000]  [G loss: 4.866809, acc: 0.171875]\n",
            "2137: [D loss: 0.048958, acc: 0.984375]  [G loss: 5.772872, acc: 0.078125]\n",
            "2138: [D loss: 0.051162, acc: 0.960938]  [G loss: 5.266846, acc: 0.187500]\n",
            "2139: [D loss: 0.035320, acc: 0.976562]  [G loss: 5.563732, acc: 0.125000]\n",
            "2140: [D loss: 0.070003, acc: 0.976562]  [G loss: 4.672450, acc: 0.140625]\n",
            "2141: [D loss: 0.178478, acc: 0.968750]  [G loss: 3.671511, acc: 0.187500]\n",
            "2142: [D loss: 0.032133, acc: 0.984375]  [G loss: 2.700165, acc: 0.234375]\n",
            "2143: [D loss: 0.049043, acc: 0.984375]  [G loss: 2.318847, acc: 0.328125]\n",
            "2144: [D loss: 0.040116, acc: 0.984375]  [G loss: 2.892213, acc: 0.312500]\n",
            "2145: [D loss: 0.024667, acc: 1.000000]  [G loss: 2.792799, acc: 0.281250]\n",
            "2146: [D loss: 0.027202, acc: 0.992188]  [G loss: 3.104597, acc: 0.281250]\n",
            "2147: [D loss: 0.052799, acc: 0.984375]  [G loss: 3.396516, acc: 0.234375]\n",
            "2148: [D loss: 0.055095, acc: 0.984375]  [G loss: 3.484358, acc: 0.265625]\n",
            "2149: [D loss: 0.044585, acc: 0.976562]  [G loss: 3.308928, acc: 0.296875]\n",
            "2150: [D loss: 0.047449, acc: 0.984375]  [G loss: 2.961478, acc: 0.359375]\n",
            "2151: [D loss: 0.055988, acc: 0.976562]  [G loss: 2.947094, acc: 0.328125]\n",
            "2152: [D loss: 0.126645, acc: 0.968750]  [G loss: 2.890432, acc: 0.281250]\n",
            "2153: [D loss: 0.051349, acc: 0.976562]  [G loss: 2.682359, acc: 0.375000]\n",
            "2154: [D loss: 0.017551, acc: 0.992188]  [G loss: 2.640259, acc: 0.406250]\n",
            "2155: [D loss: 0.035773, acc: 0.984375]  [G loss: 3.003612, acc: 0.328125]\n",
            "2156: [D loss: 0.061910, acc: 0.984375]  [G loss: 2.903139, acc: 0.359375]\n",
            "2157: [D loss: 0.090182, acc: 0.953125]  [G loss: 2.470636, acc: 0.421875]\n",
            "2158: [D loss: 0.042729, acc: 0.984375]  [G loss: 2.847464, acc: 0.437500]\n",
            "2159: [D loss: 0.049228, acc: 0.968750]  [G loss: 3.271425, acc: 0.296875]\n",
            "2160: [D loss: 0.058681, acc: 0.976562]  [G loss: 4.371126, acc: 0.218750]\n",
            "2161: [D loss: 0.107704, acc: 0.960938]  [G loss: 4.572864, acc: 0.250000]\n",
            "2162: [D loss: 0.137989, acc: 0.953125]  [G loss: 3.760664, acc: 0.187500]\n",
            "2163: [D loss: 0.226790, acc: 0.937500]  [G loss: 3.967261, acc: 0.343750]\n",
            "2164: [D loss: 0.092875, acc: 0.960938]  [G loss: 3.169203, acc: 0.328125]\n",
            "2165: [D loss: 0.217117, acc: 0.914062]  [G loss: 3.688592, acc: 0.265625]\n",
            "2166: [D loss: 0.156382, acc: 0.929688]  [G loss: 4.568586, acc: 0.265625]\n",
            "2167: [D loss: 0.170007, acc: 0.945312]  [G loss: 4.805248, acc: 0.156250]\n",
            "2168: [D loss: 0.146779, acc: 0.953125]  [G loss: 4.259442, acc: 0.218750]\n",
            "2169: [D loss: 0.205686, acc: 0.929688]  [G loss: 3.504885, acc: 0.250000]\n",
            "2170: [D loss: 0.156362, acc: 0.929688]  [G loss: 4.990659, acc: 0.171875]\n",
            "2171: [D loss: 0.094726, acc: 0.976562]  [G loss: 5.234381, acc: 0.109375]\n",
            "2172: [D loss: 0.198033, acc: 0.968750]  [G loss: 4.495730, acc: 0.109375]\n",
            "2173: [D loss: 0.115162, acc: 0.968750]  [G loss: 3.396038, acc: 0.234375]\n",
            "2174: [D loss: 0.075848, acc: 0.984375]  [G loss: 3.076165, acc: 0.265625]\n",
            "2175: [D loss: 0.107172, acc: 0.929688]  [G loss: 4.353709, acc: 0.187500]\n",
            "2176: [D loss: 0.041903, acc: 0.984375]  [G loss: 4.837395, acc: 0.046875]\n",
            "2177: [D loss: 0.095019, acc: 0.984375]  [G loss: 5.884998, acc: 0.015625]\n",
            "2178: [D loss: 0.089210, acc: 0.968750]  [G loss: 6.173981, acc: 0.015625]\n",
            "2179: [D loss: 0.024559, acc: 0.984375]  [G loss: 5.850671, acc: 0.046875]\n",
            "2180: [D loss: 0.052468, acc: 0.984375]  [G loss: 5.026143, acc: 0.078125]\n",
            "2181: [D loss: 0.219634, acc: 0.945312]  [G loss: 2.848954, acc: 0.328125]\n",
            "2182: [D loss: 0.207856, acc: 0.937500]  [G loss: 2.079873, acc: 0.421875]\n",
            "2183: [D loss: 0.156162, acc: 0.953125]  [G loss: 4.374951, acc: 0.062500]\n",
            "2184: [D loss: 0.033698, acc: 0.984375]  [G loss: 6.440097, acc: 0.031250]\n",
            "2185: [D loss: 0.092947, acc: 0.976562]  [G loss: 6.860897, acc: 0.031250]\n",
            "2186: [D loss: 0.068007, acc: 0.960938]  [G loss: 6.037968, acc: 0.046875]\n",
            "2187: [D loss: 0.089582, acc: 0.968750]  [G loss: 3.708418, acc: 0.281250]\n",
            "2188: [D loss: 0.238625, acc: 0.937500]  [G loss: 2.654626, acc: 0.375000]\n",
            "2189: [D loss: 0.133678, acc: 0.937500]  [G loss: 4.030150, acc: 0.156250]\n",
            "2190: [D loss: 0.029904, acc: 0.992188]  [G loss: 5.848273, acc: 0.093750]\n",
            "2191: [D loss: 0.049639, acc: 0.976562]  [G loss: 6.457513, acc: 0.046875]\n",
            "2192: [D loss: 0.279054, acc: 0.945312]  [G loss: 4.413175, acc: 0.156250]\n",
            "2193: [D loss: 0.037957, acc: 0.984375]  [G loss: 3.441760, acc: 0.265625]\n",
            "2194: [D loss: 0.109881, acc: 0.960938]  [G loss: 3.164249, acc: 0.265625]\n",
            "2195: [D loss: 0.138552, acc: 0.937500]  [G loss: 2.590509, acc: 0.296875]\n",
            "2196: [D loss: 0.161051, acc: 0.937500]  [G loss: 3.260666, acc: 0.265625]\n",
            "2197: [D loss: 0.067199, acc: 0.968750]  [G loss: 3.282376, acc: 0.281250]\n",
            "2198: [D loss: 0.074202, acc: 0.992188]  [G loss: 4.058105, acc: 0.265625]\n",
            "2199: [D loss: 0.211562, acc: 0.960938]  [G loss: 3.485343, acc: 0.375000]\n",
            "2200: [D loss: 0.262475, acc: 0.882812]  [G loss: 3.175744, acc: 0.265625]\n",
            "2201: [D loss: 0.195937, acc: 0.929688]  [G loss: 2.393377, acc: 0.296875]\n",
            "2202: [D loss: 0.099704, acc: 0.976562]  [G loss: 2.211919, acc: 0.406250]\n",
            "2203: [D loss: 0.324399, acc: 0.921875]  [G loss: 2.837069, acc: 0.187500]\n",
            "2204: [D loss: 0.104297, acc: 0.960938]  [G loss: 3.637109, acc: 0.250000]\n",
            "2205: [D loss: 0.102186, acc: 0.960938]  [G loss: 3.683383, acc: 0.187500]\n",
            "2206: [D loss: 0.084477, acc: 0.984375]  [G loss: 4.067064, acc: 0.203125]\n",
            "2207: [D loss: 0.097366, acc: 0.960938]  [G loss: 4.279508, acc: 0.156250]\n",
            "2208: [D loss: 0.060638, acc: 0.984375]  [G loss: 3.768398, acc: 0.203125]\n",
            "2209: [D loss: 0.159110, acc: 0.953125]  [G loss: 2.877066, acc: 0.218750]\n",
            "2210: [D loss: 0.099735, acc: 0.960938]  [G loss: 2.298625, acc: 0.296875]\n",
            "2211: [D loss: 0.087777, acc: 0.960938]  [G loss: 2.842696, acc: 0.375000]\n",
            "2212: [D loss: 0.054283, acc: 0.976562]  [G loss: 3.014357, acc: 0.281250]\n",
            "2213: [D loss: 0.082621, acc: 0.960938]  [G loss: 4.124516, acc: 0.312500]\n",
            "2214: [D loss: 0.115619, acc: 0.929688]  [G loss: 3.988474, acc: 0.343750]\n",
            "2215: [D loss: 0.062561, acc: 0.960938]  [G loss: 2.835576, acc: 0.359375]\n",
            "2216: [D loss: 0.111522, acc: 0.953125]  [G loss: 2.845277, acc: 0.390625]\n",
            "2217: [D loss: 0.101444, acc: 0.968750]  [G loss: 3.438323, acc: 0.250000]\n",
            "2218: [D loss: 0.049109, acc: 0.984375]  [G loss: 3.657050, acc: 0.218750]\n",
            "2219: [D loss: 0.021296, acc: 1.000000]  [G loss: 4.125185, acc: 0.156250]\n",
            "2220: [D loss: 0.048012, acc: 0.984375]  [G loss: 4.338873, acc: 0.156250]\n",
            "2221: [D loss: 0.053274, acc: 0.984375]  [G loss: 4.157488, acc: 0.171875]\n",
            "2222: [D loss: 0.070764, acc: 0.968750]  [G loss: 4.408570, acc: 0.140625]\n",
            "2223: [D loss: 0.123097, acc: 0.976562]  [G loss: 4.495251, acc: 0.140625]\n",
            "2224: [D loss: 0.139648, acc: 0.960938]  [G loss: 4.112244, acc: 0.140625]\n",
            "2225: [D loss: 0.053775, acc: 0.984375]  [G loss: 4.350252, acc: 0.171875]\n",
            "2226: [D loss: 0.174511, acc: 0.937500]  [G loss: 3.023325, acc: 0.234375]\n",
            "2227: [D loss: 0.151889, acc: 0.953125]  [G loss: 1.518239, acc: 0.484375]\n",
            "2228: [D loss: 0.183412, acc: 0.937500]  [G loss: 1.146473, acc: 0.671875]\n",
            "2229: [D loss: 0.089494, acc: 0.945312]  [G loss: 1.189751, acc: 0.640625]\n",
            "2230: [D loss: 0.057551, acc: 0.984375]  [G loss: 1.883375, acc: 0.546875]\n",
            "2231: [D loss: 0.050973, acc: 0.976562]  [G loss: 1.968971, acc: 0.578125]\n",
            "2232: [D loss: 0.130414, acc: 0.968750]  [G loss: 1.931286, acc: 0.578125]\n",
            "2233: [D loss: 0.071887, acc: 0.976562]  [G loss: 1.691064, acc: 0.640625]\n",
            "2234: [D loss: 0.090062, acc: 0.968750]  [G loss: 1.186842, acc: 0.671875]\n",
            "2235: [D loss: 0.058865, acc: 0.984375]  [G loss: 1.252832, acc: 0.656250]\n",
            "2236: [D loss: 0.086744, acc: 0.976562]  [G loss: 1.634274, acc: 0.656250]\n",
            "2237: [D loss: 0.030056, acc: 0.992188]  [G loss: 1.728606, acc: 0.562500]\n",
            "2238: [D loss: 0.064346, acc: 0.960938]  [G loss: 1.563064, acc: 0.609375]\n",
            "2239: [D loss: 0.094798, acc: 0.984375]  [G loss: 1.937690, acc: 0.562500]\n",
            "2240: [D loss: 0.057354, acc: 0.984375]  [G loss: 1.687908, acc: 0.640625]\n",
            "2241: [D loss: 0.054943, acc: 0.976562]  [G loss: 1.439241, acc: 0.656250]\n",
            "2242: [D loss: 0.038833, acc: 0.984375]  [G loss: 1.311797, acc: 0.625000]\n",
            "2243: [D loss: 0.077147, acc: 0.968750]  [G loss: 1.376818, acc: 0.578125]\n",
            "2244: [D loss: 0.027658, acc: 0.992188]  [G loss: 1.739007, acc: 0.546875]\n",
            "2245: [D loss: 0.080349, acc: 0.968750]  [G loss: 1.545360, acc: 0.593750]\n",
            "2246: [D loss: 0.024838, acc: 0.992188]  [G loss: 2.202373, acc: 0.406250]\n",
            "2247: [D loss: 0.093494, acc: 0.968750]  [G loss: 2.494154, acc: 0.281250]\n",
            "2248: [D loss: 0.090468, acc: 0.976562]  [G loss: 2.999315, acc: 0.156250]\n",
            "2249: [D loss: 0.018668, acc: 0.992188]  [G loss: 4.256787, acc: 0.078125]\n",
            "2250: [D loss: 0.053719, acc: 0.984375]  [G loss: 4.098575, acc: 0.109375]\n",
            "2251: [D loss: 0.067372, acc: 0.992188]  [G loss: 4.603526, acc: 0.093750]\n",
            "2252: [D loss: 0.163213, acc: 0.937500]  [G loss: 3.879447, acc: 0.156250]\n",
            "2253: [D loss: 0.084385, acc: 0.960938]  [G loss: 3.984465, acc: 0.093750]\n",
            "2254: [D loss: 0.016957, acc: 1.000000]  [G loss: 4.464842, acc: 0.125000]\n",
            "2255: [D loss: 0.129892, acc: 0.953125]  [G loss: 6.380131, acc: 0.046875]\n",
            "2256: [D loss: 0.015910, acc: 1.000000]  [G loss: 8.063284, acc: 0.000000]\n",
            "2257: [D loss: 0.148443, acc: 0.953125]  [G loss: 7.775826, acc: 0.015625]\n",
            "2258: [D loss: 0.132296, acc: 0.953125]  [G loss: 5.833510, acc: 0.046875]\n",
            "2259: [D loss: 0.202147, acc: 0.945312]  [G loss: 4.500406, acc: 0.125000]\n",
            "2260: [D loss: 0.018151, acc: 0.992188]  [G loss: 4.690610, acc: 0.140625]\n",
            "2261: [D loss: 0.087485, acc: 0.976562]  [G loss: 5.345834, acc: 0.109375]\n",
            "2262: [D loss: 0.034176, acc: 0.992188]  [G loss: 6.150579, acc: 0.000000]\n",
            "2263: [D loss: 0.090133, acc: 0.976562]  [G loss: 5.940429, acc: 0.031250]\n",
            "2264: [D loss: 0.078333, acc: 0.976562]  [G loss: 5.704958, acc: 0.046875]\n",
            "2265: [D loss: 0.062042, acc: 0.976562]  [G loss: 5.288938, acc: 0.125000]\n",
            "2266: [D loss: 0.034139, acc: 0.976562]  [G loss: 4.283949, acc: 0.156250]\n",
            "2267: [D loss: 0.071812, acc: 0.953125]  [G loss: 4.470317, acc: 0.171875]\n",
            "2268: [D loss: 0.067996, acc: 0.960938]  [G loss: 5.578432, acc: 0.125000]\n",
            "2269: [D loss: 0.049225, acc: 0.976562]  [G loss: 6.938801, acc: 0.062500]\n",
            "2270: [D loss: 0.002855, acc: 1.000000]  [G loss: 9.297905, acc: 0.031250]\n",
            "2271: [D loss: 0.093395, acc: 0.984375]  [G loss: 10.271473, acc: 0.000000]\n",
            "2272: [D loss: 0.093021, acc: 0.968750]  [G loss: 8.575869, acc: 0.015625]\n",
            "2273: [D loss: 0.178170, acc: 0.960938]  [G loss: 6.727446, acc: 0.078125]\n",
            "2274: [D loss: 0.063728, acc: 0.960938]  [G loss: 5.018041, acc: 0.234375]\n",
            "2275: [D loss: 0.096206, acc: 0.960938]  [G loss: 5.731380, acc: 0.156250]\n",
            "2276: [D loss: 0.096660, acc: 0.960938]  [G loss: 6.440834, acc: 0.109375]\n",
            "2277: [D loss: 0.076720, acc: 0.984375]  [G loss: 6.781245, acc: 0.078125]\n",
            "2278: [D loss: 0.011995, acc: 0.992188]  [G loss: 7.335866, acc: 0.000000]\n",
            "2279: [D loss: 0.068347, acc: 0.984375]  [G loss: 7.419258, acc: 0.015625]\n",
            "2280: [D loss: 0.032987, acc: 0.992188]  [G loss: 6.400395, acc: 0.125000]\n",
            "2281: [D loss: 0.108348, acc: 0.968750]  [G loss: 5.266755, acc: 0.093750]\n",
            "2282: [D loss: 0.101042, acc: 0.976562]  [G loss: 5.084943, acc: 0.109375]\n",
            "2283: [D loss: 0.062375, acc: 0.984375]  [G loss: 5.298258, acc: 0.062500]\n",
            "2284: [D loss: 0.019169, acc: 1.000000]  [G loss: 5.553916, acc: 0.078125]\n",
            "2285: [D loss: 0.035457, acc: 0.992188]  [G loss: 7.478122, acc: 0.015625]\n",
            "2286: [D loss: 0.032379, acc: 0.992188]  [G loss: 8.155041, acc: 0.000000]\n",
            "2287: [D loss: 0.057893, acc: 0.976562]  [G loss: 7.801980, acc: 0.015625]\n",
            "2288: [D loss: 0.087859, acc: 0.968750]  [G loss: 7.514891, acc: 0.015625]\n",
            "2289: [D loss: 0.056658, acc: 0.992188]  [G loss: 5.801349, acc: 0.031250]\n",
            "2290: [D loss: 0.048399, acc: 0.984375]  [G loss: 4.680330, acc: 0.140625]\n",
            "2291: [D loss: 0.145576, acc: 0.976562]  [G loss: 4.935783, acc: 0.062500]\n",
            "2292: [D loss: 0.120987, acc: 0.953125]  [G loss: 5.678291, acc: 0.078125]\n",
            "2293: [D loss: 0.063657, acc: 0.976562]  [G loss: 6.098888, acc: 0.031250]\n",
            "2294: [D loss: 0.037358, acc: 0.984375]  [G loss: 6.364788, acc: 0.093750]\n",
            "2295: [D loss: 0.069562, acc: 0.976562]  [G loss: 5.405411, acc: 0.171875]\n",
            "2296: [D loss: 0.107552, acc: 0.960938]  [G loss: 5.028216, acc: 0.203125]\n",
            "2297: [D loss: 0.179209, acc: 0.937500]  [G loss: 5.189273, acc: 0.187500]\n",
            "2298: [D loss: 0.264370, acc: 0.921875]  [G loss: 5.464780, acc: 0.125000]\n",
            "2299: [D loss: 0.079221, acc: 0.968750]  [G loss: 6.468953, acc: 0.109375]\n",
            "2300: [D loss: 0.071734, acc: 0.992188]  [G loss: 6.251993, acc: 0.062500]\n",
            "2301: [D loss: 0.082030, acc: 0.984375]  [G loss: 6.136354, acc: 0.062500]\n",
            "2302: [D loss: 0.122035, acc: 0.960938]  [G loss: 5.602975, acc: 0.109375]\n",
            "2303: [D loss: 0.072534, acc: 0.976562]  [G loss: 4.856119, acc: 0.031250]\n",
            "2304: [D loss: 0.058748, acc: 0.976562]  [G loss: 3.846492, acc: 0.109375]\n",
            "2305: [D loss: 0.045086, acc: 0.976562]  [G loss: 3.981798, acc: 0.156250]\n",
            "2306: [D loss: 0.043538, acc: 0.984375]  [G loss: 5.300782, acc: 0.062500]\n",
            "2307: [D loss: 0.071374, acc: 0.976562]  [G loss: 4.532763, acc: 0.046875]\n",
            "2308: [D loss: 0.053497, acc: 0.976562]  [G loss: 4.429818, acc: 0.125000]\n",
            "2309: [D loss: 0.050522, acc: 0.984375]  [G loss: 3.509858, acc: 0.203125]\n",
            "2310: [D loss: 0.130506, acc: 0.945312]  [G loss: 3.710959, acc: 0.140625]\n",
            "2311: [D loss: 0.169588, acc: 0.960938]  [G loss: 3.910609, acc: 0.156250]\n",
            "2312: [D loss: 0.162123, acc: 0.960938]  [G loss: 4.472373, acc: 0.156250]\n",
            "2313: [D loss: 0.088913, acc: 0.976562]  [G loss: 6.032737, acc: 0.046875]\n",
            "2314: [D loss: 0.061198, acc: 0.984375]  [G loss: 5.451017, acc: 0.078125]\n",
            "2315: [D loss: 0.054877, acc: 0.984375]  [G loss: 5.017421, acc: 0.078125]\n",
            "2316: [D loss: 0.085942, acc: 0.968750]  [G loss: 4.211673, acc: 0.125000]\n",
            "2317: [D loss: 0.022695, acc: 0.992188]  [G loss: 3.742826, acc: 0.140625]\n",
            "2318: [D loss: 0.048039, acc: 0.984375]  [G loss: 3.495376, acc: 0.234375]\n",
            "2319: [D loss: 0.072835, acc: 0.984375]  [G loss: 3.077893, acc: 0.187500]\n",
            "2320: [D loss: 0.143406, acc: 0.929688]  [G loss: 3.575810, acc: 0.187500]\n",
            "2321: [D loss: 0.051137, acc: 0.984375]  [G loss: 3.557828, acc: 0.203125]\n",
            "2322: [D loss: 0.138038, acc: 0.960938]  [G loss: 3.190592, acc: 0.156250]\n",
            "2323: [D loss: 0.038517, acc: 0.992188]  [G loss: 2.848550, acc: 0.234375]\n",
            "2324: [D loss: 0.049949, acc: 0.976562]  [G loss: 2.763288, acc: 0.265625]\n",
            "2325: [D loss: 0.085442, acc: 0.960938]  [G loss: 2.331836, acc: 0.234375]\n",
            "2326: [D loss: 0.087804, acc: 0.968750]  [G loss: 2.121757, acc: 0.296875]\n",
            "2327: [D loss: 0.105971, acc: 0.968750]  [G loss: 2.193340, acc: 0.265625]\n",
            "2328: [D loss: 0.136417, acc: 0.960938]  [G loss: 2.024201, acc: 0.343750]\n",
            "2329: [D loss: 0.140563, acc: 0.945312]  [G loss: 2.099105, acc: 0.343750]\n",
            "2330: [D loss: 0.103833, acc: 0.960938]  [G loss: 2.403223, acc: 0.312500]\n",
            "2331: [D loss: 0.105862, acc: 0.945312]  [G loss: 2.438079, acc: 0.328125]\n",
            "2332: [D loss: 0.083778, acc: 0.960938]  [G loss: 2.207918, acc: 0.328125]\n",
            "2333: [D loss: 0.039335, acc: 1.000000]  [G loss: 2.204202, acc: 0.359375]\n",
            "2334: [D loss: 0.051593, acc: 0.976562]  [G loss: 1.704070, acc: 0.421875]\n",
            "2335: [D loss: 0.069146, acc: 0.968750]  [G loss: 1.661076, acc: 0.468750]\n",
            "2336: [D loss: 0.058351, acc: 0.976562]  [G loss: 1.899278, acc: 0.421875]\n",
            "2337: [D loss: 0.054907, acc: 0.984375]  [G loss: 1.829158, acc: 0.500000]\n",
            "2338: [D loss: 0.028285, acc: 0.992188]  [G loss: 2.286232, acc: 0.406250]\n",
            "2339: [D loss: 0.080729, acc: 0.968750]  [G loss: 2.944271, acc: 0.312500]\n",
            "2340: [D loss: 0.054731, acc: 0.984375]  [G loss: 2.992431, acc: 0.359375]\n",
            "2341: [D loss: 0.025791, acc: 0.992188]  [G loss: 2.864450, acc: 0.265625]\n",
            "2342: [D loss: 0.028811, acc: 0.992188]  [G loss: 2.307151, acc: 0.375000]\n",
            "2343: [D loss: 0.020122, acc: 0.992188]  [G loss: 2.409451, acc: 0.328125]\n",
            "2344: [D loss: 0.028772, acc: 0.984375]  [G loss: 2.783421, acc: 0.343750]\n",
            "2345: [D loss: 0.017777, acc: 0.992188]  [G loss: 3.054798, acc: 0.250000]\n",
            "2346: [D loss: 0.034945, acc: 0.992188]  [G loss: 3.798103, acc: 0.218750]\n",
            "2347: [D loss: 0.011253, acc: 0.992188]  [G loss: 3.240945, acc: 0.281250]\n",
            "2348: [D loss: 0.043008, acc: 0.984375]  [G loss: 3.337182, acc: 0.296875]\n",
            "2349: [D loss: 0.037794, acc: 0.984375]  [G loss: 2.655383, acc: 0.312500]\n",
            "2350: [D loss: 0.012999, acc: 1.000000]  [G loss: 2.159333, acc: 0.390625]\n",
            "2351: [D loss: 0.050677, acc: 0.984375]  [G loss: 2.977417, acc: 0.375000]\n",
            "2352: [D loss: 0.021131, acc: 0.992188]  [G loss: 2.745407, acc: 0.328125]\n",
            "2353: [D loss: 0.038165, acc: 0.984375]  [G loss: 2.485057, acc: 0.312500]\n",
            "2354: [D loss: 0.025179, acc: 0.984375]  [G loss: 2.744043, acc: 0.359375]\n",
            "2355: [D loss: 0.041215, acc: 0.984375]  [G loss: 2.987184, acc: 0.296875]\n",
            "2356: [D loss: 0.018976, acc: 1.000000]  [G loss: 3.565523, acc: 0.234375]\n",
            "2357: [D loss: 0.080393, acc: 0.976562]  [G loss: 3.304350, acc: 0.187500]\n",
            "2358: [D loss: 0.039990, acc: 0.992188]  [G loss: 3.485882, acc: 0.218750]\n",
            "2359: [D loss: 0.097183, acc: 0.968750]  [G loss: 2.909186, acc: 0.328125]\n",
            "2360: [D loss: 0.062391, acc: 0.976562]  [G loss: 2.289420, acc: 0.421875]\n",
            "2361: [D loss: 0.129017, acc: 0.945312]  [G loss: 3.743775, acc: 0.296875]\n",
            "2362: [D loss: 0.125555, acc: 0.968750]  [G loss: 5.466701, acc: 0.140625]\n",
            "2363: [D loss: 0.086243, acc: 0.953125]  [G loss: 5.753057, acc: 0.078125]\n",
            "2364: [D loss: 0.242086, acc: 0.937500]  [G loss: 4.276430, acc: 0.203125]\n",
            "2365: [D loss: 0.054741, acc: 0.968750]  [G loss: 3.020715, acc: 0.281250]\n",
            "2366: [D loss: 0.166144, acc: 0.937500]  [G loss: 4.372583, acc: 0.093750]\n",
            "2367: [D loss: 0.089396, acc: 0.968750]  [G loss: 6.029199, acc: 0.062500]\n",
            "2368: [D loss: 0.063891, acc: 0.984375]  [G loss: 6.742311, acc: 0.046875]\n",
            "2369: [D loss: 0.099127, acc: 0.968750]  [G loss: 6.465452, acc: 0.015625]\n",
            "2370: [D loss: 0.116262, acc: 0.937500]  [G loss: 5.624053, acc: 0.093750]\n",
            "2371: [D loss: 0.312001, acc: 0.921875]  [G loss: 4.064360, acc: 0.234375]\n",
            "2372: [D loss: 0.139982, acc: 0.960938]  [G loss: 2.916223, acc: 0.218750]\n",
            "2373: [D loss: 0.096593, acc: 0.968750]  [G loss: 3.993319, acc: 0.125000]\n",
            "2374: [D loss: 0.128591, acc: 0.945312]  [G loss: 5.136436, acc: 0.015625]\n",
            "2375: [D loss: 0.018816, acc: 0.992188]  [G loss: 7.472937, acc: 0.078125]\n",
            "2376: [D loss: 0.074921, acc: 0.976562]  [G loss: 9.065122, acc: 0.015625]\n",
            "2377: [D loss: 0.064107, acc: 0.976562]  [G loss: 9.699707, acc: 0.031250]\n",
            "2378: [D loss: 0.287060, acc: 0.945312]  [G loss: 8.558113, acc: 0.093750]\n",
            "2379: [D loss: 0.036607, acc: 0.984375]  [G loss: 7.990979, acc: 0.078125]\n",
            "2380: [D loss: 0.107592, acc: 0.984375]  [G loss: 7.198405, acc: 0.109375]\n",
            "2381: [D loss: 0.087777, acc: 0.976562]  [G loss: 7.141139, acc: 0.109375]\n",
            "2382: [D loss: 0.076362, acc: 0.968750]  [G loss: 7.342630, acc: 0.140625]\n",
            "2383: [D loss: 0.030328, acc: 0.992188]  [G loss: 8.437233, acc: 0.046875]\n",
            "2384: [D loss: 0.225648, acc: 0.960938]  [G loss: 7.067318, acc: 0.078125]\n",
            "2385: [D loss: 0.088567, acc: 0.976562]  [G loss: 6.584158, acc: 0.062500]\n",
            "2386: [D loss: 0.290503, acc: 0.937500]  [G loss: 5.735450, acc: 0.093750]\n",
            "2387: [D loss: 0.189092, acc: 0.937500]  [G loss: 6.949415, acc: 0.062500]\n",
            "2388: [D loss: 0.069023, acc: 0.968750]  [G loss: 7.086341, acc: 0.031250]\n",
            "2389: [D loss: 0.060294, acc: 0.968750]  [G loss: 7.196005, acc: 0.015625]\n",
            "2390: [D loss: 0.192732, acc: 0.921875]  [G loss: 5.694919, acc: 0.062500]\n",
            "2391: [D loss: 0.138470, acc: 0.960938]  [G loss: 4.146372, acc: 0.140625]\n",
            "2392: [D loss: 0.096215, acc: 0.960938]  [G loss: 3.331162, acc: 0.234375]\n",
            "2393: [D loss: 0.170174, acc: 0.929688]  [G loss: 3.656726, acc: 0.171875]\n",
            "2394: [D loss: 0.077149, acc: 0.953125]  [G loss: 4.246418, acc: 0.125000]\n",
            "2395: [D loss: 0.172524, acc: 0.968750]  [G loss: 4.404811, acc: 0.078125]\n",
            "2396: [D loss: 0.138259, acc: 0.976562]  [G loss: 3.932319, acc: 0.109375]\n",
            "2397: [D loss: 0.210755, acc: 0.960938]  [G loss: 2.958615, acc: 0.234375]\n",
            "2398: [D loss: 0.095725, acc: 0.984375]  [G loss: 1.993381, acc: 0.421875]\n",
            "2399: [D loss: 0.075176, acc: 0.976562]  [G loss: 1.869382, acc: 0.265625]\n",
            "2400: [D loss: 0.066749, acc: 0.976562]  [G loss: 1.434886, acc: 0.484375]\n",
            "2401: [D loss: 0.131249, acc: 0.937500]  [G loss: 1.860307, acc: 0.343750]\n",
            "2402: [D loss: 0.150333, acc: 0.968750]  [G loss: 1.892553, acc: 0.312500]\n",
            "2403: [D loss: 0.108900, acc: 0.976562]  [G loss: 1.870096, acc: 0.375000]\n",
            "2404: [D loss: 0.106822, acc: 0.968750]  [G loss: 2.394217, acc: 0.281250]\n",
            "2405: [D loss: 0.095067, acc: 0.953125]  [G loss: 2.705669, acc: 0.250000]\n",
            "2406: [D loss: 0.150695, acc: 0.960938]  [G loss: 2.847646, acc: 0.281250]\n",
            "2407: [D loss: 0.043096, acc: 0.992188]  [G loss: 2.754274, acc: 0.328125]\n",
            "2408: [D loss: 0.058630, acc: 0.992188]  [G loss: 2.658079, acc: 0.281250]\n",
            "2409: [D loss: 0.086740, acc: 0.960938]  [G loss: 2.284033, acc: 0.468750]\n",
            "2410: [D loss: 0.054471, acc: 0.976562]  [G loss: 2.276718, acc: 0.375000]\n",
            "2411: [D loss: 0.026686, acc: 0.992188]  [G loss: 1.978248, acc: 0.406250]\n",
            "2412: [D loss: 0.043840, acc: 0.984375]  [G loss: 2.105311, acc: 0.390625]\n",
            "2413: [D loss: 0.057536, acc: 0.992188]  [G loss: 1.826852, acc: 0.437500]\n",
            "2414: [D loss: 0.059070, acc: 0.976562]  [G loss: 2.505028, acc: 0.375000]\n",
            "2415: [D loss: 0.044453, acc: 0.984375]  [G loss: 3.292341, acc: 0.234375]\n",
            "2416: [D loss: 0.025887, acc: 0.992188]  [G loss: 3.560965, acc: 0.156250]\n",
            "2417: [D loss: 0.030299, acc: 0.984375]  [G loss: 3.215198, acc: 0.218750]\n",
            "2418: [D loss: 0.042686, acc: 0.984375]  [G loss: 3.191545, acc: 0.218750]\n",
            "2419: [D loss: 0.102236, acc: 0.960938]  [G loss: 2.345726, acc: 0.296875]\n",
            "2420: [D loss: 0.035126, acc: 0.992188]  [G loss: 1.115135, acc: 0.609375]\n",
            "2421: [D loss: 0.119452, acc: 0.953125]  [G loss: 1.554042, acc: 0.421875]\n",
            "2422: [D loss: 0.118986, acc: 0.937500]  [G loss: 2.168502, acc: 0.328125]\n",
            "2423: [D loss: 0.020876, acc: 1.000000]  [G loss: 2.822418, acc: 0.140625]\n",
            "2424: [D loss: 0.191309, acc: 0.953125]  [G loss: 3.383806, acc: 0.156250]\n",
            "2425: [D loss: 0.135199, acc: 0.960938]  [G loss: 3.046936, acc: 0.234375]\n",
            "2426: [D loss: 0.179771, acc: 0.953125]  [G loss: 2.368400, acc: 0.359375]\n",
            "2427: [D loss: 0.081239, acc: 0.976562]  [G loss: 2.043278, acc: 0.468750]\n",
            "2428: [D loss: 0.109688, acc: 0.953125]  [G loss: 2.560585, acc: 0.312500]\n",
            "2429: [D loss: 0.093957, acc: 0.984375]  [G loss: 2.727424, acc: 0.328125]\n",
            "2430: [D loss: 0.111480, acc: 0.976562]  [G loss: 3.164996, acc: 0.265625]\n",
            "2431: [D loss: 0.049141, acc: 0.968750]  [G loss: 3.795127, acc: 0.218750]\n",
            "2432: [D loss: 0.017200, acc: 0.992188]  [G loss: 5.112540, acc: 0.109375]\n",
            "2433: [D loss: 0.144780, acc: 0.929688]  [G loss: 4.398055, acc: 0.109375]\n",
            "2434: [D loss: 0.062413, acc: 0.984375]  [G loss: 3.711056, acc: 0.156250]\n",
            "2435: [D loss: 0.066668, acc: 0.976562]  [G loss: 3.475583, acc: 0.203125]\n",
            "2436: [D loss: 0.086733, acc: 0.960938]  [G loss: 3.357176, acc: 0.250000]\n",
            "2437: [D loss: 0.042435, acc: 0.984375]  [G loss: 3.578946, acc: 0.328125]\n",
            "2438: [D loss: 0.086395, acc: 0.976562]  [G loss: 4.474243, acc: 0.109375]\n",
            "2439: [D loss: 0.099152, acc: 0.968750]  [G loss: 4.350035, acc: 0.140625]\n",
            "2440: [D loss: 0.037786, acc: 0.984375]  [G loss: 4.533384, acc: 0.093750]\n",
            "2441: [D loss: 0.128675, acc: 0.968750]  [G loss: 3.810298, acc: 0.140625]\n",
            "2442: [D loss: 0.112687, acc: 0.976562]  [G loss: 3.255682, acc: 0.156250]\n",
            "2443: [D loss: 0.127404, acc: 0.976562]  [G loss: 2.836101, acc: 0.203125]\n",
            "2444: [D loss: 0.063769, acc: 0.976562]  [G loss: 3.071343, acc: 0.125000]\n",
            "2445: [D loss: 0.042824, acc: 0.984375]  [G loss: 3.597835, acc: 0.125000]\n",
            "2446: [D loss: 0.069046, acc: 0.976562]  [G loss: 3.055392, acc: 0.125000]\n",
            "2447: [D loss: 0.085873, acc: 0.968750]  [G loss: 3.444709, acc: 0.078125]\n",
            "2448: [D loss: 0.044098, acc: 0.976562]  [G loss: 3.977340, acc: 0.062500]\n",
            "2449: [D loss: 0.062868, acc: 0.976562]  [G loss: 4.636494, acc: 0.015625]\n",
            "2450: [D loss: 0.061098, acc: 0.976562]  [G loss: 3.797575, acc: 0.078125]\n",
            "2451: [D loss: 0.058149, acc: 0.984375]  [G loss: 3.256231, acc: 0.125000]\n",
            "2452: [D loss: 0.047014, acc: 0.976562]  [G loss: 3.688775, acc: 0.140625]\n",
            "2453: [D loss: 0.020202, acc: 1.000000]  [G loss: 4.252752, acc: 0.109375]\n",
            "2454: [D loss: 0.077939, acc: 0.968750]  [G loss: 3.523227, acc: 0.187500]\n",
            "2455: [D loss: 0.040801, acc: 0.984375]  [G loss: 3.412543, acc: 0.171875]\n",
            "2456: [D loss: 0.066556, acc: 0.968750]  [G loss: 4.169405, acc: 0.093750]\n",
            "2457: [D loss: 0.062595, acc: 0.976562]  [G loss: 4.919232, acc: 0.078125]\n",
            "2458: [D loss: 0.005984, acc: 1.000000]  [G loss: 4.913874, acc: 0.109375]\n",
            "2459: [D loss: 0.041607, acc: 0.984375]  [G loss: 5.611969, acc: 0.109375]\n",
            "2460: [D loss: 0.075506, acc: 0.984375]  [G loss: 4.643080, acc: 0.125000]\n",
            "2461: [D loss: 0.078840, acc: 0.968750]  [G loss: 4.912875, acc: 0.062500]\n",
            "2462: [D loss: 0.019438, acc: 0.992188]  [G loss: 3.030745, acc: 0.281250]\n",
            "2463: [D loss: 0.033966, acc: 0.992188]  [G loss: 3.174742, acc: 0.281250]\n",
            "2464: [D loss: 0.074887, acc: 0.976562]  [G loss: 2.776895, acc: 0.359375]\n",
            "2465: [D loss: 0.090532, acc: 0.968750]  [G loss: 4.871861, acc: 0.078125]\n",
            "2466: [D loss: 0.072377, acc: 0.976562]  [G loss: 6.288548, acc: 0.015625]\n",
            "2467: [D loss: 0.014731, acc: 1.000000]  [G loss: 6.791299, acc: 0.015625]\n",
            "2468: [D loss: 0.110330, acc: 0.976562]  [G loss: 6.680861, acc: 0.015625]\n",
            "2469: [D loss: 0.071201, acc: 0.976562]  [G loss: 5.940874, acc: 0.031250]\n",
            "2470: [D loss: 0.094450, acc: 0.976562]  [G loss: 4.359534, acc: 0.171875]\n",
            "2471: [D loss: 0.126646, acc: 0.968750]  [G loss: 3.645611, acc: 0.203125]\n",
            "2472: [D loss: 0.095546, acc: 0.976562]  [G loss: 3.130516, acc: 0.265625]\n",
            "2473: [D loss: 0.190034, acc: 0.914062]  [G loss: 4.758135, acc: 0.031250]\n",
            "2474: [D loss: 0.048612, acc: 0.984375]  [G loss: 5.309724, acc: 0.046875]\n",
            "2475: [D loss: 0.013220, acc: 1.000000]  [G loss: 6.431463, acc: 0.031250]\n",
            "2476: [D loss: 0.027120, acc: 0.984375]  [G loss: 6.472101, acc: 0.015625]\n",
            "2477: [D loss: 0.041088, acc: 0.976562]  [G loss: 5.309789, acc: 0.046875]\n",
            "2478: [D loss: 0.029444, acc: 0.984375]  [G loss: 4.628118, acc: 0.093750]\n",
            "2479: [D loss: 0.030406, acc: 0.984375]  [G loss: 3.265789, acc: 0.234375]\n",
            "2480: [D loss: 0.025991, acc: 0.992188]  [G loss: 2.799871, acc: 0.265625]\n",
            "2481: [D loss: 0.042756, acc: 0.984375]  [G loss: 3.600571, acc: 0.218750]\n",
            "2482: [D loss: 0.078946, acc: 0.968750]  [G loss: 3.754821, acc: 0.140625]\n",
            "2483: [D loss: 0.028128, acc: 0.992188]  [G loss: 4.214315, acc: 0.156250]\n",
            "2484: [D loss: 0.004265, acc: 1.000000]  [G loss: 4.674103, acc: 0.156250]\n",
            "2485: [D loss: 0.004524, acc: 1.000000]  [G loss: 4.980302, acc: 0.125000]\n",
            "2486: [D loss: 0.011688, acc: 1.000000]  [G loss: 5.334665, acc: 0.140625]\n",
            "2487: [D loss: 0.042029, acc: 0.984375]  [G loss: 4.429075, acc: 0.234375]\n",
            "2488: [D loss: 0.005021, acc: 1.000000]  [G loss: 4.896060, acc: 0.109375]\n",
            "2489: [D loss: 0.015986, acc: 0.992188]  [G loss: 5.088040, acc: 0.218750]\n",
            "2490: [D loss: 0.004360, acc: 1.000000]  [G loss: 4.032819, acc: 0.281250]\n",
            "2491: [D loss: 0.018764, acc: 0.984375]  [G loss: 2.751265, acc: 0.437500]\n",
            "2492: [D loss: 0.008063, acc: 1.000000]  [G loss: 2.997160, acc: 0.468750]\n",
            "2493: [D loss: 0.055376, acc: 0.984375]  [G loss: 2.763143, acc: 0.406250]\n",
            "2494: [D loss: 0.063732, acc: 0.976562]  [G loss: 2.791985, acc: 0.453125]\n",
            "2495: [D loss: 0.015149, acc: 0.992188]  [G loss: 3.976883, acc: 0.250000]\n",
            "2496: [D loss: 0.001959, acc: 1.000000]  [G loss: 4.308513, acc: 0.250000]\n",
            "2497: [D loss: 0.048247, acc: 0.992188]  [G loss: 4.951322, acc: 0.171875]\n",
            "2498: [D loss: 0.002579, acc: 1.000000]  [G loss: 5.405993, acc: 0.171875]\n",
            "2499: [D loss: 0.077572, acc: 0.968750]  [G loss: 4.580244, acc: 0.234375]\n",
            "2500: [D loss: 0.033257, acc: 0.976562]  [G loss: 3.019772, acc: 0.453125]\n",
            "2501: [D loss: 0.006864, acc: 1.000000]  [G loss: 2.040079, acc: 0.500000]\n",
            "2502: [D loss: 0.013966, acc: 0.992188]  [G loss: 1.898847, acc: 0.546875]\n",
            "2503: [D loss: 0.002792, acc: 1.000000]  [G loss: 1.289836, acc: 0.593750]\n",
            "2504: [D loss: 0.008175, acc: 1.000000]  [G loss: 1.072769, acc: 0.671875]\n",
            "2505: [D loss: 0.036692, acc: 0.992188]  [G loss: 1.556525, acc: 0.546875]\n",
            "2506: [D loss: 0.016854, acc: 0.992188]  [G loss: 2.182750, acc: 0.468750]\n",
            "2507: [D loss: 0.018045, acc: 0.992188]  [G loss: 2.733783, acc: 0.390625]\n",
            "2508: [D loss: 0.009014, acc: 1.000000]  [G loss: 3.219481, acc: 0.296875]\n",
            "2509: [D loss: 0.045788, acc: 0.984375]  [G loss: 3.425317, acc: 0.296875]\n",
            "2510: [D loss: 0.001982, acc: 1.000000]  [G loss: 3.854626, acc: 0.312500]\n",
            "2511: [D loss: 0.021542, acc: 0.984375]  [G loss: 4.133251, acc: 0.218750]\n",
            "2512: [D loss: 0.038531, acc: 0.992188]  [G loss: 3.072175, acc: 0.328125]\n",
            "2513: [D loss: 0.008019, acc: 0.992188]  [G loss: 1.912716, acc: 0.468750]\n",
            "2514: [D loss: 0.012118, acc: 1.000000]  [G loss: 1.993613, acc: 0.453125]\n",
            "2515: [D loss: 0.012558, acc: 1.000000]  [G loss: 2.115762, acc: 0.500000]\n",
            "2516: [D loss: 0.023513, acc: 0.992188]  [G loss: 2.187943, acc: 0.390625]\n",
            "2517: [D loss: 0.053406, acc: 0.984375]  [G loss: 2.325981, acc: 0.328125]\n",
            "2518: [D loss: 0.048008, acc: 0.984375]  [G loss: 3.496558, acc: 0.234375]\n",
            "2519: [D loss: 0.111620, acc: 0.976562]  [G loss: 2.771506, acc: 0.359375]\n",
            "2520: [D loss: 0.060795, acc: 0.976562]  [G loss: 1.909821, acc: 0.468750]\n",
            "2521: [D loss: 0.017595, acc: 1.000000]  [G loss: 1.685292, acc: 0.609375]\n",
            "2522: [D loss: 0.041722, acc: 0.984375]  [G loss: 1.135517, acc: 0.656250]\n",
            "2523: [D loss: 0.088469, acc: 0.960938]  [G loss: 2.204544, acc: 0.359375]\n",
            "2524: [D loss: 0.048783, acc: 0.984375]  [G loss: 3.641840, acc: 0.234375]\n",
            "2525: [D loss: 0.238173, acc: 0.937500]  [G loss: 1.932354, acc: 0.453125]\n",
            "2526: [D loss: 0.073561, acc: 0.960938]  [G loss: 0.874732, acc: 0.703125]\n",
            "2527: [D loss: 0.231429, acc: 0.898438]  [G loss: 2.740635, acc: 0.281250]\n",
            "2528: [D loss: 0.058292, acc: 0.976562]  [G loss: 5.807936, acc: 0.031250]\n",
            "2529: [D loss: 0.301511, acc: 0.937500]  [G loss: 5.860288, acc: 0.046875]\n",
            "2530: [D loss: 0.121349, acc: 0.968750]  [G loss: 3.482428, acc: 0.203125]\n",
            "2531: [D loss: 0.085699, acc: 0.968750]  [G loss: 1.168230, acc: 0.687500]\n",
            "2532: [D loss: 0.061294, acc: 0.976562]  [G loss: 0.642593, acc: 0.796875]\n",
            "2533: [D loss: 0.170698, acc: 0.921875]  [G loss: 2.227613, acc: 0.468750]\n",
            "2534: [D loss: 0.027512, acc: 0.984375]  [G loss: 4.499402, acc: 0.093750]\n",
            "2535: [D loss: 0.027335, acc: 0.992188]  [G loss: 5.960558, acc: 0.062500]\n",
            "2536: [D loss: 0.197217, acc: 0.953125]  [G loss: 5.417629, acc: 0.062500]\n",
            "2537: [D loss: 0.090087, acc: 0.992188]  [G loss: 5.560420, acc: 0.125000]\n",
            "2538: [D loss: 0.082409, acc: 0.984375]  [G loss: 3.905879, acc: 0.218750]\n",
            "2539: [D loss: 0.015006, acc: 0.992188]  [G loss: 2.255831, acc: 0.515625]\n",
            "2540: [D loss: 0.003825, acc: 1.000000]  [G loss: 1.463692, acc: 0.515625]\n",
            "2541: [D loss: 0.015664, acc: 0.992188]  [G loss: 1.178313, acc: 0.640625]\n",
            "2542: [D loss: 0.071045, acc: 0.976562]  [G loss: 0.876489, acc: 0.671875]\n",
            "2543: [D loss: 0.030549, acc: 0.984375]  [G loss: 0.857143, acc: 0.687500]\n",
            "2544: [D loss: 0.016132, acc: 1.000000]  [G loss: 0.602852, acc: 0.703125]\n",
            "2545: [D loss: 0.005205, acc: 1.000000]  [G loss: 0.981585, acc: 0.671875]\n",
            "2546: [D loss: 0.022195, acc: 0.992188]  [G loss: 1.125980, acc: 0.531250]\n",
            "2547: [D loss: 0.019695, acc: 0.992188]  [G loss: 0.966695, acc: 0.671875]\n",
            "2548: [D loss: 0.005403, acc: 1.000000]  [G loss: 1.098938, acc: 0.546875]\n",
            "2549: [D loss: 0.014329, acc: 1.000000]  [G loss: 0.907444, acc: 0.671875]\n",
            "2550: [D loss: 0.063934, acc: 0.976562]  [G loss: 0.869314, acc: 0.656250]\n",
            "2551: [D loss: 0.029525, acc: 0.984375]  [G loss: 0.708122, acc: 0.765625]\n",
            "2552: [D loss: 0.023055, acc: 0.992188]  [G loss: 0.814652, acc: 0.750000]\n",
            "2553: [D loss: 0.013807, acc: 0.992188]  [G loss: 0.650814, acc: 0.796875]\n",
            "2554: [D loss: 0.093633, acc: 0.960938]  [G loss: 0.517162, acc: 0.781250]\n",
            "2555: [D loss: 0.026378, acc: 0.992188]  [G loss: 0.283746, acc: 0.921875]\n",
            "2556: [D loss: 0.031428, acc: 0.984375]  [G loss: 0.470315, acc: 0.859375]\n",
            "2557: [D loss: 0.017703, acc: 0.992188]  [G loss: 0.696895, acc: 0.828125]\n",
            "2558: [D loss: 0.091051, acc: 0.984375]  [G loss: 0.497907, acc: 0.843750]\n",
            "2559: [D loss: 0.031463, acc: 0.992188]  [G loss: 0.666083, acc: 0.843750]\n",
            "2560: [D loss: 0.083954, acc: 0.976562]  [G loss: 0.799729, acc: 0.796875]\n",
            "2561: [D loss: 0.052032, acc: 0.976562]  [G loss: 0.637604, acc: 0.734375]\n",
            "2562: [D loss: 0.039770, acc: 0.984375]  [G loss: 1.006404, acc: 0.796875]\n",
            "2563: [D loss: 0.016798, acc: 1.000000]  [G loss: 0.763364, acc: 0.765625]\n",
            "2564: [D loss: 0.068231, acc: 0.968750]  [G loss: 0.477101, acc: 0.875000]\n",
            "2565: [D loss: 0.052865, acc: 0.976562]  [G loss: 1.272033, acc: 0.609375]\n",
            "2566: [D loss: 0.068329, acc: 0.976562]  [G loss: 1.773007, acc: 0.437500]\n",
            "2567: [D loss: 0.095469, acc: 0.976562]  [G loss: 1.591501, acc: 0.515625]\n",
            "2568: [D loss: 0.084375, acc: 0.976562]  [G loss: 1.449181, acc: 0.531250]\n",
            "2569: [D loss: 0.011028, acc: 1.000000]  [G loss: 1.335616, acc: 0.609375]\n",
            "2570: [D loss: 0.073872, acc: 0.984375]  [G loss: 0.721321, acc: 0.718750]\n",
            "2571: [D loss: 0.071037, acc: 0.968750]  [G loss: 1.326191, acc: 0.562500]\n",
            "2572: [D loss: 0.055500, acc: 0.976562]  [G loss: 1.763397, acc: 0.437500]\n",
            "2573: [D loss: 0.103584, acc: 0.968750]  [G loss: 1.817966, acc: 0.500000]\n",
            "2574: [D loss: 0.026407, acc: 0.992188]  [G loss: 1.315450, acc: 0.609375]\n",
            "2575: [D loss: 0.044092, acc: 0.984375]  [G loss: 1.792196, acc: 0.500000]\n",
            "2576: [D loss: 0.025055, acc: 1.000000]  [G loss: 2.503999, acc: 0.406250]\n",
            "2577: [D loss: 0.051592, acc: 0.976562]  [G loss: 2.533114, acc: 0.281250]\n",
            "2578: [D loss: 0.036227, acc: 0.984375]  [G loss: 2.541523, acc: 0.437500]\n",
            "2579: [D loss: 0.046280, acc: 0.984375]  [G loss: 1.915302, acc: 0.500000]\n",
            "2580: [D loss: 0.031187, acc: 0.992188]  [G loss: 2.720363, acc: 0.406250]\n",
            "2581: [D loss: 0.048475, acc: 0.984375]  [G loss: 3.683430, acc: 0.218750]\n",
            "2582: [D loss: 0.022987, acc: 0.992188]  [G loss: 3.917401, acc: 0.171875]\n",
            "2583: [D loss: 0.051253, acc: 0.976562]  [G loss: 4.391113, acc: 0.140625]\n",
            "2584: [D loss: 0.042215, acc: 0.992188]  [G loss: 3.879255, acc: 0.140625]\n",
            "2585: [D loss: 0.009324, acc: 1.000000]  [G loss: 3.934845, acc: 0.109375]\n",
            "2586: [D loss: 0.019026, acc: 1.000000]  [G loss: 4.088856, acc: 0.125000]\n",
            "2587: [D loss: 0.011415, acc: 1.000000]  [G loss: 4.693744, acc: 0.078125]\n",
            "2588: [D loss: 0.047615, acc: 0.992188]  [G loss: 4.295321, acc: 0.046875]\n",
            "2589: [D loss: 0.048458, acc: 0.984375]  [G loss: 4.266918, acc: 0.109375]\n",
            "2590: [D loss: 0.011262, acc: 1.000000]  [G loss: 4.568178, acc: 0.031250]\n",
            "2591: [D loss: 0.032731, acc: 0.992188]  [G loss: 5.213102, acc: 0.078125]\n",
            "2592: [D loss: 0.021661, acc: 0.992188]  [G loss: 4.489759, acc: 0.046875]\n",
            "2593: [D loss: 0.025666, acc: 0.984375]  [G loss: 4.151887, acc: 0.078125]\n",
            "2594: [D loss: 0.085923, acc: 0.960938]  [G loss: 3.878085, acc: 0.125000]\n",
            "2595: [D loss: 0.035913, acc: 0.992188]  [G loss: 4.693609, acc: 0.046875]\n",
            "2596: [D loss: 0.021762, acc: 0.992188]  [G loss: 5.271323, acc: 0.078125]\n",
            "2597: [D loss: 0.009774, acc: 0.992188]  [G loss: 5.551218, acc: 0.046875]\n",
            "2598: [D loss: 0.066555, acc: 0.968750]  [G loss: 4.829860, acc: 0.109375]\n",
            "2599: [D loss: 0.024340, acc: 0.992188]  [G loss: 4.267676, acc: 0.187500]\n",
            "2600: [D loss: 0.040952, acc: 0.984375]  [G loss: 5.091552, acc: 0.125000]\n",
            "2601: [D loss: 0.056417, acc: 0.992188]  [G loss: 6.368594, acc: 0.093750]\n",
            "2602: [D loss: 0.067355, acc: 0.992188]  [G loss: 5.007707, acc: 0.140625]\n",
            "2603: [D loss: 0.035319, acc: 0.984375]  [G loss: 5.470918, acc: 0.125000]\n",
            "2604: [D loss: 0.029223, acc: 0.992188]  [G loss: 5.986234, acc: 0.140625]\n",
            "2605: [D loss: 0.052462, acc: 0.992188]  [G loss: 6.551481, acc: 0.171875]\n",
            "2606: [D loss: 0.005739, acc: 1.000000]  [G loss: 6.511308, acc: 0.140625]\n",
            "2607: [D loss: 0.076413, acc: 0.976562]  [G loss: 5.315420, acc: 0.234375]\n",
            "2608: [D loss: 0.075246, acc: 0.960938]  [G loss: 4.219071, acc: 0.312500]\n",
            "2609: [D loss: 0.064988, acc: 0.992188]  [G loss: 3.831165, acc: 0.359375]\n",
            "2610: [D loss: 0.102480, acc: 0.968750]  [G loss: 4.067013, acc: 0.343750]\n",
            "2611: [D loss: 0.031393, acc: 0.976562]  [G loss: 4.827886, acc: 0.156250]\n",
            "2612: [D loss: 0.076339, acc: 0.968750]  [G loss: 4.718581, acc: 0.171875]\n",
            "2613: [D loss: 0.072475, acc: 0.968750]  [G loss: 4.486989, acc: 0.187500]\n",
            "2614: [D loss: 0.123423, acc: 0.968750]  [G loss: 3.917156, acc: 0.171875]\n",
            "2615: [D loss: 0.040859, acc: 0.984375]  [G loss: 4.827099, acc: 0.156250]\n",
            "2616: [D loss: 0.018553, acc: 0.992188]  [G loss: 6.496235, acc: 0.062500]\n",
            "2617: [D loss: 0.106031, acc: 0.968750]  [G loss: 5.747832, acc: 0.078125]\n",
            "2618: [D loss: 0.233330, acc: 0.929688]  [G loss: 3.474548, acc: 0.250000]\n",
            "2619: [D loss: 0.203006, acc: 0.937500]  [G loss: 4.413741, acc: 0.156250]\n",
            "2620: [D loss: 0.076571, acc: 0.984375]  [G loss: 6.726004, acc: 0.062500]\n",
            "2621: [D loss: 0.079921, acc: 0.984375]  [G loss: 6.850586, acc: 0.062500]\n",
            "2622: [D loss: 0.103511, acc: 0.968750]  [G loss: 5.964649, acc: 0.140625]\n",
            "2623: [D loss: 0.130860, acc: 0.953125]  [G loss: 5.352117, acc: 0.140625]\n",
            "2624: [D loss: 0.227859, acc: 0.945312]  [G loss: 4.712471, acc: 0.187500]\n",
            "2625: [D loss: 0.141335, acc: 0.960938]  [G loss: 5.650538, acc: 0.156250]\n",
            "2626: [D loss: 0.204461, acc: 0.937500]  [G loss: 6.749924, acc: 0.125000]\n",
            "2627: [D loss: 0.135388, acc: 0.953125]  [G loss: 5.562148, acc: 0.125000]\n",
            "2628: [D loss: 0.148409, acc: 0.929688]  [G loss: 4.285762, acc: 0.281250]\n",
            "2629: [D loss: 0.193538, acc: 0.937500]  [G loss: 4.448741, acc: 0.265625]\n",
            "2630: [D loss: 0.126451, acc: 0.953125]  [G loss: 5.938798, acc: 0.125000]\n",
            "2631: [D loss: 0.218226, acc: 0.937500]  [G loss: 5.385387, acc: 0.171875]\n",
            "2632: [D loss: 0.055654, acc: 0.976562]  [G loss: 4.371812, acc: 0.328125]\n",
            "2633: [D loss: 0.146021, acc: 0.945312]  [G loss: 3.365548, acc: 0.406250]\n",
            "2634: [D loss: 0.087155, acc: 0.976562]  [G loss: 4.082314, acc: 0.250000]\n",
            "2635: [D loss: 0.036924, acc: 0.976562]  [G loss: 4.695291, acc: 0.265625]\n",
            "2636: [D loss: 0.109090, acc: 0.968750]  [G loss: 5.676620, acc: 0.265625]\n",
            "2637: [D loss: 0.063817, acc: 0.960938]  [G loss: 4.980724, acc: 0.250000]\n",
            "2638: [D loss: 0.063127, acc: 0.968750]  [G loss: 3.486748, acc: 0.343750]\n",
            "2639: [D loss: 0.104634, acc: 0.953125]  [G loss: 2.838227, acc: 0.390625]\n",
            "2640: [D loss: 0.061561, acc: 0.984375]  [G loss: 3.619659, acc: 0.328125]\n",
            "2641: [D loss: 0.063561, acc: 0.976562]  [G loss: 5.686926, acc: 0.156250]\n",
            "2642: [D loss: 0.041776, acc: 0.984375]  [G loss: 6.818480, acc: 0.140625]\n",
            "2643: [D loss: 0.131096, acc: 0.960938]  [G loss: 6.145767, acc: 0.234375]\n",
            "2644: [D loss: 0.075881, acc: 0.968750]  [G loss: 4.773854, acc: 0.265625]\n",
            "2645: [D loss: 0.024374, acc: 0.992188]  [G loss: 3.064712, acc: 0.421875]\n",
            "2646: [D loss: 0.060199, acc: 0.976562]  [G loss: 3.226775, acc: 0.406250]\n",
            "2647: [D loss: 0.040606, acc: 0.984375]  [G loss: 3.974939, acc: 0.250000]\n",
            "2648: [D loss: 0.024204, acc: 0.984375]  [G loss: 4.602498, acc: 0.218750]\n",
            "2649: [D loss: 0.037414, acc: 0.992188]  [G loss: 5.574226, acc: 0.187500]\n",
            "2650: [D loss: 0.068733, acc: 0.976562]  [G loss: 5.245370, acc: 0.156250]\n",
            "2651: [D loss: 0.017329, acc: 0.992188]  [G loss: 3.835495, acc: 0.296875]\n",
            "2652: [D loss: 0.034637, acc: 0.992188]  [G loss: 2.193713, acc: 0.437500]\n",
            "2653: [D loss: 0.155084, acc: 0.945312]  [G loss: 4.242932, acc: 0.265625]\n",
            "2654: [D loss: 0.027100, acc: 0.992188]  [G loss: 6.127257, acc: 0.140625]\n",
            "2655: [D loss: 0.124101, acc: 0.953125]  [G loss: 6.300708, acc: 0.140625]\n",
            "2656: [D loss: 0.126949, acc: 0.945312]  [G loss: 3.557329, acc: 0.328125]\n",
            "2657: [D loss: 0.125175, acc: 0.929688]  [G loss: 4.208818, acc: 0.203125]\n",
            "2658: [D loss: 0.037712, acc: 0.992188]  [G loss: 4.432097, acc: 0.203125]\n",
            "2659: [D loss: 0.115831, acc: 0.976562]  [G loss: 4.628372, acc: 0.171875]\n",
            "2660: [D loss: 0.074858, acc: 0.976562]  [G loss: 3.392666, acc: 0.250000]\n",
            "2661: [D loss: 0.049470, acc: 0.984375]  [G loss: 3.164253, acc: 0.328125]\n",
            "2662: [D loss: 0.094759, acc: 0.968750]  [G loss: 5.792787, acc: 0.031250]\n",
            "2663: [D loss: 0.079423, acc: 0.976562]  [G loss: 6.180774, acc: 0.093750]\n",
            "2664: [D loss: 0.041060, acc: 0.984375]  [G loss: 5.815156, acc: 0.093750]\n",
            "2665: [D loss: 0.183836, acc: 0.937500]  [G loss: 3.099995, acc: 0.281250]\n",
            "2666: [D loss: 0.101661, acc: 0.945312]  [G loss: 2.797435, acc: 0.312500]\n",
            "2667: [D loss: 0.188811, acc: 0.906250]  [G loss: 5.242771, acc: 0.078125]\n",
            "2668: [D loss: 0.214619, acc: 0.929688]  [G loss: 5.364542, acc: 0.109375]\n",
            "2669: [D loss: 0.104430, acc: 0.960938]  [G loss: 4.303048, acc: 0.171875]\n",
            "2670: [D loss: 0.119445, acc: 0.937500]  [G loss: 3.081534, acc: 0.218750]\n",
            "2671: [D loss: 0.141615, acc: 0.953125]  [G loss: 2.408364, acc: 0.343750]\n",
            "2672: [D loss: 0.278857, acc: 0.914062]  [G loss: 5.070059, acc: 0.109375]\n",
            "2673: [D loss: 0.245545, acc: 0.921875]  [G loss: 7.411012, acc: 0.046875]\n",
            "2674: [D loss: 0.381223, acc: 0.867188]  [G loss: 5.505077, acc: 0.078125]\n",
            "2675: [D loss: 0.259970, acc: 0.898438]  [G loss: 2.561172, acc: 0.375000]\n",
            "2676: [D loss: 0.298694, acc: 0.882812]  [G loss: 1.584017, acc: 0.421875]\n",
            "2677: [D loss: 0.418420, acc: 0.875000]  [G loss: 3.887545, acc: 0.109375]\n",
            "2678: [D loss: 0.060345, acc: 0.992188]  [G loss: 6.233165, acc: 0.015625]\n",
            "2679: [D loss: 0.082987, acc: 0.960938]  [G loss: 6.413761, acc: 0.015625]\n",
            "2680: [D loss: 0.307589, acc: 0.898438]  [G loss: 6.094386, acc: 0.046875]\n",
            "2681: [D loss: 0.173706, acc: 0.945312]  [G loss: 4.318769, acc: 0.125000]\n",
            "2682: [D loss: 0.040674, acc: 0.992188]  [G loss: 3.339138, acc: 0.234375]\n",
            "2683: [D loss: 0.089425, acc: 0.953125]  [G loss: 2.397271, acc: 0.343750]\n",
            "2684: [D loss: 0.165227, acc: 0.945312]  [G loss: 2.779626, acc: 0.296875]\n",
            "2685: [D loss: 0.121320, acc: 0.937500]  [G loss: 2.809738, acc: 0.218750]\n",
            "2686: [D loss: 0.070010, acc: 0.968750]  [G loss: 3.600944, acc: 0.109375]\n",
            "2687: [D loss: 0.060053, acc: 0.992188]  [G loss: 4.593845, acc: 0.156250]\n",
            "2688: [D loss: 0.159835, acc: 0.945312]  [G loss: 4.226130, acc: 0.109375]\n",
            "2689: [D loss: 0.183926, acc: 0.945312]  [G loss: 4.617584, acc: 0.140625]\n",
            "2690: [D loss: 0.161552, acc: 0.960938]  [G loss: 2.854728, acc: 0.343750]\n",
            "2691: [D loss: 0.037536, acc: 0.984375]  [G loss: 2.645357, acc: 0.406250]\n",
            "2692: [D loss: 0.071257, acc: 0.976562]  [G loss: 2.307263, acc: 0.468750]\n",
            "2693: [D loss: 0.128696, acc: 0.953125]  [G loss: 1.883898, acc: 0.468750]\n",
            "2694: [D loss: 0.115251, acc: 0.953125]  [G loss: 3.063675, acc: 0.390625]\n",
            "2695: [D loss: 0.128514, acc: 0.953125]  [G loss: 4.128939, acc: 0.218750]\n",
            "2696: [D loss: 0.063045, acc: 0.984375]  [G loss: 4.809038, acc: 0.203125]\n",
            "2697: [D loss: 0.113820, acc: 0.968750]  [G loss: 4.466128, acc: 0.171875]\n",
            "2698: [D loss: 0.108722, acc: 0.953125]  [G loss: 4.635201, acc: 0.093750]\n",
            "2699: [D loss: 0.052526, acc: 0.976562]  [G loss: 4.607750, acc: 0.078125]\n",
            "2700: [D loss: 0.086030, acc: 0.968750]  [G loss: 3.850245, acc: 0.078125]\n",
            "2701: [D loss: 0.295529, acc: 0.929688]  [G loss: 2.445887, acc: 0.328125]\n",
            "2702: [D loss: 0.150340, acc: 0.960938]  [G loss: 1.365039, acc: 0.562500]\n",
            "2703: [D loss: 0.164205, acc: 0.921875]  [G loss: 1.591917, acc: 0.500000]\n",
            "2704: [D loss: 0.176860, acc: 0.953125]  [G loss: 1.910538, acc: 0.375000]\n",
            "2705: [D loss: 0.068574, acc: 0.968750]  [G loss: 2.446351, acc: 0.359375]\n",
            "2706: [D loss: 0.103797, acc: 0.960938]  [G loss: 2.937749, acc: 0.281250]\n",
            "2707: [D loss: 0.080723, acc: 0.976562]  [G loss: 3.770769, acc: 0.203125]\n",
            "2708: [D loss: 0.067567, acc: 0.976562]  [G loss: 3.647692, acc: 0.250000]\n",
            "2709: [D loss: 0.168659, acc: 0.953125]  [G loss: 3.519063, acc: 0.234375]\n",
            "2710: [D loss: 0.073647, acc: 0.992188]  [G loss: 2.432197, acc: 0.343750]\n",
            "2711: [D loss: 0.110301, acc: 0.968750]  [G loss: 2.155719, acc: 0.296875]\n",
            "2712: [D loss: 0.107928, acc: 0.968750]  [G loss: 1.805941, acc: 0.468750]\n",
            "2713: [D loss: 0.113228, acc: 0.945312]  [G loss: 1.602372, acc: 0.484375]\n",
            "2714: [D loss: 0.093172, acc: 0.960938]  [G loss: 1.823351, acc: 0.468750]\n",
            "2715: [D loss: 0.032316, acc: 0.992188]  [G loss: 2.214697, acc: 0.312500]\n",
            "2716: [D loss: 0.031266, acc: 0.992188]  [G loss: 2.725197, acc: 0.265625]\n",
            "2717: [D loss: 0.047890, acc: 0.984375]  [G loss: 2.424735, acc: 0.437500]\n",
            "2718: [D loss: 0.072195, acc: 0.976562]  [G loss: 2.842110, acc: 0.343750]\n",
            "2719: [D loss: 0.072488, acc: 0.968750]  [G loss: 2.198261, acc: 0.359375]\n",
            "2720: [D loss: 0.038289, acc: 0.984375]  [G loss: 2.072441, acc: 0.453125]\n",
            "2721: [D loss: 0.046822, acc: 0.976562]  [G loss: 1.975169, acc: 0.437500]\n",
            "2722: [D loss: 0.079229, acc: 0.960938]  [G loss: 1.759694, acc: 0.453125]\n",
            "2723: [D loss: 0.057805, acc: 0.968750]  [G loss: 2.175623, acc: 0.328125]\n",
            "2724: [D loss: 0.015660, acc: 1.000000]  [G loss: 2.913103, acc: 0.281250]\n",
            "2725: [D loss: 0.045471, acc: 0.976562]  [G loss: 3.444717, acc: 0.218750]\n",
            "2726: [D loss: 0.047512, acc: 0.992188]  [G loss: 2.934319, acc: 0.343750]\n",
            "2727: [D loss: 0.040985, acc: 0.984375]  [G loss: 2.993150, acc: 0.296875]\n",
            "2728: [D loss: 0.047568, acc: 0.984375]  [G loss: 3.118693, acc: 0.375000]\n",
            "2729: [D loss: 0.061026, acc: 0.976562]  [G loss: 3.370113, acc: 0.296875]\n",
            "2730: [D loss: 0.028379, acc: 1.000000]  [G loss: 3.020717, acc: 0.406250]\n",
            "2731: [D loss: 0.099990, acc: 0.968750]  [G loss: 2.540360, acc: 0.406250]\n",
            "2732: [D loss: 0.051677, acc: 0.984375]  [G loss: 2.337730, acc: 0.437500]\n",
            "2733: [D loss: 0.064180, acc: 0.984375]  [G loss: 1.712459, acc: 0.562500]\n",
            "2734: [D loss: 0.072744, acc: 0.968750]  [G loss: 2.105618, acc: 0.453125]\n",
            "2735: [D loss: 0.040063, acc: 0.984375]  [G loss: 2.198204, acc: 0.484375]\n",
            "2736: [D loss: 0.040474, acc: 0.992188]  [G loss: 2.295650, acc: 0.406250]\n",
            "2737: [D loss: 0.061528, acc: 0.984375]  [G loss: 2.353056, acc: 0.500000]\n",
            "2738: [D loss: 0.125766, acc: 0.968750]  [G loss: 2.242992, acc: 0.390625]\n",
            "2739: [D loss: 0.294089, acc: 0.914062]  [G loss: 1.564921, acc: 0.593750]\n",
            "2740: [D loss: 0.119183, acc: 0.968750]  [G loss: 1.706459, acc: 0.609375]\n",
            "2741: [D loss: 0.093154, acc: 0.976562]  [G loss: 1.698379, acc: 0.531250]\n",
            "2742: [D loss: 0.074774, acc: 0.976562]  [G loss: 2.074944, acc: 0.453125]\n",
            "2743: [D loss: 0.121702, acc: 0.960938]  [G loss: 2.955577, acc: 0.218750]\n",
            "2744: [D loss: 0.109984, acc: 0.945312]  [G loss: 2.791340, acc: 0.281250]\n",
            "2745: [D loss: 0.127083, acc: 0.953125]  [G loss: 2.603543, acc: 0.265625]\n",
            "2746: [D loss: 0.045495, acc: 0.992188]  [G loss: 2.490582, acc: 0.187500]\n",
            "2747: [D loss: 0.059024, acc: 0.968750]  [G loss: 2.832286, acc: 0.250000]\n",
            "2748: [D loss: 0.018745, acc: 1.000000]  [G loss: 2.962806, acc: 0.218750]\n",
            "2749: [D loss: 0.101406, acc: 0.976562]  [G loss: 3.566292, acc: 0.140625]\n",
            "2750: [D loss: 0.049265, acc: 0.992188]  [G loss: 3.327752, acc: 0.156250]\n",
            "2751: [D loss: 0.097201, acc: 0.968750]  [G loss: 3.472548, acc: 0.171875]\n",
            "2752: [D loss: 0.033476, acc: 0.984375]  [G loss: 2.415115, acc: 0.328125]\n",
            "2753: [D loss: 0.125137, acc: 0.945312]  [G loss: 2.257262, acc: 0.406250]\n",
            "2754: [D loss: 0.008354, acc: 1.000000]  [G loss: 3.479277, acc: 0.156250]\n",
            "2755: [D loss: 0.053647, acc: 0.960938]  [G loss: 4.323162, acc: 0.156250]\n",
            "2756: [D loss: 0.105024, acc: 0.960938]  [G loss: 3.776120, acc: 0.140625]\n",
            "2757: [D loss: 0.157510, acc: 0.937500]  [G loss: 2.189977, acc: 0.453125]\n",
            "2758: [D loss: 0.055034, acc: 0.968750]  [G loss: 1.311513, acc: 0.656250]\n",
            "2759: [D loss: 0.188363, acc: 0.929688]  [G loss: 1.827696, acc: 0.531250]\n",
            "2760: [D loss: 0.090280, acc: 0.960938]  [G loss: 2.965863, acc: 0.234375]\n",
            "2761: [D loss: 0.024392, acc: 0.992188]  [G loss: 5.193827, acc: 0.093750]\n",
            "2762: [D loss: 0.078689, acc: 0.976562]  [G loss: 6.335254, acc: 0.000000]\n",
            "2763: [D loss: 0.181536, acc: 0.960938]  [G loss: 5.464197, acc: 0.093750]\n",
            "2764: [D loss: 0.108395, acc: 0.976562]  [G loss: 4.056289, acc: 0.187500]\n",
            "2765: [D loss: 0.044682, acc: 0.984375]  [G loss: 3.421429, acc: 0.250000]\n",
            "2766: [D loss: 0.049986, acc: 0.976562]  [G loss: 3.305109, acc: 0.250000]\n",
            "2767: [D loss: 0.201778, acc: 0.937500]  [G loss: 4.304879, acc: 0.078125]\n",
            "2768: [D loss: 0.091060, acc: 0.960938]  [G loss: 4.417616, acc: 0.093750]\n",
            "2769: [D loss: 0.023018, acc: 0.984375]  [G loss: 4.570035, acc: 0.125000]\n",
            "2770: [D loss: 0.034295, acc: 0.984375]  [G loss: 3.516948, acc: 0.140625]\n",
            "2771: [D loss: 0.141003, acc: 0.976562]  [G loss: 3.177120, acc: 0.265625]\n",
            "2772: [D loss: 0.121870, acc: 0.960938]  [G loss: 1.775137, acc: 0.437500]\n",
            "2773: [D loss: 0.062596, acc: 0.960938]  [G loss: 1.576812, acc: 0.500000]\n",
            "2774: [D loss: 0.076850, acc: 0.960938]  [G loss: 1.435919, acc: 0.625000]\n",
            "2775: [D loss: 0.173913, acc: 0.937500]  [G loss: 2.008527, acc: 0.453125]\n",
            "2776: [D loss: 0.050932, acc: 0.976562]  [G loss: 3.015330, acc: 0.312500]\n",
            "2777: [D loss: 0.123323, acc: 0.976562]  [G loss: 3.166599, acc: 0.296875]\n",
            "2778: [D loss: 0.048159, acc: 0.976562]  [G loss: 3.814017, acc: 0.187500]\n",
            "2779: [D loss: 0.108522, acc: 0.945312]  [G loss: 4.592585, acc: 0.156250]\n",
            "2780: [D loss: 0.198757, acc: 0.976562]  [G loss: 4.419049, acc: 0.140625]\n",
            "2781: [D loss: 0.060667, acc: 0.976562]  [G loss: 4.168980, acc: 0.125000]\n",
            "2782: [D loss: 0.064752, acc: 0.960938]  [G loss: 3.492816, acc: 0.187500]\n",
            "2783: [D loss: 0.098375, acc: 0.960938]  [G loss: 3.620796, acc: 0.156250]\n",
            "2784: [D loss: 0.141051, acc: 0.960938]  [G loss: 2.931140, acc: 0.265625]\n",
            "2785: [D loss: 0.044780, acc: 0.976562]  [G loss: 3.840528, acc: 0.171875]\n",
            "2786: [D loss: 0.042659, acc: 0.984375]  [G loss: 3.686065, acc: 0.140625]\n",
            "2787: [D loss: 0.114048, acc: 0.953125]  [G loss: 5.087153, acc: 0.046875]\n",
            "2788: [D loss: 0.065746, acc: 0.976562]  [G loss: 4.726779, acc: 0.109375]\n",
            "2789: [D loss: 0.019912, acc: 0.992188]  [G loss: 4.736532, acc: 0.156250]\n",
            "2790: [D loss: 0.038266, acc: 0.984375]  [G loss: 4.363021, acc: 0.140625]\n",
            "2791: [D loss: 0.035134, acc: 0.992188]  [G loss: 3.527602, acc: 0.156250]\n",
            "2792: [D loss: 0.020947, acc: 0.992188]  [G loss: 3.696555, acc: 0.156250]\n",
            "2793: [D loss: 0.028561, acc: 0.984375]  [G loss: 4.286529, acc: 0.125000]\n",
            "2794: [D loss: 0.026436, acc: 0.992188]  [G loss: 4.751316, acc: 0.093750]\n",
            "2795: [D loss: 0.033052, acc: 0.992188]  [G loss: 5.662109, acc: 0.046875]\n",
            "2796: [D loss: 0.084694, acc: 0.984375]  [G loss: 5.681768, acc: 0.031250]\n",
            "2797: [D loss: 0.043132, acc: 0.984375]  [G loss: 4.774908, acc: 0.062500]\n",
            "2798: [D loss: 0.085762, acc: 0.976562]  [G loss: 3.665066, acc: 0.203125]\n",
            "2799: [D loss: 0.007597, acc: 1.000000]  [G loss: 2.917470, acc: 0.250000]\n",
            "2800: [D loss: 0.120652, acc: 0.968750]  [G loss: 3.044374, acc: 0.250000]\n",
            "2801: [D loss: 0.064001, acc: 0.976562]  [G loss: 3.824754, acc: 0.187500]\n",
            "2802: [D loss: 0.046815, acc: 0.992188]  [G loss: 4.710372, acc: 0.109375]\n",
            "2803: [D loss: 0.021522, acc: 0.992188]  [G loss: 5.740812, acc: 0.031250]\n",
            "2804: [D loss: 0.133480, acc: 0.976562]  [G loss: 5.104007, acc: 0.171875]\n",
            "2805: [D loss: 0.011025, acc: 1.000000]  [G loss: 4.019946, acc: 0.203125]\n",
            "2806: [D loss: 0.077333, acc: 0.976562]  [G loss: 2.056289, acc: 0.437500]\n",
            "2807: [D loss: 0.034154, acc: 0.992188]  [G loss: 1.411712, acc: 0.578125]\n",
            "2808: [D loss: 0.014735, acc: 1.000000]  [G loss: 0.647159, acc: 0.812500]\n",
            "2809: [D loss: 0.060579, acc: 0.976562]  [G loss: 1.151455, acc: 0.671875]\n",
            "2810: [D loss: 0.008583, acc: 1.000000]  [G loss: 1.712822, acc: 0.578125]\n",
            "2811: [D loss: 0.046638, acc: 0.992188]  [G loss: 2.163024, acc: 0.421875]\n",
            "2812: [D loss: 0.003842, acc: 1.000000]  [G loss: 2.812510, acc: 0.390625]\n",
            "2813: [D loss: 0.075327, acc: 0.984375]  [G loss: 2.557879, acc: 0.484375]\n",
            "2814: [D loss: 0.021892, acc: 0.992188]  [G loss: 2.100336, acc: 0.468750]\n",
            "2815: [D loss: 0.004904, acc: 1.000000]  [G loss: 1.588247, acc: 0.546875]\n",
            "2816: [D loss: 0.032382, acc: 0.984375]  [G loss: 1.308149, acc: 0.578125]\n",
            "2817: [D loss: 0.071446, acc: 0.976562]  [G loss: 1.458318, acc: 0.609375]\n",
            "2818: [D loss: 0.022362, acc: 0.992188]  [G loss: 1.351626, acc: 0.593750]\n",
            "2819: [D loss: 0.023246, acc: 0.984375]  [G loss: 1.627532, acc: 0.578125]\n",
            "2820: [D loss: 0.021599, acc: 0.992188]  [G loss: 2.218748, acc: 0.375000]\n",
            "2821: [D loss: 0.028748, acc: 0.992188]  [G loss: 1.177096, acc: 0.656250]\n",
            "2822: [D loss: 0.030564, acc: 0.992188]  [G loss: 1.097717, acc: 0.734375]\n",
            "2823: [D loss: 0.138253, acc: 0.960938]  [G loss: 1.227653, acc: 0.750000]\n",
            "2824: [D loss: 0.049973, acc: 0.984375]  [G loss: 1.221639, acc: 0.687500]\n",
            "2825: [D loss: 0.021543, acc: 0.992188]  [G loss: 1.969027, acc: 0.609375]\n",
            "2826: [D loss: 0.063738, acc: 0.976562]  [G loss: 2.140778, acc: 0.531250]\n",
            "2827: [D loss: 0.058300, acc: 0.992188]  [G loss: 2.055060, acc: 0.531250]\n",
            "2828: [D loss: 0.047207, acc: 0.976562]  [G loss: 1.921856, acc: 0.484375]\n",
            "2829: [D loss: 0.188252, acc: 0.960938]  [G loss: 1.428121, acc: 0.609375]\n",
            "2830: [D loss: 0.083056, acc: 0.976562]  [G loss: 0.999188, acc: 0.750000]\n",
            "2831: [D loss: 0.321715, acc: 0.914062]  [G loss: 5.365007, acc: 0.093750]\n",
            "2832: [D loss: 0.237593, acc: 0.953125]  [G loss: 9.017984, acc: 0.000000]\n",
            "2833: [D loss: 0.780613, acc: 0.875000]  [G loss: 4.970762, acc: 0.125000]\n",
            "2834: [D loss: 0.232963, acc: 0.960938]  [G loss: 1.193571, acc: 0.640625]\n",
            "2835: [D loss: 0.251934, acc: 0.914062]  [G loss: 1.606912, acc: 0.562500]\n",
            "2836: [D loss: 0.124580, acc: 0.945312]  [G loss: 4.231521, acc: 0.140625]\n",
            "2837: [D loss: 0.015851, acc: 0.992188]  [G loss: 6.643263, acc: 0.031250]\n",
            "2838: [D loss: 0.042564, acc: 0.984375]  [G loss: 7.596148, acc: 0.000000]\n",
            "2839: [D loss: 0.046035, acc: 0.968750]  [G loss: 7.441642, acc: 0.000000]\n",
            "2840: [D loss: 0.099580, acc: 0.960938]  [G loss: 6.425090, acc: 0.015625]\n",
            "2841: [D loss: 0.041724, acc: 0.992188]  [G loss: 5.162375, acc: 0.031250]\n",
            "2842: [D loss: 0.026006, acc: 0.992188]  [G loss: 3.292486, acc: 0.218750]\n",
            "2843: [D loss: 0.008898, acc: 1.000000]  [G loss: 2.329727, acc: 0.421875]\n",
            "2844: [D loss: 0.019869, acc: 1.000000]  [G loss: 1.933551, acc: 0.484375]\n",
            "2845: [D loss: 0.088229, acc: 0.960938]  [G loss: 1.745263, acc: 0.562500]\n",
            "2846: [D loss: 0.078001, acc: 0.976562]  [G loss: 1.874411, acc: 0.546875]\n",
            "2847: [D loss: 0.022679, acc: 1.000000]  [G loss: 2.001399, acc: 0.468750]\n",
            "2848: [D loss: 0.036511, acc: 0.992188]  [G loss: 2.024611, acc: 0.437500]\n",
            "2849: [D loss: 0.006533, acc: 1.000000]  [G loss: 1.490818, acc: 0.531250]\n",
            "2850: [D loss: 0.012517, acc: 0.992188]  [G loss: 1.321627, acc: 0.546875]\n",
            "2851: [D loss: 0.001462, acc: 1.000000]  [G loss: 1.421571, acc: 0.546875]\n",
            "2852: [D loss: 0.002251, acc: 1.000000]  [G loss: 1.425061, acc: 0.593750]\n",
            "2853: [D loss: 0.001542, acc: 1.000000]  [G loss: 1.609044, acc: 0.390625]\n",
            "2854: [D loss: 0.012087, acc: 0.992188]  [G loss: 1.024078, acc: 0.546875]\n",
            "2855: [D loss: 0.035762, acc: 0.984375]  [G loss: 0.656592, acc: 0.734375]\n",
            "2856: [D loss: 0.000896, acc: 1.000000]  [G loss: 0.311355, acc: 0.765625]\n",
            "2857: [D loss: 0.001735, acc: 1.000000]  [G loss: 0.230600, acc: 0.875000]\n",
            "2858: [D loss: 0.015855, acc: 0.992188]  [G loss: 0.194173, acc: 0.921875]\n",
            "2859: [D loss: 0.001178, acc: 1.000000]  [G loss: 0.046070, acc: 0.984375]\n",
            "2860: [D loss: 0.000740, acc: 1.000000]  [G loss: 0.024216, acc: 1.000000]\n",
            "2861: [D loss: 0.000799, acc: 1.000000]  [G loss: 0.015032, acc: 1.000000]\n",
            "2862: [D loss: 0.000988, acc: 1.000000]  [G loss: 0.015558, acc: 1.000000]\n",
            "2863: [D loss: 0.003008, acc: 1.000000]  [G loss: 0.012391, acc: 1.000000]\n",
            "2864: [D loss: 0.006953, acc: 1.000000]  [G loss: 0.004987, acc: 1.000000]\n",
            "2865: [D loss: 0.014178, acc: 1.000000]  [G loss: 0.007719, acc: 1.000000]\n",
            "2866: [D loss: 0.001950, acc: 1.000000]  [G loss: 0.041050, acc: 0.984375]\n",
            "2867: [D loss: 0.001898, acc: 1.000000]  [G loss: 0.008094, acc: 1.000000]\n",
            "2868: [D loss: 0.000739, acc: 1.000000]  [G loss: 0.041091, acc: 0.968750]\n",
            "2869: [D loss: 0.000354, acc: 1.000000]  [G loss: 0.014790, acc: 1.000000]\n",
            "2870: [D loss: 0.000705, acc: 1.000000]  [G loss: 0.034203, acc: 0.984375]\n",
            "2871: [D loss: 0.002353, acc: 1.000000]  [G loss: 0.036877, acc: 0.984375]\n",
            "2872: [D loss: 0.000199, acc: 1.000000]  [G loss: 0.046618, acc: 0.968750]\n",
            "2873: [D loss: 0.000200, acc: 1.000000]  [G loss: 0.019537, acc: 1.000000]\n",
            "2874: [D loss: 0.000417, acc: 1.000000]  [G loss: 0.044160, acc: 0.968750]\n",
            "2875: [D loss: 0.000317, acc: 1.000000]  [G loss: 0.022923, acc: 1.000000]\n",
            "2876: [D loss: 0.002068, acc: 1.000000]  [G loss: 0.032998, acc: 1.000000]\n",
            "2877: [D loss: 0.000124, acc: 1.000000]  [G loss: 0.024782, acc: 0.984375]\n",
            "2878: [D loss: 0.011259, acc: 0.992188]  [G loss: 0.017284, acc: 1.000000]\n",
            "2879: [D loss: 0.000361, acc: 1.000000]  [G loss: 0.028539, acc: 0.984375]\n",
            "2880: [D loss: 0.000322, acc: 1.000000]  [G loss: 0.030702, acc: 1.000000]\n",
            "2881: [D loss: 0.001040, acc: 1.000000]  [G loss: 0.021803, acc: 0.984375]\n",
            "2882: [D loss: 0.011821, acc: 0.992188]  [G loss: 0.029649, acc: 0.984375]\n",
            "2883: [D loss: 0.000453, acc: 1.000000]  [G loss: 0.010916, acc: 1.000000]\n",
            "2884: [D loss: 0.001395, acc: 1.000000]  [G loss: 0.006941, acc: 1.000000]\n",
            "2885: [D loss: 0.007627, acc: 0.992188]  [G loss: 0.000152, acc: 1.000000]\n",
            "2886: [D loss: 0.001718, acc: 1.000000]  [G loss: 0.004829, acc: 1.000000]\n",
            "2887: [D loss: 0.008290, acc: 1.000000]  [G loss: 0.001497, acc: 1.000000]\n",
            "2888: [D loss: 0.001950, acc: 1.000000]  [G loss: 0.005002, acc: 1.000000]\n",
            "2889: [D loss: 0.006870, acc: 1.000000]  [G loss: 0.009261, acc: 1.000000]\n",
            "2890: [D loss: 0.002312, acc: 1.000000]  [G loss: 0.012096, acc: 1.000000]\n",
            "2891: [D loss: 0.002295, acc: 1.000000]  [G loss: 0.000739, acc: 1.000000]\n",
            "2892: [D loss: 0.004964, acc: 1.000000]  [G loss: 0.003002, acc: 1.000000]\n",
            "2893: [D loss: 0.004621, acc: 1.000000]  [G loss: 0.007091, acc: 1.000000]\n",
            "2894: [D loss: 0.012149, acc: 0.992188]  [G loss: 0.002216, acc: 1.000000]\n",
            "2895: [D loss: 0.000428, acc: 1.000000]  [G loss: 0.016486, acc: 1.000000]\n",
            "2896: [D loss: 0.002510, acc: 1.000000]  [G loss: 0.030198, acc: 1.000000]\n",
            "2897: [D loss: 0.001225, acc: 1.000000]  [G loss: 0.055780, acc: 0.984375]\n",
            "2898: [D loss: 0.000476, acc: 1.000000]  [G loss: 0.036858, acc: 1.000000]\n",
            "2899: [D loss: 0.000907, acc: 1.000000]  [G loss: 0.183735, acc: 0.937500]\n",
            "2900: [D loss: 0.007974, acc: 0.992188]  [G loss: 0.106881, acc: 0.953125]\n",
            "2901: [D loss: 0.036988, acc: 0.992188]  [G loss: 0.087965, acc: 0.984375]\n",
            "2902: [D loss: 0.000517, acc: 1.000000]  [G loss: 0.111796, acc: 0.921875]\n",
            "2903: [D loss: 0.010082, acc: 0.992188]  [G loss: 0.006414, acc: 1.000000]\n",
            "2904: [D loss: 0.003930, acc: 1.000000]  [G loss: 0.014412, acc: 1.000000]\n",
            "2905: [D loss: 0.009079, acc: 0.992188]  [G loss: 0.054245, acc: 0.984375]\n",
            "2906: [D loss: 0.012981, acc: 0.992188]  [G loss: 0.009675, acc: 1.000000]\n",
            "2907: [D loss: 0.012876, acc: 1.000000]  [G loss: 0.023412, acc: 1.000000]\n",
            "2908: [D loss: 0.008161, acc: 1.000000]  [G loss: 0.128944, acc: 0.953125]\n",
            "2909: [D loss: 0.003683, acc: 1.000000]  [G loss: 0.130695, acc: 0.906250]\n",
            "2910: [D loss: 0.030847, acc: 0.992188]  [G loss: 0.081719, acc: 0.968750]\n",
            "2911: [D loss: 0.013882, acc: 0.992188]  [G loss: 0.238116, acc: 0.875000]\n",
            "2912: [D loss: 0.011201, acc: 0.992188]  [G loss: 0.220744, acc: 0.921875]\n",
            "2913: [D loss: 0.020273, acc: 0.984375]  [G loss: 0.273933, acc: 0.890625]\n",
            "2914: [D loss: 0.015999, acc: 1.000000]  [G loss: 0.488156, acc: 0.859375]\n",
            "2915: [D loss: 0.024478, acc: 0.992188]  [G loss: 0.472521, acc: 0.796875]\n",
            "2916: [D loss: 0.010132, acc: 1.000000]  [G loss: 0.617245, acc: 0.812500]\n",
            "2917: [D loss: 0.040154, acc: 0.992188]  [G loss: 0.531049, acc: 0.765625]\n",
            "2918: [D loss: 0.015972, acc: 0.992188]  [G loss: 0.459648, acc: 0.859375]\n",
            "2919: [D loss: 0.018048, acc: 0.992188]  [G loss: 0.928242, acc: 0.640625]\n",
            "2920: [D loss: 0.009117, acc: 1.000000]  [G loss: 1.024768, acc: 0.640625]\n",
            "2921: [D loss: 0.051722, acc: 0.984375]  [G loss: 1.123054, acc: 0.625000]\n",
            "2922: [D loss: 0.009252, acc: 1.000000]  [G loss: 1.274537, acc: 0.656250]\n",
            "2923: [D loss: 0.015947, acc: 0.992188]  [G loss: 1.160874, acc: 0.640625]\n",
            "2924: [D loss: 0.045409, acc: 0.984375]  [G loss: 1.113884, acc: 0.625000]\n",
            "2925: [D loss: 0.085405, acc: 0.968750]  [G loss: 1.104937, acc: 0.671875]\n",
            "2926: [D loss: 0.124122, acc: 0.953125]  [G loss: 2.792537, acc: 0.296875]\n",
            "2927: [D loss: 0.095484, acc: 0.968750]  [G loss: 3.306202, acc: 0.171875]\n",
            "2928: [D loss: 0.143248, acc: 0.960938]  [G loss: 1.559537, acc: 0.546875]\n",
            "2929: [D loss: 0.074135, acc: 0.976562]  [G loss: 1.409094, acc: 0.562500]\n",
            "2930: [D loss: 0.123749, acc: 0.945312]  [G loss: 2.920412, acc: 0.234375]\n",
            "2931: [D loss: 0.068199, acc: 0.984375]  [G loss: 5.646795, acc: 0.078125]\n",
            "2932: [D loss: 0.196446, acc: 0.945312]  [G loss: 4.857219, acc: 0.031250]\n",
            "2933: [D loss: 0.019889, acc: 0.992188]  [G loss: 4.680014, acc: 0.031250]\n",
            "2934: [D loss: 0.014183, acc: 1.000000]  [G loss: 3.443191, acc: 0.156250]\n",
            "2935: [D loss: 0.032812, acc: 0.992188]  [G loss: 3.797111, acc: 0.125000]\n",
            "2936: [D loss: 0.079500, acc: 0.968750]  [G loss: 4.374231, acc: 0.031250]\n",
            "2937: [D loss: 0.037733, acc: 0.992188]  [G loss: 4.154721, acc: 0.156250]\n",
            "2938: [D loss: 0.075452, acc: 0.968750]  [G loss: 4.568886, acc: 0.062500]\n",
            "2939: [D loss: 0.101319, acc: 0.976562]  [G loss: 3.467809, acc: 0.156250]\n",
            "2940: [D loss: 0.099155, acc: 0.976562]  [G loss: 3.278294, acc: 0.265625]\n",
            "2941: [D loss: 0.050103, acc: 0.984375]  [G loss: 3.029156, acc: 0.234375]\n",
            "2942: [D loss: 0.061809, acc: 0.984375]  [G loss: 2.790046, acc: 0.250000]\n",
            "2943: [D loss: 0.176953, acc: 0.945312]  [G loss: 2.561803, acc: 0.281250]\n",
            "2944: [D loss: 0.063009, acc: 0.984375]  [G loss: 2.703184, acc: 0.312500]\n",
            "2945: [D loss: 0.143753, acc: 0.945312]  [G loss: 3.155071, acc: 0.312500]\n",
            "2946: [D loss: 0.083351, acc: 0.968750]  [G loss: 3.298038, acc: 0.187500]\n",
            "2947: [D loss: 0.020544, acc: 1.000000]  [G loss: 3.965072, acc: 0.171875]\n",
            "2948: [D loss: 0.057425, acc: 0.976562]  [G loss: 4.197603, acc: 0.171875]\n",
            "2949: [D loss: 0.106719, acc: 0.960938]  [G loss: 3.709389, acc: 0.203125]\n",
            "2950: [D loss: 0.082919, acc: 0.976562]  [G loss: 3.005356, acc: 0.390625]\n",
            "2951: [D loss: 0.029934, acc: 0.992188]  [G loss: 2.809616, acc: 0.359375]\n",
            "2952: [D loss: 0.087703, acc: 0.968750]  [G loss: 2.530928, acc: 0.328125]\n",
            "2953: [D loss: 0.118051, acc: 0.953125]  [G loss: 2.926904, acc: 0.312500]\n",
            "2954: [D loss: 0.141439, acc: 0.960938]  [G loss: 3.813576, acc: 0.234375]\n",
            "2955: [D loss: 0.088713, acc: 0.945312]  [G loss: 5.578097, acc: 0.125000]\n",
            "2956: [D loss: 0.095541, acc: 0.976562]  [G loss: 4.524087, acc: 0.218750]\n",
            "2957: [D loss: 0.076739, acc: 0.976562]  [G loss: 4.212893, acc: 0.265625]\n",
            "2958: [D loss: 0.045317, acc: 0.984375]  [G loss: 3.958971, acc: 0.250000]\n",
            "2959: [D loss: 0.117663, acc: 0.945312]  [G loss: 3.522965, acc: 0.375000]\n",
            "2960: [D loss: 0.054120, acc: 0.976562]  [G loss: 3.785902, acc: 0.281250]\n",
            "2961: [D loss: 0.062334, acc: 0.960938]  [G loss: 4.625863, acc: 0.171875]\n",
            "2962: [D loss: 0.170505, acc: 0.960938]  [G loss: 4.033368, acc: 0.250000]\n",
            "2963: [D loss: 0.116614, acc: 0.976562]  [G loss: 2.983661, acc: 0.312500]\n",
            "2964: [D loss: 0.042446, acc: 0.984375]  [G loss: 3.578797, acc: 0.218750]\n",
            "2965: [D loss: 0.109524, acc: 0.960938]  [G loss: 3.650874, acc: 0.218750]\n",
            "2966: [D loss: 0.109910, acc: 0.960938]  [G loss: 4.641212, acc: 0.187500]\n",
            "2967: [D loss: 0.075655, acc: 0.984375]  [G loss: 5.184065, acc: 0.109375]\n",
            "2968: [D loss: 0.111944, acc: 0.968750]  [G loss: 4.816668, acc: 0.078125]\n",
            "2969: [D loss: 0.141669, acc: 0.953125]  [G loss: 2.958106, acc: 0.359375]\n",
            "2970: [D loss: 0.147347, acc: 0.937500]  [G loss: 1.656657, acc: 0.500000]\n",
            "2971: [D loss: 0.166864, acc: 0.945312]  [G loss: 1.533316, acc: 0.531250]\n",
            "2972: [D loss: 0.068934, acc: 0.960938]  [G loss: 2.074248, acc: 0.546875]\n",
            "2973: [D loss: 0.075681, acc: 0.960938]  [G loss: 2.612781, acc: 0.437500]\n",
            "2974: [D loss: 0.075409, acc: 0.984375]  [G loss: 3.644617, acc: 0.281250]\n",
            "2975: [D loss: 0.028813, acc: 0.992188]  [G loss: 3.838683, acc: 0.234375]\n",
            "2976: [D loss: 0.023330, acc: 0.992188]  [G loss: 4.636945, acc: 0.203125]\n",
            "2977: [D loss: 0.092670, acc: 0.976562]  [G loss: 4.507646, acc: 0.203125]\n",
            "2978: [D loss: 0.037612, acc: 0.984375]  [G loss: 4.067579, acc: 0.250000]\n",
            "2979: [D loss: 0.038802, acc: 0.984375]  [G loss: 2.972286, acc: 0.359375]\n",
            "2980: [D loss: 0.068746, acc: 0.960938]  [G loss: 2.317150, acc: 0.468750]\n",
            "2981: [D loss: 0.137007, acc: 0.960938]  [G loss: 2.214698, acc: 0.468750]\n",
            "2982: [D loss: 0.175849, acc: 0.937500]  [G loss: 3.578970, acc: 0.265625]\n",
            "2983: [D loss: 0.117291, acc: 0.945312]  [G loss: 4.524764, acc: 0.171875]\n",
            "2984: [D loss: 0.066441, acc: 0.976562]  [G loss: 3.889555, acc: 0.203125]\n",
            "2985: [D loss: 0.097475, acc: 0.968750]  [G loss: 4.228570, acc: 0.281250]\n",
            "2986: [D loss: 0.112712, acc: 0.945312]  [G loss: 2.613192, acc: 0.390625]\n",
            "2987: [D loss: 0.114867, acc: 0.968750]  [G loss: 2.588556, acc: 0.343750]\n",
            "2988: [D loss: 0.317276, acc: 0.937500]  [G loss: 4.084980, acc: 0.187500]\n",
            "2989: [D loss: 0.084033, acc: 0.976562]  [G loss: 5.239407, acc: 0.046875]\n",
            "2990: [D loss: 0.071177, acc: 0.968750]  [G loss: 6.705753, acc: 0.015625]\n",
            "2991: [D loss: 0.214401, acc: 0.929688]  [G loss: 5.316817, acc: 0.078125]\n",
            "2992: [D loss: 0.069191, acc: 0.968750]  [G loss: 2.903125, acc: 0.343750]\n",
            "2993: [D loss: 0.124207, acc: 0.968750]  [G loss: 2.464624, acc: 0.406250]\n",
            "2994: [D loss: 0.162554, acc: 0.921875]  [G loss: 4.548568, acc: 0.171875]\n",
            "2995: [D loss: 0.014586, acc: 1.000000]  [G loss: 6.661170, acc: 0.031250]\n",
            "2996: [D loss: 0.178680, acc: 0.921875]  [G loss: 6.035340, acc: 0.031250]\n",
            "2997: [D loss: 0.074321, acc: 0.960938]  [G loss: 5.575429, acc: 0.031250]\n",
            "2998: [D loss: 0.171449, acc: 0.960938]  [G loss: 3.349192, acc: 0.203125]\n",
            "2999: [D loss: 0.065563, acc: 0.984375]  [G loss: 2.635199, acc: 0.234375]\n",
            "3000: [D loss: 0.219371, acc: 0.890625]  [G loss: 3.576398, acc: 0.203125]\n",
            "3001: [D loss: 0.111721, acc: 0.968750]  [G loss: 5.815391, acc: 0.062500]\n",
            "3002: [D loss: 0.050700, acc: 0.976562]  [G loss: 5.983294, acc: 0.000000]\n",
            "3003: [D loss: 0.099327, acc: 0.960938]  [G loss: 5.283500, acc: 0.062500]\n",
            "3004: [D loss: 0.108739, acc: 0.968750]  [G loss: 3.630122, acc: 0.140625]\n",
            "3005: [D loss: 0.225881, acc: 0.937500]  [G loss: 2.919972, acc: 0.296875]\n",
            "3006: [D loss: 0.239806, acc: 0.890625]  [G loss: 3.904540, acc: 0.281250]\n",
            "3007: [D loss: 0.165546, acc: 0.914062]  [G loss: 4.318029, acc: 0.187500]\n",
            "3008: [D loss: 0.091375, acc: 0.976562]  [G loss: 4.266842, acc: 0.218750]\n",
            "3009: [D loss: 0.080395, acc: 0.968750]  [G loss: 4.558081, acc: 0.187500]\n",
            "3010: [D loss: 0.088122, acc: 0.976562]  [G loss: 3.578449, acc: 0.312500]\n",
            "3011: [D loss: 0.111038, acc: 0.937500]  [G loss: 3.201545, acc: 0.359375]\n",
            "3012: [D loss: 0.127439, acc: 0.945312]  [G loss: 4.211808, acc: 0.281250]\n",
            "3013: [D loss: 0.068098, acc: 0.976562]  [G loss: 4.739598, acc: 0.250000]\n",
            "3014: [D loss: 0.101836, acc: 0.968750]  [G loss: 4.465560, acc: 0.171875]\n",
            "3015: [D loss: 0.106532, acc: 0.984375]  [G loss: 5.492067, acc: 0.187500]\n",
            "3016: [D loss: 0.082766, acc: 0.976562]  [G loss: 4.664883, acc: 0.156250]\n",
            "3017: [D loss: 0.016216, acc: 0.992188]  [G loss: 3.895502, acc: 0.265625]\n",
            "3018: [D loss: 0.145621, acc: 0.953125]  [G loss: 3.794162, acc: 0.250000]\n",
            "3019: [D loss: 0.111102, acc: 0.968750]  [G loss: 3.759477, acc: 0.234375]\n",
            "3020: [D loss: 0.108633, acc: 0.968750]  [G loss: 3.837716, acc: 0.218750]\n",
            "3021: [D loss: 0.095968, acc: 0.976562]  [G loss: 2.748515, acc: 0.390625]\n",
            "3022: [D loss: 0.226533, acc: 0.945312]  [G loss: 2.843966, acc: 0.375000]\n",
            "3023: [D loss: 0.046936, acc: 0.984375]  [G loss: 2.621699, acc: 0.421875]\n",
            "3024: [D loss: 0.111483, acc: 0.968750]  [G loss: 2.213792, acc: 0.500000]\n",
            "3025: [D loss: 0.204833, acc: 0.937500]  [G loss: 2.593570, acc: 0.437500]\n",
            "3026: [D loss: 0.047610, acc: 0.984375]  [G loss: 2.854003, acc: 0.375000]\n",
            "3027: [D loss: 0.074524, acc: 0.968750]  [G loss: 2.492906, acc: 0.421875]\n",
            "3028: [D loss: 0.028822, acc: 0.992188]  [G loss: 2.752526, acc: 0.343750]\n",
            "3029: [D loss: 0.112699, acc: 0.960938]  [G loss: 3.313732, acc: 0.265625]\n",
            "3030: [D loss: 0.089887, acc: 0.960938]  [G loss: 4.512530, acc: 0.218750]\n",
            "3031: [D loss: 0.129233, acc: 0.968750]  [G loss: 3.972568, acc: 0.234375]\n",
            "3032: [D loss: 0.088412, acc: 0.976562]  [G loss: 4.961926, acc: 0.203125]\n",
            "3033: [D loss: 0.087411, acc: 0.953125]  [G loss: 4.827936, acc: 0.203125]\n",
            "3034: [D loss: 0.102870, acc: 0.953125]  [G loss: 2.946572, acc: 0.296875]\n",
            "3035: [D loss: 0.057575, acc: 0.976562]  [G loss: 2.504308, acc: 0.343750]\n",
            "3036: [D loss: 0.066452, acc: 0.984375]  [G loss: 2.062519, acc: 0.390625]\n",
            "3037: [D loss: 0.238521, acc: 0.882812]  [G loss: 4.502740, acc: 0.125000]\n",
            "3038: [D loss: 0.062763, acc: 0.976562]  [G loss: 6.598761, acc: 0.093750]\n",
            "3039: [D loss: 0.260693, acc: 0.929688]  [G loss: 6.022351, acc: 0.046875]\n",
            "3040: [D loss: 0.213208, acc: 0.921875]  [G loss: 4.754445, acc: 0.140625]\n",
            "3041: [D loss: 0.107157, acc: 0.976562]  [G loss: 3.903719, acc: 0.250000]\n",
            "3042: [D loss: 0.104343, acc: 0.945312]  [G loss: 2.684750, acc: 0.281250]\n",
            "3043: [D loss: 0.117988, acc: 0.960938]  [G loss: 2.474991, acc: 0.343750]\n",
            "3044: [D loss: 0.194374, acc: 0.898438]  [G loss: 3.047338, acc: 0.250000]\n",
            "3045: [D loss: 0.102708, acc: 0.976562]  [G loss: 3.974363, acc: 0.171875]\n",
            "3046: [D loss: 0.082244, acc: 0.976562]  [G loss: 4.837703, acc: 0.156250]\n",
            "3047: [D loss: 0.038836, acc: 0.984375]  [G loss: 6.289405, acc: 0.078125]\n",
            "3048: [D loss: 0.103396, acc: 0.960938]  [G loss: 4.977450, acc: 0.140625]\n",
            "3049: [D loss: 0.111997, acc: 0.960938]  [G loss: 5.319276, acc: 0.062500]\n",
            "3050: [D loss: 0.075146, acc: 0.968750]  [G loss: 4.057880, acc: 0.140625]\n",
            "3051: [D loss: 0.027614, acc: 0.992188]  [G loss: 3.886025, acc: 0.218750]\n",
            "3052: [D loss: 0.078383, acc: 0.976562]  [G loss: 3.128374, acc: 0.265625]\n",
            "3053: [D loss: 0.182422, acc: 0.914062]  [G loss: 2.972524, acc: 0.250000]\n",
            "3054: [D loss: 0.084448, acc: 0.984375]  [G loss: 3.793347, acc: 0.265625]\n",
            "3055: [D loss: 0.015193, acc: 1.000000]  [G loss: 4.699034, acc: 0.218750]\n",
            "3056: [D loss: 0.136300, acc: 0.953125]  [G loss: 4.655206, acc: 0.218750]\n",
            "3057: [D loss: 0.063566, acc: 0.968750]  [G loss: 4.870575, acc: 0.234375]\n",
            "3058: [D loss: 0.067446, acc: 0.960938]  [G loss: 4.943060, acc: 0.171875]\n",
            "3059: [D loss: 0.037707, acc: 0.992188]  [G loss: 4.671173, acc: 0.218750]\n",
            "3060: [D loss: 0.079213, acc: 0.960938]  [G loss: 3.922282, acc: 0.265625]\n",
            "3061: [D loss: 0.110640, acc: 0.953125]  [G loss: 3.695996, acc: 0.265625]\n",
            "3062: [D loss: 0.140329, acc: 0.945312]  [G loss: 3.280259, acc: 0.250000]\n",
            "3063: [D loss: 0.044943, acc: 0.984375]  [G loss: 2.687045, acc: 0.328125]\n",
            "3064: [D loss: 0.087798, acc: 0.968750]  [G loss: 2.937959, acc: 0.312500]\n",
            "3065: [D loss: 0.051460, acc: 0.984375]  [G loss: 4.025159, acc: 0.265625]\n",
            "3066: [D loss: 0.035441, acc: 0.984375]  [G loss: 4.695243, acc: 0.296875]\n",
            "3067: [D loss: 0.075486, acc: 0.976562]  [G loss: 4.575065, acc: 0.312500]\n",
            "3068: [D loss: 0.066953, acc: 0.976562]  [G loss: 3.912224, acc: 0.265625]\n",
            "3069: [D loss: 0.096466, acc: 0.960938]  [G loss: 4.471954, acc: 0.234375]\n",
            "3070: [D loss: 0.053671, acc: 0.976562]  [G loss: 4.281044, acc: 0.218750]\n",
            "3071: [D loss: 0.084687, acc: 0.953125]  [G loss: 4.588215, acc: 0.203125]\n",
            "3072: [D loss: 0.107349, acc: 0.960938]  [G loss: 4.578919, acc: 0.250000]\n",
            "3073: [D loss: 0.090301, acc: 0.945312]  [G loss: 5.534019, acc: 0.234375]\n",
            "3074: [D loss: 0.180917, acc: 0.968750]  [G loss: 4.990389, acc: 0.203125]\n",
            "3075: [D loss: 0.135932, acc: 0.945312]  [G loss: 5.166252, acc: 0.218750]\n",
            "3076: [D loss: 0.102294, acc: 0.976562]  [G loss: 5.346457, acc: 0.218750]\n",
            "3077: [D loss: 0.098560, acc: 0.960938]  [G loss: 5.011542, acc: 0.250000]\n",
            "3078: [D loss: 0.087026, acc: 0.960938]  [G loss: 4.717482, acc: 0.234375]\n",
            "3079: [D loss: 0.160846, acc: 0.953125]  [G loss: 4.931876, acc: 0.171875]\n",
            "3080: [D loss: 0.085019, acc: 0.968750]  [G loss: 5.600359, acc: 0.171875]\n",
            "3081: [D loss: 0.052093, acc: 0.976562]  [G loss: 5.551911, acc: 0.234375]\n",
            "3082: [D loss: 0.168576, acc: 0.960938]  [G loss: 4.662457, acc: 0.156250]\n",
            "3083: [D loss: 0.136015, acc: 0.960938]  [G loss: 3.888671, acc: 0.234375]\n",
            "3084: [D loss: 0.153177, acc: 0.960938]  [G loss: 2.488089, acc: 0.468750]\n",
            "3085: [D loss: 0.176225, acc: 0.937500]  [G loss: 3.801728, acc: 0.203125]\n",
            "3086: [D loss: 0.187604, acc: 0.937500]  [G loss: 3.816845, acc: 0.250000]\n",
            "3087: [D loss: 0.104801, acc: 0.968750]  [G loss: 4.555335, acc: 0.187500]\n",
            "3088: [D loss: 0.164417, acc: 0.945312]  [G loss: 4.027705, acc: 0.281250]\n",
            "3089: [D loss: 0.175159, acc: 0.945312]  [G loss: 2.383061, acc: 0.484375]\n",
            "3090: [D loss: 0.272011, acc: 0.906250]  [G loss: 2.218251, acc: 0.390625]\n",
            "3091: [D loss: 0.115408, acc: 0.937500]  [G loss: 3.010164, acc: 0.281250]\n",
            "3092: [D loss: 0.081208, acc: 0.976562]  [G loss: 4.079960, acc: 0.218750]\n",
            "3093: [D loss: 0.056605, acc: 0.976562]  [G loss: 4.709283, acc: 0.171875]\n",
            "3094: [D loss: 0.095358, acc: 0.968750]  [G loss: 4.969864, acc: 0.140625]\n",
            "3095: [D loss: 0.052422, acc: 0.976562]  [G loss: 5.165786, acc: 0.125000]\n",
            "3096: [D loss: 0.120815, acc: 0.960938]  [G loss: 4.568802, acc: 0.109375]\n",
            "3097: [D loss: 0.032481, acc: 0.992188]  [G loss: 5.063944, acc: 0.125000]\n",
            "3098: [D loss: 0.051938, acc: 0.968750]  [G loss: 4.320129, acc: 0.187500]\n",
            "3099: [D loss: 0.033456, acc: 0.984375]  [G loss: 4.219462, acc: 0.171875]\n",
            "3100: [D loss: 0.025894, acc: 0.992188]  [G loss: 3.755769, acc: 0.296875]\n",
            "3101: [D loss: 0.026853, acc: 1.000000]  [G loss: 3.507377, acc: 0.312500]\n",
            "3102: [D loss: 0.043252, acc: 0.968750]  [G loss: 3.199338, acc: 0.265625]\n",
            "3103: [D loss: 0.045423, acc: 0.984375]  [G loss: 2.762513, acc: 0.406250]\n",
            "3104: [D loss: 0.032184, acc: 0.992188]  [G loss: 2.448578, acc: 0.296875]\n",
            "3105: [D loss: 0.102857, acc: 0.945312]  [G loss: 2.547847, acc: 0.343750]\n",
            "3106: [D loss: 0.087561, acc: 0.984375]  [G loss: 2.620088, acc: 0.390625]\n",
            "3107: [D loss: 0.018266, acc: 1.000000]  [G loss: 3.242498, acc: 0.265625]\n",
            "3108: [D loss: 0.039675, acc: 0.984375]  [G loss: 2.704437, acc: 0.406250]\n",
            "3109: [D loss: 0.015649, acc: 1.000000]  [G loss: 3.588034, acc: 0.312500]\n",
            "3110: [D loss: 0.023327, acc: 0.984375]  [G loss: 4.062698, acc: 0.234375]\n",
            "3111: [D loss: 0.040567, acc: 0.984375]  [G loss: 4.122959, acc: 0.203125]\n",
            "3112: [D loss: 0.092818, acc: 0.968750]  [G loss: 4.018410, acc: 0.187500]\n",
            "3113: [D loss: 0.008393, acc: 1.000000]  [G loss: 3.226894, acc: 0.328125]\n",
            "3114: [D loss: 0.027507, acc: 0.992188]  [G loss: 3.480579, acc: 0.281250]\n",
            "3115: [D loss: 0.089967, acc: 0.953125]  [G loss: 3.685180, acc: 0.218750]\n",
            "3116: [D loss: 0.070588, acc: 0.968750]  [G loss: 4.669448, acc: 0.093750]\n",
            "3117: [D loss: 0.045897, acc: 0.976562]  [G loss: 4.750934, acc: 0.171875]\n",
            "3118: [D loss: 0.015364, acc: 1.000000]  [G loss: 4.785806, acc: 0.140625]\n",
            "3119: [D loss: 0.043200, acc: 0.984375]  [G loss: 4.835525, acc: 0.109375]\n",
            "3120: [D loss: 0.027287, acc: 0.992188]  [G loss: 4.804488, acc: 0.203125]\n",
            "3121: [D loss: 0.009181, acc: 1.000000]  [G loss: 5.519945, acc: 0.093750]\n",
            "3122: [D loss: 0.020753, acc: 1.000000]  [G loss: 4.966090, acc: 0.140625]\n",
            "3123: [D loss: 0.016315, acc: 1.000000]  [G loss: 5.133039, acc: 0.140625]\n",
            "3124: [D loss: 0.046302, acc: 0.984375]  [G loss: 4.901927, acc: 0.140625]\n",
            "3125: [D loss: 0.051381, acc: 0.984375]  [G loss: 4.879850, acc: 0.203125]\n",
            "3126: [D loss: 0.048408, acc: 0.976562]  [G loss: 4.555786, acc: 0.140625]\n",
            "3127: [D loss: 0.151900, acc: 0.976562]  [G loss: 4.365068, acc: 0.156250]\n",
            "3128: [D loss: 0.116071, acc: 0.953125]  [G loss: 5.800075, acc: 0.156250]\n",
            "3129: [D loss: 0.069389, acc: 0.976562]  [G loss: 5.635497, acc: 0.125000]\n",
            "3130: [D loss: 0.087335, acc: 0.960938]  [G loss: 5.265532, acc: 0.140625]\n",
            "3131: [D loss: 0.115818, acc: 0.953125]  [G loss: 6.656542, acc: 0.109375]\n",
            "3132: [D loss: 0.072657, acc: 0.984375]  [G loss: 7.086762, acc: 0.187500]\n",
            "3133: [D loss: 0.111910, acc: 0.953125]  [G loss: 6.585067, acc: 0.171875]\n",
            "3134: [D loss: 0.147658, acc: 0.960938]  [G loss: 5.855686, acc: 0.218750]\n",
            "3135: [D loss: 0.091400, acc: 0.968750]  [G loss: 4.554572, acc: 0.265625]\n",
            "3136: [D loss: 0.146566, acc: 0.953125]  [G loss: 4.100147, acc: 0.343750]\n",
            "3137: [D loss: 0.119610, acc: 0.945312]  [G loss: 5.699939, acc: 0.234375]\n",
            "3138: [D loss: 0.088369, acc: 0.968750]  [G loss: 6.085463, acc: 0.125000]\n",
            "3139: [D loss: 0.100970, acc: 0.968750]  [G loss: 6.564260, acc: 0.109375]\n",
            "3140: [D loss: 0.074029, acc: 0.976562]  [G loss: 5.414682, acc: 0.125000]\n",
            "3141: [D loss: 0.167198, acc: 0.937500]  [G loss: 5.717853, acc: 0.140625]\n",
            "3142: [D loss: 0.118910, acc: 0.937500]  [G loss: 8.056617, acc: 0.078125]\n",
            "3143: [D loss: 0.188846, acc: 0.906250]  [G loss: 5.661791, acc: 0.140625]\n",
            "3144: [D loss: 0.076721, acc: 0.984375]  [G loss: 3.968733, acc: 0.281250]\n",
            "3145: [D loss: 0.242764, acc: 0.882812]  [G loss: 7.518871, acc: 0.109375]\n",
            "3146: [D loss: 0.129342, acc: 0.960938]  [G loss: 8.195847, acc: 0.046875]\n",
            "3147: [D loss: 0.042056, acc: 0.992188]  [G loss: 9.645575, acc: 0.015625]\n",
            "3148: [D loss: 0.302361, acc: 0.945312]  [G loss: 7.114671, acc: 0.062500]\n",
            "3149: [D loss: 0.213970, acc: 0.953125]  [G loss: 3.991268, acc: 0.281250]\n",
            "3150: [D loss: 0.265451, acc: 0.906250]  [G loss: 3.867248, acc: 0.312500]\n",
            "3151: [D loss: 0.057226, acc: 0.968750]  [G loss: 4.372566, acc: 0.171875]\n",
            "3152: [D loss: 0.038310, acc: 0.976562]  [G loss: 6.759564, acc: 0.062500]\n",
            "3153: [D loss: 0.128764, acc: 0.953125]  [G loss: 6.350313, acc: 0.109375]\n",
            "3154: [D loss: 0.075493, acc: 0.976562]  [G loss: 5.822270, acc: 0.140625]\n",
            "3155: [D loss: 0.038296, acc: 0.984375]  [G loss: 5.321595, acc: 0.187500]\n",
            "3156: [D loss: 0.030826, acc: 0.984375]  [G loss: 5.565018, acc: 0.171875]\n",
            "3157: [D loss: 0.076364, acc: 0.968750]  [G loss: 5.226097, acc: 0.203125]\n",
            "3158: [D loss: 0.060938, acc: 0.968750]  [G loss: 6.313035, acc: 0.093750]\n",
            "3159: [D loss: 0.095059, acc: 0.976562]  [G loss: 6.863266, acc: 0.125000]\n",
            "3160: [D loss: 0.031968, acc: 0.984375]  [G loss: 6.290209, acc: 0.125000]\n",
            "3161: [D loss: 0.017557, acc: 0.992188]  [G loss: 5.814992, acc: 0.125000]\n",
            "3162: [D loss: 0.038146, acc: 0.984375]  [G loss: 5.270274, acc: 0.109375]\n",
            "3163: [D loss: 0.042427, acc: 0.984375]  [G loss: 5.241112, acc: 0.140625]\n",
            "3164: [D loss: 0.073345, acc: 0.984375]  [G loss: 5.474728, acc: 0.140625]\n",
            "3165: [D loss: 0.076907, acc: 0.976562]  [G loss: 5.227054, acc: 0.093750]\n",
            "3166: [D loss: 0.117681, acc: 0.937500]  [G loss: 6.420720, acc: 0.140625]\n",
            "3167: [D loss: 0.072939, acc: 0.984375]  [G loss: 5.388480, acc: 0.156250]\n",
            "3168: [D loss: 0.091451, acc: 0.968750]  [G loss: 4.735926, acc: 0.187500]\n",
            "3169: [D loss: 0.078872, acc: 0.960938]  [G loss: 4.630396, acc: 0.250000]\n",
            "3170: [D loss: 0.023267, acc: 1.000000]  [G loss: 4.561780, acc: 0.171875]\n",
            "3171: [D loss: 0.091586, acc: 0.976562]  [G loss: 4.602588, acc: 0.140625]\n",
            "3172: [D loss: 0.180827, acc: 0.953125]  [G loss: 4.597124, acc: 0.265625]\n",
            "3173: [D loss: 0.040574, acc: 0.984375]  [G loss: 6.450968, acc: 0.203125]\n",
            "3174: [D loss: 0.075198, acc: 0.976562]  [G loss: 5.991028, acc: 0.156250]\n",
            "3175: [D loss: 0.123963, acc: 0.960938]  [G loss: 4.597452, acc: 0.234375]\n",
            "3176: [D loss: 0.053030, acc: 0.968750]  [G loss: 3.921005, acc: 0.281250]\n",
            "3177: [D loss: 0.171130, acc: 0.921875]  [G loss: 3.995144, acc: 0.281250]\n",
            "3178: [D loss: 0.148853, acc: 0.945312]  [G loss: 5.576504, acc: 0.171875]\n",
            "3179: [D loss: 0.170849, acc: 0.960938]  [G loss: 5.758100, acc: 0.109375]\n",
            "3180: [D loss: 0.109171, acc: 0.968750]  [G loss: 5.397039, acc: 0.171875]\n",
            "3181: [D loss: 0.057828, acc: 0.984375]  [G loss: 4.501158, acc: 0.250000]\n",
            "3182: [D loss: 0.031920, acc: 0.992188]  [G loss: 4.085537, acc: 0.343750]\n",
            "3183: [D loss: 0.063226, acc: 0.984375]  [G loss: 2.890204, acc: 0.390625]\n",
            "3184: [D loss: 0.063824, acc: 0.968750]  [G loss: 3.285953, acc: 0.296875]\n",
            "3185: [D loss: 0.034741, acc: 1.000000]  [G loss: 3.532476, acc: 0.375000]\n",
            "3186: [D loss: 0.064105, acc: 0.968750]  [G loss: 3.575049, acc: 0.250000]\n",
            "3187: [D loss: 0.060177, acc: 0.968750]  [G loss: 3.003876, acc: 0.328125]\n",
            "3188: [D loss: 0.131302, acc: 0.960938]  [G loss: 2.652010, acc: 0.390625]\n",
            "3189: [D loss: 0.080646, acc: 0.960938]  [G loss: 2.450086, acc: 0.390625]\n",
            "3190: [D loss: 0.093099, acc: 0.960938]  [G loss: 2.783907, acc: 0.421875]\n",
            "3191: [D loss: 0.085056, acc: 0.968750]  [G loss: 2.968192, acc: 0.390625]\n",
            "3192: [D loss: 0.077620, acc: 0.960938]  [G loss: 3.187829, acc: 0.421875]\n",
            "3193: [D loss: 0.024962, acc: 0.992188]  [G loss: 2.817347, acc: 0.437500]\n",
            "3194: [D loss: 0.035934, acc: 0.992188]  [G loss: 2.097366, acc: 0.484375]\n",
            "3195: [D loss: 0.022602, acc: 0.992188]  [G loss: 2.291127, acc: 0.515625]\n",
            "3196: [D loss: 0.088835, acc: 0.968750]  [G loss: 2.410774, acc: 0.531250]\n",
            "3197: [D loss: 0.059295, acc: 0.976562]  [G loss: 2.576945, acc: 0.437500]\n",
            "3198: [D loss: 0.019590, acc: 0.992188]  [G loss: 2.459654, acc: 0.453125]\n",
            "3199: [D loss: 0.096904, acc: 0.953125]  [G loss: 2.485991, acc: 0.468750]\n",
            "3200: [D loss: 0.040146, acc: 0.976562]  [G loss: 1.840894, acc: 0.609375]\n",
            "3201: [D loss: 0.036713, acc: 0.992188]  [G loss: 1.822038, acc: 0.593750]\n",
            "3202: [D loss: 0.071817, acc: 0.968750]  [G loss: 1.378994, acc: 0.640625]\n",
            "3203: [D loss: 0.117563, acc: 0.960938]  [G loss: 1.025301, acc: 0.703125]\n",
            "3204: [D loss: 0.127824, acc: 0.960938]  [G loss: 1.249868, acc: 0.687500]\n",
            "3205: [D loss: 0.037260, acc: 0.984375]  [G loss: 2.017941, acc: 0.562500]\n",
            "3206: [D loss: 0.039348, acc: 0.992188]  [G loss: 1.937882, acc: 0.500000]\n",
            "3207: [D loss: 0.070567, acc: 0.984375]  [G loss: 1.623669, acc: 0.562500]\n",
            "3208: [D loss: 0.067510, acc: 0.976562]  [G loss: 1.423657, acc: 0.546875]\n",
            "3209: [D loss: 0.038426, acc: 0.984375]  [G loss: 1.093279, acc: 0.656250]\n",
            "3210: [D loss: 0.038487, acc: 0.976562]  [G loss: 1.347390, acc: 0.593750]\n",
            "3211: [D loss: 0.065182, acc: 0.976562]  [G loss: 1.644001, acc: 0.609375]\n",
            "3212: [D loss: 0.026250, acc: 0.992188]  [G loss: 1.457515, acc: 0.546875]\n",
            "3213: [D loss: 0.090920, acc: 0.976562]  [G loss: 1.936753, acc: 0.546875]\n",
            "3214: [D loss: 0.047930, acc: 0.992188]  [G loss: 1.997079, acc: 0.390625]\n",
            "3215: [D loss: 0.011674, acc: 0.992188]  [G loss: 2.143119, acc: 0.468750]\n",
            "3216: [D loss: 0.058910, acc: 0.968750]  [G loss: 2.083013, acc: 0.531250]\n",
            "3217: [D loss: 0.047406, acc: 0.976562]  [G loss: 1.897822, acc: 0.421875]\n",
            "3218: [D loss: 0.061457, acc: 0.968750]  [G loss: 1.753461, acc: 0.453125]\n",
            "3219: [D loss: 0.170359, acc: 0.929688]  [G loss: 1.590829, acc: 0.515625]\n",
            "3220: [D loss: 0.180654, acc: 0.914062]  [G loss: 2.362831, acc: 0.406250]\n",
            "3221: [D loss: 0.054918, acc: 0.976562]  [G loss: 4.192025, acc: 0.281250]\n",
            "3222: [D loss: 0.156888, acc: 0.953125]  [G loss: 3.644629, acc: 0.250000]\n",
            "3223: [D loss: 0.115670, acc: 0.937500]  [G loss: 3.214678, acc: 0.375000]\n",
            "3224: [D loss: 0.335855, acc: 0.914062]  [G loss: 2.882210, acc: 0.343750]\n",
            "3225: [D loss: 0.152291, acc: 0.914062]  [G loss: 3.065577, acc: 0.234375]\n",
            "3226: [D loss: 0.090513, acc: 0.960938]  [G loss: 4.584301, acc: 0.156250]\n",
            "3227: [D loss: 0.063853, acc: 0.984375]  [G loss: 5.085083, acc: 0.109375]\n",
            "3228: [D loss: 0.169720, acc: 0.968750]  [G loss: 5.579954, acc: 0.125000]\n",
            "3229: [D loss: 0.066814, acc: 0.984375]  [G loss: 5.016508, acc: 0.187500]\n",
            "3230: [D loss: 0.229821, acc: 0.937500]  [G loss: 6.691380, acc: 0.140625]\n",
            "3231: [D loss: 0.067865, acc: 0.976562]  [G loss: 8.087575, acc: 0.015625]\n",
            "3232: [D loss: 0.135115, acc: 0.937500]  [G loss: 7.419884, acc: 0.046875]\n",
            "3233: [D loss: 0.260214, acc: 0.929688]  [G loss: 4.987287, acc: 0.093750]\n",
            "3234: [D loss: 0.088196, acc: 0.968750]  [G loss: 2.978924, acc: 0.296875]\n",
            "3235: [D loss: 0.179676, acc: 0.914062]  [G loss: 3.573583, acc: 0.187500]\n",
            "3236: [D loss: 0.073466, acc: 0.960938]  [G loss: 3.908548, acc: 0.203125]\n",
            "3237: [D loss: 0.079595, acc: 0.992188]  [G loss: 5.412220, acc: 0.109375]\n",
            "3238: [D loss: 0.050338, acc: 0.968750]  [G loss: 5.751665, acc: 0.093750]\n",
            "3239: [D loss: 0.073750, acc: 0.976562]  [G loss: 4.613198, acc: 0.218750]\n",
            "3240: [D loss: 0.089127, acc: 0.968750]  [G loss: 3.804766, acc: 0.390625]\n",
            "3241: [D loss: 0.053919, acc: 0.984375]  [G loss: 2.817169, acc: 0.343750]\n",
            "3242: [D loss: 0.055934, acc: 0.984375]  [G loss: 2.771867, acc: 0.421875]\n",
            "3243: [D loss: 0.192510, acc: 0.914062]  [G loss: 3.675709, acc: 0.296875]\n",
            "3244: [D loss: 0.025324, acc: 1.000000]  [G loss: 4.504256, acc: 0.218750]\n",
            "3245: [D loss: 0.059647, acc: 0.984375]  [G loss: 5.460032, acc: 0.093750]\n",
            "3246: [D loss: 0.066845, acc: 0.968750]  [G loss: 5.744337, acc: 0.109375]\n",
            "3247: [D loss: 0.028066, acc: 0.992188]  [G loss: 5.201497, acc: 0.140625]\n",
            "3248: [D loss: 0.096820, acc: 0.976562]  [G loss: 4.832246, acc: 0.171875]\n",
            "3249: [D loss: 0.092584, acc: 0.960938]  [G loss: 3.890934, acc: 0.203125]\n",
            "3250: [D loss: 0.046998, acc: 0.992188]  [G loss: 4.542668, acc: 0.156250]\n",
            "3251: [D loss: 0.045276, acc: 0.984375]  [G loss: 3.756148, acc: 0.125000]\n",
            "3252: [D loss: 0.039653, acc: 0.984375]  [G loss: 3.375507, acc: 0.296875]\n",
            "3253: [D loss: 0.092828, acc: 0.960938]  [G loss: 3.503382, acc: 0.234375]\n",
            "3254: [D loss: 0.025564, acc: 0.992188]  [G loss: 4.412798, acc: 0.171875]\n",
            "3255: [D loss: 0.101621, acc: 0.960938]  [G loss: 4.118980, acc: 0.203125]\n",
            "3256: [D loss: 0.133676, acc: 0.929688]  [G loss: 1.951469, acc: 0.453125]\n",
            "3257: [D loss: 0.109231, acc: 0.976562]  [G loss: 2.408554, acc: 0.406250]\n",
            "3258: [D loss: 0.149693, acc: 0.945312]  [G loss: 2.553847, acc: 0.359375]\n",
            "3259: [D loss: 0.070911, acc: 0.968750]  [G loss: 3.230547, acc: 0.312500]\n",
            "3260: [D loss: 0.021403, acc: 0.984375]  [G loss: 3.775266, acc: 0.265625]\n",
            "3261: [D loss: 0.086281, acc: 0.968750]  [G loss: 3.285585, acc: 0.265625]\n",
            "3262: [D loss: 0.081006, acc: 0.984375]  [G loss: 3.623254, acc: 0.203125]\n",
            "3263: [D loss: 0.013897, acc: 0.992188]  [G loss: 2.784050, acc: 0.406250]\n",
            "3264: [D loss: 0.075237, acc: 0.976562]  [G loss: 2.281602, acc: 0.453125]\n",
            "3265: [D loss: 0.051010, acc: 0.992188]  [G loss: 2.250412, acc: 0.500000]\n",
            "3266: [D loss: 0.104975, acc: 0.953125]  [G loss: 1.922827, acc: 0.484375]\n",
            "3267: [D loss: 0.085093, acc: 0.976562]  [G loss: 2.096492, acc: 0.343750]\n",
            "3268: [D loss: 0.125218, acc: 0.953125]  [G loss: 1.816216, acc: 0.468750]\n",
            "3269: [D loss: 0.070656, acc: 0.984375]  [G loss: 2.648574, acc: 0.312500]\n",
            "3270: [D loss: 0.051115, acc: 0.968750]  [G loss: 3.147696, acc: 0.296875]\n",
            "3271: [D loss: 0.025353, acc: 1.000000]  [G loss: 3.327299, acc: 0.156250]\n",
            "3272: [D loss: 0.094676, acc: 0.960938]  [G loss: 3.127885, acc: 0.203125]\n",
            "3273: [D loss: 0.173298, acc: 0.953125]  [G loss: 2.542210, acc: 0.265625]\n",
            "3274: [D loss: 0.102518, acc: 0.960938]  [G loss: 2.117709, acc: 0.390625]\n",
            "3275: [D loss: 0.093137, acc: 0.976562]  [G loss: 2.219713, acc: 0.421875]\n",
            "3276: [D loss: 0.106599, acc: 0.968750]  [G loss: 2.511275, acc: 0.390625]\n",
            "3277: [D loss: 0.021141, acc: 1.000000]  [G loss: 2.594340, acc: 0.359375]\n",
            "3278: [D loss: 0.036209, acc: 0.992188]  [G loss: 3.594268, acc: 0.203125]\n",
            "3279: [D loss: 0.032616, acc: 0.992188]  [G loss: 4.441431, acc: 0.187500]\n",
            "3280: [D loss: 0.019687, acc: 0.992188]  [G loss: 3.994775, acc: 0.203125]\n",
            "3281: [D loss: 0.077761, acc: 0.984375]  [G loss: 4.566214, acc: 0.218750]\n",
            "3282: [D loss: 0.007709, acc: 1.000000]  [G loss: 4.187272, acc: 0.265625]\n",
            "3283: [D loss: 0.100858, acc: 0.968750]  [G loss: 3.817068, acc: 0.312500]\n",
            "3284: [D loss: 0.046408, acc: 0.984375]  [G loss: 3.535385, acc: 0.312500]\n",
            "3285: [D loss: 0.100204, acc: 0.968750]  [G loss: 2.961668, acc: 0.359375]\n",
            "3286: [D loss: 0.155325, acc: 0.945312]  [G loss: 3.648012, acc: 0.265625]\n",
            "3287: [D loss: 0.016257, acc: 1.000000]  [G loss: 3.821818, acc: 0.203125]\n",
            "3288: [D loss: 0.029919, acc: 0.984375]  [G loss: 4.576166, acc: 0.093750]\n",
            "3289: [D loss: 0.045254, acc: 0.984375]  [G loss: 5.778119, acc: 0.015625]\n",
            "3290: [D loss: 0.008372, acc: 1.000000]  [G loss: 6.845975, acc: 0.000000]\n",
            "3291: [D loss: 0.033033, acc: 0.984375]  [G loss: 7.645530, acc: 0.000000]\n",
            "3292: [D loss: 0.122072, acc: 0.992188]  [G loss: 8.084886, acc: 0.000000]\n",
            "3293: [D loss: 0.048947, acc: 0.976562]  [G loss: 7.820915, acc: 0.000000]\n",
            "3294: [D loss: 0.047201, acc: 0.984375]  [G loss: 7.311900, acc: 0.015625]\n",
            "3295: [D loss: 0.130654, acc: 0.960938]  [G loss: 6.774940, acc: 0.000000]\n",
            "3296: [D loss: 0.029920, acc: 0.984375]  [G loss: 5.473865, acc: 0.031250]\n",
            "3297: [D loss: 0.059110, acc: 0.976562]  [G loss: 5.580506, acc: 0.031250]\n",
            "3298: [D loss: 0.051820, acc: 0.984375]  [G loss: 5.597776, acc: 0.015625]\n",
            "3299: [D loss: 0.056303, acc: 0.968750]  [G loss: 6.943295, acc: 0.015625]\n",
            "3300: [D loss: 0.027535, acc: 0.984375]  [G loss: 7.205373, acc: 0.000000]\n",
            "3301: [D loss: 0.069888, acc: 0.984375]  [G loss: 7.230883, acc: 0.000000]\n",
            "3302: [D loss: 0.052431, acc: 0.976562]  [G loss: 5.396600, acc: 0.093750]\n",
            "3303: [D loss: 0.053001, acc: 0.984375]  [G loss: 4.822824, acc: 0.140625]\n",
            "3304: [D loss: 0.066017, acc: 0.976562]  [G loss: 4.477185, acc: 0.250000]\n",
            "3305: [D loss: 0.070902, acc: 0.968750]  [G loss: 4.436082, acc: 0.203125]\n",
            "3306: [D loss: 0.018143, acc: 1.000000]  [G loss: 4.932837, acc: 0.125000]\n",
            "3307: [D loss: 0.101358, acc: 0.968750]  [G loss: 6.547901, acc: 0.031250]\n",
            "3308: [D loss: 0.078408, acc: 0.984375]  [G loss: 6.787388, acc: 0.046875]\n",
            "3309: [D loss: 0.088494, acc: 0.976562]  [G loss: 5.170684, acc: 0.109375]\n",
            "3310: [D loss: 0.045292, acc: 0.976562]  [G loss: 5.310257, acc: 0.109375]\n",
            "3311: [D loss: 0.052295, acc: 0.984375]  [G loss: 4.736295, acc: 0.156250]\n",
            "3312: [D loss: 0.032630, acc: 0.984375]  [G loss: 4.513831, acc: 0.171875]\n",
            "3313: [D loss: 0.015542, acc: 1.000000]  [G loss: 5.546350, acc: 0.109375]\n",
            "3314: [D loss: 0.009015, acc: 1.000000]  [G loss: 6.011386, acc: 0.046875]\n",
            "3315: [D loss: 0.106095, acc: 0.984375]  [G loss: 5.805920, acc: 0.015625]\n",
            "3316: [D loss: 0.023546, acc: 0.992188]  [G loss: 5.772837, acc: 0.062500]\n",
            "3317: [D loss: 0.044766, acc: 0.976562]  [G loss: 5.483545, acc: 0.109375]\n",
            "3318: [D loss: 0.052711, acc: 0.976562]  [G loss: 6.268014, acc: 0.046875]\n",
            "3319: [D loss: 0.073721, acc: 0.968750]  [G loss: 6.497758, acc: 0.000000]\n",
            "3320: [D loss: 0.020863, acc: 0.992188]  [G loss: 6.331326, acc: 0.046875]\n",
            "3321: [D loss: 0.081082, acc: 0.984375]  [G loss: 6.187375, acc: 0.015625]\n",
            "3322: [D loss: 0.045150, acc: 0.984375]  [G loss: 6.759381, acc: 0.031250]\n",
            "3323: [D loss: 0.027348, acc: 0.984375]  [G loss: 7.125980, acc: 0.062500]\n",
            "3324: [D loss: 0.068611, acc: 0.984375]  [G loss: 6.406841, acc: 0.015625]\n",
            "3325: [D loss: 0.018375, acc: 0.992188]  [G loss: 5.460615, acc: 0.046875]\n",
            "3326: [D loss: 0.110064, acc: 0.968750]  [G loss: 5.514524, acc: 0.000000]\n",
            "3327: [D loss: 0.081986, acc: 0.960938]  [G loss: 5.069826, acc: 0.093750]\n",
            "3328: [D loss: 0.070345, acc: 0.976562]  [G loss: 5.145217, acc: 0.078125]\n",
            "3329: [D loss: 0.132640, acc: 0.960938]  [G loss: 4.524639, acc: 0.078125]\n",
            "3330: [D loss: 0.111297, acc: 0.960938]  [G loss: 4.313850, acc: 0.156250]\n",
            "3331: [D loss: 0.159810, acc: 0.937500]  [G loss: 5.250003, acc: 0.093750]\n",
            "3332: [D loss: 0.064210, acc: 0.984375]  [G loss: 5.913168, acc: 0.046875]\n",
            "3333: [D loss: 0.140528, acc: 0.953125]  [G loss: 4.807278, acc: 0.093750]\n",
            "3334: [D loss: 0.193097, acc: 0.937500]  [G loss: 3.051511, acc: 0.312500]\n",
            "3335: [D loss: 0.137650, acc: 0.953125]  [G loss: 2.851396, acc: 0.343750]\n",
            "3336: [D loss: 0.095222, acc: 0.984375]  [G loss: 3.624464, acc: 0.218750]\n",
            "3337: [D loss: 0.101972, acc: 0.953125]  [G loss: 5.453279, acc: 0.093750]\n",
            "3338: [D loss: 0.024509, acc: 0.992188]  [G loss: 7.769480, acc: 0.015625]\n",
            "3339: [D loss: 0.139526, acc: 0.929688]  [G loss: 8.131463, acc: 0.015625]\n",
            "3340: [D loss: 0.165576, acc: 0.929688]  [G loss: 5.052301, acc: 0.031250]\n",
            "3341: [D loss: 0.083543, acc: 0.968750]  [G loss: 2.762336, acc: 0.328125]\n",
            "3342: [D loss: 0.099636, acc: 0.953125]  [G loss: 2.241622, acc: 0.453125]\n",
            "3343: [D loss: 0.126924, acc: 0.968750]  [G loss: 3.114175, acc: 0.250000]\n",
            "3344: [D loss: 0.104843, acc: 0.953125]  [G loss: 4.865818, acc: 0.078125]\n",
            "3345: [D loss: 0.080431, acc: 0.960938]  [G loss: 4.382804, acc: 0.078125]\n",
            "3346: [D loss: 0.061463, acc: 0.984375]  [G loss: 4.591314, acc: 0.125000]\n",
            "3347: [D loss: 0.132425, acc: 0.960938]  [G loss: 4.000259, acc: 0.125000]\n",
            "3348: [D loss: 0.080231, acc: 0.984375]  [G loss: 3.048641, acc: 0.281250]\n",
            "3349: [D loss: 0.098857, acc: 0.976562]  [G loss: 1.820216, acc: 0.515625]\n",
            "3350: [D loss: 0.047990, acc: 0.992188]  [G loss: 1.226700, acc: 0.546875]\n",
            "3351: [D loss: 0.080998, acc: 0.984375]  [G loss: 1.199099, acc: 0.625000]\n",
            "3352: [D loss: 0.095178, acc: 0.976562]  [G loss: 1.521271, acc: 0.500000]\n",
            "3353: [D loss: 0.044831, acc: 0.984375]  [G loss: 2.518291, acc: 0.265625]\n",
            "3354: [D loss: 0.017500, acc: 1.000000]  [G loss: 3.245741, acc: 0.125000]\n",
            "3355: [D loss: 0.023165, acc: 1.000000]  [G loss: 4.209977, acc: 0.093750]\n",
            "3356: [D loss: 0.093245, acc: 0.968750]  [G loss: 3.915792, acc: 0.109375]\n",
            "3357: [D loss: 0.095390, acc: 0.976562]  [G loss: 3.628632, acc: 0.156250]\n",
            "3358: [D loss: 0.050470, acc: 0.992188]  [G loss: 2.642548, acc: 0.296875]\n",
            "3359: [D loss: 0.057184, acc: 0.976562]  [G loss: 3.118133, acc: 0.265625]\n",
            "3360: [D loss: 0.016972, acc: 1.000000]  [G loss: 3.423456, acc: 0.125000]\n",
            "3361: [D loss: 0.090117, acc: 0.968750]  [G loss: 4.382961, acc: 0.078125]\n",
            "3362: [D loss: 0.058710, acc: 0.984375]  [G loss: 4.933343, acc: 0.015625]\n",
            "3363: [D loss: 0.181283, acc: 0.976562]  [G loss: 4.564187, acc: 0.093750]\n",
            "3364: [D loss: 0.090034, acc: 0.976562]  [G loss: 4.136258, acc: 0.093750]\n",
            "3365: [D loss: 0.131572, acc: 0.960938]  [G loss: 3.097871, acc: 0.171875]\n",
            "3366: [D loss: 0.104209, acc: 0.968750]  [G loss: 4.615759, acc: 0.015625]\n",
            "3367: [D loss: 0.066113, acc: 0.968750]  [G loss: 6.368422, acc: 0.015625]\n",
            "3368: [D loss: 0.017316, acc: 0.992188]  [G loss: 6.736815, acc: 0.015625]\n",
            "3369: [D loss: 0.035255, acc: 0.984375]  [G loss: 7.360133, acc: 0.015625]\n",
            "3370: [D loss: 0.073730, acc: 0.976562]  [G loss: 7.884653, acc: 0.031250]\n",
            "3371: [D loss: 0.085605, acc: 0.960938]  [G loss: 7.449497, acc: 0.046875]\n",
            "3372: [D loss: 0.034531, acc: 0.992188]  [G loss: 7.310022, acc: 0.031250]\n",
            "3373: [D loss: 0.163248, acc: 0.968750]  [G loss: 6.238507, acc: 0.078125]\n",
            "3374: [D loss: 0.038985, acc: 0.984375]  [G loss: 6.268914, acc: 0.046875]\n",
            "3375: [D loss: 0.126557, acc: 0.968750]  [G loss: 7.093093, acc: 0.046875]\n",
            "3376: [D loss: 0.107090, acc: 0.968750]  [G loss: 8.050075, acc: 0.015625]\n",
            "3377: [D loss: 0.037334, acc: 0.984375]  [G loss: 7.938893, acc: 0.046875]\n",
            "3378: [D loss: 0.070805, acc: 0.992188]  [G loss: 7.911660, acc: 0.031250]\n",
            "3379: [D loss: 0.071951, acc: 0.984375]  [G loss: 6.736413, acc: 0.109375]\n",
            "3380: [D loss: 0.016976, acc: 0.992188]  [G loss: 5.723142, acc: 0.187500]\n",
            "3381: [D loss: 0.049200, acc: 0.984375]  [G loss: 4.616723, acc: 0.234375]\n",
            "3382: [D loss: 0.011249, acc: 1.000000]  [G loss: 4.540812, acc: 0.250000]\n",
            "3383: [D loss: 0.063819, acc: 0.976562]  [G loss: 5.100837, acc: 0.234375]\n",
            "3384: [D loss: 0.009349, acc: 1.000000]  [G loss: 5.957794, acc: 0.156250]\n",
            "3385: [D loss: 0.006811, acc: 1.000000]  [G loss: 7.098676, acc: 0.109375]\n",
            "3386: [D loss: 0.043016, acc: 0.976562]  [G loss: 7.996087, acc: 0.062500]\n",
            "3387: [D loss: 0.070923, acc: 0.992188]  [G loss: 8.117653, acc: 0.093750]\n",
            "3388: [D loss: 0.001856, acc: 1.000000]  [G loss: 8.225366, acc: 0.109375]\n",
            "3389: [D loss: 0.028736, acc: 0.992188]  [G loss: 8.047247, acc: 0.093750]\n",
            "3390: [D loss: 0.052864, acc: 0.984375]  [G loss: 6.901224, acc: 0.109375]\n",
            "3391: [D loss: 0.103494, acc: 0.968750]  [G loss: 6.777662, acc: 0.218750]\n",
            "3392: [D loss: 0.056278, acc: 0.976562]  [G loss: 6.814484, acc: 0.171875]\n",
            "3393: [D loss: 0.019646, acc: 0.992188]  [G loss: 6.349689, acc: 0.218750]\n",
            "3394: [D loss: 0.022647, acc: 0.992188]  [G loss: 6.041915, acc: 0.218750]\n",
            "3395: [D loss: 0.031445, acc: 0.992188]  [G loss: 6.802100, acc: 0.140625]\n",
            "3396: [D loss: 0.017063, acc: 0.992188]  [G loss: 6.532176, acc: 0.218750]\n",
            "3397: [D loss: 0.112117, acc: 0.976562]  [G loss: 5.606734, acc: 0.250000]\n",
            "3398: [D loss: 0.048247, acc: 0.984375]  [G loss: 5.461064, acc: 0.265625]\n",
            "3399: [D loss: 0.107108, acc: 0.976562]  [G loss: 4.564726, acc: 0.343750]\n",
            "3400: [D loss: 0.068898, acc: 0.968750]  [G loss: 5.320667, acc: 0.234375]\n",
            "3401: [D loss: 0.105904, acc: 0.976562]  [G loss: 7.111439, acc: 0.156250]\n",
            "3402: [D loss: 0.056424, acc: 0.968750]  [G loss: 6.750358, acc: 0.093750]\n",
            "3403: [D loss: 0.109766, acc: 0.976562]  [G loss: 6.666540, acc: 0.125000]\n",
            "3404: [D loss: 0.083283, acc: 0.976562]  [G loss: 6.447954, acc: 0.078125]\n",
            "3405: [D loss: 0.119447, acc: 0.984375]  [G loss: 4.404772, acc: 0.171875]\n",
            "3406: [D loss: 0.131276, acc: 0.968750]  [G loss: 2.329210, acc: 0.343750]\n",
            "3407: [D loss: 0.412619, acc: 0.914062]  [G loss: 4.055533, acc: 0.156250]\n",
            "3408: [D loss: 0.073698, acc: 0.976562]  [G loss: 6.883897, acc: 0.031250]\n",
            "3409: [D loss: 0.061994, acc: 0.968750]  [G loss: 6.457006, acc: 0.015625]\n",
            "3410: [D loss: 0.180688, acc: 0.960938]  [G loss: 5.827152, acc: 0.078125]\n",
            "3411: [D loss: 0.199838, acc: 0.945312]  [G loss: 4.736607, acc: 0.171875]\n",
            "3412: [D loss: 0.137184, acc: 0.960938]  [G loss: 3.155989, acc: 0.281250]\n",
            "3413: [D loss: 0.130204, acc: 0.968750]  [G loss: 3.077142, acc: 0.250000]\n",
            "3414: [D loss: 0.080837, acc: 0.968750]  [G loss: 2.815743, acc: 0.312500]\n",
            "3415: [D loss: 0.040767, acc: 0.992188]  [G loss: 3.817286, acc: 0.171875]\n",
            "3416: [D loss: 0.074468, acc: 0.968750]  [G loss: 4.366512, acc: 0.171875]\n",
            "3417: [D loss: 0.088316, acc: 0.976562]  [G loss: 4.691689, acc: 0.109375]\n",
            "3418: [D loss: 0.026497, acc: 0.992188]  [G loss: 4.657940, acc: 0.093750]\n",
            "3419: [D loss: 0.084788, acc: 0.976562]  [G loss: 3.920758, acc: 0.093750]\n",
            "3420: [D loss: 0.066052, acc: 0.984375]  [G loss: 3.269087, acc: 0.281250]\n",
            "3421: [D loss: 0.086605, acc: 0.968750]  [G loss: 2.543198, acc: 0.281250]\n",
            "3422: [D loss: 0.074204, acc: 0.968750]  [G loss: 2.191807, acc: 0.296875]\n",
            "3423: [D loss: 0.092674, acc: 0.960938]  [G loss: 2.282055, acc: 0.312500]\n",
            "3424: [D loss: 0.075424, acc: 0.976562]  [G loss: 2.642534, acc: 0.296875]\n",
            "3425: [D loss: 0.064206, acc: 0.968750]  [G loss: 3.237864, acc: 0.281250]\n",
            "3426: [D loss: 0.009997, acc: 1.000000]  [G loss: 3.353806, acc: 0.250000]\n",
            "3427: [D loss: 0.016703, acc: 0.992188]  [G loss: 3.486094, acc: 0.250000]\n",
            "3428: [D loss: 0.033512, acc: 0.984375]  [G loss: 3.548233, acc: 0.218750]\n",
            "3429: [D loss: 0.022858, acc: 1.000000]  [G loss: 3.750162, acc: 0.281250]\n",
            "3430: [D loss: 0.020490, acc: 0.992188]  [G loss: 3.257245, acc: 0.234375]\n",
            "3431: [D loss: 0.078582, acc: 0.984375]  [G loss: 3.107507, acc: 0.296875]\n",
            "3432: [D loss: 0.049982, acc: 0.984375]  [G loss: 2.577821, acc: 0.343750]\n",
            "3433: [D loss: 0.042570, acc: 0.976562]  [G loss: 2.201028, acc: 0.437500]\n",
            "3434: [D loss: 0.016615, acc: 0.992188]  [G loss: 2.072055, acc: 0.437500]\n",
            "3435: [D loss: 0.043143, acc: 0.984375]  [G loss: 2.428387, acc: 0.437500]\n",
            "3436: [D loss: 0.072430, acc: 0.976562]  [G loss: 2.346839, acc: 0.406250]\n",
            "3437: [D loss: 0.063808, acc: 0.984375]  [G loss: 2.312614, acc: 0.390625]\n",
            "3438: [D loss: 0.032750, acc: 0.984375]  [G loss: 2.858841, acc: 0.390625]\n",
            "3439: [D loss: 0.027196, acc: 0.992188]  [G loss: 3.120775, acc: 0.421875]\n",
            "3440: [D loss: 0.067419, acc: 0.976562]  [G loss: 2.769870, acc: 0.390625]\n",
            "3441: [D loss: 0.025987, acc: 0.984375]  [G loss: 2.955564, acc: 0.406250]\n",
            "3442: [D loss: 0.027769, acc: 0.984375]  [G loss: 2.127789, acc: 0.453125]\n",
            "3443: [D loss: 0.059892, acc: 0.984375]  [G loss: 2.301804, acc: 0.515625]\n",
            "3444: [D loss: 0.022409, acc: 0.992188]  [G loss: 2.433311, acc: 0.359375]\n",
            "3445: [D loss: 0.037305, acc: 0.992188]  [G loss: 2.605797, acc: 0.406250]\n",
            "3446: [D loss: 0.013321, acc: 0.992188]  [G loss: 2.055725, acc: 0.468750]\n",
            "3447: [D loss: 0.018927, acc: 1.000000]  [G loss: 2.722498, acc: 0.375000]\n",
            "3448: [D loss: 0.024326, acc: 0.992188]  [G loss: 3.077218, acc: 0.343750]\n",
            "3449: [D loss: 0.017749, acc: 0.992188]  [G loss: 3.388962, acc: 0.296875]\n",
            "3450: [D loss: 0.031723, acc: 0.992188]  [G loss: 3.087103, acc: 0.343750]\n",
            "3451: [D loss: 0.066852, acc: 0.984375]  [G loss: 2.670560, acc: 0.375000]\n",
            "3452: [D loss: 0.025326, acc: 0.984375]  [G loss: 2.653393, acc: 0.375000]\n",
            "3453: [D loss: 0.085937, acc: 0.976562]  [G loss: 2.514910, acc: 0.343750]\n",
            "3454: [D loss: 0.043873, acc: 0.992188]  [G loss: 2.234784, acc: 0.500000]\n",
            "3455: [D loss: 0.045244, acc: 0.984375]  [G loss: 2.476840, acc: 0.375000]\n",
            "3456: [D loss: 0.056267, acc: 0.968750]  [G loss: 3.313279, acc: 0.234375]\n",
            "3457: [D loss: 0.174574, acc: 0.953125]  [G loss: 4.386124, acc: 0.203125]\n",
            "3458: [D loss: 0.059960, acc: 0.976562]  [G loss: 4.285790, acc: 0.234375]\n",
            "3459: [D loss: 0.182234, acc: 0.953125]  [G loss: 2.832522, acc: 0.328125]\n",
            "3460: [D loss: 0.157614, acc: 0.960938]  [G loss: 2.339067, acc: 0.343750]\n",
            "3461: [D loss: 0.091542, acc: 0.976562]  [G loss: 3.048424, acc: 0.250000]\n",
            "3462: [D loss: 0.135123, acc: 0.953125]  [G loss: 3.421126, acc: 0.156250]\n",
            "3463: [D loss: 0.034171, acc: 1.000000]  [G loss: 4.380005, acc: 0.140625]\n",
            "3464: [D loss: 0.020681, acc: 1.000000]  [G loss: 4.716625, acc: 0.109375]\n",
            "3465: [D loss: 0.022418, acc: 0.984375]  [G loss: 4.876571, acc: 0.078125]\n",
            "3466: [D loss: 0.097002, acc: 0.960938]  [G loss: 4.602571, acc: 0.062500]\n",
            "3467: [D loss: 0.029142, acc: 0.992188]  [G loss: 3.672211, acc: 0.093750]\n",
            "3468: [D loss: 0.027680, acc: 1.000000]  [G loss: 3.324646, acc: 0.203125]\n",
            "3469: [D loss: 0.056653, acc: 0.984375]  [G loss: 3.170075, acc: 0.250000]\n",
            "3470: [D loss: 0.063537, acc: 0.976562]  [G loss: 3.508115, acc: 0.218750]\n",
            "3471: [D loss: 0.053296, acc: 0.976562]  [G loss: 3.689155, acc: 0.156250]\n",
            "3472: [D loss: 0.044720, acc: 0.976562]  [G loss: 3.456804, acc: 0.250000]\n",
            "3473: [D loss: 0.024196, acc: 1.000000]  [G loss: 3.561996, acc: 0.203125]\n",
            "3474: [D loss: 0.083078, acc: 0.968750]  [G loss: 2.996127, acc: 0.218750]\n",
            "3475: [D loss: 0.034873, acc: 0.992188]  [G loss: 2.678155, acc: 0.312500]\n",
            "3476: [D loss: 0.023662, acc: 0.992188]  [G loss: 2.577783, acc: 0.390625]\n",
            "3477: [D loss: 0.021719, acc: 1.000000]  [G loss: 1.956223, acc: 0.500000]\n",
            "3478: [D loss: 0.076666, acc: 0.976562]  [G loss: 2.655234, acc: 0.359375]\n",
            "3479: [D loss: 0.030332, acc: 0.992188]  [G loss: 3.676190, acc: 0.171875]\n",
            "3480: [D loss: 0.015724, acc: 1.000000]  [G loss: 4.285960, acc: 0.203125]\n",
            "3481: [D loss: 0.127963, acc: 0.976562]  [G loss: 3.779158, acc: 0.187500]\n",
            "3482: [D loss: 0.079607, acc: 0.976562]  [G loss: 2.984196, acc: 0.375000]\n",
            "3483: [D loss: 0.045234, acc: 0.976562]  [G loss: 2.149246, acc: 0.500000]\n",
            "3484: [D loss: 0.095116, acc: 0.945312]  [G loss: 2.810633, acc: 0.375000]\n",
            "3485: [D loss: 0.099942, acc: 0.953125]  [G loss: 3.088726, acc: 0.218750]\n",
            "3486: [D loss: 0.118152, acc: 0.976562]  [G loss: 3.990203, acc: 0.187500]\n",
            "3487: [D loss: 0.030234, acc: 0.992188]  [G loss: 4.236227, acc: 0.140625]\n",
            "3488: [D loss: 0.147868, acc: 0.984375]  [G loss: 4.118458, acc: 0.187500]\n",
            "3489: [D loss: 0.084210, acc: 0.960938]  [G loss: 3.795622, acc: 0.187500]\n",
            "3490: [D loss: 0.122027, acc: 0.945312]  [G loss: 2.485360, acc: 0.265625]\n",
            "3491: [D loss: 0.065527, acc: 0.976562]  [G loss: 1.917774, acc: 0.359375]\n",
            "3492: [D loss: 0.131089, acc: 0.945312]  [G loss: 2.119718, acc: 0.312500]\n",
            "3493: [D loss: 0.063030, acc: 0.984375]  [G loss: 3.254276, acc: 0.218750]\n",
            "3494: [D loss: 0.032271, acc: 0.992188]  [G loss: 4.602585, acc: 0.093750]\n",
            "3495: [D loss: 0.089455, acc: 0.960938]  [G loss: 4.301511, acc: 0.093750]\n",
            "3496: [D loss: 0.069454, acc: 0.968750]  [G loss: 4.171535, acc: 0.140625]\n",
            "3497: [D loss: 0.106693, acc: 0.960938]  [G loss: 3.293465, acc: 0.187500]\n",
            "3498: [D loss: 0.034046, acc: 0.992188]  [G loss: 3.589821, acc: 0.171875]\n",
            "3499: [D loss: 0.038956, acc: 0.992188]  [G loss: 2.861948, acc: 0.203125]\n",
            "3500: [D loss: 0.064167, acc: 0.968750]  [G loss: 3.257730, acc: 0.140625]\n",
            "3501: [D loss: 0.040370, acc: 0.976562]  [G loss: 3.648404, acc: 0.046875]\n",
            "3502: [D loss: 0.020967, acc: 1.000000]  [G loss: 3.799167, acc: 0.031250]\n",
            "3503: [D loss: 0.016737, acc: 1.000000]  [G loss: 4.459056, acc: 0.000000]\n",
            "3504: [D loss: 0.018383, acc: 1.000000]  [G loss: 5.391007, acc: 0.031250]\n",
            "3505: [D loss: 0.046388, acc: 0.968750]  [G loss: 4.960053, acc: 0.015625]\n",
            "3506: [D loss: 0.040474, acc: 0.984375]  [G loss: 5.500262, acc: 0.015625]\n",
            "3507: [D loss: 0.036399, acc: 0.984375]  [G loss: 5.573863, acc: 0.000000]\n",
            "3508: [D loss: 0.030836, acc: 1.000000]  [G loss: 5.433256, acc: 0.000000]\n",
            "3509: [D loss: 0.046113, acc: 0.992188]  [G loss: 5.653384, acc: 0.000000]\n",
            "3510: [D loss: 0.038145, acc: 0.984375]  [G loss: 5.919310, acc: 0.000000]\n",
            "3511: [D loss: 0.006631, acc: 1.000000]  [G loss: 6.239131, acc: 0.000000]\n",
            "3512: [D loss: 0.005517, acc: 1.000000]  [G loss: 6.104895, acc: 0.000000]\n",
            "3513: [D loss: 0.003297, acc: 1.000000]  [G loss: 6.121785, acc: 0.000000]\n",
            "3514: [D loss: 0.008214, acc: 1.000000]  [G loss: 6.839933, acc: 0.015625]\n",
            "3515: [D loss: 0.038303, acc: 0.984375]  [G loss: 6.445587, acc: 0.000000]\n",
            "3516: [D loss: 0.005803, acc: 1.000000]  [G loss: 6.171437, acc: 0.015625]\n",
            "3517: [D loss: 0.007185, acc: 1.000000]  [G loss: 5.792624, acc: 0.015625]\n",
            "3518: [D loss: 0.011340, acc: 1.000000]  [G loss: 6.314122, acc: 0.015625]\n",
            "3519: [D loss: 0.007238, acc: 1.000000]  [G loss: 6.035556, acc: 0.015625]\n",
            "3520: [D loss: 0.012434, acc: 1.000000]  [G loss: 6.331561, acc: 0.015625]\n",
            "3521: [D loss: 0.009378, acc: 1.000000]  [G loss: 6.490408, acc: 0.015625]\n",
            "3522: [D loss: 0.016826, acc: 0.992188]  [G loss: 7.066181, acc: 0.015625]\n",
            "3523: [D loss: 0.025166, acc: 0.992188]  [G loss: 7.422095, acc: 0.015625]\n",
            "3524: [D loss: 0.003599, acc: 1.000000]  [G loss: 8.043573, acc: 0.031250]\n",
            "3525: [D loss: 0.041930, acc: 0.984375]  [G loss: 8.353083, acc: 0.000000]\n",
            "3526: [D loss: 0.063474, acc: 0.984375]  [G loss: 7.679525, acc: 0.031250]\n",
            "3527: [D loss: 0.066592, acc: 0.984375]  [G loss: 6.786546, acc: 0.031250]\n",
            "3528: [D loss: 0.096548, acc: 0.968750]  [G loss: 6.368069, acc: 0.046875]\n",
            "3529: [D loss: 0.033335, acc: 0.984375]  [G loss: 5.225370, acc: 0.062500]\n",
            "3530: [D loss: 0.043531, acc: 0.984375]  [G loss: 5.146378, acc: 0.140625]\n",
            "3531: [D loss: 0.076522, acc: 0.968750]  [G loss: 6.197555, acc: 0.031250]\n",
            "3532: [D loss: 0.002188, acc: 1.000000]  [G loss: 7.250063, acc: 0.062500]\n",
            "3533: [D loss: 0.033904, acc: 0.976562]  [G loss: 7.704114, acc: 0.015625]\n",
            "3534: [D loss: 0.053337, acc: 0.992188]  [G loss: 7.101693, acc: 0.031250]\n",
            "3535: [D loss: 0.073326, acc: 0.976562]  [G loss: 5.953223, acc: 0.109375]\n",
            "3536: [D loss: 0.035549, acc: 0.976562]  [G loss: 4.941026, acc: 0.156250]\n",
            "3537: [D loss: 0.032734, acc: 0.992188]  [G loss: 4.105722, acc: 0.250000]\n",
            "3538: [D loss: 0.032413, acc: 0.984375]  [G loss: 2.909424, acc: 0.421875]\n",
            "3539: [D loss: 0.063221, acc: 0.976562]  [G loss: 2.607887, acc: 0.453125]\n",
            "3540: [D loss: 0.058812, acc: 0.976562]  [G loss: 2.348778, acc: 0.546875]\n",
            "3541: [D loss: 0.017243, acc: 0.992188]  [G loss: 2.332622, acc: 0.453125]\n",
            "3542: [D loss: 0.014292, acc: 1.000000]  [G loss: 3.127658, acc: 0.453125]\n",
            "3543: [D loss: 0.021407, acc: 0.992188]  [G loss: 3.143004, acc: 0.390625]\n",
            "3544: [D loss: 0.012615, acc: 0.992188]  [G loss: 2.682577, acc: 0.328125]\n",
            "3545: [D loss: 0.023970, acc: 0.984375]  [G loss: 2.487592, acc: 0.406250]\n",
            "3546: [D loss: 0.026566, acc: 0.992188]  [G loss: 2.453631, acc: 0.421875]\n",
            "3547: [D loss: 0.028944, acc: 0.992188]  [G loss: 1.817643, acc: 0.531250]\n",
            "3548: [D loss: 0.065047, acc: 0.984375]  [G loss: 1.214874, acc: 0.609375]\n",
            "3549: [D loss: 0.059207, acc: 0.984375]  [G loss: 1.895074, acc: 0.500000]\n",
            "3550: [D loss: 0.044631, acc: 0.976562]  [G loss: 2.532709, acc: 0.500000]\n",
            "3551: [D loss: 0.113051, acc: 0.968750]  [G loss: 4.385559, acc: 0.328125]\n",
            "3552: [D loss: 0.078244, acc: 0.984375]  [G loss: 4.893143, acc: 0.203125]\n",
            "3553: [D loss: 0.184113, acc: 0.945312]  [G loss: 3.939774, acc: 0.328125]\n",
            "3554: [D loss: 0.218382, acc: 0.945312]  [G loss: 2.400435, acc: 0.531250]\n",
            "3555: [D loss: 0.381522, acc: 0.867188]  [G loss: 4.326883, acc: 0.312500]\n",
            "3556: [D loss: 0.191287, acc: 0.953125]  [G loss: 6.165514, acc: 0.171875]\n",
            "3557: [D loss: 0.190749, acc: 0.960938]  [G loss: 5.911608, acc: 0.109375]\n",
            "3558: [D loss: 0.154515, acc: 0.968750]  [G loss: 3.760513, acc: 0.281250]\n",
            "3559: [D loss: 0.268903, acc: 0.890625]  [G loss: 3.333588, acc: 0.312500]\n",
            "3560: [D loss: 0.223787, acc: 0.898438]  [G loss: 8.485651, acc: 0.046875]\n",
            "3561: [D loss: 0.144087, acc: 0.960938]  [G loss: 10.798986, acc: 0.000000]\n",
            "3562: [D loss: 0.264096, acc: 0.921875]  [G loss: 7.576255, acc: 0.046875]\n",
            "3563: [D loss: 0.143381, acc: 0.960938]  [G loss: 3.990482, acc: 0.296875]\n",
            "3564: [D loss: 0.488133, acc: 0.835938]  [G loss: 7.529269, acc: 0.046875]\n",
            "3565: [D loss: 0.142375, acc: 0.953125]  [G loss: 9.828138, acc: 0.015625]\n",
            "3566: [D loss: 0.330224, acc: 0.953125]  [G loss: 5.908815, acc: 0.156250]\n",
            "3567: [D loss: 0.079924, acc: 0.960938]  [G loss: 4.609254, acc: 0.203125]\n",
            "3568: [D loss: 0.057897, acc: 0.968750]  [G loss: 5.684740, acc: 0.156250]\n",
            "3569: [D loss: 0.044251, acc: 0.976562]  [G loss: 5.730297, acc: 0.140625]\n",
            "3570: [D loss: 0.091921, acc: 0.984375]  [G loss: 6.027090, acc: 0.171875]\n",
            "3571: [D loss: 0.098953, acc: 0.968750]  [G loss: 6.006214, acc: 0.140625]\n",
            "3572: [D loss: 0.052611, acc: 0.992188]  [G loss: 6.169684, acc: 0.203125]\n",
            "3573: [D loss: 0.094171, acc: 0.992188]  [G loss: 5.418243, acc: 0.218750]\n",
            "3574: [D loss: 0.038325, acc: 0.992188]  [G loss: 3.996259, acc: 0.296875]\n",
            "3575: [D loss: 0.029428, acc: 0.984375]  [G loss: 4.782497, acc: 0.312500]\n",
            "3576: [D loss: 0.109209, acc: 0.953125]  [G loss: 4.340709, acc: 0.328125]\n",
            "3577: [D loss: 0.066806, acc: 0.984375]  [G loss: 3.882189, acc: 0.406250]\n",
            "3578: [D loss: 0.035122, acc: 0.984375]  [G loss: 4.333076, acc: 0.250000]\n",
            "3579: [D loss: 0.052483, acc: 0.968750]  [G loss: 5.730016, acc: 0.265625]\n",
            "3580: [D loss: 0.149863, acc: 0.968750]  [G loss: 3.956829, acc: 0.312500]\n",
            "3581: [D loss: 0.063381, acc: 0.984375]  [G loss: 3.552485, acc: 0.375000]\n",
            "3582: [D loss: 0.094188, acc: 0.968750]  [G loss: 4.392941, acc: 0.281250]\n",
            "3583: [D loss: 0.023860, acc: 0.992188]  [G loss: 5.246713, acc: 0.218750]\n",
            "3584: [D loss: 0.067613, acc: 0.968750]  [G loss: 4.852876, acc: 0.250000]\n",
            "3585: [D loss: 0.008000, acc: 1.000000]  [G loss: 3.970979, acc: 0.312500]\n",
            "3586: [D loss: 0.113654, acc: 0.976562]  [G loss: 3.432323, acc: 0.359375]\n",
            "3587: [D loss: 0.109406, acc: 0.976562]  [G loss: 3.331353, acc: 0.328125]\n",
            "3588: [D loss: 0.047600, acc: 0.976562]  [G loss: 4.313124, acc: 0.296875]\n",
            "3589: [D loss: 0.038911, acc: 0.984375]  [G loss: 5.792496, acc: 0.203125]\n",
            "3590: [D loss: 0.085547, acc: 0.960938]  [G loss: 5.119459, acc: 0.203125]\n",
            "3591: [D loss: 0.057211, acc: 0.984375]  [G loss: 4.261481, acc: 0.296875]\n",
            "3592: [D loss: 0.110488, acc: 0.953125]  [G loss: 4.061691, acc: 0.250000]\n",
            "3593: [D loss: 0.134753, acc: 0.960938]  [G loss: 4.886533, acc: 0.218750]\n",
            "3594: [D loss: 0.065504, acc: 0.976562]  [G loss: 3.152613, acc: 0.343750]\n",
            "3595: [D loss: 0.045976, acc: 0.984375]  [G loss: 4.088180, acc: 0.218750]\n",
            "3596: [D loss: 0.021862, acc: 1.000000]  [G loss: 3.559077, acc: 0.406250]\n",
            "3597: [D loss: 0.155297, acc: 0.953125]  [G loss: 5.349228, acc: 0.218750]\n",
            "3598: [D loss: 0.084776, acc: 0.976562]  [G loss: 6.897664, acc: 0.156250]\n",
            "3599: [D loss: 0.020783, acc: 0.992188]  [G loss: 8.068733, acc: 0.031250]\n",
            "3600: [D loss: 0.262051, acc: 0.929688]  [G loss: 3.846741, acc: 0.312500]\n",
            "3601: [D loss: 0.051576, acc: 0.976562]  [G loss: 2.474307, acc: 0.437500]\n",
            "3602: [D loss: 0.079368, acc: 0.968750]  [G loss: 2.417530, acc: 0.546875]\n",
            "3603: [D loss: 0.209071, acc: 0.937500]  [G loss: 5.758074, acc: 0.156250]\n",
            "3604: [D loss: 0.105352, acc: 0.968750]  [G loss: 8.236143, acc: 0.015625]\n",
            "3605: [D loss: 0.451558, acc: 0.921875]  [G loss: 7.970611, acc: 0.031250]\n",
            "3606: [D loss: 0.376569, acc: 0.914062]  [G loss: 4.802511, acc: 0.171875]\n",
            "3607: [D loss: 0.215928, acc: 0.945312]  [G loss: 1.685725, acc: 0.578125]\n",
            "3608: [D loss: 0.364391, acc: 0.882812]  [G loss: 2.563103, acc: 0.359375]\n",
            "3609: [D loss: 0.084230, acc: 0.976562]  [G loss: 3.587599, acc: 0.203125]\n",
            "3610: [D loss: 0.081079, acc: 0.976562]  [G loss: 4.182116, acc: 0.203125]\n",
            "3611: [D loss: 0.036413, acc: 0.992188]  [G loss: 4.590605, acc: 0.156250]\n",
            "3612: [D loss: 0.049092, acc: 0.976562]  [G loss: 4.790501, acc: 0.140625]\n",
            "3613: [D loss: 0.149178, acc: 0.937500]  [G loss: 3.229956, acc: 0.265625]\n",
            "3614: [D loss: 0.081529, acc: 0.992188]  [G loss: 2.695756, acc: 0.281250]\n",
            "3615: [D loss: 0.053150, acc: 0.968750]  [G loss: 2.226615, acc: 0.265625]\n",
            "3616: [D loss: 0.099135, acc: 0.976562]  [G loss: 2.292819, acc: 0.312500]\n",
            "3617: [D loss: 0.093971, acc: 0.960938]  [G loss: 2.290470, acc: 0.468750]\n",
            "3618: [D loss: 0.052344, acc: 0.984375]  [G loss: 2.518137, acc: 0.375000]\n",
            "3619: [D loss: 0.054549, acc: 0.984375]  [G loss: 2.568145, acc: 0.296875]\n",
            "3620: [D loss: 0.089675, acc: 0.968750]  [G loss: 2.954397, acc: 0.312500]\n",
            "3621: [D loss: 0.056273, acc: 0.984375]  [G loss: 2.889232, acc: 0.406250]\n",
            "3622: [D loss: 0.049667, acc: 0.984375]  [G loss: 2.810815, acc: 0.281250]\n",
            "3623: [D loss: 0.063924, acc: 0.984375]  [G loss: 3.129419, acc: 0.328125]\n",
            "3624: [D loss: 0.060777, acc: 0.968750]  [G loss: 2.889884, acc: 0.421875]\n",
            "3625: [D loss: 0.061101, acc: 0.968750]  [G loss: 2.741426, acc: 0.328125]\n",
            "3626: [D loss: 0.050087, acc: 0.984375]  [G loss: 2.817503, acc: 0.328125]\n",
            "3627: [D loss: 0.032370, acc: 0.984375]  [G loss: 3.116466, acc: 0.296875]\n",
            "3628: [D loss: 0.026311, acc: 1.000000]  [G loss: 2.854825, acc: 0.296875]\n",
            "3629: [D loss: 0.037246, acc: 0.984375]  [G loss: 3.849580, acc: 0.281250]\n",
            "3630: [D loss: 0.059956, acc: 0.968750]  [G loss: 3.517243, acc: 0.375000]\n",
            "3631: [D loss: 0.132878, acc: 0.960938]  [G loss: 3.955969, acc: 0.265625]\n",
            "3632: [D loss: 0.053614, acc: 0.968750]  [G loss: 4.845535, acc: 0.218750]\n",
            "3633: [D loss: 0.110191, acc: 0.968750]  [G loss: 4.178791, acc: 0.203125]\n",
            "3634: [D loss: 0.052555, acc: 0.984375]  [G loss: 3.950714, acc: 0.250000]\n",
            "3635: [D loss: 0.044483, acc: 0.968750]  [G loss: 3.348615, acc: 0.390625]\n",
            "3636: [D loss: 0.057955, acc: 0.968750]  [G loss: 2.882960, acc: 0.468750]\n",
            "3637: [D loss: 0.131698, acc: 0.953125]  [G loss: 3.176607, acc: 0.390625]\n",
            "3638: [D loss: 0.021268, acc: 0.992188]  [G loss: 4.360188, acc: 0.296875]\n",
            "3639: [D loss: 0.138140, acc: 0.968750]  [G loss: 4.183794, acc: 0.234375]\n",
            "3640: [D loss: 0.152189, acc: 0.945312]  [G loss: 3.032086, acc: 0.390625]\n",
            "3641: [D loss: 0.147278, acc: 0.953125]  [G loss: 2.887476, acc: 0.296875]\n",
            "3642: [D loss: 0.075160, acc: 0.968750]  [G loss: 2.796893, acc: 0.375000]\n",
            "3643: [D loss: 0.114494, acc: 0.945312]  [G loss: 3.362592, acc: 0.312500]\n",
            "3644: [D loss: 0.112977, acc: 0.921875]  [G loss: 5.438592, acc: 0.062500]\n",
            "3645: [D loss: 0.048352, acc: 0.976562]  [G loss: 5.967205, acc: 0.031250]\n",
            "3646: [D loss: 0.071945, acc: 0.976562]  [G loss: 6.487265, acc: 0.062500]\n",
            "3647: [D loss: 0.091084, acc: 0.968750]  [G loss: 6.117323, acc: 0.046875]\n",
            "3648: [D loss: 0.053113, acc: 0.976562]  [G loss: 5.684919, acc: 0.062500]\n",
            "3649: [D loss: 0.086318, acc: 0.937500]  [G loss: 4.283238, acc: 0.125000]\n",
            "3650: [D loss: 0.075309, acc: 0.976562]  [G loss: 3.817994, acc: 0.265625]\n",
            "3651: [D loss: 0.057011, acc: 0.976562]  [G loss: 2.504914, acc: 0.312500]\n",
            "3652: [D loss: 0.085128, acc: 0.968750]  [G loss: 3.325178, acc: 0.328125]\n",
            "3653: [D loss: 0.071341, acc: 0.968750]  [G loss: 3.893593, acc: 0.296875]\n",
            "3654: [D loss: 0.070963, acc: 0.960938]  [G loss: 4.975612, acc: 0.093750]\n",
            "3655: [D loss: 0.019838, acc: 0.992188]  [G loss: 5.271246, acc: 0.109375]\n",
            "3656: [D loss: 0.099642, acc: 0.968750]  [G loss: 6.205029, acc: 0.093750]\n",
            "3657: [D loss: 0.071929, acc: 0.976562]  [G loss: 6.095408, acc: 0.062500]\n",
            "3658: [D loss: 0.100324, acc: 0.984375]  [G loss: 5.293949, acc: 0.156250]\n",
            "3659: [D loss: 0.130364, acc: 0.960938]  [G loss: 4.483966, acc: 0.203125]\n",
            "3660: [D loss: 0.065695, acc: 0.968750]  [G loss: 4.035732, acc: 0.218750]\n",
            "3661: [D loss: 0.024210, acc: 0.992188]  [G loss: 3.651415, acc: 0.375000]\n",
            "3662: [D loss: 0.045828, acc: 0.984375]  [G loss: 2.919404, acc: 0.406250]\n",
            "3663: [D loss: 0.014508, acc: 1.000000]  [G loss: 2.858239, acc: 0.406250]\n",
            "3664: [D loss: 0.106498, acc: 0.945312]  [G loss: 3.203488, acc: 0.359375]\n",
            "3665: [D loss: 0.112737, acc: 0.968750]  [G loss: 4.071097, acc: 0.265625]\n",
            "3666: [D loss: 0.091793, acc: 0.960938]  [G loss: 5.056469, acc: 0.265625]\n",
            "3667: [D loss: 0.009290, acc: 1.000000]  [G loss: 4.612036, acc: 0.390625]\n",
            "3668: [D loss: 0.134794, acc: 0.968750]  [G loss: 4.653471, acc: 0.312500]\n",
            "3669: [D loss: 0.090529, acc: 0.976562]  [G loss: 3.810526, acc: 0.343750]\n",
            "3670: [D loss: 0.120002, acc: 0.976562]  [G loss: 3.504735, acc: 0.390625]\n",
            "3671: [D loss: 0.049648, acc: 0.976562]  [G loss: 3.125378, acc: 0.421875]\n",
            "3672: [D loss: 0.038327, acc: 0.992188]  [G loss: 3.030822, acc: 0.375000]\n",
            "3673: [D loss: 0.055493, acc: 0.984375]  [G loss: 3.466337, acc: 0.296875]\n",
            "3674: [D loss: 0.063505, acc: 0.968750]  [G loss: 2.833495, acc: 0.312500]\n",
            "3675: [D loss: 0.041494, acc: 0.984375]  [G loss: 3.502729, acc: 0.234375]\n",
            "3676: [D loss: 0.034623, acc: 0.984375]  [G loss: 4.042664, acc: 0.218750]\n",
            "3677: [D loss: 0.026469, acc: 0.992188]  [G loss: 3.931764, acc: 0.218750]\n",
            "3678: [D loss: 0.054485, acc: 0.976562]  [G loss: 5.409828, acc: 0.171875]\n",
            "3679: [D loss: 0.121953, acc: 0.984375]  [G loss: 5.157466, acc: 0.140625]\n",
            "3680: [D loss: 0.116354, acc: 0.960938]  [G loss: 5.747154, acc: 0.125000]\n",
            "3681: [D loss: 0.089871, acc: 0.976562]  [G loss: 4.295951, acc: 0.187500]\n",
            "3682: [D loss: 0.087951, acc: 0.976562]  [G loss: 4.288116, acc: 0.250000]\n",
            "3683: [D loss: 0.077095, acc: 0.976562]  [G loss: 3.588018, acc: 0.250000]\n",
            "3684: [D loss: 0.026181, acc: 1.000000]  [G loss: 3.147830, acc: 0.328125]\n",
            "3685: [D loss: 0.025730, acc: 1.000000]  [G loss: 3.262966, acc: 0.312500]\n",
            "3686: [D loss: 0.111586, acc: 0.968750]  [G loss: 3.448091, acc: 0.203125]\n",
            "3687: [D loss: 0.184508, acc: 0.945312]  [G loss: 5.968572, acc: 0.093750]\n",
            "3688: [D loss: 0.027238, acc: 0.976562]  [G loss: 6.232802, acc: 0.078125]\n",
            "3689: [D loss: 0.212022, acc: 0.945312]  [G loss: 5.960497, acc: 0.140625]\n",
            "3690: [D loss: 0.140256, acc: 0.968750]  [G loss: 4.765508, acc: 0.203125]\n",
            "3691: [D loss: 0.043850, acc: 0.976562]  [G loss: 4.203149, acc: 0.281250]\n",
            "3692: [D loss: 0.087489, acc: 0.960938]  [G loss: 3.585055, acc: 0.312500]\n",
            "3693: [D loss: 0.114190, acc: 0.945312]  [G loss: 4.522160, acc: 0.234375]\n",
            "3694: [D loss: 0.082072, acc: 0.968750]  [G loss: 4.549781, acc: 0.187500]\n",
            "3695: [D loss: 0.067448, acc: 0.968750]  [G loss: 5.800816, acc: 0.046875]\n",
            "3696: [D loss: 0.063066, acc: 0.976562]  [G loss: 5.731134, acc: 0.109375]\n",
            "3697: [D loss: 0.047954, acc: 0.984375]  [G loss: 5.932946, acc: 0.046875]\n",
            "3698: [D loss: 0.016065, acc: 1.000000]  [G loss: 6.494608, acc: 0.031250]\n",
            "3699: [D loss: 0.026130, acc: 1.000000]  [G loss: 6.056959, acc: 0.062500]\n",
            "3700: [D loss: 0.060227, acc: 0.984375]  [G loss: 5.854061, acc: 0.015625]\n",
            "3701: [D loss: 0.036661, acc: 0.984375]  [G loss: 4.999359, acc: 0.093750]\n",
            "3702: [D loss: 0.032831, acc: 0.992188]  [G loss: 5.195284, acc: 0.062500]\n",
            "3703: [D loss: 0.140450, acc: 0.945312]  [G loss: 5.738389, acc: 0.046875]\n",
            "3704: [D loss: 0.087874, acc: 0.968750]  [G loss: 5.853498, acc: 0.031250]\n",
            "3705: [D loss: 0.087952, acc: 0.968750]  [G loss: 6.006629, acc: 0.031250]\n",
            "3706: [D loss: 0.060178, acc: 0.976562]  [G loss: 4.932266, acc: 0.078125]\n",
            "3707: [D loss: 0.062152, acc: 0.960938]  [G loss: 4.354728, acc: 0.187500]\n",
            "3708: [D loss: 0.102775, acc: 0.945312]  [G loss: 3.250817, acc: 0.265625]\n",
            "3709: [D loss: 0.192362, acc: 0.921875]  [G loss: 3.121994, acc: 0.296875]\n",
            "3710: [D loss: 0.064685, acc: 0.968750]  [G loss: 3.645808, acc: 0.156250]\n",
            "3711: [D loss: 0.068299, acc: 0.976562]  [G loss: 4.708111, acc: 0.093750]\n",
            "3712: [D loss: 0.056101, acc: 0.984375]  [G loss: 5.303100, acc: 0.062500]\n",
            "3713: [D loss: 0.156991, acc: 0.937500]  [G loss: 5.030089, acc: 0.062500]\n",
            "3714: [D loss: 0.072824, acc: 0.984375]  [G loss: 4.262471, acc: 0.218750]\n",
            "3715: [D loss: 0.080263, acc: 0.968750]  [G loss: 4.816935, acc: 0.125000]\n",
            "3716: [D loss: 0.080593, acc: 0.992188]  [G loss: 4.064346, acc: 0.218750]\n",
            "3717: [D loss: 0.047171, acc: 0.968750]  [G loss: 3.661963, acc: 0.234375]\n",
            "3718: [D loss: 0.040707, acc: 0.976562]  [G loss: 3.288065, acc: 0.265625]\n",
            "3719: [D loss: 0.029959, acc: 0.984375]  [G loss: 3.637607, acc: 0.343750]\n",
            "3720: [D loss: 0.106460, acc: 0.953125]  [G loss: 3.576712, acc: 0.265625]\n",
            "3721: [D loss: 0.043826, acc: 0.984375]  [G loss: 2.625539, acc: 0.375000]\n",
            "3722: [D loss: 0.206227, acc: 0.945312]  [G loss: 2.504458, acc: 0.421875]\n",
            "3723: [D loss: 0.092198, acc: 0.968750]  [G loss: 3.294490, acc: 0.375000]\n",
            "3724: [D loss: 0.044121, acc: 0.976562]  [G loss: 3.689272, acc: 0.265625]\n",
            "3725: [D loss: 0.053372, acc: 0.984375]  [G loss: 3.961877, acc: 0.265625]\n",
            "3726: [D loss: 0.046031, acc: 0.976562]  [G loss: 3.684637, acc: 0.203125]\n",
            "3727: [D loss: 0.121791, acc: 0.960938]  [G loss: 4.585360, acc: 0.218750]\n",
            "3728: [D loss: 0.048073, acc: 0.976562]  [G loss: 4.133615, acc: 0.281250]\n",
            "3729: [D loss: 0.062126, acc: 0.984375]  [G loss: 4.682631, acc: 0.250000]\n",
            "3730: [D loss: 0.029071, acc: 0.984375]  [G loss: 4.678506, acc: 0.281250]\n",
            "3731: [D loss: 0.018449, acc: 0.992188]  [G loss: 4.260897, acc: 0.250000]\n",
            "3732: [D loss: 0.062691, acc: 0.976562]  [G loss: 4.936928, acc: 0.187500]\n",
            "3733: [D loss: 0.056365, acc: 0.984375]  [G loss: 4.599522, acc: 0.359375]\n",
            "3734: [D loss: 0.040714, acc: 0.984375]  [G loss: 4.292423, acc: 0.218750]\n",
            "3735: [D loss: 0.133040, acc: 0.968750]  [G loss: 4.133415, acc: 0.250000]\n",
            "3736: [D loss: 0.034298, acc: 0.992188]  [G loss: 3.879317, acc: 0.265625]\n",
            "3737: [D loss: 0.082300, acc: 0.976562]  [G loss: 4.069523, acc: 0.218750]\n",
            "3738: [D loss: 0.123033, acc: 0.960938]  [G loss: 4.652535, acc: 0.156250]\n",
            "3739: [D loss: 0.083593, acc: 0.960938]  [G loss: 4.789046, acc: 0.093750]\n",
            "3740: [D loss: 0.157864, acc: 0.960938]  [G loss: 4.337853, acc: 0.187500]\n",
            "3741: [D loss: 0.150979, acc: 0.953125]  [G loss: 3.694149, acc: 0.281250]\n",
            "3742: [D loss: 0.136217, acc: 0.953125]  [G loss: 3.061943, acc: 0.250000]\n",
            "3743: [D loss: 0.150607, acc: 0.937500]  [G loss: 3.797674, acc: 0.250000]\n",
            "3744: [D loss: 0.120026, acc: 0.953125]  [G loss: 4.681143, acc: 0.125000]\n",
            "3745: [D loss: 0.092411, acc: 0.976562]  [G loss: 6.128173, acc: 0.031250]\n",
            "3746: [D loss: 0.166244, acc: 0.945312]  [G loss: 5.197211, acc: 0.062500]\n",
            "3747: [D loss: 0.049409, acc: 0.976562]  [G loss: 4.423351, acc: 0.156250]\n",
            "3748: [D loss: 0.233382, acc: 0.929688]  [G loss: 3.462109, acc: 0.187500]\n",
            "3749: [D loss: 0.196222, acc: 0.914062]  [G loss: 4.407527, acc: 0.187500]\n",
            "3750: [D loss: 0.206110, acc: 0.953125]  [G loss: 6.312963, acc: 0.078125]\n",
            "3751: [D loss: 0.042662, acc: 0.984375]  [G loss: 6.676152, acc: 0.093750]\n",
            "3752: [D loss: 0.101166, acc: 0.937500]  [G loss: 5.658020, acc: 0.062500]\n",
            "3753: [D loss: 0.182059, acc: 0.945312]  [G loss: 3.769815, acc: 0.296875]\n",
            "3754: [D loss: 0.095900, acc: 0.953125]  [G loss: 3.084162, acc: 0.296875]\n",
            "3755: [D loss: 0.189959, acc: 0.945312]  [G loss: 4.653037, acc: 0.125000]\n",
            "3756: [D loss: 0.093446, acc: 0.953125]  [G loss: 5.455494, acc: 0.062500]\n",
            "3757: [D loss: 0.037441, acc: 0.992188]  [G loss: 6.605788, acc: 0.015625]\n",
            "3758: [D loss: 0.127206, acc: 0.960938]  [G loss: 7.447847, acc: 0.000000]\n",
            "3759: [D loss: 0.090671, acc: 0.968750]  [G loss: 5.952118, acc: 0.015625]\n",
            "3760: [D loss: 0.108445, acc: 0.976562]  [G loss: 4.983670, acc: 0.046875]\n",
            "3761: [D loss: 0.089864, acc: 0.960938]  [G loss: 3.981952, acc: 0.031250]\n",
            "3762: [D loss: 0.030250, acc: 0.992188]  [G loss: 3.951359, acc: 0.078125]\n",
            "3763: [D loss: 0.062443, acc: 0.968750]  [G loss: 5.799036, acc: 0.015625]\n",
            "3764: [D loss: 0.066108, acc: 0.976562]  [G loss: 6.014054, acc: 0.000000]\n",
            "3765: [D loss: 0.121484, acc: 0.945312]  [G loss: 5.194768, acc: 0.062500]\n",
            "3766: [D loss: 0.032310, acc: 0.992188]  [G loss: 4.921576, acc: 0.109375]\n",
            "3767: [D loss: 0.020941, acc: 0.992188]  [G loss: 5.211333, acc: 0.031250]\n",
            "3768: [D loss: 0.021523, acc: 0.984375]  [G loss: 4.493865, acc: 0.187500]\n",
            "3769: [D loss: 0.065422, acc: 0.976562]  [G loss: 4.792484, acc: 0.140625]\n",
            "3770: [D loss: 0.040804, acc: 0.992188]  [G loss: 4.915573, acc: 0.156250]\n",
            "3771: [D loss: 0.022702, acc: 1.000000]  [G loss: 4.506908, acc: 0.125000]\n",
            "3772: [D loss: 0.087790, acc: 0.960938]  [G loss: 4.425450, acc: 0.156250]\n",
            "3773: [D loss: 0.063993, acc: 0.984375]  [G loss: 5.038285, acc: 0.125000]\n",
            "3774: [D loss: 0.044024, acc: 0.984375]  [G loss: 4.885839, acc: 0.109375]\n",
            "3775: [D loss: 0.032944, acc: 0.984375]  [G loss: 4.431651, acc: 0.187500]\n",
            "3776: [D loss: 0.012161, acc: 1.000000]  [G loss: 4.423260, acc: 0.171875]\n",
            "3777: [D loss: 0.088457, acc: 0.968750]  [G loss: 3.253893, acc: 0.265625]\n",
            "3778: [D loss: 0.030649, acc: 1.000000]  [G loss: 3.018450, acc: 0.296875]\n",
            "3779: [D loss: 0.123792, acc: 0.953125]  [G loss: 2.467976, acc: 0.390625]\n",
            "3780: [D loss: 0.017714, acc: 1.000000]  [G loss: 3.550566, acc: 0.234375]\n",
            "3781: [D loss: 0.334546, acc: 0.906250]  [G loss: 2.648180, acc: 0.390625]\n",
            "3782: [D loss: 0.024522, acc: 1.000000]  [G loss: 3.130102, acc: 0.343750]\n",
            "3783: [D loss: 0.122186, acc: 0.976562]  [G loss: 2.746413, acc: 0.343750]\n",
            "3784: [D loss: 0.162608, acc: 0.937500]  [G loss: 1.990276, acc: 0.421875]\n",
            "3785: [D loss: 0.046869, acc: 0.992188]  [G loss: 1.916472, acc: 0.531250]\n",
            "3786: [D loss: 0.100261, acc: 0.945312]  [G loss: 2.362381, acc: 0.390625]\n",
            "3787: [D loss: 0.086935, acc: 0.960938]  [G loss: 2.641753, acc: 0.484375]\n",
            "3788: [D loss: 0.079634, acc: 0.968750]  [G loss: 2.264582, acc: 0.453125]\n",
            "3789: [D loss: 0.109196, acc: 0.953125]  [G loss: 1.581929, acc: 0.562500]\n",
            "3790: [D loss: 0.088600, acc: 0.968750]  [G loss: 0.897113, acc: 0.671875]\n",
            "3791: [D loss: 0.210529, acc: 0.914062]  [G loss: 1.471949, acc: 0.546875]\n",
            "3792: [D loss: 0.172794, acc: 0.968750]  [G loss: 2.901890, acc: 0.375000]\n",
            "3793: [D loss: 0.125007, acc: 0.953125]  [G loss: 3.356196, acc: 0.296875]\n",
            "3794: [D loss: 0.089669, acc: 0.984375]  [G loss: 3.196595, acc: 0.281250]\n",
            "3795: [D loss: 0.199621, acc: 0.953125]  [G loss: 1.921601, acc: 0.468750]\n",
            "3796: [D loss: 0.263599, acc: 0.921875]  [G loss: 1.335766, acc: 0.593750]\n",
            "3797: [D loss: 0.150015, acc: 0.953125]  [G loss: 2.474593, acc: 0.328125]\n",
            "3798: [D loss: 0.181539, acc: 0.953125]  [G loss: 3.225964, acc: 0.250000]\n",
            "3799: [D loss: 0.065431, acc: 0.984375]  [G loss: 5.704575, acc: 0.062500]\n",
            "3800: [D loss: 0.052528, acc: 0.968750]  [G loss: 4.774312, acc: 0.078125]\n",
            "3801: [D loss: 0.266943, acc: 0.906250]  [G loss: 3.502465, acc: 0.265625]\n",
            "3802: [D loss: 0.115145, acc: 0.968750]  [G loss: 2.946562, acc: 0.296875]\n",
            "3803: [D loss: 0.414127, acc: 0.882812]  [G loss: 6.071619, acc: 0.062500]\n",
            "3804: [D loss: 0.068872, acc: 0.968750]  [G loss: 8.259844, acc: 0.000000]\n",
            "3805: [D loss: 0.181898, acc: 0.937500]  [G loss: 8.690386, acc: 0.000000]\n",
            "3806: [D loss: 0.169287, acc: 0.945312]  [G loss: 5.657215, acc: 0.109375]\n",
            "3807: [D loss: 0.109073, acc: 0.953125]  [G loss: 3.857771, acc: 0.203125]\n",
            "3808: [D loss: 0.163277, acc: 0.929688]  [G loss: 4.393380, acc: 0.140625]\n",
            "3809: [D loss: 0.184018, acc: 0.960938]  [G loss: 5.645183, acc: 0.078125]\n",
            "3810: [D loss: 0.068268, acc: 0.984375]  [G loss: 6.483562, acc: 0.062500]\n",
            "3811: [D loss: 0.088914, acc: 0.976562]  [G loss: 7.613263, acc: 0.031250]\n",
            "3812: [D loss: 0.160134, acc: 0.945312]  [G loss: 7.526871, acc: 0.031250]\n",
            "3813: [D loss: 0.041149, acc: 0.984375]  [G loss: 6.872233, acc: 0.046875]\n",
            "3814: [D loss: 0.038741, acc: 0.984375]  [G loss: 5.260900, acc: 0.125000]\n",
            "3815: [D loss: 0.039011, acc: 0.992188]  [G loss: 4.131867, acc: 0.156250]\n",
            "3816: [D loss: 0.102757, acc: 0.968750]  [G loss: 4.957355, acc: 0.156250]\n",
            "3817: [D loss: 0.079331, acc: 0.984375]  [G loss: 5.581211, acc: 0.031250]\n",
            "3818: [D loss: 0.199014, acc: 0.929688]  [G loss: 6.300486, acc: 0.015625]\n",
            "3819: [D loss: 0.153609, acc: 0.968750]  [G loss: 5.825318, acc: 0.093750]\n",
            "3820: [D loss: 0.124722, acc: 0.953125]  [G loss: 4.751740, acc: 0.078125]\n",
            "3821: [D loss: 0.105551, acc: 0.953125]  [G loss: 4.639476, acc: 0.093750]\n",
            "3822: [D loss: 0.089394, acc: 0.976562]  [G loss: 4.612261, acc: 0.093750]\n",
            "3823: [D loss: 0.082429, acc: 0.968750]  [G loss: 5.407601, acc: 0.093750]\n",
            "3824: [D loss: 0.176114, acc: 0.953125]  [G loss: 4.837280, acc: 0.046875]\n",
            "3825: [D loss: 0.069636, acc: 0.968750]  [G loss: 4.695651, acc: 0.109375]\n",
            "3826: [D loss: 0.068215, acc: 0.976562]  [G loss: 4.692187, acc: 0.078125]\n",
            "3827: [D loss: 0.058457, acc: 0.976562]  [G loss: 5.141048, acc: 0.062500]\n",
            "3828: [D loss: 0.052549, acc: 0.992188]  [G loss: 4.068871, acc: 0.109375]\n",
            "3829: [D loss: 0.094987, acc: 0.953125]  [G loss: 4.590944, acc: 0.093750]\n",
            "3830: [D loss: 0.129263, acc: 0.953125]  [G loss: 4.129741, acc: 0.093750]\n",
            "3831: [D loss: 0.044308, acc: 0.992188]  [G loss: 3.413174, acc: 0.187500]\n",
            "3832: [D loss: 0.064192, acc: 0.976562]  [G loss: 3.027234, acc: 0.265625]\n",
            "3833: [D loss: 0.031234, acc: 0.984375]  [G loss: 2.625336, acc: 0.312500]\n",
            "3834: [D loss: 0.043756, acc: 0.992188]  [G loss: 3.075795, acc: 0.265625]\n",
            "3835: [D loss: 0.055561, acc: 0.976562]  [G loss: 3.629359, acc: 0.187500]\n",
            "3836: [D loss: 0.169709, acc: 0.929688]  [G loss: 3.196869, acc: 0.234375]\n",
            "3837: [D loss: 0.013141, acc: 0.992188]  [G loss: 3.345963, acc: 0.250000]\n",
            "3838: [D loss: 0.026193, acc: 0.992188]  [G loss: 3.577987, acc: 0.218750]\n",
            "3839: [D loss: 0.104327, acc: 0.976562]  [G loss: 2.749795, acc: 0.328125]\n",
            "3840: [D loss: 0.034118, acc: 0.992188]  [G loss: 2.437034, acc: 0.296875]\n",
            "3841: [D loss: 0.026609, acc: 0.992188]  [G loss: 2.565235, acc: 0.296875]\n",
            "3842: [D loss: 0.071698, acc: 0.960938]  [G loss: 1.750450, acc: 0.421875]\n",
            "3843: [D loss: 0.097369, acc: 0.953125]  [G loss: 2.431595, acc: 0.312500]\n",
            "3844: [D loss: 0.062161, acc: 0.976562]  [G loss: 3.479939, acc: 0.203125]\n",
            "3845: [D loss: 0.039069, acc: 0.984375]  [G loss: 5.522895, acc: 0.093750]\n",
            "3846: [D loss: 0.160116, acc: 0.937500]  [G loss: 4.020214, acc: 0.171875]\n",
            "3847: [D loss: 0.209863, acc: 0.929688]  [G loss: 2.753622, acc: 0.265625]\n",
            "3848: [D loss: 0.270686, acc: 0.906250]  [G loss: 2.755791, acc: 0.234375]\n",
            "3849: [D loss: 0.201069, acc: 0.945312]  [G loss: 3.479514, acc: 0.218750]\n",
            "3850: [D loss: 0.141695, acc: 0.968750]  [G loss: 4.041234, acc: 0.156250]\n",
            "3851: [D loss: 0.211646, acc: 0.914062]  [G loss: 3.808399, acc: 0.156250]\n",
            "3852: [D loss: 0.126776, acc: 0.960938]  [G loss: 3.340731, acc: 0.171875]\n",
            "3853: [D loss: 0.195388, acc: 0.921875]  [G loss: 3.710253, acc: 0.125000]\n",
            "3854: [D loss: 0.153347, acc: 0.953125]  [G loss: 3.641860, acc: 0.203125]\n",
            "3855: [D loss: 0.156567, acc: 0.960938]  [G loss: 3.552101, acc: 0.203125]\n",
            "3856: [D loss: 0.157005, acc: 0.953125]  [G loss: 3.631598, acc: 0.125000]\n",
            "3857: [D loss: 0.188632, acc: 0.945312]  [G loss: 3.284306, acc: 0.218750]\n",
            "3858: [D loss: 0.164817, acc: 0.921875]  [G loss: 3.160198, acc: 0.281250]\n",
            "3859: [D loss: 0.107072, acc: 0.929688]  [G loss: 3.874461, acc: 0.140625]\n",
            "3860: [D loss: 0.156582, acc: 0.937500]  [G loss: 3.896149, acc: 0.171875]\n",
            "3861: [D loss: 0.124959, acc: 0.968750]  [G loss: 4.318361, acc: 0.203125]\n",
            "3862: [D loss: 0.157250, acc: 0.898438]  [G loss: 4.120728, acc: 0.171875]\n",
            "3863: [D loss: 0.101661, acc: 0.960938]  [G loss: 3.496560, acc: 0.234375]\n",
            "3864: [D loss: 0.097085, acc: 0.960938]  [G loss: 4.194816, acc: 0.156250]\n",
            "3865: [D loss: 0.163877, acc: 0.937500]  [G loss: 3.079386, acc: 0.234375]\n",
            "3866: [D loss: 0.083257, acc: 0.976562]  [G loss: 2.598382, acc: 0.390625]\n",
            "3867: [D loss: 0.078516, acc: 0.984375]  [G loss: 1.845361, acc: 0.453125]\n",
            "3868: [D loss: 0.092479, acc: 0.968750]  [G loss: 2.358568, acc: 0.421875]\n",
            "3869: [D loss: 0.094758, acc: 0.953125]  [G loss: 2.200447, acc: 0.468750]\n",
            "3870: [D loss: 0.049312, acc: 0.984375]  [G loss: 2.424887, acc: 0.390625]\n",
            "3871: [D loss: 0.059225, acc: 0.976562]  [G loss: 2.981698, acc: 0.359375]\n",
            "3872: [D loss: 0.030774, acc: 0.992188]  [G loss: 2.319972, acc: 0.468750]\n",
            "3873: [D loss: 0.050364, acc: 0.976562]  [G loss: 2.268723, acc: 0.484375]\n",
            "3874: [D loss: 0.012699, acc: 1.000000]  [G loss: 2.073238, acc: 0.515625]\n",
            "3875: [D loss: 0.103947, acc: 0.984375]  [G loss: 2.180067, acc: 0.546875]\n",
            "3876: [D loss: 0.021688, acc: 0.984375]  [G loss: 1.601146, acc: 0.656250]\n",
            "3877: [D loss: 0.012940, acc: 1.000000]  [G loss: 1.584431, acc: 0.578125]\n",
            "3878: [D loss: 0.011002, acc: 1.000000]  [G loss: 1.465173, acc: 0.609375]\n",
            "3879: [D loss: 0.022144, acc: 0.992188]  [G loss: 1.602655, acc: 0.609375]\n",
            "3880: [D loss: 0.026102, acc: 1.000000]  [G loss: 1.410304, acc: 0.609375]\n",
            "3881: [D loss: 0.079043, acc: 0.968750]  [G loss: 1.552700, acc: 0.703125]\n",
            "3882: [D loss: 0.006854, acc: 1.000000]  [G loss: 1.450052, acc: 0.625000]\n",
            "3883: [D loss: 0.018307, acc: 0.992188]  [G loss: 1.695959, acc: 0.656250]\n",
            "3884: [D loss: 0.058227, acc: 0.976562]  [G loss: 1.959012, acc: 0.640625]\n",
            "3885: [D loss: 0.010706, acc: 1.000000]  [G loss: 1.992072, acc: 0.593750]\n",
            "3886: [D loss: 0.029589, acc: 0.992188]  [G loss: 1.614993, acc: 0.546875]\n",
            "3887: [D loss: 0.085633, acc: 0.984375]  [G loss: 1.795867, acc: 0.500000]\n",
            "3888: [D loss: 0.050437, acc: 0.992188]  [G loss: 1.480767, acc: 0.671875]\n",
            "3889: [D loss: 0.021613, acc: 0.992188]  [G loss: 1.699959, acc: 0.593750]\n",
            "3890: [D loss: 0.033478, acc: 0.992188]  [G loss: 1.511019, acc: 0.656250]\n",
            "3891: [D loss: 0.039252, acc: 0.984375]  [G loss: 1.795092, acc: 0.593750]\n",
            "3892: [D loss: 0.038403, acc: 0.984375]  [G loss: 1.771005, acc: 0.625000]\n",
            "3893: [D loss: 0.021578, acc: 0.984375]  [G loss: 2.627606, acc: 0.406250]\n",
            "3894: [D loss: 0.042500, acc: 0.976562]  [G loss: 2.757700, acc: 0.328125]\n",
            "3895: [D loss: 0.103468, acc: 0.960938]  [G loss: 2.893953, acc: 0.343750]\n",
            "3896: [D loss: 0.024925, acc: 0.992188]  [G loss: 2.968956, acc: 0.281250]\n",
            "3897: [D loss: 0.048632, acc: 0.984375]  [G loss: 2.440731, acc: 0.421875]\n",
            "3898: [D loss: 0.052572, acc: 0.976562]  [G loss: 2.226544, acc: 0.468750]\n",
            "3899: [D loss: 0.173193, acc: 0.953125]  [G loss: 2.294009, acc: 0.437500]\n",
            "3900: [D loss: 0.060031, acc: 0.984375]  [G loss: 2.986616, acc: 0.343750]\n",
            "3901: [D loss: 0.076048, acc: 0.976562]  [G loss: 3.476657, acc: 0.281250]\n",
            "3902: [D loss: 0.008321, acc: 1.000000]  [G loss: 4.042527, acc: 0.218750]\n",
            "3903: [D loss: 0.093939, acc: 0.976562]  [G loss: 4.033835, acc: 0.218750]\n",
            "3904: [D loss: 0.025091, acc: 0.984375]  [G loss: 4.055576, acc: 0.218750]\n",
            "3905: [D loss: 0.019262, acc: 0.992188]  [G loss: 4.967273, acc: 0.140625]\n",
            "3906: [D loss: 0.041982, acc: 0.976562]  [G loss: 5.555281, acc: 0.125000]\n",
            "3907: [D loss: 0.015800, acc: 0.992188]  [G loss: 6.132210, acc: 0.062500]\n",
            "3908: [D loss: 0.042540, acc: 0.984375]  [G loss: 4.164620, acc: 0.125000]\n",
            "3909: [D loss: 0.084141, acc: 0.976562]  [G loss: 3.482083, acc: 0.187500]\n",
            "3910: [D loss: 0.075589, acc: 0.960938]  [G loss: 3.342867, acc: 0.281250]\n",
            "3911: [D loss: 0.049274, acc: 0.976562]  [G loss: 4.280138, acc: 0.187500]\n",
            "3912: [D loss: 0.177778, acc: 0.976562]  [G loss: 5.236188, acc: 0.156250]\n",
            "3913: [D loss: 0.117369, acc: 0.945312]  [G loss: 3.631453, acc: 0.296875]\n",
            "3914: [D loss: 0.104840, acc: 0.960938]  [G loss: 2.594389, acc: 0.375000]\n",
            "3915: [D loss: 0.121778, acc: 0.960938]  [G loss: 4.802366, acc: 0.171875]\n",
            "3916: [D loss: 0.079384, acc: 0.976562]  [G loss: 4.445200, acc: 0.250000]\n",
            "3917: [D loss: 0.085813, acc: 0.953125]  [G loss: 2.949626, acc: 0.406250]\n",
            "3918: [D loss: 0.144998, acc: 0.945312]  [G loss: 2.603548, acc: 0.390625]\n",
            "3919: [D loss: 0.139824, acc: 0.960938]  [G loss: 4.208182, acc: 0.218750]\n",
            "3920: [D loss: 0.032809, acc: 0.992188]  [G loss: 5.502803, acc: 0.109375]\n",
            "3921: [D loss: 0.166846, acc: 0.968750]  [G loss: 4.661695, acc: 0.187500]\n",
            "3922: [D loss: 0.146644, acc: 0.953125]  [G loss: 1.864732, acc: 0.500000]\n",
            "3923: [D loss: 0.144913, acc: 0.945312]  [G loss: 2.292177, acc: 0.468750]\n",
            "3924: [D loss: 0.401247, acc: 0.882812]  [G loss: 4.209524, acc: 0.203125]\n",
            "3925: [D loss: 0.063772, acc: 0.984375]  [G loss: 6.205823, acc: 0.046875]\n",
            "3926: [D loss: 0.275378, acc: 0.929688]  [G loss: 5.970680, acc: 0.078125]\n",
            "3927: [D loss: 0.165081, acc: 0.945312]  [G loss: 3.627833, acc: 0.312500]\n",
            "3928: [D loss: 0.110016, acc: 0.968750]  [G loss: 2.371279, acc: 0.531250]\n",
            "3929: [D loss: 0.415111, acc: 0.921875]  [G loss: 3.317596, acc: 0.406250]\n",
            "3930: [D loss: 0.154991, acc: 0.945312]  [G loss: 4.649461, acc: 0.171875]\n",
            "3931: [D loss: 0.042255, acc: 0.984375]  [G loss: 5.894549, acc: 0.031250]\n",
            "3932: [D loss: 0.137774, acc: 0.960938]  [G loss: 6.407568, acc: 0.062500]\n",
            "3933: [D loss: 0.185903, acc: 0.937500]  [G loss: 5.564952, acc: 0.125000]\n",
            "3934: [D loss: 0.133748, acc: 0.968750]  [G loss: 3.846750, acc: 0.203125]\n",
            "3935: [D loss: 0.158126, acc: 0.945312]  [G loss: 3.931944, acc: 0.203125]\n",
            "3936: [D loss: 0.094846, acc: 0.945312]  [G loss: 3.838338, acc: 0.250000]\n",
            "3937: [D loss: 0.214473, acc: 0.906250]  [G loss: 3.592798, acc: 0.281250]\n",
            "3938: [D loss: 0.038282, acc: 0.984375]  [G loss: 4.355780, acc: 0.109375]\n",
            "3939: [D loss: 0.057723, acc: 0.992188]  [G loss: 5.247046, acc: 0.125000]\n",
            "3940: [D loss: 0.047059, acc: 0.984375]  [G loss: 6.055702, acc: 0.046875]\n",
            "3941: [D loss: 0.099756, acc: 0.984375]  [G loss: 5.730649, acc: 0.171875]\n",
            "3942: [D loss: 0.026169, acc: 1.000000]  [G loss: 5.500904, acc: 0.093750]\n",
            "3943: [D loss: 0.102983, acc: 0.984375]  [G loss: 4.440802, acc: 0.171875]\n",
            "3944: [D loss: 0.062694, acc: 0.976562]  [G loss: 4.586725, acc: 0.156250]\n",
            "3945: [D loss: 0.107920, acc: 0.960938]  [G loss: 4.813286, acc: 0.125000]\n",
            "3946: [D loss: 0.165518, acc: 0.945312]  [G loss: 3.354761, acc: 0.187500]\n",
            "3947: [D loss: 0.263300, acc: 0.906250]  [G loss: 4.726946, acc: 0.078125]\n",
            "3948: [D loss: 0.045373, acc: 0.984375]  [G loss: 6.371866, acc: 0.015625]\n",
            "3949: [D loss: 0.040699, acc: 0.976562]  [G loss: 6.216892, acc: 0.000000]\n",
            "3950: [D loss: 0.009485, acc: 1.000000]  [G loss: 6.849244, acc: 0.015625]\n",
            "3951: [D loss: 0.040267, acc: 0.984375]  [G loss: 6.977952, acc: 0.000000]\n",
            "3952: [D loss: 0.159328, acc: 0.953125]  [G loss: 5.953065, acc: 0.062500]\n",
            "3953: [D loss: 0.049747, acc: 0.992188]  [G loss: 5.179608, acc: 0.062500]\n",
            "3954: [D loss: 0.046853, acc: 0.976562]  [G loss: 4.104434, acc: 0.062500]\n",
            "3955: [D loss: 0.034713, acc: 0.992188]  [G loss: 3.015402, acc: 0.187500]\n",
            "3956: [D loss: 0.060666, acc: 0.968750]  [G loss: 3.302968, acc: 0.171875]\n",
            "3957: [D loss: 0.071640, acc: 0.968750]  [G loss: 4.499819, acc: 0.031250]\n",
            "3958: [D loss: 0.045735, acc: 0.984375]  [G loss: 5.441130, acc: 0.031250]\n",
            "3959: [D loss: 0.007768, acc: 1.000000]  [G loss: 6.496809, acc: 0.015625]\n",
            "3960: [D loss: 0.004678, acc: 1.000000]  [G loss: 7.133901, acc: 0.000000]\n",
            "3961: [D loss: 0.108342, acc: 0.976562]  [G loss: 7.304085, acc: 0.000000]\n",
            "3962: [D loss: 0.074127, acc: 0.968750]  [G loss: 6.466383, acc: 0.000000]\n",
            "3963: [D loss: 0.021600, acc: 0.992188]  [G loss: 5.712790, acc: 0.000000]\n",
            "3964: [D loss: 0.018107, acc: 1.000000]  [G loss: 4.448540, acc: 0.171875]\n",
            "3965: [D loss: 0.109303, acc: 0.976562]  [G loss: 3.425868, acc: 0.156250]\n",
            "3966: [D loss: 0.021377, acc: 0.992188]  [G loss: 2.961349, acc: 0.343750]\n",
            "3967: [D loss: 0.024915, acc: 0.992188]  [G loss: 2.565764, acc: 0.421875]\n",
            "3968: [D loss: 0.059246, acc: 0.984375]  [G loss: 2.801891, acc: 0.359375]\n",
            "3969: [D loss: 0.021484, acc: 1.000000]  [G loss: 2.851767, acc: 0.359375]\n",
            "3970: [D loss: 0.029469, acc: 0.992188]  [G loss: 2.974370, acc: 0.421875]\n",
            "3971: [D loss: 0.043168, acc: 0.992188]  [G loss: 3.416673, acc: 0.375000]\n",
            "3972: [D loss: 0.003928, acc: 1.000000]  [G loss: 3.411725, acc: 0.281250]\n",
            "3973: [D loss: 0.044955, acc: 0.992188]  [G loss: 3.755309, acc: 0.296875]\n",
            "3974: [D loss: 0.002499, acc: 1.000000]  [G loss: 3.353848, acc: 0.296875]\n",
            "3975: [D loss: 0.008207, acc: 0.992188]  [G loss: 3.668521, acc: 0.281250]\n",
            "3976: [D loss: 0.033204, acc: 0.984375]  [G loss: 3.117566, acc: 0.343750]\n",
            "3977: [D loss: 0.036711, acc: 0.984375]  [G loss: 2.187370, acc: 0.500000]\n",
            "3978: [D loss: 0.026507, acc: 0.984375]  [G loss: 2.268755, acc: 0.437500]\n",
            "3979: [D loss: 0.008922, acc: 1.000000]  [G loss: 2.850124, acc: 0.375000]\n",
            "3980: [D loss: 0.080110, acc: 0.984375]  [G loss: 2.749449, acc: 0.359375]\n",
            "3981: [D loss: 0.007888, acc: 1.000000]  [G loss: 2.698197, acc: 0.359375]\n",
            "3982: [D loss: 0.003808, acc: 1.000000]  [G loss: 2.275977, acc: 0.281250]\n",
            "3983: [D loss: 0.029301, acc: 0.992188]  [G loss: 3.205100, acc: 0.234375]\n",
            "3984: [D loss: 0.004292, acc: 1.000000]  [G loss: 3.964766, acc: 0.187500]\n",
            "3985: [D loss: 0.003713, acc: 1.000000]  [G loss: 4.115358, acc: 0.109375]\n",
            "3986: [D loss: 0.042986, acc: 0.984375]  [G loss: 4.109062, acc: 0.125000]\n",
            "3987: [D loss: 0.021467, acc: 0.992188]  [G loss: 4.434041, acc: 0.203125]\n",
            "3988: [D loss: 0.156943, acc: 0.976562]  [G loss: 3.702715, acc: 0.265625]\n",
            "3989: [D loss: 0.037962, acc: 0.992188]  [G loss: 2.904471, acc: 0.328125]\n",
            "3990: [D loss: 0.091444, acc: 0.960938]  [G loss: 3.781859, acc: 0.156250]\n",
            "3991: [D loss: 0.005683, acc: 1.000000]  [G loss: 4.534411, acc: 0.171875]\n",
            "3992: [D loss: 0.094378, acc: 0.976562]  [G loss: 5.350105, acc: 0.109375]\n",
            "3993: [D loss: 0.024365, acc: 0.984375]  [G loss: 4.788493, acc: 0.218750]\n",
            "3994: [D loss: 0.053250, acc: 0.984375]  [G loss: 3.835958, acc: 0.265625]\n",
            "3995: [D loss: 0.018496, acc: 0.992188]  [G loss: 4.056969, acc: 0.218750]\n",
            "3996: [D loss: 0.051014, acc: 0.976562]  [G loss: 4.712658, acc: 0.171875]\n",
            "3997: [D loss: 0.023817, acc: 0.992188]  [G loss: 5.291274, acc: 0.109375]\n",
            "3998: [D loss: 0.111591, acc: 0.976562]  [G loss: 5.593462, acc: 0.093750]\n",
            "3999: [D loss: 0.035146, acc: 0.992188]  [G loss: 7.335815, acc: 0.031250]\n",
            "4000: [D loss: 0.200288, acc: 0.945312]  [G loss: 5.984792, acc: 0.078125]\n",
            "4001: [D loss: 0.029054, acc: 0.992188]  [G loss: 5.949803, acc: 0.062500]\n",
            "4002: [D loss: 0.101325, acc: 0.968750]  [G loss: 4.161869, acc: 0.078125]\n",
            "4003: [D loss: 0.113402, acc: 0.953125]  [G loss: 4.107115, acc: 0.109375]\n",
            "4004: [D loss: 0.061497, acc: 0.976562]  [G loss: 4.469821, acc: 0.203125]\n",
            "4005: [D loss: 0.111980, acc: 0.960938]  [G loss: 5.389166, acc: 0.031250]\n",
            "4006: [D loss: 0.142662, acc: 0.968750]  [G loss: 4.820729, acc: 0.203125]\n",
            "4007: [D loss: 0.126023, acc: 0.945312]  [G loss: 4.535285, acc: 0.187500]\n",
            "4008: [D loss: 0.061762, acc: 0.960938]  [G loss: 5.191943, acc: 0.140625]\n",
            "4009: [D loss: 0.076192, acc: 0.968750]  [G loss: 5.187086, acc: 0.093750]\n",
            "4010: [D loss: 0.071740, acc: 0.984375]  [G loss: 5.252381, acc: 0.078125]\n",
            "4011: [D loss: 0.039041, acc: 0.984375]  [G loss: 4.671918, acc: 0.140625]\n",
            "4012: [D loss: 0.105266, acc: 0.953125]  [G loss: 4.112128, acc: 0.218750]\n",
            "4013: [D loss: 0.040502, acc: 0.976562]  [G loss: 3.648535, acc: 0.203125]\n",
            "4014: [D loss: 0.083820, acc: 0.968750]  [G loss: 3.883834, acc: 0.234375]\n",
            "4015: [D loss: 0.031541, acc: 0.992188]  [G loss: 5.132803, acc: 0.125000]\n",
            "4016: [D loss: 0.063729, acc: 0.984375]  [G loss: 5.567968, acc: 0.109375]\n",
            "4017: [D loss: 0.054774, acc: 0.976562]  [G loss: 5.090017, acc: 0.031250]\n",
            "4018: [D loss: 0.052058, acc: 0.968750]  [G loss: 4.657213, acc: 0.109375]\n",
            "4019: [D loss: 0.079212, acc: 0.976562]  [G loss: 4.819084, acc: 0.125000]\n",
            "4020: [D loss: 0.047991, acc: 0.984375]  [G loss: 4.673069, acc: 0.078125]\n",
            "4021: [D loss: 0.031647, acc: 0.992188]  [G loss: 3.743434, acc: 0.187500]\n",
            "4022: [D loss: 0.059516, acc: 0.976562]  [G loss: 4.367942, acc: 0.125000]\n",
            "4023: [D loss: 0.074255, acc: 0.960938]  [G loss: 3.309875, acc: 0.109375]\n",
            "4024: [D loss: 0.058686, acc: 0.976562]  [G loss: 3.934446, acc: 0.109375]\n",
            "4025: [D loss: 0.036447, acc: 0.984375]  [G loss: 4.068430, acc: 0.109375]\n",
            "4026: [D loss: 0.079431, acc: 0.984375]  [G loss: 4.739015, acc: 0.078125]\n",
            "4027: [D loss: 0.053489, acc: 0.992188]  [G loss: 6.085907, acc: 0.015625]\n",
            "4028: [D loss: 0.066365, acc: 0.968750]  [G loss: 6.070074, acc: 0.000000]\n",
            "4029: [D loss: 0.109134, acc: 0.945312]  [G loss: 3.865156, acc: 0.140625]\n",
            "4030: [D loss: 0.070762, acc: 0.984375]  [G loss: 3.598960, acc: 0.203125]\n",
            "4031: [D loss: 0.054027, acc: 0.968750]  [G loss: 3.172271, acc: 0.296875]\n",
            "4032: [D loss: 0.103318, acc: 0.953125]  [G loss: 4.211135, acc: 0.140625]\n",
            "4033: [D loss: 0.076401, acc: 0.976562]  [G loss: 5.307883, acc: 0.062500]\n",
            "4034: [D loss: 0.030602, acc: 0.976562]  [G loss: 5.817576, acc: 0.046875]\n",
            "4035: [D loss: 0.016728, acc: 1.000000]  [G loss: 6.489218, acc: 0.000000]\n",
            "4036: [D loss: 0.165546, acc: 0.960938]  [G loss: 6.513140, acc: 0.000000]\n",
            "4037: [D loss: 0.068038, acc: 0.984375]  [G loss: 4.684558, acc: 0.046875]\n",
            "4038: [D loss: 0.028652, acc: 0.992188]  [G loss: 4.133263, acc: 0.140625]\n",
            "4039: [D loss: 0.139052, acc: 0.960938]  [G loss: 3.864681, acc: 0.203125]\n",
            "4040: [D loss: 0.066810, acc: 0.976562]  [G loss: 4.157630, acc: 0.109375]\n",
            "4041: [D loss: 0.055572, acc: 0.992188]  [G loss: 4.676404, acc: 0.140625]\n",
            "4042: [D loss: 0.081290, acc: 0.968750]  [G loss: 5.572832, acc: 0.046875]\n",
            "4043: [D loss: 0.081699, acc: 0.953125]  [G loss: 5.922380, acc: 0.015625]\n",
            "4044: [D loss: 0.029557, acc: 0.992188]  [G loss: 5.690176, acc: 0.031250]\n",
            "4045: [D loss: 0.184913, acc: 0.945312]  [G loss: 3.934333, acc: 0.140625]\n",
            "4046: [D loss: 0.061036, acc: 0.976562]  [G loss: 4.323116, acc: 0.156250]\n",
            "4047: [D loss: 0.113215, acc: 0.968750]  [G loss: 3.525153, acc: 0.187500]\n",
            "4048: [D loss: 0.151162, acc: 0.937500]  [G loss: 4.236138, acc: 0.218750]\n",
            "4049: [D loss: 0.149050, acc: 0.960938]  [G loss: 4.400994, acc: 0.125000]\n",
            "4050: [D loss: 0.070951, acc: 0.984375]  [G loss: 4.815185, acc: 0.093750]\n",
            "4051: [D loss: 0.045238, acc: 0.968750]  [G loss: 4.925469, acc: 0.093750]\n",
            "4052: [D loss: 0.117356, acc: 0.953125]  [G loss: 4.513912, acc: 0.109375]\n",
            "4053: [D loss: 0.023564, acc: 0.992188]  [G loss: 3.733840, acc: 0.093750]\n",
            "4054: [D loss: 0.074232, acc: 0.968750]  [G loss: 3.674567, acc: 0.187500]\n",
            "4055: [D loss: 0.045739, acc: 0.984375]  [G loss: 2.894892, acc: 0.296875]\n",
            "4056: [D loss: 0.117684, acc: 0.976562]  [G loss: 2.693141, acc: 0.343750]\n",
            "4057: [D loss: 0.054769, acc: 0.976562]  [G loss: 2.691237, acc: 0.375000]\n",
            "4058: [D loss: 0.040044, acc: 0.984375]  [G loss: 2.695441, acc: 0.265625]\n",
            "4059: [D loss: 0.057423, acc: 0.984375]  [G loss: 2.837327, acc: 0.406250]\n",
            "4060: [D loss: 0.048161, acc: 0.984375]  [G loss: 3.064675, acc: 0.390625]\n",
            "4061: [D loss: 0.095683, acc: 0.968750]  [G loss: 2.658769, acc: 0.390625]\n",
            "4062: [D loss: 0.041230, acc: 0.992188]  [G loss: 2.362963, acc: 0.406250]\n",
            "4063: [D loss: 0.082882, acc: 0.968750]  [G loss: 2.127823, acc: 0.406250]\n",
            "4064: [D loss: 0.053366, acc: 0.992188]  [G loss: 2.698993, acc: 0.437500]\n",
            "4065: [D loss: 0.036630, acc: 0.992188]  [G loss: 2.902588, acc: 0.390625]\n",
            "4066: [D loss: 0.038071, acc: 0.984375]  [G loss: 3.004551, acc: 0.328125]\n",
            "4067: [D loss: 0.027881, acc: 0.992188]  [G loss: 3.515347, acc: 0.250000]\n",
            "4068: [D loss: 0.134904, acc: 0.984375]  [G loss: 3.009622, acc: 0.359375]\n",
            "4069: [D loss: 0.059976, acc: 0.968750]  [G loss: 3.432650, acc: 0.250000]\n",
            "4070: [D loss: 0.012583, acc: 1.000000]  [G loss: 2.844386, acc: 0.296875]\n",
            "4071: [D loss: 0.039927, acc: 0.984375]  [G loss: 2.415867, acc: 0.406250]\n",
            "4072: [D loss: 0.102303, acc: 0.968750]  [G loss: 2.025913, acc: 0.484375]\n",
            "4073: [D loss: 0.088665, acc: 0.953125]  [G loss: 2.148187, acc: 0.453125]\n",
            "4074: [D loss: 0.033180, acc: 0.992188]  [G loss: 2.975110, acc: 0.390625]\n",
            "4075: [D loss: 0.057270, acc: 0.968750]  [G loss: 2.438684, acc: 0.453125]\n",
            "4076: [D loss: 0.056846, acc: 0.968750]  [G loss: 2.636019, acc: 0.406250]\n",
            "4077: [D loss: 0.106618, acc: 0.953125]  [G loss: 2.882161, acc: 0.437500]\n",
            "4078: [D loss: 0.051493, acc: 0.992188]  [G loss: 2.516444, acc: 0.484375]\n",
            "4079: [D loss: 0.217838, acc: 0.945312]  [G loss: 2.523231, acc: 0.437500]\n",
            "4080: [D loss: 0.073967, acc: 0.953125]  [G loss: 2.985565, acc: 0.265625]\n",
            "4081: [D loss: 0.085421, acc: 0.968750]  [G loss: 3.294181, acc: 0.343750]\n",
            "4082: [D loss: 0.102624, acc: 0.968750]  [G loss: 3.825828, acc: 0.171875]\n",
            "4083: [D loss: 0.108726, acc: 0.960938]  [G loss: 4.392422, acc: 0.156250]\n",
            "4084: [D loss: 0.193091, acc: 0.937500]  [G loss: 5.535302, acc: 0.093750]\n",
            "4085: [D loss: 0.149536, acc: 0.945312]  [G loss: 4.819772, acc: 0.078125]\n",
            "4086: [D loss: 0.275347, acc: 0.921875]  [G loss: 5.677647, acc: 0.031250]\n",
            "4087: [D loss: 0.061325, acc: 0.984375]  [G loss: 6.120431, acc: 0.031250]\n",
            "4088: [D loss: 0.159395, acc: 0.937500]  [G loss: 6.670787, acc: 0.031250]\n",
            "4089: [D loss: 0.111109, acc: 0.968750]  [G loss: 8.269627, acc: 0.015625]\n",
            "4090: [D loss: 0.081785, acc: 0.976562]  [G loss: 6.943969, acc: 0.031250]\n",
            "4091: [D loss: 0.154299, acc: 0.968750]  [G loss: 6.197393, acc: 0.046875]\n",
            "4092: [D loss: 0.069941, acc: 0.984375]  [G loss: 6.311814, acc: 0.062500]\n",
            "4093: [D loss: 0.032302, acc: 0.992188]  [G loss: 6.042522, acc: 0.062500]\n",
            "4094: [D loss: 0.062020, acc: 0.976562]  [G loss: 5.787900, acc: 0.046875]\n",
            "4095: [D loss: 0.044214, acc: 0.992188]  [G loss: 7.171868, acc: 0.031250]\n",
            "4096: [D loss: 0.068455, acc: 0.976562]  [G loss: 6.695465, acc: 0.062500]\n",
            "4097: [D loss: 0.053357, acc: 0.992188]  [G loss: 7.709716, acc: 0.078125]\n",
            "4098: [D loss: 0.049647, acc: 0.968750]  [G loss: 5.574166, acc: 0.140625]\n",
            "4099: [D loss: 0.055637, acc: 0.984375]  [G loss: 4.598425, acc: 0.250000]\n",
            "4100: [D loss: 0.044623, acc: 0.984375]  [G loss: 4.513722, acc: 0.265625]\n",
            "4101: [D loss: 0.093497, acc: 0.976562]  [G loss: 5.158294, acc: 0.140625]\n",
            "4102: [D loss: 0.142634, acc: 0.968750]  [G loss: 6.402884, acc: 0.078125]\n",
            "4103: [D loss: 0.066950, acc: 0.976562]  [G loss: 6.005486, acc: 0.062500]\n",
            "4104: [D loss: 0.014249, acc: 1.000000]  [G loss: 6.068154, acc: 0.046875]\n",
            "4105: [D loss: 0.046520, acc: 0.984375]  [G loss: 6.837886, acc: 0.046875]\n",
            "4106: [D loss: 0.018709, acc: 0.992188]  [G loss: 6.142522, acc: 0.062500]\n",
            "4107: [D loss: 0.025718, acc: 0.984375]  [G loss: 5.098018, acc: 0.125000]\n",
            "4108: [D loss: 0.012469, acc: 0.992188]  [G loss: 3.602316, acc: 0.218750]\n",
            "4109: [D loss: 0.031307, acc: 0.984375]  [G loss: 2.495113, acc: 0.390625]\n",
            "4110: [D loss: 0.058639, acc: 0.984375]  [G loss: 2.847391, acc: 0.406250]\n",
            "4111: [D loss: 0.040643, acc: 0.984375]  [G loss: 2.401745, acc: 0.453125]\n",
            "4112: [D loss: 0.109428, acc: 0.968750]  [G loss: 2.735862, acc: 0.453125]\n",
            "4113: [D loss: 0.047005, acc: 0.984375]  [G loss: 3.010092, acc: 0.515625]\n",
            "4114: [D loss: 0.017272, acc: 0.992188]  [G loss: 3.323319, acc: 0.500000]\n",
            "4115: [D loss: 0.012810, acc: 1.000000]  [G loss: 3.203598, acc: 0.484375]\n",
            "4116: [D loss: 0.087631, acc: 0.984375]  [G loss: 2.876761, acc: 0.531250]\n",
            "4117: [D loss: 0.055488, acc: 0.992188]  [G loss: 2.669580, acc: 0.546875]\n",
            "4118: [D loss: 0.033586, acc: 0.984375]  [G loss: 1.743042, acc: 0.609375]\n",
            "4119: [D loss: 0.034642, acc: 0.992188]  [G loss: 1.238034, acc: 0.671875]\n",
            "4120: [D loss: 0.044376, acc: 0.984375]  [G loss: 1.507913, acc: 0.671875]\n",
            "4121: [D loss: 0.038689, acc: 0.992188]  [G loss: 1.278217, acc: 0.703125]\n",
            "4122: [D loss: 0.021806, acc: 0.992188]  [G loss: 1.150263, acc: 0.718750]\n",
            "4123: [D loss: 0.019676, acc: 0.992188]  [G loss: 0.975504, acc: 0.781250]\n",
            "4124: [D loss: 0.009638, acc: 1.000000]  [G loss: 0.608688, acc: 0.843750]\n",
            "4125: [D loss: 0.002891, acc: 1.000000]  [G loss: 0.818106, acc: 0.734375]\n",
            "4126: [D loss: 0.031004, acc: 0.984375]  [G loss: 0.734489, acc: 0.765625]\n",
            "4127: [D loss: 0.006808, acc: 1.000000]  [G loss: 0.872000, acc: 0.734375]\n",
            "4128: [D loss: 0.034492, acc: 0.992188]  [G loss: 1.303926, acc: 0.687500]\n",
            "4129: [D loss: 0.000995, acc: 1.000000]  [G loss: 1.107394, acc: 0.703125]\n",
            "4130: [D loss: 0.020272, acc: 0.992188]  [G loss: 1.163831, acc: 0.703125]\n",
            "4131: [D loss: 0.105609, acc: 0.968750]  [G loss: 0.660187, acc: 0.843750]\n",
            "4132: [D loss: 0.135280, acc: 0.968750]  [G loss: 0.514245, acc: 0.859375]\n",
            "4133: [D loss: 0.124829, acc: 0.960938]  [G loss: 0.717398, acc: 0.812500]\n",
            "4134: [D loss: 0.025477, acc: 0.992188]  [G loss: 0.858088, acc: 0.750000]\n",
            "4135: [D loss: 0.001705, acc: 1.000000]  [G loss: 1.361164, acc: 0.609375]\n",
            "4136: [D loss: 0.137580, acc: 0.968750]  [G loss: 1.088923, acc: 0.671875]\n",
            "4137: [D loss: 0.101936, acc: 0.992188]  [G loss: 0.872134, acc: 0.765625]\n",
            "4138: [D loss: 0.070510, acc: 0.984375]  [G loss: 1.015085, acc: 0.718750]\n",
            "4139: [D loss: 0.031460, acc: 0.984375]  [G loss: 1.652138, acc: 0.546875]\n",
            "4140: [D loss: 0.067980, acc: 0.968750]  [G loss: 1.942881, acc: 0.531250]\n",
            "4141: [D loss: 0.046245, acc: 0.984375]  [G loss: 3.080867, acc: 0.390625]\n",
            "4142: [D loss: 0.059324, acc: 0.992188]  [G loss: 3.237602, acc: 0.375000]\n",
            "4143: [D loss: 0.178284, acc: 0.968750]  [G loss: 1.925604, acc: 0.421875]\n",
            "4144: [D loss: 0.071006, acc: 0.968750]  [G loss: 2.270044, acc: 0.453125]\n",
            "4145: [D loss: 0.043709, acc: 0.976562]  [G loss: 2.768231, acc: 0.328125]\n",
            "4146: [D loss: 0.033284, acc: 0.992188]  [G loss: 3.312073, acc: 0.296875]\n",
            "4147: [D loss: 0.049586, acc: 0.976562]  [G loss: 3.796752, acc: 0.265625]\n",
            "4148: [D loss: 0.072097, acc: 0.984375]  [G loss: 3.522750, acc: 0.187500]\n",
            "4149: [D loss: 0.029177, acc: 0.992188]  [G loss: 3.920466, acc: 0.265625]\n",
            "4150: [D loss: 0.080873, acc: 0.984375]  [G loss: 3.992429, acc: 0.156250]\n",
            "4151: [D loss: 0.174996, acc: 0.968750]  [G loss: 2.351185, acc: 0.375000]\n",
            "4152: [D loss: 0.082843, acc: 0.976562]  [G loss: 2.195855, acc: 0.343750]\n",
            "4153: [D loss: 0.109911, acc: 0.953125]  [G loss: 2.896900, acc: 0.281250]\n",
            "4154: [D loss: 0.077733, acc: 0.960938]  [G loss: 4.129692, acc: 0.156250]\n",
            "4155: [D loss: 0.084572, acc: 0.968750]  [G loss: 4.353846, acc: 0.093750]\n",
            "4156: [D loss: 0.100367, acc: 0.960938]  [G loss: 4.380818, acc: 0.062500]\n",
            "4157: [D loss: 0.092953, acc: 0.976562]  [G loss: 2.813059, acc: 0.265625]\n",
            "4158: [D loss: 0.046278, acc: 0.976562]  [G loss: 2.728437, acc: 0.265625]\n",
            "4159: [D loss: 0.133102, acc: 0.968750]  [G loss: 2.346060, acc: 0.406250]\n",
            "4160: [D loss: 0.082888, acc: 0.984375]  [G loss: 2.280715, acc: 0.390625]\n",
            "4161: [D loss: 0.114251, acc: 0.960938]  [G loss: 2.603776, acc: 0.312500]\n",
            "4162: [D loss: 0.046624, acc: 0.976562]  [G loss: 2.944669, acc: 0.265625]\n",
            "4163: [D loss: 0.041049, acc: 1.000000]  [G loss: 3.748571, acc: 0.265625]\n",
            "4164: [D loss: 0.072376, acc: 0.976562]  [G loss: 3.420468, acc: 0.281250]\n",
            "4165: [D loss: 0.095873, acc: 0.976562]  [G loss: 3.349178, acc: 0.312500]\n",
            "4166: [D loss: 0.061908, acc: 0.984375]  [G loss: 2.515903, acc: 0.343750]\n",
            "4167: [D loss: 0.105554, acc: 0.976562]  [G loss: 2.387248, acc: 0.312500]\n",
            "4168: [D loss: 0.115652, acc: 0.960938]  [G loss: 2.300626, acc: 0.359375]\n",
            "4169: [D loss: 0.118582, acc: 0.960938]  [G loss: 2.653369, acc: 0.328125]\n",
            "4170: [D loss: 0.126796, acc: 0.945312]  [G loss: 2.830711, acc: 0.343750]\n",
            "4171: [D loss: 0.063818, acc: 0.984375]  [G loss: 2.793712, acc: 0.375000]\n",
            "4172: [D loss: 0.192484, acc: 0.945312]  [G loss: 2.355535, acc: 0.500000]\n",
            "4173: [D loss: 0.155948, acc: 0.953125]  [G loss: 2.155778, acc: 0.484375]\n",
            "4174: [D loss: 0.102529, acc: 0.953125]  [G loss: 2.580933, acc: 0.343750]\n",
            "4175: [D loss: 0.049602, acc: 0.984375]  [G loss: 2.627318, acc: 0.343750]\n",
            "4176: [D loss: 0.054802, acc: 0.984375]  [G loss: 3.434913, acc: 0.171875]\n",
            "4177: [D loss: 0.022815, acc: 0.992188]  [G loss: 3.450389, acc: 0.250000]\n",
            "4178: [D loss: 0.110381, acc: 0.968750]  [G loss: 2.847887, acc: 0.296875]\n",
            "4179: [D loss: 0.112509, acc: 0.968750]  [G loss: 1.790419, acc: 0.437500]\n",
            "4180: [D loss: 0.043831, acc: 0.984375]  [G loss: 1.126765, acc: 0.609375]\n",
            "4181: [D loss: 0.059112, acc: 0.992188]  [G loss: 0.932655, acc: 0.718750]\n",
            "4182: [D loss: 0.083475, acc: 0.968750]  [G loss: 0.982123, acc: 0.718750]\n",
            "4183: [D loss: 0.102266, acc: 0.976562]  [G loss: 0.827337, acc: 0.734375]\n",
            "4184: [D loss: 0.110002, acc: 0.976562]  [G loss: 1.588839, acc: 0.546875]\n",
            "4185: [D loss: 0.070709, acc: 0.968750]  [G loss: 1.877154, acc: 0.437500]\n",
            "4186: [D loss: 0.040571, acc: 0.976562]  [G loss: 1.657207, acc: 0.484375]\n",
            "4187: [D loss: 0.069891, acc: 0.976562]  [G loss: 2.239997, acc: 0.390625]\n",
            "4188: [D loss: 0.025206, acc: 0.992188]  [G loss: 2.472128, acc: 0.328125]\n",
            "4189: [D loss: 0.047213, acc: 0.984375]  [G loss: 2.304020, acc: 0.390625]\n",
            "4190: [D loss: 0.023853, acc: 0.984375]  [G loss: 1.877499, acc: 0.531250]\n",
            "4191: [D loss: 0.077507, acc: 0.992188]  [G loss: 1.927957, acc: 0.562500]\n",
            "4192: [D loss: 0.099802, acc: 0.976562]  [G loss: 1.323202, acc: 0.640625]\n",
            "4193: [D loss: 0.072449, acc: 0.976562]  [G loss: 1.110732, acc: 0.625000]\n",
            "4194: [D loss: 0.054887, acc: 0.968750]  [G loss: 1.113516, acc: 0.640625]\n",
            "4195: [D loss: 0.160758, acc: 0.960938]  [G loss: 1.248293, acc: 0.640625]\n",
            "4196: [D loss: 0.044537, acc: 0.992188]  [G loss: 1.184964, acc: 0.593750]\n",
            "4197: [D loss: 0.077948, acc: 0.968750]  [G loss: 2.022526, acc: 0.515625]\n",
            "4198: [D loss: 0.077600, acc: 0.968750]  [G loss: 2.189489, acc: 0.453125]\n",
            "4199: [D loss: 0.066184, acc: 0.984375]  [G loss: 1.661224, acc: 0.468750]\n",
            "4200: [D loss: 0.155511, acc: 0.960938]  [G loss: 1.168316, acc: 0.625000]\n",
            "4201: [D loss: 0.103028, acc: 0.937500]  [G loss: 1.725604, acc: 0.593750]\n",
            "4202: [D loss: 0.116421, acc: 0.976562]  [G loss: 2.871399, acc: 0.312500]\n",
            "4203: [D loss: 0.055770, acc: 0.976562]  [G loss: 3.986409, acc: 0.250000]\n",
            "4204: [D loss: 0.447811, acc: 0.898438]  [G loss: 2.435588, acc: 0.328125]\n",
            "4205: [D loss: 0.213862, acc: 0.906250]  [G loss: 2.023662, acc: 0.375000]\n",
            "4206: [D loss: 0.231212, acc: 0.906250]  [G loss: 4.298059, acc: 0.125000]\n",
            "4207: [D loss: 0.202873, acc: 0.945312]  [G loss: 6.162091, acc: 0.000000]\n",
            "4208: [D loss: 0.398768, acc: 0.890625]  [G loss: 4.046665, acc: 0.171875]\n",
            "4209: [D loss: 0.294987, acc: 0.937500]  [G loss: 2.361119, acc: 0.312500]\n",
            "4210: [D loss: 0.265203, acc: 0.898438]  [G loss: 3.531932, acc: 0.203125]\n",
            "4211: [D loss: 0.116876, acc: 0.976562]  [G loss: 6.317740, acc: 0.078125]\n",
            "4212: [D loss: 0.160280, acc: 0.953125]  [G loss: 6.902237, acc: 0.078125]\n",
            "4213: [D loss: 0.080367, acc: 0.976562]  [G loss: 6.503035, acc: 0.062500]\n",
            "4214: [D loss: 0.227539, acc: 0.929688]  [G loss: 4.579736, acc: 0.171875]\n",
            "4215: [D loss: 0.047274, acc: 0.992188]  [G loss: 4.452394, acc: 0.234375]\n",
            "4216: [D loss: 0.191939, acc: 0.929688]  [G loss: 4.968670, acc: 0.187500]\n",
            "4217: [D loss: 0.043268, acc: 0.984375]  [G loss: 6.362278, acc: 0.015625]\n",
            "4218: [D loss: 0.091743, acc: 0.960938]  [G loss: 6.667686, acc: 0.046875]\n",
            "4219: [D loss: 0.093945, acc: 0.968750]  [G loss: 6.581961, acc: 0.015625]\n",
            "4220: [D loss: 0.091001, acc: 0.945312]  [G loss: 5.832257, acc: 0.015625]\n",
            "4221: [D loss: 0.181639, acc: 0.953125]  [G loss: 4.327055, acc: 0.156250]\n",
            "4222: [D loss: 0.083213, acc: 0.960938]  [G loss: 3.381652, acc: 0.218750]\n",
            "4223: [D loss: 0.193813, acc: 0.937500]  [G loss: 4.661510, acc: 0.140625]\n",
            "4224: [D loss: 0.144852, acc: 0.960938]  [G loss: 5.990771, acc: 0.062500]\n",
            "4225: [D loss: 0.092039, acc: 0.968750]  [G loss: 6.166592, acc: 0.015625]\n",
            "4226: [D loss: 0.175807, acc: 0.953125]  [G loss: 4.966350, acc: 0.140625]\n",
            "4227: [D loss: 0.023963, acc: 1.000000]  [G loss: 4.546374, acc: 0.078125]\n",
            "4228: [D loss: 0.040956, acc: 0.976562]  [G loss: 3.152613, acc: 0.203125]\n",
            "4229: [D loss: 0.065861, acc: 0.976562]  [G loss: 2.719222, acc: 0.328125]\n",
            "4230: [D loss: 0.129188, acc: 0.960938]  [G loss: 3.744710, acc: 0.171875]\n",
            "4231: [D loss: 0.091887, acc: 0.976562]  [G loss: 4.408334, acc: 0.109375]\n",
            "4232: [D loss: 0.034251, acc: 1.000000]  [G loss: 4.278799, acc: 0.140625]\n",
            "4233: [D loss: 0.129794, acc: 0.953125]  [G loss: 3.842753, acc: 0.187500]\n",
            "4234: [D loss: 0.091114, acc: 0.976562]  [G loss: 4.342136, acc: 0.187500]\n",
            "4235: [D loss: 0.057420, acc: 0.976562]  [G loss: 4.576143, acc: 0.171875]\n",
            "4236: [D loss: 0.055528, acc: 0.984375]  [G loss: 4.119785, acc: 0.250000]\n",
            "4237: [D loss: 0.171050, acc: 0.937500]  [G loss: 3.493173, acc: 0.265625]\n",
            "4238: [D loss: 0.183385, acc: 0.953125]  [G loss: 2.901073, acc: 0.328125]\n",
            "4239: [D loss: 0.079779, acc: 0.992188]  [G loss: 3.037838, acc: 0.296875]\n",
            "4240: [D loss: 0.107783, acc: 0.960938]  [G loss: 3.330632, acc: 0.328125]\n",
            "4241: [D loss: 0.067994, acc: 0.968750]  [G loss: 3.951936, acc: 0.296875]\n",
            "4242: [D loss: 0.140057, acc: 0.953125]  [G loss: 4.192215, acc: 0.218750]\n",
            "4243: [D loss: 0.069183, acc: 0.968750]  [G loss: 4.106513, acc: 0.187500]\n",
            "4244: [D loss: 0.123388, acc: 0.960938]  [G loss: 4.484031, acc: 0.171875]\n",
            "4245: [D loss: 0.018641, acc: 0.992188]  [G loss: 4.541755, acc: 0.093750]\n",
            "4246: [D loss: 0.139479, acc: 0.953125]  [G loss: 3.736048, acc: 0.109375]\n",
            "4247: [D loss: 0.042226, acc: 0.976562]  [G loss: 3.273746, acc: 0.234375]\n",
            "4248: [D loss: 0.121893, acc: 0.968750]  [G loss: 2.794651, acc: 0.265625]\n",
            "4249: [D loss: 0.070670, acc: 0.968750]  [G loss: 3.613451, acc: 0.187500]\n",
            "4250: [D loss: 0.086890, acc: 0.960938]  [G loss: 3.579901, acc: 0.140625]\n",
            "4251: [D loss: 0.057189, acc: 0.984375]  [G loss: 3.571455, acc: 0.203125]\n",
            "4252: [D loss: 0.096658, acc: 0.984375]  [G loss: 4.426864, acc: 0.125000]\n",
            "4253: [D loss: 0.067619, acc: 0.984375]  [G loss: 3.926797, acc: 0.187500]\n",
            "4254: [D loss: 0.047456, acc: 0.984375]  [G loss: 3.719492, acc: 0.125000]\n",
            "4255: [D loss: 0.091352, acc: 0.976562]  [G loss: 2.857623, acc: 0.312500]\n",
            "4256: [D loss: 0.103477, acc: 0.953125]  [G loss: 1.997943, acc: 0.453125]\n",
            "4257: [D loss: 0.085581, acc: 0.968750]  [G loss: 2.481898, acc: 0.328125]\n",
            "4258: [D loss: 0.096571, acc: 0.968750]  [G loss: 3.357810, acc: 0.234375]\n",
            "4259: [D loss: 0.023587, acc: 0.984375]  [G loss: 3.987592, acc: 0.187500]\n",
            "4260: [D loss: 0.083249, acc: 0.960938]  [G loss: 4.816170, acc: 0.093750]\n",
            "4261: [D loss: 0.140531, acc: 0.968750]  [G loss: 3.801797, acc: 0.250000]\n",
            "4262: [D loss: 0.032423, acc: 0.984375]  [G loss: 3.514251, acc: 0.250000]\n",
            "4263: [D loss: 0.097212, acc: 0.976562]  [G loss: 2.912036, acc: 0.328125]\n",
            "4264: [D loss: 0.026746, acc: 1.000000]  [G loss: 2.432796, acc: 0.421875]\n",
            "4265: [D loss: 0.092193, acc: 0.984375]  [G loss: 1.921276, acc: 0.390625]\n",
            "4266: [D loss: 0.061359, acc: 0.992188]  [G loss: 1.967734, acc: 0.406250]\n",
            "4267: [D loss: 0.100882, acc: 0.968750]  [G loss: 2.369631, acc: 0.359375]\n",
            "4268: [D loss: 0.020255, acc: 1.000000]  [G loss: 2.870409, acc: 0.265625]\n",
            "4269: [D loss: 0.068251, acc: 0.976562]  [G loss: 3.412495, acc: 0.218750]\n",
            "4270: [D loss: 0.024621, acc: 0.984375]  [G loss: 3.883461, acc: 0.187500]\n",
            "4271: [D loss: 0.073825, acc: 0.984375]  [G loss: 3.084239, acc: 0.218750]\n",
            "4272: [D loss: 0.122931, acc: 0.960938]  [G loss: 2.877964, acc: 0.265625]\n",
            "4273: [D loss: 0.058567, acc: 0.976562]  [G loss: 1.520429, acc: 0.453125]\n",
            "4274: [D loss: 0.080602, acc: 0.968750]  [G loss: 1.446872, acc: 0.453125]\n",
            "4275: [D loss: 0.087884, acc: 0.984375]  [G loss: 1.798338, acc: 0.375000]\n",
            "4276: [D loss: 0.081198, acc: 0.976562]  [G loss: 2.732318, acc: 0.187500]\n",
            "4277: [D loss: 0.101206, acc: 0.953125]  [G loss: 2.289557, acc: 0.312500]\n",
            "4278: [D loss: 0.108479, acc: 0.968750]  [G loss: 1.804584, acc: 0.437500]\n",
            "4279: [D loss: 0.090226, acc: 0.968750]  [G loss: 1.352368, acc: 0.625000]\n",
            "4280: [D loss: 0.080576, acc: 0.968750]  [G loss: 1.341382, acc: 0.531250]\n",
            "4281: [D loss: 0.039861, acc: 0.992188]  [G loss: 2.363633, acc: 0.453125]\n",
            "4282: [D loss: 0.043896, acc: 0.984375]  [G loss: 2.357203, acc: 0.375000]\n",
            "4283: [D loss: 0.049253, acc: 0.992188]  [G loss: 3.381883, acc: 0.171875]\n",
            "4284: [D loss: 0.070691, acc: 0.976562]  [G loss: 2.861252, acc: 0.265625]\n",
            "4285: [D loss: 0.105354, acc: 0.960938]  [G loss: 2.860768, acc: 0.281250]\n",
            "4286: [D loss: 0.026880, acc: 0.992188]  [G loss: 2.568505, acc: 0.296875]\n",
            "4287: [D loss: 0.074479, acc: 0.960938]  [G loss: 1.785794, acc: 0.453125]\n",
            "4288: [D loss: 0.071832, acc: 0.984375]  [G loss: 1.518225, acc: 0.437500]\n",
            "4289: [D loss: 0.049371, acc: 0.992188]  [G loss: 2.074023, acc: 0.296875]\n",
            "4290: [D loss: 0.067650, acc: 0.976562]  [G loss: 2.796401, acc: 0.203125]\n",
            "4291: [D loss: 0.051977, acc: 0.976562]  [G loss: 2.342972, acc: 0.234375]\n",
            "4292: [D loss: 0.118090, acc: 0.945312]  [G loss: 3.300962, acc: 0.109375]\n",
            "4293: [D loss: 0.069114, acc: 0.968750]  [G loss: 2.606784, acc: 0.265625]\n",
            "4294: [D loss: 0.041784, acc: 0.992188]  [G loss: 1.987438, acc: 0.375000]\n",
            "4295: [D loss: 0.046405, acc: 0.984375]  [G loss: 2.437415, acc: 0.265625]\n",
            "4296: [D loss: 0.108815, acc: 0.968750]  [G loss: 2.690034, acc: 0.171875]\n",
            "4297: [D loss: 0.080584, acc: 0.968750]  [G loss: 2.720926, acc: 0.250000]\n",
            "4298: [D loss: 0.064449, acc: 0.984375]  [G loss: 3.227542, acc: 0.125000]\n",
            "4299: [D loss: 0.077832, acc: 0.960938]  [G loss: 2.798216, acc: 0.093750]\n",
            "4300: [D loss: 0.135384, acc: 0.945312]  [G loss: 2.485473, acc: 0.218750]\n",
            "4301: [D loss: 0.079226, acc: 0.968750]  [G loss: 2.171167, acc: 0.437500]\n",
            "4302: [D loss: 0.082088, acc: 0.953125]  [G loss: 1.616372, acc: 0.421875]\n",
            "4303: [D loss: 0.025447, acc: 0.992188]  [G loss: 1.451243, acc: 0.406250]\n",
            "4304: [D loss: 0.107289, acc: 0.968750]  [G loss: 1.724175, acc: 0.421875]\n",
            "4305: [D loss: 0.060021, acc: 0.976562]  [G loss: 2.986331, acc: 0.187500]\n",
            "4306: [D loss: 0.052739, acc: 0.992188]  [G loss: 4.151282, acc: 0.109375]\n",
            "4307: [D loss: 0.178463, acc: 0.929688]  [G loss: 3.087220, acc: 0.218750]\n",
            "4308: [D loss: 0.087133, acc: 0.968750]  [G loss: 2.485602, acc: 0.281250]\n",
            "4309: [D loss: 0.086854, acc: 0.960938]  [G loss: 2.176688, acc: 0.296875]\n",
            "4310: [D loss: 0.059296, acc: 0.976562]  [G loss: 2.331580, acc: 0.218750]\n",
            "4311: [D loss: 0.068529, acc: 0.976562]  [G loss: 2.380930, acc: 0.281250]\n",
            "4312: [D loss: 0.104970, acc: 0.960938]  [G loss: 3.142102, acc: 0.125000]\n",
            "4313: [D loss: 0.041996, acc: 0.992188]  [G loss: 3.168813, acc: 0.125000]\n",
            "4314: [D loss: 0.019796, acc: 1.000000]  [G loss: 3.970378, acc: 0.031250]\n",
            "4315: [D loss: 0.081058, acc: 0.960938]  [G loss: 3.878321, acc: 0.046875]\n",
            "4316: [D loss: 0.083300, acc: 0.976562]  [G loss: 2.871230, acc: 0.171875]\n",
            "4317: [D loss: 0.123385, acc: 0.953125]  [G loss: 3.035337, acc: 0.187500]\n",
            "4318: [D loss: 0.079113, acc: 0.960938]  [G loss: 2.760892, acc: 0.156250]\n",
            "4319: [D loss: 0.043109, acc: 0.992188]  [G loss: 3.557987, acc: 0.109375]\n",
            "4320: [D loss: 0.058404, acc: 0.976562]  [G loss: 3.214729, acc: 0.171875]\n",
            "4321: [D loss: 0.093489, acc: 0.953125]  [G loss: 3.208244, acc: 0.156250]\n",
            "4322: [D loss: 0.058997, acc: 0.976562]  [G loss: 2.844342, acc: 0.281250]\n",
            "4323: [D loss: 0.078282, acc: 0.968750]  [G loss: 2.880555, acc: 0.328125]\n",
            "4324: [D loss: 0.068398, acc: 0.960938]  [G loss: 2.807942, acc: 0.234375]\n",
            "4325: [D loss: 0.102625, acc: 0.968750]  [G loss: 2.501777, acc: 0.281250]\n",
            "4326: [D loss: 0.030866, acc: 1.000000]  [G loss: 3.177579, acc: 0.203125]\n",
            "4327: [D loss: 0.023646, acc: 0.992188]  [G loss: 3.258952, acc: 0.156250]\n",
            "4328: [D loss: 0.021167, acc: 0.992188]  [G loss: 3.830194, acc: 0.062500]\n",
            "4329: [D loss: 0.039679, acc: 0.984375]  [G loss: 4.459687, acc: 0.156250]\n",
            "4330: [D loss: 0.069802, acc: 0.968750]  [G loss: 3.937814, acc: 0.140625]\n",
            "4331: [D loss: 0.029805, acc: 0.992188]  [G loss: 3.191680, acc: 0.187500]\n",
            "4332: [D loss: 0.097817, acc: 0.968750]  [G loss: 2.478465, acc: 0.171875]\n",
            "4333: [D loss: 0.073783, acc: 0.968750]  [G loss: 3.253328, acc: 0.234375]\n",
            "4334: [D loss: 0.048267, acc: 0.976562]  [G loss: 3.092939, acc: 0.218750]\n",
            "4335: [D loss: 0.063403, acc: 0.984375]  [G loss: 3.358414, acc: 0.203125]\n",
            "4336: [D loss: 0.038893, acc: 0.984375]  [G loss: 3.253045, acc: 0.265625]\n",
            "4337: [D loss: 0.052092, acc: 0.992188]  [G loss: 3.242268, acc: 0.234375]\n",
            "4338: [D loss: 0.040458, acc: 0.976562]  [G loss: 3.409863, acc: 0.218750]\n",
            "4339: [D loss: 0.025745, acc: 1.000000]  [G loss: 3.632789, acc: 0.234375]\n",
            "4340: [D loss: 0.034043, acc: 0.992188]  [G loss: 3.193862, acc: 0.265625]\n",
            "4341: [D loss: 0.020430, acc: 1.000000]  [G loss: 3.555021, acc: 0.312500]\n",
            "4342: [D loss: 0.041680, acc: 0.992188]  [G loss: 3.260894, acc: 0.281250]\n",
            "4343: [D loss: 0.019264, acc: 0.992188]  [G loss: 3.163387, acc: 0.265625]\n",
            "4344: [D loss: 0.065777, acc: 0.984375]  [G loss: 3.452360, acc: 0.281250]\n",
            "4345: [D loss: 0.025501, acc: 0.992188]  [G loss: 3.239721, acc: 0.312500]\n",
            "4346: [D loss: 0.069947, acc: 0.976562]  [G loss: 3.489704, acc: 0.281250]\n",
            "4347: [D loss: 0.007358, acc: 1.000000]  [G loss: 3.758262, acc: 0.281250]\n",
            "4348: [D loss: 0.021561, acc: 0.992188]  [G loss: 3.712006, acc: 0.265625]\n",
            "4349: [D loss: 0.005047, acc: 1.000000]  [G loss: 3.478367, acc: 0.234375]\n",
            "4350: [D loss: 0.005654, acc: 1.000000]  [G loss: 3.415311, acc: 0.296875]\n",
            "4351: [D loss: 0.014439, acc: 0.992188]  [G loss: 3.183051, acc: 0.343750]\n",
            "4352: [D loss: 0.008975, acc: 1.000000]  [G loss: 3.002332, acc: 0.312500]\n",
            "4353: [D loss: 0.056031, acc: 0.992188]  [G loss: 2.842374, acc: 0.359375]\n",
            "4354: [D loss: 0.010771, acc: 0.992188]  [G loss: 3.071104, acc: 0.359375]\n",
            "4355: [D loss: 0.010562, acc: 1.000000]  [G loss: 2.819367, acc: 0.484375]\n",
            "4356: [D loss: 0.053683, acc: 0.992188]  [G loss: 3.105446, acc: 0.437500]\n",
            "4357: [D loss: 0.010448, acc: 1.000000]  [G loss: 3.399711, acc: 0.406250]\n",
            "4358: [D loss: 0.009997, acc: 0.992188]  [G loss: 3.056760, acc: 0.406250]\n",
            "4359: [D loss: 0.016550, acc: 0.992188]  [G loss: 3.585287, acc: 0.312500]\n",
            "4360: [D loss: 0.037504, acc: 0.984375]  [G loss: 3.401373, acc: 0.359375]\n",
            "4361: [D loss: 0.029000, acc: 0.992188]  [G loss: 3.629745, acc: 0.406250]\n",
            "4362: [D loss: 0.010622, acc: 1.000000]  [G loss: 2.910107, acc: 0.406250]\n",
            "4363: [D loss: 0.033557, acc: 0.984375]  [G loss: 3.129599, acc: 0.343750]\n",
            "4364: [D loss: 0.072646, acc: 0.976562]  [G loss: 2.606359, acc: 0.468750]\n",
            "4365: [D loss: 0.048974, acc: 0.976562]  [G loss: 2.946492, acc: 0.375000]\n",
            "4366: [D loss: 0.023893, acc: 0.992188]  [G loss: 3.053348, acc: 0.390625]\n",
            "4367: [D loss: 0.061984, acc: 0.968750]  [G loss: 3.637159, acc: 0.218750]\n",
            "4368: [D loss: 0.037425, acc: 0.984375]  [G loss: 3.148207, acc: 0.328125]\n",
            "4369: [D loss: 0.066013, acc: 0.984375]  [G loss: 3.681916, acc: 0.265625]\n",
            "4370: [D loss: 0.097335, acc: 0.968750]  [G loss: 3.247828, acc: 0.328125]\n",
            "4371: [D loss: 0.041778, acc: 0.976562]  [G loss: 3.666268, acc: 0.250000]\n",
            "4372: [D loss: 0.089197, acc: 0.968750]  [G loss: 3.787234, acc: 0.234375]\n",
            "4373: [D loss: 0.118770, acc: 0.968750]  [G loss: 2.951061, acc: 0.312500]\n",
            "4374: [D loss: 0.094464, acc: 0.953125]  [G loss: 4.620326, acc: 0.140625]\n",
            "4375: [D loss: 0.061101, acc: 0.968750]  [G loss: 5.015596, acc: 0.046875]\n",
            "4376: [D loss: 0.184840, acc: 0.968750]  [G loss: 5.072763, acc: 0.156250]\n",
            "4377: [D loss: 0.073253, acc: 0.976562]  [G loss: 4.288762, acc: 0.171875]\n",
            "4378: [D loss: 0.038119, acc: 0.984375]  [G loss: 3.690466, acc: 0.234375]\n",
            "4379: [D loss: 0.118961, acc: 0.976562]  [G loss: 4.127587, acc: 0.218750]\n",
            "4380: [D loss: 0.026126, acc: 0.984375]  [G loss: 5.864340, acc: 0.109375]\n",
            "4381: [D loss: 0.031075, acc: 0.984375]  [G loss: 5.446569, acc: 0.046875]\n",
            "4382: [D loss: 0.031130, acc: 0.992188]  [G loss: 6.930199, acc: 0.031250]\n",
            "4383: [D loss: 0.023619, acc: 0.984375]  [G loss: 5.841166, acc: 0.156250]\n",
            "4384: [D loss: 0.028745, acc: 0.984375]  [G loss: 4.837686, acc: 0.140625]\n",
            "4385: [D loss: 0.028817, acc: 0.984375]  [G loss: 3.915943, acc: 0.156250]\n",
            "4386: [D loss: 0.016266, acc: 1.000000]  [G loss: 4.922038, acc: 0.171875]\n",
            "4387: [D loss: 0.021447, acc: 0.992188]  [G loss: 5.041247, acc: 0.203125]\n",
            "4388: [D loss: 0.097329, acc: 0.953125]  [G loss: 7.290545, acc: 0.062500]\n",
            "4389: [D loss: 0.048414, acc: 0.984375]  [G loss: 8.157686, acc: 0.031250]\n",
            "4390: [D loss: 0.025858, acc: 0.992188]  [G loss: 8.949078, acc: 0.031250]\n",
            "4391: [D loss: 0.083403, acc: 0.976562]  [G loss: 7.030271, acc: 0.109375]\n",
            "4392: [D loss: 0.140982, acc: 0.953125]  [G loss: 3.997867, acc: 0.296875]\n",
            "4393: [D loss: 0.193434, acc: 0.953125]  [G loss: 4.252490, acc: 0.250000]\n",
            "4394: [D loss: 0.088040, acc: 0.968750]  [G loss: 7.174606, acc: 0.140625]\n",
            "4395: [D loss: 0.044368, acc: 0.976562]  [G loss: 7.050308, acc: 0.078125]\n",
            "4396: [D loss: 0.033908, acc: 0.984375]  [G loss: 8.027045, acc: 0.093750]\n",
            "4397: [D loss: 0.154677, acc: 0.953125]  [G loss: 6.432848, acc: 0.109375]\n",
            "4398: [D loss: 0.016251, acc: 0.992188]  [G loss: 4.548123, acc: 0.156250]\n",
            "4399: [D loss: 0.077882, acc: 0.976562]  [G loss: 3.198789, acc: 0.312500]\n",
            "4400: [D loss: 0.050483, acc: 0.984375]  [G loss: 3.495545, acc: 0.328125]\n",
            "4401: [D loss: 0.135444, acc: 0.968750]  [G loss: 3.214519, acc: 0.484375]\n",
            "4402: [D loss: 0.055227, acc: 0.976562]  [G loss: 3.749723, acc: 0.265625]\n",
            "4403: [D loss: 0.004939, acc: 1.000000]  [G loss: 4.670619, acc: 0.218750]\n",
            "4404: [D loss: 0.030845, acc: 0.992188]  [G loss: 5.240211, acc: 0.234375]\n",
            "4405: [D loss: 0.036646, acc: 0.992188]  [G loss: 4.664104, acc: 0.281250]\n",
            "4406: [D loss: 0.087675, acc: 0.968750]  [G loss: 3.399676, acc: 0.406250]\n",
            "4407: [D loss: 0.035921, acc: 0.968750]  [G loss: 2.126912, acc: 0.640625]\n",
            "4408: [D loss: 0.036112, acc: 0.984375]  [G loss: 1.433407, acc: 0.625000]\n",
            "4409: [D loss: 0.037744, acc: 0.984375]  [G loss: 1.867430, acc: 0.593750]\n",
            "4410: [D loss: 0.033500, acc: 0.992188]  [G loss: 1.719667, acc: 0.609375]\n",
            "4411: [D loss: 0.070721, acc: 0.976562]  [G loss: 2.515997, acc: 0.546875]\n",
            "4412: [D loss: 0.042755, acc: 0.976562]  [G loss: 4.452824, acc: 0.312500]\n",
            "4413: [D loss: 0.094558, acc: 0.968750]  [G loss: 5.068318, acc: 0.343750]\n",
            "4414: [D loss: 0.096439, acc: 0.960938]  [G loss: 4.125259, acc: 0.406250]\n",
            "4415: [D loss: 0.123576, acc: 0.953125]  [G loss: 3.332425, acc: 0.484375]\n",
            "4416: [D loss: 0.051530, acc: 0.984375]  [G loss: 1.953697, acc: 0.640625]\n",
            "4417: [D loss: 0.115644, acc: 0.953125]  [G loss: 1.641896, acc: 0.656250]\n",
            "4418: [D loss: 0.201150, acc: 0.921875]  [G loss: 2.708417, acc: 0.515625]\n",
            "4419: [D loss: 0.027559, acc: 0.992188]  [G loss: 4.405698, acc: 0.296875]\n",
            "4420: [D loss: 0.121022, acc: 0.953125]  [G loss: 5.015178, acc: 0.281250]\n",
            "4421: [D loss: 0.273400, acc: 0.914062]  [G loss: 3.764278, acc: 0.437500]\n",
            "4422: [D loss: 0.076858, acc: 0.968750]  [G loss: 1.700124, acc: 0.625000]\n",
            "4423: [D loss: 0.108804, acc: 0.968750]  [G loss: 1.318751, acc: 0.687500]\n",
            "4424: [D loss: 0.095750, acc: 0.968750]  [G loss: 1.485047, acc: 0.656250]\n",
            "4425: [D loss: 0.189132, acc: 0.929688]  [G loss: 2.385528, acc: 0.515625]\n",
            "4426: [D loss: 0.052371, acc: 0.984375]  [G loss: 3.400380, acc: 0.421875]\n",
            "4427: [D loss: 0.035991, acc: 0.976562]  [G loss: 4.173712, acc: 0.234375]\n",
            "4428: [D loss: 0.135535, acc: 0.953125]  [G loss: 3.786547, acc: 0.328125]\n",
            "4429: [D loss: 0.098206, acc: 0.992188]  [G loss: 3.177460, acc: 0.359375]\n",
            "4430: [D loss: 0.161513, acc: 0.937500]  [G loss: 2.299797, acc: 0.546875]\n",
            "4431: [D loss: 0.114851, acc: 0.968750]  [G loss: 2.104841, acc: 0.562500]\n",
            "4432: [D loss: 0.060343, acc: 0.984375]  [G loss: 1.922379, acc: 0.593750]\n",
            "4433: [D loss: 0.048213, acc: 0.984375]  [G loss: 2.340888, acc: 0.437500]\n",
            "4434: [D loss: 0.165353, acc: 0.945312]  [G loss: 3.191685, acc: 0.234375]\n",
            "4435: [D loss: 0.078024, acc: 0.976562]  [G loss: 3.523967, acc: 0.250000]\n",
            "4436: [D loss: 0.157498, acc: 0.968750]  [G loss: 4.702739, acc: 0.156250]\n",
            "4437: [D loss: 0.116497, acc: 0.953125]  [G loss: 5.120813, acc: 0.140625]\n",
            "4438: [D loss: 0.052388, acc: 0.976562]  [G loss: 4.179351, acc: 0.265625]\n",
            "4439: [D loss: 0.052384, acc: 0.976562]  [G loss: 4.137085, acc: 0.312500]\n",
            "4440: [D loss: 0.051668, acc: 0.968750]  [G loss: 2.935482, acc: 0.390625]\n",
            "4441: [D loss: 0.130245, acc: 0.960938]  [G loss: 4.182581, acc: 0.296875]\n",
            "4442: [D loss: 0.143706, acc: 0.953125]  [G loss: 3.931489, acc: 0.328125]\n",
            "4443: [D loss: 0.055273, acc: 0.984375]  [G loss: 4.260501, acc: 0.281250]\n",
            "4444: [D loss: 0.045750, acc: 0.984375]  [G loss: 4.343327, acc: 0.296875]\n",
            "4445: [D loss: 0.151472, acc: 0.968750]  [G loss: 5.275912, acc: 0.109375]\n",
            "4446: [D loss: 0.049958, acc: 0.976562]  [G loss: 5.707994, acc: 0.078125]\n",
            "4447: [D loss: 0.056448, acc: 0.984375]  [G loss: 5.868808, acc: 0.062500]\n",
            "4448: [D loss: 0.132398, acc: 0.968750]  [G loss: 6.667885, acc: 0.046875]\n",
            "4449: [D loss: 0.020871, acc: 0.992188]  [G loss: 6.183064, acc: 0.062500]\n",
            "4450: [D loss: 0.112180, acc: 0.976562]  [G loss: 5.754192, acc: 0.031250]\n",
            "4451: [D loss: 0.092022, acc: 0.976562]  [G loss: 3.745369, acc: 0.171875]\n",
            "4452: [D loss: 0.093142, acc: 0.984375]  [G loss: 3.773169, acc: 0.234375]\n",
            "4453: [D loss: 0.161638, acc: 0.945312]  [G loss: 3.521885, acc: 0.234375]\n",
            "4454: [D loss: 0.116388, acc: 0.968750]  [G loss: 4.242708, acc: 0.203125]\n",
            "4455: [D loss: 0.023337, acc: 0.992188]  [G loss: 4.947153, acc: 0.140625]\n",
            "4456: [D loss: 0.045449, acc: 0.976562]  [G loss: 5.734935, acc: 0.078125]\n",
            "4457: [D loss: 0.067573, acc: 0.968750]  [G loss: 4.909348, acc: 0.140625]\n",
            "4458: [D loss: 0.066330, acc: 0.960938]  [G loss: 3.934992, acc: 0.250000]\n",
            "4459: [D loss: 0.041816, acc: 0.984375]  [G loss: 2.833476, acc: 0.250000]\n",
            "4460: [D loss: 0.071250, acc: 0.960938]  [G loss: 3.349405, acc: 0.390625]\n",
            "4461: [D loss: 0.110210, acc: 0.945312]  [G loss: 3.249398, acc: 0.296875]\n",
            "4462: [D loss: 0.073503, acc: 0.984375]  [G loss: 4.297469, acc: 0.109375]\n",
            "4463: [D loss: 0.052951, acc: 0.984375]  [G loss: 4.812100, acc: 0.031250]\n",
            "4464: [D loss: 0.030845, acc: 0.992188]  [G loss: 5.592949, acc: 0.046875]\n",
            "4465: [D loss: 0.085614, acc: 0.953125]  [G loss: 5.355799, acc: 0.046875]\n",
            "4466: [D loss: 0.119822, acc: 0.960938]  [G loss: 4.933558, acc: 0.046875]\n",
            "4467: [D loss: 0.095318, acc: 0.968750]  [G loss: 4.007801, acc: 0.109375]\n",
            "4468: [D loss: 0.061188, acc: 0.976562]  [G loss: 3.635190, acc: 0.125000]\n",
            "4469: [D loss: 0.123322, acc: 0.953125]  [G loss: 5.002574, acc: 0.046875]\n",
            "4470: [D loss: 0.084637, acc: 0.976562]  [G loss: 5.836070, acc: 0.031250]\n",
            "4471: [D loss: 0.175841, acc: 0.960938]  [G loss: 7.058599, acc: 0.000000]\n",
            "4472: [D loss: 0.006131, acc: 1.000000]  [G loss: 7.340143, acc: 0.000000]\n",
            "4473: [D loss: 0.013571, acc: 0.992188]  [G loss: 8.347944, acc: 0.000000]\n",
            "4474: [D loss: 0.035940, acc: 0.984375]  [G loss: 7.962699, acc: 0.000000]\n",
            "4475: [D loss: 0.028716, acc: 0.984375]  [G loss: 6.817933, acc: 0.031250]\n",
            "4476: [D loss: 0.032927, acc: 0.984375]  [G loss: 5.363188, acc: 0.062500]\n",
            "4477: [D loss: 0.092978, acc: 0.984375]  [G loss: 4.001720, acc: 0.187500]\n",
            "4478: [D loss: 0.139231, acc: 0.976562]  [G loss: 4.751054, acc: 0.171875]\n",
            "4479: [D loss: 0.068534, acc: 0.992188]  [G loss: 5.813205, acc: 0.062500]\n",
            "4480: [D loss: 0.123495, acc: 0.984375]  [G loss: 6.953018, acc: 0.046875]\n",
            "4481: [D loss: 0.026429, acc: 0.992188]  [G loss: 7.506698, acc: 0.015625]\n",
            "4482: [D loss: 0.009991, acc: 0.992188]  [G loss: 7.203709, acc: 0.015625]\n",
            "4483: [D loss: 0.057040, acc: 0.976562]  [G loss: 5.994475, acc: 0.062500]\n",
            "4484: [D loss: 0.117887, acc: 0.976562]  [G loss: 5.338588, acc: 0.171875]\n",
            "4485: [D loss: 0.063172, acc: 0.984375]  [G loss: 3.735533, acc: 0.281250]\n",
            "4486: [D loss: 0.080011, acc: 0.968750]  [G loss: 4.095996, acc: 0.203125]\n",
            "4487: [D loss: 0.037586, acc: 0.984375]  [G loss: 4.374791, acc: 0.203125]\n",
            "4488: [D loss: 0.029830, acc: 0.992188]  [G loss: 5.380548, acc: 0.125000]\n",
            "4489: [D loss: 0.058181, acc: 0.992188]  [G loss: 6.042848, acc: 0.078125]\n",
            "4490: [D loss: 0.009841, acc: 0.992188]  [G loss: 6.592231, acc: 0.031250]\n",
            "4491: [D loss: 0.029591, acc: 0.992188]  [G loss: 6.713858, acc: 0.062500]\n",
            "4492: [D loss: 0.073733, acc: 0.968750]  [G loss: 5.042771, acc: 0.078125]\n",
            "4493: [D loss: 0.043078, acc: 0.992188]  [G loss: 5.024970, acc: 0.218750]\n",
            "4494: [D loss: 0.030071, acc: 0.984375]  [G loss: 3.719278, acc: 0.328125]\n",
            "4495: [D loss: 0.012882, acc: 0.992188]  [G loss: 3.568566, acc: 0.250000]\n",
            "4496: [D loss: 0.017035, acc: 0.992188]  [G loss: 3.165381, acc: 0.265625]\n",
            "4497: [D loss: 0.033405, acc: 0.984375]  [G loss: 2.538251, acc: 0.359375]\n",
            "4498: [D loss: 0.020352, acc: 0.992188]  [G loss: 2.841974, acc: 0.296875]\n",
            "4499: [D loss: 0.039810, acc: 0.984375]  [G loss: 2.866027, acc: 0.328125]\n",
            "4500: [D loss: 0.072743, acc: 0.953125]  [G loss: 3.738374, acc: 0.296875]\n",
            "4501: [D loss: 0.091350, acc: 0.976562]  [G loss: 3.857596, acc: 0.250000]\n",
            "4502: [D loss: 0.022463, acc: 0.992188]  [G loss: 4.259194, acc: 0.234375]\n",
            "4503: [D loss: 0.055268, acc: 0.992188]  [G loss: 3.259492, acc: 0.328125]\n",
            "4504: [D loss: 0.060696, acc: 0.992188]  [G loss: 3.000835, acc: 0.406250]\n",
            "4505: [D loss: 0.073066, acc: 0.976562]  [G loss: 2.209095, acc: 0.468750]\n",
            "4506: [D loss: 0.013159, acc: 1.000000]  [G loss: 1.268606, acc: 0.625000]\n",
            "4507: [D loss: 0.035101, acc: 0.984375]  [G loss: 1.345348, acc: 0.609375]\n",
            "4508: [D loss: 0.035049, acc: 0.992188]  [G loss: 1.273899, acc: 0.625000]\n",
            "4509: [D loss: 0.034348, acc: 0.992188]  [G loss: 1.917623, acc: 0.515625]\n",
            "4510: [D loss: 0.013113, acc: 0.992188]  [G loss: 2.877230, acc: 0.484375]\n",
            "4511: [D loss: 0.063355, acc: 0.992188]  [G loss: 3.057468, acc: 0.453125]\n",
            "4512: [D loss: 0.036574, acc: 0.984375]  [G loss: 2.872096, acc: 0.453125]\n",
            "4513: [D loss: 0.006251, acc: 1.000000]  [G loss: 2.989409, acc: 0.453125]\n",
            "4514: [D loss: 0.007747, acc: 0.992188]  [G loss: 2.439216, acc: 0.500000]\n",
            "4515: [D loss: 0.017476, acc: 0.984375]  [G loss: 2.303380, acc: 0.546875]\n",
            "4516: [D loss: 0.023560, acc: 0.992188]  [G loss: 1.685250, acc: 0.671875]\n",
            "4517: [D loss: 0.039850, acc: 0.976562]  [G loss: 1.502630, acc: 0.687500]\n",
            "4518: [D loss: 0.056401, acc: 0.976562]  [G loss: 1.239565, acc: 0.734375]\n",
            "4519: [D loss: 0.052199, acc: 0.992188]  [G loss: 1.986473, acc: 0.640625]\n",
            "4520: [D loss: 0.021232, acc: 0.992188]  [G loss: 1.691288, acc: 0.796875]\n",
            "4521: [D loss: 0.045534, acc: 0.968750]  [G loss: 1.859144, acc: 0.718750]\n",
            "4522: [D loss: 0.023967, acc: 0.992188]  [G loss: 2.258650, acc: 0.593750]\n",
            "4523: [D loss: 0.016043, acc: 0.992188]  [G loss: 2.662150, acc: 0.562500]\n",
            "4524: [D loss: 0.025800, acc: 0.992188]  [G loss: 2.956341, acc: 0.468750]\n",
            "4525: [D loss: 0.049618, acc: 0.992188]  [G loss: 2.933955, acc: 0.468750]\n",
            "4526: [D loss: 0.045125, acc: 0.992188]  [G loss: 2.955716, acc: 0.515625]\n",
            "4527: [D loss: 0.001052, acc: 1.000000]  [G loss: 2.750729, acc: 0.468750]\n",
            "4528: [D loss: 0.087359, acc: 0.968750]  [G loss: 2.180651, acc: 0.500000]\n",
            "4529: [D loss: 0.125107, acc: 0.960938]  [G loss: 1.659862, acc: 0.515625]\n",
            "4530: [D loss: 0.138538, acc: 0.976562]  [G loss: 1.871090, acc: 0.562500]\n",
            "4531: [D loss: 0.028272, acc: 0.976562]  [G loss: 2.387090, acc: 0.437500]\n",
            "4532: [D loss: 0.093127, acc: 0.976562]  [G loss: 2.619692, acc: 0.437500]\n",
            "4533: [D loss: 0.092394, acc: 0.984375]  [G loss: 3.538115, acc: 0.218750]\n",
            "4534: [D loss: 0.078716, acc: 0.984375]  [G loss: 3.284585, acc: 0.312500]\n",
            "4535: [D loss: 0.052243, acc: 0.984375]  [G loss: 2.578606, acc: 0.484375]\n",
            "4536: [D loss: 0.060507, acc: 0.976562]  [G loss: 2.830261, acc: 0.453125]\n",
            "4537: [D loss: 0.100078, acc: 0.968750]  [G loss: 2.783869, acc: 0.453125]\n",
            "4538: [D loss: 0.065608, acc: 0.960938]  [G loss: 3.327070, acc: 0.359375]\n",
            "4539: [D loss: 0.026647, acc: 0.984375]  [G loss: 2.925327, acc: 0.281250]\n",
            "4540: [D loss: 0.041216, acc: 0.976562]  [G loss: 2.944738, acc: 0.296875]\n",
            "4541: [D loss: 0.107577, acc: 0.984375]  [G loss: 2.794996, acc: 0.328125]\n",
            "4542: [D loss: 0.007302, acc: 1.000000]  [G loss: 2.875880, acc: 0.218750]\n",
            "4543: [D loss: 0.044319, acc: 0.976562]  [G loss: 2.780056, acc: 0.359375]\n",
            "4544: [D loss: 0.029475, acc: 0.992188]  [G loss: 3.611552, acc: 0.187500]\n",
            "4545: [D loss: 0.109972, acc: 0.968750]  [G loss: 2.799375, acc: 0.312500]\n",
            "4546: [D loss: 0.124397, acc: 0.960938]  [G loss: 1.964437, acc: 0.468750]\n",
            "4547: [D loss: 0.041539, acc: 0.976562]  [G loss: 3.427304, acc: 0.218750]\n",
            "4548: [D loss: 0.056784, acc: 0.976562]  [G loss: 4.298288, acc: 0.078125]\n",
            "4549: [D loss: 0.213978, acc: 0.945312]  [G loss: 4.045907, acc: 0.218750]\n",
            "4550: [D loss: 0.185620, acc: 0.945312]  [G loss: 2.846608, acc: 0.437500]\n",
            "4551: [D loss: 0.081791, acc: 0.960938]  [G loss: 3.136869, acc: 0.296875]\n",
            "4552: [D loss: 0.104588, acc: 0.968750]  [G loss: 2.543000, acc: 0.328125]\n",
            "4553: [D loss: 0.084948, acc: 0.968750]  [G loss: 3.377519, acc: 0.187500]\n",
            "4554: [D loss: 0.054458, acc: 0.984375]  [G loss: 3.382002, acc: 0.250000]\n",
            "4555: [D loss: 0.079446, acc: 0.976562]  [G loss: 3.049793, acc: 0.234375]\n",
            "4556: [D loss: 0.082514, acc: 0.960938]  [G loss: 3.083522, acc: 0.218750]\n",
            "4557: [D loss: 0.062071, acc: 0.968750]  [G loss: 3.513121, acc: 0.187500]\n",
            "4558: [D loss: 0.062239, acc: 0.960938]  [G loss: 4.145350, acc: 0.078125]\n",
            "4559: [D loss: 0.055610, acc: 0.976562]  [G loss: 3.021058, acc: 0.296875]\n",
            "4560: [D loss: 0.048159, acc: 0.984375]  [G loss: 4.156372, acc: 0.125000]\n",
            "4561: [D loss: 0.054831, acc: 0.968750]  [G loss: 3.779489, acc: 0.093750]\n",
            "4562: [D loss: 0.118712, acc: 0.960938]  [G loss: 3.029563, acc: 0.203125]\n",
            "4563: [D loss: 0.104234, acc: 0.953125]  [G loss: 3.551116, acc: 0.156250]\n",
            "4564: [D loss: 0.111455, acc: 0.960938]  [G loss: 4.476542, acc: 0.140625]\n",
            "4565: [D loss: 0.066899, acc: 0.968750]  [G loss: 5.319561, acc: 0.062500]\n",
            "4566: [D loss: 0.119980, acc: 0.960938]  [G loss: 5.222385, acc: 0.078125]\n",
            "4567: [D loss: 0.113636, acc: 0.960938]  [G loss: 3.388068, acc: 0.093750]\n",
            "4568: [D loss: 0.100283, acc: 0.953125]  [G loss: 2.327962, acc: 0.296875]\n",
            "4569: [D loss: 0.105614, acc: 0.960938]  [G loss: 3.613166, acc: 0.156250]\n",
            "4570: [D loss: 0.063797, acc: 0.968750]  [G loss: 5.355302, acc: 0.015625]\n",
            "4571: [D loss: 0.025148, acc: 0.992188]  [G loss: 7.389964, acc: 0.000000]\n",
            "4572: [D loss: 0.087782, acc: 0.960938]  [G loss: 6.682418, acc: 0.000000]\n",
            "4573: [D loss: 0.305984, acc: 0.921875]  [G loss: 3.505376, acc: 0.187500]\n",
            "4574: [D loss: 0.150016, acc: 0.929688]  [G loss: 2.365388, acc: 0.343750]\n",
            "4575: [D loss: 0.359798, acc: 0.843750]  [G loss: 3.796856, acc: 0.187500]\n",
            "4576: [D loss: 0.052208, acc: 0.976562]  [G loss: 5.704640, acc: 0.031250]\n",
            "4577: [D loss: 0.241736, acc: 0.929688]  [G loss: 4.753268, acc: 0.093750]\n",
            "4578: [D loss: 0.087506, acc: 0.960938]  [G loss: 3.518287, acc: 0.187500]\n",
            "4579: [D loss: 0.087426, acc: 0.968750]  [G loss: 2.268458, acc: 0.359375]\n",
            "4580: [D loss: 0.162521, acc: 0.929688]  [G loss: 1.939796, acc: 0.484375]\n",
            "4581: [D loss: 0.043122, acc: 0.984375]  [G loss: 2.935359, acc: 0.296875]\n",
            "4582: [D loss: 0.150422, acc: 0.937500]  [G loss: 3.374887, acc: 0.109375]\n",
            "4583: [D loss: 0.031507, acc: 0.992188]  [G loss: 4.349361, acc: 0.125000]\n",
            "4584: [D loss: 0.049149, acc: 0.984375]  [G loss: 4.185367, acc: 0.078125]\n",
            "4585: [D loss: 0.082062, acc: 0.976562]  [G loss: 4.291493, acc: 0.046875]\n",
            "4586: [D loss: 0.222830, acc: 0.953125]  [G loss: 3.420943, acc: 0.140625]\n",
            "4587: [D loss: 0.069136, acc: 0.976562]  [G loss: 2.336248, acc: 0.218750]\n",
            "4588: [D loss: 0.072703, acc: 0.960938]  [G loss: 1.808916, acc: 0.359375]\n",
            "4589: [D loss: 0.126737, acc: 0.960938]  [G loss: 1.760008, acc: 0.453125]\n",
            "4590: [D loss: 0.085464, acc: 0.968750]  [G loss: 1.957649, acc: 0.359375]\n",
            "4591: [D loss: 0.084275, acc: 0.976562]  [G loss: 1.994893, acc: 0.375000]\n",
            "4592: [D loss: 0.025518, acc: 0.992188]  [G loss: 3.357917, acc: 0.203125]\n",
            "4593: [D loss: 0.051931, acc: 0.992188]  [G loss: 3.779289, acc: 0.187500]\n",
            "4594: [D loss: 0.103644, acc: 0.968750]  [G loss: 3.905555, acc: 0.187500]\n",
            "4595: [D loss: 0.089886, acc: 0.960938]  [G loss: 3.960976, acc: 0.250000]\n",
            "4596: [D loss: 0.096363, acc: 0.968750]  [G loss: 3.186254, acc: 0.281250]\n",
            "4597: [D loss: 0.024002, acc: 0.992188]  [G loss: 2.971975, acc: 0.328125]\n",
            "4598: [D loss: 0.062091, acc: 0.984375]  [G loss: 2.234756, acc: 0.515625]\n",
            "4599: [D loss: 0.068135, acc: 0.984375]  [G loss: 2.112192, acc: 0.437500]\n",
            "4600: [D loss: 0.134523, acc: 0.968750]  [G loss: 2.375569, acc: 0.437500]\n",
            "4601: [D loss: 0.084848, acc: 0.968750]  [G loss: 2.969594, acc: 0.312500]\n",
            "4602: [D loss: 0.045997, acc: 0.968750]  [G loss: 2.859706, acc: 0.343750]\n",
            "4603: [D loss: 0.034379, acc: 0.984375]  [G loss: 3.184634, acc: 0.250000]\n",
            "4604: [D loss: 0.050947, acc: 0.968750]  [G loss: 3.267658, acc: 0.296875]\n",
            "4605: [D loss: 0.133618, acc: 0.960938]  [G loss: 2.764527, acc: 0.406250]\n",
            "4606: [D loss: 0.020547, acc: 0.992188]  [G loss: 2.315061, acc: 0.437500]\n",
            "4607: [D loss: 0.042561, acc: 0.992188]  [G loss: 2.538479, acc: 0.421875]\n",
            "4608: [D loss: 0.107152, acc: 0.976562]  [G loss: 2.132574, acc: 0.375000]\n",
            "4609: [D loss: 0.033186, acc: 0.992188]  [G loss: 2.295767, acc: 0.515625]\n",
            "4610: [D loss: 0.031484, acc: 0.992188]  [G loss: 2.843963, acc: 0.375000]\n",
            "4611: [D loss: 0.029535, acc: 0.984375]  [G loss: 3.189756, acc: 0.250000]\n",
            "4612: [D loss: 0.034287, acc: 0.984375]  [G loss: 3.944008, acc: 0.187500]\n",
            "4613: [D loss: 0.018746, acc: 0.984375]  [G loss: 3.421664, acc: 0.250000]\n",
            "4614: [D loss: 0.057508, acc: 0.976562]  [G loss: 3.542232, acc: 0.281250]\n",
            "4615: [D loss: 0.041056, acc: 0.976562]  [G loss: 2.866690, acc: 0.343750]\n",
            "4616: [D loss: 0.035687, acc: 0.984375]  [G loss: 2.350174, acc: 0.546875]\n",
            "4617: [D loss: 0.055539, acc: 0.976562]  [G loss: 2.100657, acc: 0.500000]\n",
            "4618: [D loss: 0.015202, acc: 1.000000]  [G loss: 2.450536, acc: 0.421875]\n",
            "4619: [D loss: 0.056646, acc: 0.984375]  [G loss: 2.768252, acc: 0.468750]\n",
            "4620: [D loss: 0.067260, acc: 0.953125]  [G loss: 3.209623, acc: 0.421875]\n",
            "4621: [D loss: 0.055720, acc: 0.992188]  [G loss: 3.381516, acc: 0.250000]\n",
            "4622: [D loss: 0.062055, acc: 0.968750]  [G loss: 4.526692, acc: 0.171875]\n",
            "4623: [D loss: 0.076331, acc: 0.976562]  [G loss: 5.307909, acc: 0.156250]\n",
            "4624: [D loss: 0.075121, acc: 0.968750]  [G loss: 4.056595, acc: 0.203125]\n",
            "4625: [D loss: 0.173263, acc: 0.945312]  [G loss: 2.922147, acc: 0.359375]\n",
            "4626: [D loss: 0.051076, acc: 0.968750]  [G loss: 2.565506, acc: 0.421875]\n",
            "4627: [D loss: 0.122634, acc: 0.953125]  [G loss: 2.243084, acc: 0.437500]\n",
            "4628: [D loss: 0.095359, acc: 0.960938]  [G loss: 2.564045, acc: 0.390625]\n",
            "4629: [D loss: 0.189054, acc: 0.945312]  [G loss: 3.885525, acc: 0.250000]\n",
            "4630: [D loss: 0.052009, acc: 0.976562]  [G loss: 4.483465, acc: 0.250000]\n",
            "4631: [D loss: 0.014263, acc: 0.992188]  [G loss: 5.090261, acc: 0.187500]\n",
            "4632: [D loss: 0.150283, acc: 0.953125]  [G loss: 4.998045, acc: 0.140625]\n",
            "4633: [D loss: 0.122458, acc: 0.953125]  [G loss: 4.748644, acc: 0.265625]\n",
            "4634: [D loss: 0.161632, acc: 0.945312]  [G loss: 4.059371, acc: 0.218750]\n",
            "4635: [D loss: 0.047983, acc: 0.984375]  [G loss: 4.818877, acc: 0.125000]\n",
            "4636: [D loss: 0.138579, acc: 0.960938]  [G loss: 4.410745, acc: 0.093750]\n",
            "4637: [D loss: 0.070269, acc: 0.976562]  [G loss: 3.921073, acc: 0.203125]\n",
            "4638: [D loss: 0.073236, acc: 0.984375]  [G loss: 3.532423, acc: 0.203125]\n",
            "4639: [D loss: 0.089065, acc: 0.976562]  [G loss: 4.614681, acc: 0.109375]\n",
            "4640: [D loss: 0.068087, acc: 0.984375]  [G loss: 6.020026, acc: 0.015625]\n",
            "4641: [D loss: 0.049793, acc: 0.976562]  [G loss: 5.735745, acc: 0.046875]\n",
            "4642: [D loss: 0.044510, acc: 0.976562]  [G loss: 5.387779, acc: 0.046875]\n",
            "4643: [D loss: 0.132057, acc: 0.953125]  [G loss: 4.709571, acc: 0.078125]\n",
            "4644: [D loss: 0.045889, acc: 0.976562]  [G loss: 4.958529, acc: 0.093750]\n",
            "4645: [D loss: 0.221064, acc: 0.898438]  [G loss: 5.388928, acc: 0.078125]\n",
            "4646: [D loss: 0.108956, acc: 0.953125]  [G loss: 6.231738, acc: 0.031250]\n",
            "4647: [D loss: 0.093090, acc: 0.984375]  [G loss: 5.772438, acc: 0.062500]\n",
            "4648: [D loss: 0.144077, acc: 0.953125]  [G loss: 4.253272, acc: 0.218750]\n",
            "4649: [D loss: 0.044478, acc: 0.992188]  [G loss: 3.165692, acc: 0.359375]\n",
            "4650: [D loss: 0.126712, acc: 0.945312]  [G loss: 2.475134, acc: 0.453125]\n",
            "4651: [D loss: 0.135570, acc: 0.953125]  [G loss: 2.897563, acc: 0.343750]\n",
            "4652: [D loss: 0.047745, acc: 0.976562]  [G loss: 2.827929, acc: 0.359375]\n",
            "4653: [D loss: 0.037115, acc: 0.984375]  [G loss: 4.073905, acc: 0.171875]\n",
            "4654: [D loss: 0.031392, acc: 0.984375]  [G loss: 4.494020, acc: 0.109375]\n",
            "4655: [D loss: 0.109098, acc: 0.976562]  [G loss: 4.375932, acc: 0.125000]\n",
            "4656: [D loss: 0.061438, acc: 0.968750]  [G loss: 3.541782, acc: 0.203125]\n",
            "4657: [D loss: 0.057658, acc: 0.976562]  [G loss: 3.572924, acc: 0.234375]\n",
            "4658: [D loss: 0.032224, acc: 0.984375]  [G loss: 1.987626, acc: 0.562500]\n",
            "4659: [D loss: 0.101038, acc: 0.984375]  [G loss: 2.178987, acc: 0.515625]\n",
            "4660: [D loss: 0.131477, acc: 0.960938]  [G loss: 1.915297, acc: 0.515625]\n",
            "4661: [D loss: 0.116181, acc: 0.953125]  [G loss: 2.893950, acc: 0.375000]\n",
            "4662: [D loss: 0.125672, acc: 0.960938]  [G loss: 4.238719, acc: 0.218750]\n",
            "4663: [D loss: 0.108348, acc: 0.953125]  [G loss: 4.562582, acc: 0.203125]\n",
            "4664: [D loss: 0.172454, acc: 0.937500]  [G loss: 4.988108, acc: 0.156250]\n",
            "4665: [D loss: 0.031366, acc: 0.984375]  [G loss: 4.874382, acc: 0.140625]\n",
            "4666: [D loss: 0.189256, acc: 0.945312]  [G loss: 3.980129, acc: 0.265625]\n",
            "4667: [D loss: 0.068957, acc: 0.968750]  [G loss: 2.983022, acc: 0.296875]\n",
            "4668: [D loss: 0.094404, acc: 0.960938]  [G loss: 2.598757, acc: 0.515625]\n",
            "4669: [D loss: 0.112429, acc: 0.937500]  [G loss: 2.993727, acc: 0.312500]\n",
            "4670: [D loss: 0.131584, acc: 0.968750]  [G loss: 4.558768, acc: 0.203125]\n",
            "4671: [D loss: 0.111825, acc: 0.968750]  [G loss: 5.762720, acc: 0.078125]\n",
            "4672: [D loss: 0.029861, acc: 0.984375]  [G loss: 7.433728, acc: 0.015625]\n",
            "4673: [D loss: 0.041490, acc: 0.984375]  [G loss: 7.625505, acc: 0.000000]\n",
            "4674: [D loss: 0.201222, acc: 0.953125]  [G loss: 7.351406, acc: 0.000000]\n",
            "4675: [D loss: 0.053367, acc: 0.976562]  [G loss: 5.732690, acc: 0.062500]\n",
            "4676: [D loss: 0.054370, acc: 0.984375]  [G loss: 4.531972, acc: 0.140625]\n",
            "4677: [D loss: 0.063048, acc: 0.976562]  [G loss: 4.133354, acc: 0.125000]\n",
            "4678: [D loss: 0.060927, acc: 0.976562]  [G loss: 3.277110, acc: 0.250000]\n",
            "4679: [D loss: 0.142206, acc: 0.953125]  [G loss: 4.243847, acc: 0.156250]\n",
            "4680: [D loss: 0.025723, acc: 0.992188]  [G loss: 3.882723, acc: 0.171875]\n",
            "4681: [D loss: 0.040679, acc: 0.984375]  [G loss: 4.206964, acc: 0.156250]\n",
            "4682: [D loss: 0.011669, acc: 1.000000]  [G loss: 4.462609, acc: 0.078125]\n",
            "4683: [D loss: 0.122597, acc: 0.976562]  [G loss: 3.775402, acc: 0.296875]\n",
            "4684: [D loss: 0.033651, acc: 0.976562]  [G loss: 4.231558, acc: 0.234375]\n",
            "4685: [D loss: 0.059185, acc: 0.968750]  [G loss: 3.666350, acc: 0.375000]\n",
            "4686: [D loss: 0.145208, acc: 0.945312]  [G loss: 4.235250, acc: 0.250000]\n",
            "4687: [D loss: 0.045624, acc: 0.984375]  [G loss: 4.369035, acc: 0.234375]\n",
            "4688: [D loss: 0.062951, acc: 0.976562]  [G loss: 4.667109, acc: 0.171875]\n",
            "4689: [D loss: 0.099062, acc: 0.968750]  [G loss: 4.057333, acc: 0.281250]\n",
            "4690: [D loss: 0.069586, acc: 0.976562]  [G loss: 4.009886, acc: 0.218750]\n",
            "4691: [D loss: 0.092195, acc: 0.968750]  [G loss: 3.879955, acc: 0.093750]\n",
            "4692: [D loss: 0.050485, acc: 0.992188]  [G loss: 4.889554, acc: 0.187500]\n",
            "4693: [D loss: 0.027052, acc: 0.992188]  [G loss: 5.365470, acc: 0.093750]\n",
            "4694: [D loss: 0.033109, acc: 0.992188]  [G loss: 5.680463, acc: 0.062500]\n",
            "4695: [D loss: 0.015710, acc: 1.000000]  [G loss: 6.103273, acc: 0.015625]\n",
            "4696: [D loss: 0.072710, acc: 0.953125]  [G loss: 6.145533, acc: 0.015625]\n",
            "4697: [D loss: 0.082314, acc: 0.968750]  [G loss: 6.126840, acc: 0.015625]\n",
            "4698: [D loss: 0.033594, acc: 0.984375]  [G loss: 4.942510, acc: 0.078125]\n",
            "4699: [D loss: 0.192816, acc: 0.937500]  [G loss: 4.513637, acc: 0.062500]\n",
            "4700: [D loss: 0.074272, acc: 0.960938]  [G loss: 5.799150, acc: 0.046875]\n",
            "4701: [D loss: 0.090250, acc: 0.976562]  [G loss: 4.551136, acc: 0.156250]\n",
            "4702: [D loss: 0.079252, acc: 0.960938]  [G loss: 3.680749, acc: 0.140625]\n",
            "4703: [D loss: 0.028152, acc: 0.984375]  [G loss: 3.132235, acc: 0.281250]\n",
            "4704: [D loss: 0.079499, acc: 0.960938]  [G loss: 4.593088, acc: 0.140625]\n",
            "4705: [D loss: 0.070446, acc: 0.968750]  [G loss: 4.724191, acc: 0.109375]\n",
            "4706: [D loss: 0.024092, acc: 0.992188]  [G loss: 5.263022, acc: 0.062500]\n",
            "4707: [D loss: 0.049148, acc: 0.984375]  [G loss: 5.139750, acc: 0.109375]\n",
            "4708: [D loss: 0.018482, acc: 0.992188]  [G loss: 5.417485, acc: 0.046875]\n",
            "4709: [D loss: 0.139576, acc: 0.976562]  [G loss: 4.602816, acc: 0.109375]\n",
            "4710: [D loss: 0.029976, acc: 0.984375]  [G loss: 4.641963, acc: 0.125000]\n",
            "4711: [D loss: 0.048413, acc: 0.984375]  [G loss: 3.663492, acc: 0.171875]\n",
            "4712: [D loss: 0.071100, acc: 0.976562]  [G loss: 3.788153, acc: 0.125000]\n",
            "4713: [D loss: 0.112778, acc: 0.984375]  [G loss: 4.598996, acc: 0.109375]\n",
            "4714: [D loss: 0.093632, acc: 0.984375]  [G loss: 5.953010, acc: 0.031250]\n",
            "4715: [D loss: 0.104379, acc: 0.960938]  [G loss: 5.314159, acc: 0.062500]\n",
            "4716: [D loss: 0.097398, acc: 0.968750]  [G loss: 5.144193, acc: 0.000000]\n",
            "4717: [D loss: 0.069872, acc: 0.960938]  [G loss: 4.469937, acc: 0.125000]\n",
            "4718: [D loss: 0.054444, acc: 0.976562]  [G loss: 4.530582, acc: 0.109375]\n",
            "4719: [D loss: 0.181087, acc: 0.945312]  [G loss: 4.806935, acc: 0.078125]\n",
            "4720: [D loss: 0.065529, acc: 0.976562]  [G loss: 4.751709, acc: 0.156250]\n",
            "4721: [D loss: 0.040633, acc: 0.992188]  [G loss: 4.834692, acc: 0.140625]\n",
            "4722: [D loss: 0.074813, acc: 0.968750]  [G loss: 4.311207, acc: 0.187500]\n",
            "4723: [D loss: 0.130309, acc: 0.953125]  [G loss: 2.997079, acc: 0.375000]\n",
            "4724: [D loss: 0.060653, acc: 0.976562]  [G loss: 2.937747, acc: 0.312500]\n",
            "4725: [D loss: 0.167079, acc: 0.960938]  [G loss: 3.151052, acc: 0.296875]\n",
            "4726: [D loss: 0.093491, acc: 0.968750]  [G loss: 4.290455, acc: 0.078125]\n",
            "4727: [D loss: 0.018728, acc: 0.992188]  [G loss: 4.502931, acc: 0.156250]\n",
            "4728: [D loss: 0.077603, acc: 0.976562]  [G loss: 5.060667, acc: 0.046875]\n",
            "4729: [D loss: 0.041922, acc: 0.984375]  [G loss: 4.477582, acc: 0.093750]\n",
            "4730: [D loss: 0.022496, acc: 1.000000]  [G loss: 3.773767, acc: 0.140625]\n",
            "4731: [D loss: 0.061098, acc: 0.984375]  [G loss: 3.075810, acc: 0.312500]\n",
            "4732: [D loss: 0.124933, acc: 0.953125]  [G loss: 3.980577, acc: 0.171875]\n",
            "4733: [D loss: 0.079941, acc: 0.968750]  [G loss: 4.697352, acc: 0.062500]\n",
            "4734: [D loss: 0.112990, acc: 0.968750]  [G loss: 5.811320, acc: 0.062500]\n",
            "4735: [D loss: 0.073547, acc: 0.968750]  [G loss: 6.044354, acc: 0.015625]\n",
            "4736: [D loss: 0.062488, acc: 0.976562]  [G loss: 6.266056, acc: 0.000000]\n",
            "4737: [D loss: 0.096793, acc: 0.976562]  [G loss: 4.524758, acc: 0.109375]\n",
            "4738: [D loss: 0.083768, acc: 0.960938]  [G loss: 4.544983, acc: 0.109375]\n",
            "4739: [D loss: 0.111005, acc: 0.960938]  [G loss: 3.945179, acc: 0.125000]\n",
            "4740: [D loss: 0.061493, acc: 0.984375]  [G loss: 4.873758, acc: 0.078125]\n",
            "4741: [D loss: 0.054018, acc: 0.968750]  [G loss: 4.492392, acc: 0.062500]\n",
            "4742: [D loss: 0.038779, acc: 0.984375]  [G loss: 4.895078, acc: 0.093750]\n",
            "4743: [D loss: 0.212147, acc: 0.953125]  [G loss: 4.179901, acc: 0.062500]\n",
            "4744: [D loss: 0.097311, acc: 0.960938]  [G loss: 4.163401, acc: 0.062500]\n",
            "4745: [D loss: 0.054712, acc: 0.968750]  [G loss: 3.055070, acc: 0.281250]\n",
            "4746: [D loss: 0.052211, acc: 0.992188]  [G loss: 2.158673, acc: 0.390625]\n",
            "4747: [D loss: 0.132532, acc: 0.929688]  [G loss: 2.947303, acc: 0.250000]\n",
            "4748: [D loss: 0.027698, acc: 0.992188]  [G loss: 4.369506, acc: 0.078125]\n",
            "4749: [D loss: 0.061323, acc: 0.976562]  [G loss: 4.439691, acc: 0.093750]\n",
            "4750: [D loss: 0.053920, acc: 0.960938]  [G loss: 5.750411, acc: 0.046875]\n",
            "4751: [D loss: 0.158120, acc: 0.960938]  [G loss: 4.997394, acc: 0.093750]\n",
            "4752: [D loss: 0.096565, acc: 0.960938]  [G loss: 4.745912, acc: 0.062500]\n",
            "4753: [D loss: 0.092633, acc: 0.968750]  [G loss: 3.852785, acc: 0.156250]\n",
            "4754: [D loss: 0.051946, acc: 0.984375]  [G loss: 2.949053, acc: 0.234375]\n",
            "4755: [D loss: 0.066902, acc: 0.984375]  [G loss: 3.557726, acc: 0.203125]\n",
            "4756: [D loss: 0.062699, acc: 0.984375]  [G loss: 3.693011, acc: 0.171875]\n",
            "4757: [D loss: 0.095728, acc: 0.960938]  [G loss: 4.171386, acc: 0.140625]\n",
            "4758: [D loss: 0.080611, acc: 0.976562]  [G loss: 4.253350, acc: 0.093750]\n",
            "4759: [D loss: 0.050395, acc: 0.976562]  [G loss: 4.490074, acc: 0.031250]\n",
            "4760: [D loss: 0.094683, acc: 0.968750]  [G loss: 4.157016, acc: 0.078125]\n",
            "4761: [D loss: 0.096699, acc: 0.984375]  [G loss: 3.454748, acc: 0.156250]\n",
            "4762: [D loss: 0.089947, acc: 0.960938]  [G loss: 3.297557, acc: 0.093750]\n",
            "4763: [D loss: 0.157422, acc: 0.945312]  [G loss: 4.608688, acc: 0.046875]\n",
            "4764: [D loss: 0.057947, acc: 0.976562]  [G loss: 6.570042, acc: 0.015625]\n",
            "4765: [D loss: 0.096354, acc: 0.968750]  [G loss: 7.421997, acc: 0.000000]\n",
            "4766: [D loss: 0.174898, acc: 0.968750]  [G loss: 6.944099, acc: 0.015625]\n",
            "4767: [D loss: 0.099557, acc: 0.960938]  [G loss: 6.342160, acc: 0.046875]\n",
            "4768: [D loss: 0.104418, acc: 0.945312]  [G loss: 6.196119, acc: 0.000000]\n",
            "4769: [D loss: 0.053110, acc: 0.992188]  [G loss: 4.365674, acc: 0.046875]\n",
            "4770: [D loss: 0.039949, acc: 0.984375]  [G loss: 3.752020, acc: 0.046875]\n",
            "4771: [D loss: 0.121531, acc: 0.960938]  [G loss: 4.649752, acc: 0.031250]\n",
            "4772: [D loss: 0.075513, acc: 0.976562]  [G loss: 5.119054, acc: 0.000000]\n",
            "4773: [D loss: 0.108486, acc: 0.984375]  [G loss: 5.380223, acc: 0.015625]\n",
            "4774: [D loss: 0.084785, acc: 0.976562]  [G loss: 6.374866, acc: 0.015625]\n",
            "4775: [D loss: 0.044264, acc: 0.968750]  [G loss: 6.217730, acc: 0.000000]\n",
            "4776: [D loss: 0.062110, acc: 0.976562]  [G loss: 6.273991, acc: 0.015625]\n",
            "4777: [D loss: 0.086433, acc: 0.976562]  [G loss: 5.887156, acc: 0.031250]\n",
            "4778: [D loss: 0.025714, acc: 0.984375]  [G loss: 5.461856, acc: 0.062500]\n",
            "4779: [D loss: 0.065600, acc: 0.960938]  [G loss: 6.719722, acc: 0.031250]\n",
            "4780: [D loss: 0.068884, acc: 0.968750]  [G loss: 6.960059, acc: 0.000000]\n",
            "4781: [D loss: 0.059542, acc: 0.992188]  [G loss: 7.285627, acc: 0.000000]\n",
            "4782: [D loss: 0.010509, acc: 1.000000]  [G loss: 7.860363, acc: 0.000000]\n",
            "4783: [D loss: 0.020462, acc: 0.992188]  [G loss: 8.076901, acc: 0.000000]\n",
            "4784: [D loss: 0.019337, acc: 0.992188]  [G loss: 8.624235, acc: 0.000000]\n",
            "4785: [D loss: 0.050935, acc: 0.992188]  [G loss: 8.628404, acc: 0.000000]\n",
            "4786: [D loss: 0.073448, acc: 0.992188]  [G loss: 8.606514, acc: 0.000000]\n",
            "4787: [D loss: 0.070383, acc: 0.984375]  [G loss: 7.277600, acc: 0.000000]\n",
            "4788: [D loss: 0.019029, acc: 0.992188]  [G loss: 6.486575, acc: 0.000000]\n",
            "4789: [D loss: 0.086828, acc: 0.960938]  [G loss: 6.801329, acc: 0.000000]\n",
            "4790: [D loss: 0.032821, acc: 0.984375]  [G loss: 6.449501, acc: 0.046875]\n",
            "4791: [D loss: 0.021824, acc: 0.992188]  [G loss: 6.452744, acc: 0.062500]\n",
            "4792: [D loss: 0.122845, acc: 0.960938]  [G loss: 5.457054, acc: 0.078125]\n",
            "4793: [D loss: 0.022540, acc: 0.992188]  [G loss: 5.282309, acc: 0.140625]\n",
            "4794: [D loss: 0.052213, acc: 0.976562]  [G loss: 4.348453, acc: 0.187500]\n",
            "4795: [D loss: 0.010408, acc: 0.992188]  [G loss: 4.607627, acc: 0.203125]\n",
            "4796: [D loss: 0.004748, acc: 1.000000]  [G loss: 5.024695, acc: 0.187500]\n",
            "4797: [D loss: 0.131532, acc: 0.968750]  [G loss: 5.073116, acc: 0.140625]\n",
            "4798: [D loss: 0.010441, acc: 1.000000]  [G loss: 5.943289, acc: 0.171875]\n",
            "4799: [D loss: 0.033611, acc: 0.984375]  [G loss: 5.319118, acc: 0.218750]\n",
            "4800: [D loss: 0.114077, acc: 0.968750]  [G loss: 4.144545, acc: 0.218750]\n",
            "4801: [D loss: 0.027914, acc: 0.992188]  [G loss: 3.880038, acc: 0.296875]\n",
            "4802: [D loss: 0.026216, acc: 0.992188]  [G loss: 3.619186, acc: 0.359375]\n",
            "4803: [D loss: 0.023591, acc: 0.992188]  [G loss: 3.470981, acc: 0.437500]\n",
            "4804: [D loss: 0.049632, acc: 0.984375]  [G loss: 2.642863, acc: 0.468750]\n",
            "4805: [D loss: 0.025470, acc: 0.984375]  [G loss: 1.902754, acc: 0.640625]\n",
            "4806: [D loss: 0.048797, acc: 0.984375]  [G loss: 2.214478, acc: 0.531250]\n",
            "4807: [D loss: 0.039688, acc: 0.984375]  [G loss: 3.528716, acc: 0.390625]\n",
            "4808: [D loss: 0.007727, acc: 1.000000]  [G loss: 3.318021, acc: 0.437500]\n",
            "4809: [D loss: 0.002264, acc: 1.000000]  [G loss: 3.856575, acc: 0.406250]\n",
            "4810: [D loss: 0.010813, acc: 0.992188]  [G loss: 4.387437, acc: 0.343750]\n",
            "4811: [D loss: 0.002317, acc: 1.000000]  [G loss: 3.341829, acc: 0.421875]\n",
            "4812: [D loss: 0.015345, acc: 0.992188]  [G loss: 4.062651, acc: 0.390625]\n",
            "4813: [D loss: 0.018249, acc: 0.992188]  [G loss: 3.410752, acc: 0.437500]\n",
            "4814: [D loss: 0.027136, acc: 0.992188]  [G loss: 3.298153, acc: 0.531250]\n",
            "4815: [D loss: 0.015584, acc: 0.992188]  [G loss: 2.486133, acc: 0.640625]\n",
            "4816: [D loss: 0.006675, acc: 1.000000]  [G loss: 2.065499, acc: 0.671875]\n",
            "4817: [D loss: 0.020629, acc: 0.992188]  [G loss: 1.805106, acc: 0.718750]\n",
            "4818: [D loss: 0.015501, acc: 1.000000]  [G loss: 2.845181, acc: 0.578125]\n",
            "4819: [D loss: 0.013830, acc: 0.992188]  [G loss: 2.019100, acc: 0.609375]\n",
            "4820: [D loss: 0.018501, acc: 0.992188]  [G loss: 2.656391, acc: 0.546875]\n",
            "4821: [D loss: 0.003315, acc: 1.000000]  [G loss: 1.923008, acc: 0.593750]\n",
            "4822: [D loss: 0.006836, acc: 1.000000]  [G loss: 1.775579, acc: 0.640625]\n",
            "4823: [D loss: 0.002559, acc: 1.000000]  [G loss: 1.929853, acc: 0.546875]\n",
            "4824: [D loss: 0.002550, acc: 1.000000]  [G loss: 2.039001, acc: 0.531250]\n",
            "4825: [D loss: 0.001746, acc: 1.000000]  [G loss: 2.528854, acc: 0.484375]\n",
            "4826: [D loss: 0.000948, acc: 1.000000]  [G loss: 2.705877, acc: 0.515625]\n",
            "4827: [D loss: 0.008840, acc: 0.992188]  [G loss: 2.708281, acc: 0.546875]\n",
            "4828: [D loss: 0.032759, acc: 0.984375]  [G loss: 3.371339, acc: 0.406250]\n",
            "4829: [D loss: 0.020109, acc: 0.992188]  [G loss: 2.339053, acc: 0.578125]\n",
            "4830: [D loss: 0.072872, acc: 0.992188]  [G loss: 1.638126, acc: 0.734375]\n",
            "4831: [D loss: 0.075280, acc: 0.976562]  [G loss: 1.522480, acc: 0.781250]\n",
            "4832: [D loss: 0.046671, acc: 0.992188]  [G loss: 0.891535, acc: 0.859375]\n",
            "4833: [D loss: 0.243336, acc: 0.945312]  [G loss: 1.847630, acc: 0.609375]\n",
            "4834: [D loss: 0.037408, acc: 0.992188]  [G loss: 4.176791, acc: 0.312500]\n",
            "4835: [D loss: 0.109294, acc: 0.984375]  [G loss: 5.335653, acc: 0.203125]\n",
            "4836: [D loss: 0.087397, acc: 0.992188]  [G loss: 6.104959, acc: 0.171875]\n",
            "4837: [D loss: 0.365346, acc: 0.945312]  [G loss: 3.878826, acc: 0.281250]\n",
            "4838: [D loss: 0.391376, acc: 0.953125]  [G loss: 0.998716, acc: 0.671875]\n",
            "4839: [D loss: 0.135974, acc: 0.976562]  [G loss: 0.216028, acc: 0.921875]\n",
            "4840: [D loss: 0.082900, acc: 0.968750]  [G loss: 0.035133, acc: 0.984375]\n",
            "4841: [D loss: 0.131147, acc: 0.945312]  [G loss: 0.096226, acc: 0.937500]\n",
            "4842: [D loss: 0.038420, acc: 0.984375]  [G loss: 0.466118, acc: 0.812500]\n",
            "4843: [D loss: 0.011028, acc: 0.992188]  [G loss: 1.389470, acc: 0.625000]\n",
            "4844: [D loss: 0.027382, acc: 0.984375]  [G loss: 1.978369, acc: 0.578125]\n",
            "4845: [D loss: 0.000579, acc: 1.000000]  [G loss: 2.676267, acc: 0.531250]\n",
            "4846: [D loss: 0.006475, acc: 1.000000]  [G loss: 3.158348, acc: 0.437500]\n",
            "4847: [D loss: 0.000963, acc: 1.000000]  [G loss: 3.491939, acc: 0.421875]\n",
            "4848: [D loss: 0.055771, acc: 0.984375]  [G loss: 3.367548, acc: 0.484375]\n",
            "4849: [D loss: 0.094033, acc: 0.968750]  [G loss: 2.204831, acc: 0.593750]\n",
            "4850: [D loss: 0.049402, acc: 0.984375]  [G loss: 1.626245, acc: 0.687500]\n",
            "4851: [D loss: 0.048059, acc: 0.976562]  [G loss: 1.547129, acc: 0.718750]\n",
            "4852: [D loss: 0.050292, acc: 0.976562]  [G loss: 1.289382, acc: 0.734375]\n",
            "4853: [D loss: 0.081649, acc: 0.960938]  [G loss: 2.007072, acc: 0.718750]\n",
            "4854: [D loss: 0.062467, acc: 0.976562]  [G loss: 3.476824, acc: 0.390625]\n",
            "4855: [D loss: 0.041314, acc: 0.976562]  [G loss: 4.197875, acc: 0.343750]\n",
            "4856: [D loss: 0.105342, acc: 0.945312]  [G loss: 3.833022, acc: 0.343750]\n",
            "4857: [D loss: 0.296328, acc: 0.953125]  [G loss: 2.367124, acc: 0.390625]\n",
            "4858: [D loss: 0.081401, acc: 0.960938]  [G loss: 1.381479, acc: 0.687500]\n",
            "4859: [D loss: 0.087787, acc: 0.960938]  [G loss: 0.900928, acc: 0.781250]\n",
            "4860: [D loss: 0.171502, acc: 0.945312]  [G loss: 1.531537, acc: 0.531250]\n",
            "4861: [D loss: 0.040529, acc: 0.976562]  [G loss: 2.976203, acc: 0.250000]\n",
            "4862: [D loss: 0.058304, acc: 0.984375]  [G loss: 4.319541, acc: 0.171875]\n",
            "4863: [D loss: 0.070063, acc: 0.960938]  [G loss: 3.878467, acc: 0.093750]\n",
            "4864: [D loss: 0.157341, acc: 0.953125]  [G loss: 2.893990, acc: 0.187500]\n",
            "4865: [D loss: 0.086718, acc: 0.976562]  [G loss: 1.322248, acc: 0.640625]\n",
            "4866: [D loss: 0.056005, acc: 0.984375]  [G loss: 1.548399, acc: 0.609375]\n",
            "4867: [D loss: 0.139683, acc: 0.937500]  [G loss: 1.615084, acc: 0.453125]\n",
            "4868: [D loss: 0.040212, acc: 0.984375]  [G loss: 3.233587, acc: 0.203125]\n",
            "4869: [D loss: 0.064078, acc: 0.992188]  [G loss: 3.766447, acc: 0.250000]\n",
            "4870: [D loss: 0.122471, acc: 0.960938]  [G loss: 3.724348, acc: 0.234375]\n",
            "4871: [D loss: 0.104427, acc: 0.968750]  [G loss: 2.811291, acc: 0.296875]\n",
            "4872: [D loss: 0.039974, acc: 0.984375]  [G loss: 1.586811, acc: 0.515625]\n",
            "4873: [D loss: 0.025888, acc: 0.984375]  [G loss: 1.034481, acc: 0.656250]\n",
            "4874: [D loss: 0.056137, acc: 0.984375]  [G loss: 0.920820, acc: 0.750000]\n",
            "4875: [D loss: 0.085713, acc: 0.968750]  [G loss: 1.115396, acc: 0.578125]\n",
            "4876: [D loss: 0.049949, acc: 0.984375]  [G loss: 1.408884, acc: 0.562500]\n",
            "4877: [D loss: 0.020099, acc: 1.000000]  [G loss: 1.826410, acc: 0.484375]\n",
            "4878: [D loss: 0.057363, acc: 0.984375]  [G loss: 1.864887, acc: 0.515625]\n",
            "4879: [D loss: 0.123771, acc: 0.976562]  [G loss: 1.553514, acc: 0.500000]\n",
            "4880: [D loss: 0.025545, acc: 0.992188]  [G loss: 1.540443, acc: 0.578125]\n",
            "4881: [D loss: 0.096758, acc: 0.968750]  [G loss: 1.338283, acc: 0.609375]\n",
            "4882: [D loss: 0.031467, acc: 0.992188]  [G loss: 1.483746, acc: 0.515625]\n",
            "4883: [D loss: 0.150126, acc: 0.937500]  [G loss: 1.994839, acc: 0.453125]\n",
            "4884: [D loss: 0.032747, acc: 0.984375]  [G loss: 4.367000, acc: 0.218750]\n",
            "4885: [D loss: 0.052339, acc: 0.976562]  [G loss: 5.100160, acc: 0.187500]\n",
            "4886: [D loss: 0.147435, acc: 0.945312]  [G loss: 3.802906, acc: 0.265625]\n",
            "4887: [D loss: 0.167253, acc: 0.945312]  [G loss: 2.003830, acc: 0.500000]\n",
            "4888: [D loss: 0.079308, acc: 0.976562]  [G loss: 1.384009, acc: 0.640625]\n",
            "4889: [D loss: 0.323951, acc: 0.867188]  [G loss: 2.729973, acc: 0.343750]\n",
            "4890: [D loss: 0.070565, acc: 0.976562]  [G loss: 4.747925, acc: 0.109375]\n",
            "4891: [D loss: 0.075887, acc: 0.968750]  [G loss: 5.396484, acc: 0.031250]\n",
            "4892: [D loss: 0.098956, acc: 0.960938]  [G loss: 5.040794, acc: 0.078125]\n",
            "4893: [D loss: 0.123567, acc: 0.937500]  [G loss: 4.914941, acc: 0.031250]\n",
            "4894: [D loss: 0.063800, acc: 0.984375]  [G loss: 3.112905, acc: 0.171875]\n",
            "4895: [D loss: 0.160859, acc: 0.945312]  [G loss: 2.034339, acc: 0.375000]\n",
            "4896: [D loss: 0.084004, acc: 0.968750]  [G loss: 2.058588, acc: 0.328125]\n",
            "4897: [D loss: 0.099140, acc: 0.968750]  [G loss: 2.158550, acc: 0.234375]\n",
            "4898: [D loss: 0.124784, acc: 0.953125]  [G loss: 2.903253, acc: 0.281250]\n",
            "4899: [D loss: 0.026439, acc: 0.992188]  [G loss: 3.587698, acc: 0.234375]\n",
            "4900: [D loss: 0.100393, acc: 0.984375]  [G loss: 4.203834, acc: 0.109375]\n",
            "4901: [D loss: 0.041244, acc: 0.984375]  [G loss: 4.307184, acc: 0.156250]\n",
            "4902: [D loss: 0.022447, acc: 0.992188]  [G loss: 4.847029, acc: 0.140625]\n",
            "4903: [D loss: 0.093174, acc: 0.968750]  [G loss: 4.180598, acc: 0.250000]\n",
            "4904: [D loss: 0.106166, acc: 0.976562]  [G loss: 3.879609, acc: 0.218750]\n",
            "4905: [D loss: 0.048768, acc: 0.984375]  [G loss: 2.556230, acc: 0.328125]\n",
            "4906: [D loss: 0.051808, acc: 0.976562]  [G loss: 2.772483, acc: 0.250000]\n",
            "4907: [D loss: 0.022280, acc: 0.992188]  [G loss: 1.746321, acc: 0.453125]\n",
            "4908: [D loss: 0.037531, acc: 0.984375]  [G loss: 2.000273, acc: 0.437500]\n",
            "4909: [D loss: 0.084781, acc: 0.968750]  [G loss: 2.224714, acc: 0.312500]\n",
            "4910: [D loss: 0.079353, acc: 0.945312]  [G loss: 3.498077, acc: 0.125000]\n",
            "4911: [D loss: 0.012384, acc: 1.000000]  [G loss: 4.000892, acc: 0.031250]\n",
            "4912: [D loss: 0.043183, acc: 0.984375]  [G loss: 3.990522, acc: 0.031250]\n",
            "4913: [D loss: 0.025231, acc: 0.992188]  [G loss: 3.640144, acc: 0.093750]\n",
            "4914: [D loss: 0.044200, acc: 0.968750]  [G loss: 3.748727, acc: 0.000000]\n",
            "4915: [D loss: 0.043066, acc: 0.984375]  [G loss: 3.425611, acc: 0.062500]\n",
            "4916: [D loss: 0.033083, acc: 0.984375]  [G loss: 3.865463, acc: 0.078125]\n",
            "4917: [D loss: 0.015208, acc: 0.992188]  [G loss: 3.151724, acc: 0.109375]\n",
            "4918: [D loss: 0.027585, acc: 0.992188]  [G loss: 2.930551, acc: 0.140625]\n",
            "4919: [D loss: 0.022018, acc: 1.000000]  [G loss: 2.784275, acc: 0.203125]\n",
            "4920: [D loss: 0.034024, acc: 0.992188]  [G loss: 2.836285, acc: 0.187500]\n",
            "4921: [D loss: 0.012318, acc: 1.000000]  [G loss: 2.897973, acc: 0.265625]\n",
            "4922: [D loss: 0.061396, acc: 0.984375]  [G loss: 3.919775, acc: 0.140625]\n",
            "4923: [D loss: 0.018128, acc: 0.992188]  [G loss: 4.399957, acc: 0.062500]\n",
            "4924: [D loss: 0.060016, acc: 0.984375]  [G loss: 4.824812, acc: 0.031250]\n",
            "4925: [D loss: 0.020593, acc: 0.992188]  [G loss: 5.632056, acc: 0.031250]\n",
            "4926: [D loss: 0.041597, acc: 0.984375]  [G loss: 4.723213, acc: 0.062500]\n",
            "4927: [D loss: 0.073532, acc: 0.968750]  [G loss: 4.562775, acc: 0.093750]\n",
            "4928: [D loss: 0.073714, acc: 0.984375]  [G loss: 3.673386, acc: 0.187500]\n",
            "4929: [D loss: 0.016393, acc: 0.992188]  [G loss: 3.915865, acc: 0.140625]\n",
            "4930: [D loss: 0.033159, acc: 0.984375]  [G loss: 3.157622, acc: 0.203125]\n",
            "4931: [D loss: 0.049115, acc: 0.984375]  [G loss: 3.725117, acc: 0.109375]\n",
            "4932: [D loss: 0.040473, acc: 0.984375]  [G loss: 3.852900, acc: 0.140625]\n",
            "4933: [D loss: 0.039650, acc: 0.984375]  [G loss: 4.736738, acc: 0.125000]\n",
            "4934: [D loss: 0.011304, acc: 0.992188]  [G loss: 5.525281, acc: 0.046875]\n",
            "4935: [D loss: 0.049423, acc: 0.968750]  [G loss: 5.602835, acc: 0.078125]\n",
            "4936: [D loss: 0.073876, acc: 0.976562]  [G loss: 6.441761, acc: 0.093750]\n",
            "4937: [D loss: 0.030366, acc: 0.984375]  [G loss: 5.970634, acc: 0.093750]\n",
            "4938: [D loss: 0.006434, acc: 1.000000]  [G loss: 6.407609, acc: 0.015625]\n",
            "4939: [D loss: 0.014119, acc: 0.992188]  [G loss: 5.789636, acc: 0.062500]\n",
            "4940: [D loss: 0.006636, acc: 1.000000]  [G loss: 5.585819, acc: 0.062500]\n",
            "4941: [D loss: 0.016455, acc: 0.992188]  [G loss: 5.150469, acc: 0.093750]\n",
            "4942: [D loss: 0.019124, acc: 0.992188]  [G loss: 4.836933, acc: 0.171875]\n",
            "4943: [D loss: 0.036887, acc: 0.984375]  [G loss: 4.821201, acc: 0.140625]\n",
            "4944: [D loss: 0.026824, acc: 0.992188]  [G loss: 4.991165, acc: 0.171875]\n",
            "4945: [D loss: 0.037073, acc: 0.984375]  [G loss: 4.466254, acc: 0.078125]\n",
            "4946: [D loss: 0.027525, acc: 0.984375]  [G loss: 5.637584, acc: 0.046875]\n",
            "4947: [D loss: 0.005871, acc: 1.000000]  [G loss: 4.641521, acc: 0.078125]\n",
            "4948: [D loss: 0.014714, acc: 1.000000]  [G loss: 5.832800, acc: 0.015625]\n",
            "4949: [D loss: 0.030835, acc: 0.992188]  [G loss: 6.184720, acc: 0.000000]\n",
            "4950: [D loss: 0.041244, acc: 0.984375]  [G loss: 5.596144, acc: 0.046875]\n",
            "4951: [D loss: 0.069245, acc: 0.992188]  [G loss: 5.488302, acc: 0.046875]\n",
            "4952: [D loss: 0.012503, acc: 1.000000]  [G loss: 5.284026, acc: 0.062500]\n",
            "4953: [D loss: 0.161548, acc: 0.953125]  [G loss: 4.475190, acc: 0.093750]\n",
            "4954: [D loss: 0.065660, acc: 0.960938]  [G loss: 3.997659, acc: 0.046875]\n",
            "4955: [D loss: 0.045238, acc: 0.992188]  [G loss: 5.300229, acc: 0.093750]\n",
            "4956: [D loss: 0.021073, acc: 0.992188]  [G loss: 5.232013, acc: 0.093750]\n",
            "4957: [D loss: 0.021783, acc: 0.992188]  [G loss: 5.680454, acc: 0.000000]\n",
            "4958: [D loss: 0.050547, acc: 0.968750]  [G loss: 4.619119, acc: 0.062500]\n",
            "4959: [D loss: 0.016327, acc: 1.000000]  [G loss: 4.677923, acc: 0.093750]\n",
            "4960: [D loss: 0.032126, acc: 0.984375]  [G loss: 4.249125, acc: 0.140625]\n",
            "4961: [D loss: 0.018569, acc: 1.000000]  [G loss: 5.029574, acc: 0.109375]\n",
            "4962: [D loss: 0.010998, acc: 1.000000]  [G loss: 4.773886, acc: 0.078125]\n",
            "4963: [D loss: 0.055261, acc: 0.976562]  [G loss: 4.121840, acc: 0.156250]\n",
            "4964: [D loss: 0.042221, acc: 0.984375]  [G loss: 3.306382, acc: 0.187500]\n",
            "4965: [D loss: 0.044957, acc: 0.992188]  [G loss: 3.528140, acc: 0.187500]\n",
            "4966: [D loss: 0.089361, acc: 0.968750]  [G loss: 2.668414, acc: 0.265625]\n",
            "4967: [D loss: 0.154935, acc: 0.953125]  [G loss: 4.318974, acc: 0.078125]\n",
            "4968: [D loss: 0.058189, acc: 0.976562]  [G loss: 6.273305, acc: 0.078125]\n",
            "4969: [D loss: 0.017469, acc: 1.000000]  [G loss: 6.924529, acc: 0.000000]\n",
            "4970: [D loss: 0.403075, acc: 0.921875]  [G loss: 5.561490, acc: 0.109375]\n",
            "4971: [D loss: 0.203573, acc: 0.937500]  [G loss: 2.603870, acc: 0.375000]\n",
            "4972: [D loss: 0.091404, acc: 0.953125]  [G loss: 1.042227, acc: 0.625000]\n",
            "4973: [D loss: 0.565379, acc: 0.875000]  [G loss: 3.154800, acc: 0.296875]\n",
            "4974: [D loss: 0.172950, acc: 0.960938]  [G loss: 5.795257, acc: 0.078125]\n",
            "4975: [D loss: 0.234930, acc: 0.921875]  [G loss: 5.379193, acc: 0.046875]\n",
            "4976: [D loss: 0.103095, acc: 0.960938]  [G loss: 4.727100, acc: 0.078125]\n",
            "4977: [D loss: 0.189330, acc: 0.937500]  [G loss: 3.082262, acc: 0.250000]\n",
            "4978: [D loss: 0.191234, acc: 0.937500]  [G loss: 2.531071, acc: 0.375000]\n",
            "4979: [D loss: 0.116856, acc: 0.937500]  [G loss: 2.471031, acc: 0.312500]\n",
            "4980: [D loss: 0.080055, acc: 0.968750]  [G loss: 2.069153, acc: 0.343750]\n",
            "4981: [D loss: 0.098473, acc: 0.960938]  [G loss: 3.019279, acc: 0.234375]\n",
            "4982: [D loss: 0.098008, acc: 0.976562]  [G loss: 2.374948, acc: 0.312500]\n",
            "4983: [D loss: 0.117973, acc: 0.968750]  [G loss: 2.290828, acc: 0.390625]\n",
            "4984: [D loss: 0.053471, acc: 0.984375]  [G loss: 2.256770, acc: 0.343750]\n",
            "4985: [D loss: 0.057213, acc: 0.976562]  [G loss: 2.763207, acc: 0.328125]\n",
            "4986: [D loss: 0.048544, acc: 0.976562]  [G loss: 3.904305, acc: 0.281250]\n",
            "4987: [D loss: 0.114860, acc: 0.960938]  [G loss: 3.534646, acc: 0.312500]\n",
            "4988: [D loss: 0.127088, acc: 0.953125]  [G loss: 2.436148, acc: 0.484375]\n",
            "4989: [D loss: 0.171973, acc: 0.929688]  [G loss: 1.480909, acc: 0.593750]\n",
            "4990: [D loss: 0.162527, acc: 0.906250]  [G loss: 1.895905, acc: 0.500000]\n",
            "4991: [D loss: 0.170614, acc: 0.921875]  [G loss: 2.898542, acc: 0.328125]\n",
            "4992: [D loss: 0.180839, acc: 0.937500]  [G loss: 3.439924, acc: 0.265625]\n",
            "4993: [D loss: 0.103241, acc: 0.976562]  [G loss: 4.000535, acc: 0.187500]\n",
            "4994: [D loss: 0.216101, acc: 0.937500]  [G loss: 2.821801, acc: 0.265625]\n",
            "4995: [D loss: 0.163743, acc: 0.937500]  [G loss: 1.864192, acc: 0.390625]\n",
            "4996: [D loss: 0.103409, acc: 0.945312]  [G loss: 1.589561, acc: 0.515625]\n",
            "4997: [D loss: 0.132390, acc: 0.960938]  [G loss: 2.143816, acc: 0.406250]\n",
            "4998: [D loss: 0.121228, acc: 0.960938]  [G loss: 2.533320, acc: 0.343750]\n",
            "4999: [D loss: 0.151375, acc: 0.953125]  [G loss: 3.811964, acc: 0.171875]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8OZt5dMiWCET",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_discriminador = [] \n",
        "acc_discriminador = [] \n",
        "loss_generador = []\n",
        "acc_generador = []\n",
        "\n",
        "for loss, acc in hist['d']:\n",
        "  loss_discriminador.append(loss)\n",
        "  acc_discriminador.append(acc)\n",
        "\n",
        "for loss, acc in hist['g']:\n",
        "  loss_generador.append(loss)\n",
        "  acc_generador.append(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CW8J2AbhIf55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "c85077f2-fcbd-401b-ab7e-25a4891428d6"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(6, figsize=(16,8))\n",
        "plt.suptitle(\"Loss y Acc para redes discriminadora y generadora (lr = 0.0001)\", y=1.03)\n",
        "plt.subplot(121)\n",
        "plt.title(\"Loss para distintas redes\")\n",
        "plt.plot(loss_discriminador, label = \"Discriminador\")\n",
        "plt.plot(loss_generador, label=\"Generador\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"N° de epoch\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.subplot(122)\n",
        "plt.title(\"Acc para distintas redes\")\n",
        "plt.plot(acc_discriminador, label = \"Discriminador\")\n",
        "plt.plot(acc_generador, label=\"Generador\")\n",
        "plt.ylabel(\"Acc\")\n",
        "plt.xlabel(\"N° de epoch\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show();"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABHQAAAJSCAYAAACiKZKnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEIIIW6GAjqEEEIIIYQQQgghbub/AUudmKZGspVEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f3764cc0630>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Apvh47g0T-tY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "b46fda97-6fe1-4a29-9f93-e6e4f602a76e"
      },
      "cell_type": "code",
      "source": [
        "N = 10\n",
        "noise = np.random.uniform(-1.0, 1.0, size=[N, input_dim]) \n",
        "images = G.predict(noise)\n",
        "import matplotlib.pyplot as plt\n",
        "for i in range(images.shape[0]):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    image = images[i, :, :, :]\n",
        "    image = np.reshape(image, [img_rows, img_cols])\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAAEtCAYAAABtdZh7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAiQIAAAgQIIAAAAC/wdtbU4jnocQGgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f3764cc0b38>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}